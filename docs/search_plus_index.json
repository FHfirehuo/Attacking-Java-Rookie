{"./":{"url":"./","title":"介绍","keywords":"","body":"介绍 　　这是一本java程序员写的必备知识积累之书。可以说是一本进击之书。命名为《进击的java菜鸟》。 本书地址https://fhfirehuo.github.io/Attacking-Java-Rookie/   This is a book of necessary knowledge accumulated by java programmers. It can be said that it is a book of attack. named《Attacking java rookie》.Book addresshttps://fhfirehuo.github.io/Attacking-Java-Rookie/ 为什么写这本书 现状的IT行业，真的不缺“工具人”了。 每年毕业这么多计算机专业的从业者，还有数不清报速成培训班的，你的优势真的那么明晰、自信吗？ 当下，不缺熟练使用各种流行框架和组件的，但缺懂得该框架技术背后原理的长久沉淀。 当下，不缺CRUD+各种Ctrl+V的人，但缺少对技术选择和使用时，乐于较真优劣的意识。 当下，不缺自称掌握了很多开源技术的同学，缺的是能体系化理解和输出这些技术栈的能力。 向大厂进击 当下大厂面试造火箭,进场拧螺丝的现象很普遍。 本书括数据结构、操作系统原理、计算机网络、计算机安全等基础课程，未来在职场你不一定有机会会用到这些，但会让你在选择时更自信、效率、稳固。 目录 第一章 心得篇 第二章 数据结构篇 第三章 算法篇 第四章 设计模式 第五章 JAVA篇 第六章 JVM篇 第七章 并发篇 第八章 中间件篇 第九章 数据库篇 第十章 网络篇 第十一章 SPRING篇 第十二章 CLOUD篇 第十三章 架构篇 第十四章 权限篇 第十五章 登录篇 第十六章 系统篇 第十七章 netty篇 第十八章 计算机原理篇 附录 "},"Chapter01/experience.html":{"url":"Chapter01/experience.html","title":"Part I 心得篇","keywords":"","body":"第一章 心得篇 程序 计算机基础知识对程序员来说有多重要? 提问题的艺术 书单 "},"Chapter01/program.html":{"url":"Chapter01/program.html","title":"程序","keywords":"","body":"程序 程序 = 数据结构 + 算法 program = dataStructure + alogorthm 所以本书开篇两章先来一遍数据结构和算法 第一章：数据结构 第二章：依赖于数据结构的算法;这里依赖的数据结构可能史jdk的也可能是自己实现的 "},"Chapter01/experience1.html":{"url":"Chapter01/experience1.html","title":"计算机基础知识对程序员来说有多重要?","keywords":"","body":"计算机基础知识对程序员来说有多重要？ 程序员都说自己平时日常的工作就是搬砖，后端说自己就是写写CRUD，前端说自己就是对着设计稿调调CSS参数。 但这样永远都无法向高级工程师岗位迈进，也就是业界所说的三年一大坎。 如果你工作了三年，还依旧被迫或迷恋于做这些事情，那你的工资仍然没有长进也是没有理由抱怨的。 那么如何区分一个程序员的水平在一个什么级别上呢，关键的一个指标就是对算法的掌握程度， 这就涉及到标题中所说的计算机基础知识。 如果你是一个计算机相关专业毕业的本科生，你应该听过或学过《高等数学》和《离散数学》这两门课。 当学生的时候你可能会觉得学习这东西对我以后工作有啥用呢， 下面我们就来举个例子证明这些理论知识对编程具体有什么用。 假如给你一个非空数组（注意哦是非空，不需要考虑判空），然后里面是N个无序的整数， 其中只有一个数只出现了一次，其他的都出现了两次，现在让你找出这个只出现过一次的数。 那么现在你可能打开IDE就开始敲，这问题很常见啊，我遍历一下数组，依次把元素放到另一个空数组中， 遇到不存在于新数组的元素，我就扔进去，遇到已经存在于新数组的，那我就不往新数组扔了， 而且我还要把新数组里这个数丢出来，因为它不是我要找的只出现过一次的数，等遍历结束， 自然最后新数组就只剩下那个只出现过一次的数了。好，那么用代码写出来： var singleNumber = function(nums) { var list = []; for (var i = 0; i 实现了，但是有什么问题呢？这时候就是第一个区分程序员水平的分水岭 普通程序员会觉得这方法很好，代码简洁、易懂、好维护， 然后就提交代码了，剩下的交给测试，提了bug再解决就好了。 那么这样的程序员如果不是到某一天突然觉悟，如此下去，就会变成业界所说的那种工作三年但只有一年经的人。 想要自我救赎，不沦为咸鱼应该怎么办呢？写完这段代码，你盯着它再看看，用批判性的眼光挑挑毛病。 或许你就会发现，如果我们要找的数在数组的最后一个，那你前面的数组遍历、数组插入、数组移出的操作就都是在浪费计算机的时间和资源。 如果这个待处理的数组有几百万几千万的长度呢，那你要拿到结果可能就会感觉你的电脑卡住了一样， 其实它在疯狂地运行你写的代码，去找到结果，只不过在找到结果之前它在疯狂地写操作、读操作，么得办法。 这就是你导致的啊，你写的表面看上去岁月静好的程序放到实际应用中去跑，有可能就被用户吐槽是辣鸡的APP。 那么想要对得起起公司产品的质量，又想提高自己作为程序员的编程能力，我们再来品这段代码，细细地品。 if (list.indexOf(nums[i]) 编程语言中是不是不只有数组这种存储结构，不要需求告诉我们要处理数组，我们就用数组去思考， 编程语言本身提供了很多强有力的基础工具，数组只是最普通的数据结构， 那还有一种可以快速查找的数据结构是什么呢？ 就是字典，意如其名，我们如果使用一个字典找某个字，是不是通过索引目录就可以快速翻到某页找到要找的字， 而不是像字谱一样从头到尾找一遍。 那么我们试着把if (list.indexOf(nums[i]) var map = {}; for (var i = 0; i 程序的运行变得稍微快了一点，并且没有影响程序的可读性，只是巧用了不同的数据结构，这是一名中级程序员应该做到的。 那么有没有更厉害的办法呢，这时就是所谓大牛的思路了（是时候展现真正的技术了！）大牛都是学过高数的， 而且能把高数中的知识实实在在用到编程中的 （不然怎么说学数学的如果转行做程序员，起步就很高，很多时候大家都是知道一些事实，但是不知道为什么， 当你知道足够多的场景后你就会知道为什么） 看一下这个数学公式2 * （a + b + c） - （a + a + b + b + c） = c是不是很简单， 但是这公式里就藏着我们这道题的答案。不捉迷藏，解释一下，我们换个好理解的场景，给你N个物品， 然后这里面只有一种类型数量为1，其他类型的数量都为2， 那么你把所有类型加起来再乘2是不是就会比所有物品加起来多出那个只有一个的物品。 （原谅我已经尽力了，这个例子也能解释为什么教别人和自己理解东西的难度不能相提并论，因为你理解一个东西， 也许某个点你就顿悟了，但你要把你理解的东西清晰传达给别人， 你需要想出足够简洁又有说服力的例子来让别人也能通过看你举得例子就能明白你想讲的东西， 而且不同听者在听同样的东西时，达到理解的时机也不同。教授不是件容易的事， 不然就不会有老师的水平参差不齐了，有时候你学不懂一个东西，也许不是你理解能力的问题， 而是你的老师讲解能力的问题） 那么我们根据这个数学公式编写出如下的代码 ``javascript var sumOfNums = 0, sumOfSet = 0; var set = new Set(); for (var i = 0; i 这就是单纯利用数学公式得出的结果，感受到数学的力量了吗。 但这也没多厉害，因为数学的本质是通过挖掘规律总结出一系列的公式，从而使得计算变得更快， 但数学没有数据结构的概念，也就是数学的公式是不提供存储功能，只是一个公式，你给它输入，它给你结果。 这就要提到计算机为什么会成为第三次工业革命的标志物了，因为计算机不仅可以把人类交给它的公式用电来计算出来，它还可以提供存储。 存储这个词对于学过计算机的人太熟悉了，从操作系统中学过计算机基本组成中就包括存储，其中包含硬盘用于固定存储随机的数据，即RAM，内存条用于存储计算机运行时的过程数据，即ROM，这是物理层面上的存储。 还有宏观上的存储，比如数据库，用于存储计算机运行产生的数据，对于软件，也就是保存我们使用计算机所产生的一切内容，包括我此时写的这些文字，以及我是谁，我是在什么时间写的这篇文章，还有我的修改时间，修改过几次，这些都存储在数据库中。最初在没有计算机的时候，我们所使用的计算器，也曾提供过存储功能（我指的是30块钱以上的那种按键很多的计算器），可能有人没用过，但我当时发现它可以把135 + 324的结果先存储到一个存储器中，然后我再读取存储器中的内容继续去乘3256，因为计算器是顺序输入的，所以如果你直接输入135 + 324 x 3256，会按照输入顺序计算，也就是会先计算加法，但我们知道这是不对的，当然你也可以使用括号，或者把中间过程记在纸上，我只是举个例子解释当时的计算器就已经有了设计存储这个概念，但其只能说是帮助人类生活加速的一个尝试。 到后来计算机普及，才真正加速了人们的生活，它把前面铺好的路都整合了起来，成为一个功能巨大的机器。此时应该插播一句每个计算机专业学生都听过的话“编程=算法+数据结构”，到这里你应该知道为什么不是只有算法或只有数据结构。因为只有算法，你无法存储过程数据，而只有数据结构，你无法利用更优秀的数学公式，所以二者结合才是利用计算机为你做事的最佳实践。其产物也就是程序，通过编程所产生，有了程序，人们通过计算机才诞生今天的千姿百态的玩法。 好了，继续说这个例子，第三种解法利用数学公式，虽然更巧妙，但实际对于计算机运行来说，并没有变得更快，也没有节省内存。 那么我们想想还有没有其他解法，这就要利用文首提到的另一门计算机课程《离散数学》了，这门课是只针对于计算机专业的，虽然名字里有代数，听着像数学相关的课程，但其讲的内容都是计算机里的数学，说白了就是计算机的核心基本——二进制运算。 可能稍微了解过的人都知道，电脑虽然功能这么多，但它还是靠电运行的，也就是第三次工业革命是在第二次工业革命的基础上发展而出的，如果没有电，第三次工业革命的一切产物都是空谈，无法运作。你把你家电闸拉了，你看你还能玩游戏吗，你还能看视频听歌吗，都不能。 计算机是如何利用电实现了这么多功能呢，其实就回到了初中物理——通路和短路，电路正常接电就是通路，电路中间有断开的部分，整个电路都不工作，就是断路，这就是二进制的理论基础，计算机就是通过不断地组合线路板上数量非常庞大的电子元件实现了不同的逻辑组合，那数量有多庞大呢，一开始是没多大的，所以电脑所产生的功能也有限，但后来有了集成电路，计算机主板上能放的电子元件数量爆炸性增长，电脑的功能随之变得越来越多。甚至有一个叫摩尔的外国人提出“集成电路上可容纳的晶体管数量，约每隔两年就会增加一倍”（摩尔定律）这样的预言，这个预言到现在都一直没被打破，的确是按照这个规律发展的。 这样我们就好理解为什么在今天有了人工智能、大数据，为什么阿尔法狗可以在围棋上战胜人类，答案就是因为同样大小的一台电脑，每过两年，其功能数量就能翻倍，其计算能力地增长更是不可预测。这样就把计算机从组成到发展串起来了，这样再来看看今天你手中的手机，手机中的APP，是不是就更好理解了。 那么回归正题，我们看看离散数学是怎么用于日常编程的，离散数学里有一种很强但是稍微有些难理解的运算叫做“异或”，我们试着从名字拆解以理解，“异或”即“不同”+“或”，或运算是离散数学中的基本运算，比如1或0就是1，0或0还是0，1或1也是1，0或1即是1，有没有发现规律，这就是初中物理中的并联，如果两条电路是并联的，其中一条电路断掉是不影响整个电路通电的，这相当于什么呢，就相当于备用电器，你用两个插板都接着电，其中一个坏掉是不会影响你正常用电的，代码中也常见，比如 if (a || b) { console.log('hello world') } 这段代码中a或b有一个为真，就会打印出“hello world”。当然，程序员看到这里还会指出一个知识点就是如果a已经是真了，那么计算机就不会再去看b是否为真了，就直接打印“hello world”了，俗称“断路”，即或操作中先为真的数会导致后面判断断掉而不去进行判断。当然，这都是程序员关心的小知识了，无足挂齿。 那么我们继续看“异或”，先说结果，”异或“意味着两个数相同的时候结果就为假，而不同的时候则为真，即1异或0是1，0异或1也是1，但1异或1是0，0异或0也是0，纵观其规律就是两个字”拧巴“，就是两个数闹别扭，我偏不和你一样，这也符合当今时代年轻人的个性，不喜欢苟同，喜欢个性，与众不同。这样记“异或”就够了，就不用去咬文嚼字理解为什么这样的操作叫“异或”，记住“异或”重点在于“ 异”。只要“异”了就是真，“异”了就像在做“或”操作了，而“或”操作的精髓在于“有真则真”（后面这段解释可以不看，免得产生误导） 那么怎么利用”异或计算“这个东西呢，有以下这样的推导公式： a ^ 0 == a a ^ a == 0 a ^ b ^ a == a ^ a ^ b == 0 ^ b == b 一个数和0进行异或操作，如果这个数是0，那么结果为0，因为0异或0是0，那么结果是不是就和这个数相同了，如果这个数不是0呢，那就和0不同，结果就是这个数了；那如果这个数和自身进行异或操作呢，那永远是0，因为和自身异或，相当于两个相同的数进行异或；同时多个数进行异或操作允许交换顺序，利用交换顺序，结合上面两个规律，便得出如果有两个相同的数和一个只出现一次的数进行异或操作，最后得出的便是那个只出现过一次的数，也就是本题的解。 那我们只需要依次将数组的每个数都进行异或操作，即能得到那个”另类“的数，但给我们的是一个数组，我们只能依次遍历每一个数，那么如何记住上一次两个数的遍历结果呢，我们可以用一个变量存储这个值，那么这个数初始化为多少合适呢，只能是0，假如是别的数，比如是1，那么数组第一个数如果是1，那么第一次异或结果就是0，这样外部数据影响了数组内部数据的异或结果。但如果我们把外部变量初始化为0，那么数组第一个数无论是几，第一次的异或结果都是数组第一个数本身，因为有理论做支撑（a异或0等于a）。因为所给数组中只存在出现两次的数和一个只出现一次的数，所以结果必然符合我们的异或推到公式（a ^ b ^ a == a ^ a ^ b == 0 ^ b == b），即多个出现两次的数和一个单独的仅出现一次的数在一次，都进行异或，顺序无所谓，最后只等于那个只出现一次的数。 最终代码如下： var a = 0; for (var i = 0; i 这样地计算既利用了计算机的数据结构，也利用到了算法，是应该优先考虑的解法，至于代码可读性自然没有前面的容易理解，这就要求编程人员掌握足够的计算机基础知识，知道什么”异或“操作，以及”异或“操作可以推导多个数进行”异或“的规律，而“异或”的知识就藏在《离散数学》那本书中。（这里我也不是推荐程序员都去写一些晦涩难懂的代码，从而其他和你水平不相当的人接手骂爹，因为看不懂你写的东西而全部重写。我们提供的是一种思路，作为程序员，应该拥有程序员精神，即是要有意识地提高你程序运行的速度，并使用尽可能少的内存资源，这种追求极致的精神其实在任何行业都是高手的基本素养） 总算是首尾呼应，自圆其说了。曾经有很多人说大学学习的东西和毕业后工作的内容没什么联系，也就是大学学的东西都没什么用。我也曾慢慢加入到这样的呼声中，但可能再过一个阶段，你回过头再去看一切，也许就会理解到这世上不可能存在没有任何联系的事物，一切都在一个大的网络联络之中。如果你觉得两个事情没有任何联系，那可能只是你还没找到联络其两物之间的那复杂的联系。 作者：秋水 链接：https://www.zhihu.com/question/300650155/answer/1122468842 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 "},"Chapter01/question.html":{"url":"Chapter01/question.html","title":"提问题的艺术","keywords":"","body":"提问题的艺术 "},"Chapter01/bookList.html":{"url":"Chapter01/bookList.html","title":"书单","keywords":"","body":"读书书单 2021.3 看了两页领域驱动，很抽象，暂时放弃。后面主要是看《clickhouse原理解析与应用实战》。3.23看完第一遍，后面开始实践，一边实践一边学习吧。然后接着看《领域驱动设计》和《实现领域驱动设计》这两本书。这次换工作发现大数据已经是基础知识了，所以这里立一个flag，今年要看关于 zookeeper、kafka、Hadoop（yarn、hdfs）、flink各一本书。 2021.2 一月份入职了，开启新的篇章。春节是回不了家了。第一次过年在外地过。总感觉少点啥。后面准备学习clickhouse和领域驱动。领域驱动是面阿里时提到了。然而我对这玩意是一头雾水啊。 2021.1 过完元旦准备换房、入职了。回了趟家又被隔离了。疫情以来的第四次隔离。 2020.12 最先拿到个jd的offer，后面mt，ali给了给P6，最后拿了个soguo的后面没心情面了。 2020.11.9 spring到31篇SpringMVC，要准备面试了，多看些基础知识。 2020.10 第十八章 netty篇 第十一章 spring篇 比较可惜的是明明《TCP/IP详解 卷1：协议 》已经看到了至关重要的TCP章节但是这个月一点没看。 比较重要的一个事情是本数增加了mygitalk插件可以直接和github联系起来评论。调整了目录结构，左侧菜单可以展开收起。添加了donate可以进行打赏。添加search-pro可以进行中文搜索。 2020.9 《深入理解Java虚拟机 》看完第5章 《TCP/IP详解 卷1：协议》 看完第16章 2020.8.4 看见其它人的书单决定自己也弄一个，所以之前的都是回忆。其实也看了一些书但是很零散就不记录了。 2020.1-2020.7 《MySQL技术内幕：InnoDB存储引擎（第2版）》(从2月看到7月底看完) 3月完成本书第15章sso篇 4月完成本书第4章设计模式前34小结 5月完成本书第3章算法篇前22小结 6月完成本书第2章数据结构篇的前40小结 其中穿插很多其它章节的零散小结 2020.2 由于疫情突发奇想开始进行本书 2019.1-2019.12 Spring Cloud 官方文档 《Redis 深度历险：核心原理与应用实践》 《Spring实战 第4版》 《Netty实战 (1、2、3 章)》 《深入理解Java虚拟机_JVM高级特性与最佳实践 第2版 (第二章)》 2018.4-2018.12 Spring Boot官方文档 2017-2018.3 《JavaEE开发的颠覆者：Spring Boot实战》(真正意义上看完的第一本书) Spring官方文档 2014-2017 根据需求各种百度(打下了一定基础，但是效果最差，没有形成系统的知识体系，之后基本到18年底主要还是依靠百度求生。19年开始真正意义上看书) 《Hadoop in action》(本想转大数据方向，2016开始读,大概是看到ZooKeeper, 但是5月换工作还是和大数据无关从此就断了这个念头) "},"Chapter01/complaints.html":{"url":"Chapter01/complaints.html","title":"关于投诉举报","keywords":"","body":"投诉举报 "},"Chapter01/SelfIntroduction.html":{"url":"Chapter01/SelfIntroduction.html","title":"面试官说手里有简历为什么还要先做个自我介绍","keywords":"","body":"面试官说手里有简历为什么还要先做个自我介绍 很多面试者跟我说：我一边自我介绍一边怀疑，面试官是不是根本没有看我的简历呀？！有这种可能性，不过，请相信我：大部分面试官是负责任的，他会提前看你的简历、了解你的基本情况！ 只是，你也别太急啦，面试是面试官通过面对面交流的方式认识你的过程，可不是看完设备使用说明书，直接拿起来就用的过程呀。所以，当面试官要求你进行自我介绍的时候，不用有抵触情绪，人家希望你自己当面说说有啥问题呢？ 可是，到底有啥可听的呢，也就是面试官愿意亲耳听你自我介绍的目的是什么呢？ 第一，当然是语言表达能力。 这是毋庸置疑的，肯定是有这一项。不过，其他问题也同样可以可以考察你的语言表达能力呀。所以，我认为这不是最重要的目的。 第二，逻辑思维能力。 一个自我介绍，你东扯一句、西扯一句，讲不清楚自己到底有什么特点。这肯定是逻辑思维能力有问题。所以，你在进行自我介绍的时候，肯定会暴露你的逻辑思维能力。但是，同样的，这个问题也不是自我介绍所特有的。 第三，自我认知+用户思维（或者需求分析能力）。 这一点不是特别容易被提到，但却是自我介绍所独有的。自我介绍一般只能有1-3分钟的时间，大部分是1分钟的时间，说再多对方就要走神啦。在这么短的时间内，你需要介绍清楚自己的基本情况、主要特点以及最为关键的岗位匹配度。这就需要既要了解自己的主要优势，又要了解岗位需求、了解面试官最想要什么样的人。 所以，你看，自我介绍没有想象的那么简单，是一个特别综合的考核项目。作为面试者一定要认真对待啦。 第四，作为开场的缓冲期 这一点也是自我介绍所特有的功能。理论上说，你对自己的最熟悉的，说自己应该是相对比较放松的。坐在你对面正襟危坐的面试官，也许是临时被叫来面试人，确实没有看过你的简历；也许看了太多简历，根本记不住谁对谁；也许面试了一天了，大脑都已经木了。正好，用一个相对轻松的方式开始，也给面试官自己的一个缓冲期。 不过，不要以为面试官放松你就也放松啦，实际上从你进入这家公司到离开，你所有的行为、语言都是在面试，“说者无心，听者有意”一旦发现异常（可能是好消息、可能是坏消息），都会立即警觉起来，作为进一步考察的切入点。 "},"Chapter02/DataStructure.html":{"url":"Chapter02/DataStructure.html","title":"Part II 数据结构篇","keywords":"","body":"第二章 数据结构 人们在使用计算机解决客观世界中存在的具体问题时，通常过程如下： 首先通过对客观世界的认知形成印象和概念从而得到了信息，在此基础上建立概念模型，它必须能够如实地反映客观世界中的事物以及事物间的联系； 根据概念模型将实际问题转化为计算机能够理解的形式，然后设计程序； 用户通过人机交互界面与系统交流，使系统执行相应操作，最后解决实际的问题。 数据结构主要与在上述过程中从建立概念模型到实现模型转化并为后续程序设计提供基础的内容相关。 它是用来反映一个概念模型的内部构成，即一个概念模型由那些成分数据构成，以什么方式构成，呈现什么结构。 数据结构主要是研究程序设计问题中计算机的操作对象以及它们之间的关系和操作的学科。 基本概念 数据（data是描述客观事物的数值、字符以及能输入机器且能被处理的各种符号集合。 数据的含义非常广泛，除了通常的数值数据、字符、字符串是数据以外，声音、图像等一切可以输入计算机并能被处理的都是数据。 例如除了表示人的姓名、身高、体重等的字符、数字是数据，人的照片、指纹、三维模型、语音指令等也都是数据。 数据元素（dataelement是数据的基本单位，是数据集合的个体，在计算机程序中通常作为一个整体来进行处理。例如一条描述一位学生的完整信息的数据记录就是一个数据元素； 空间中一点的三维坐标也可以是一个数据元素。数据元素通常由若干个数据项组成，例如描述学生相关信息的姓名、性别、学号等都是数据项； 三维坐标中的每一维坐标值也是数据项。数据项具有原子性，是不可分割的最小单位。 数据对象（dataobject是性质相同的数据元素的集合，是数据的子集。 例如一个学校的所有学生的集合就是数据对象，空间中所有点的集合也是数据对象。 数据结构（datastructure是指相互之间存在一种或多种特定关系的数据元素的集合。 是组织并存储数据以便能够有效使用的一种专门格式，它用来反映一个数据的内部构成， 即一个数据由那些成分数据构成，以什么方式构成，呈什么结构。 由于信息可以存在于逻辑思维领域，也可以存在于计算机世界，因此作为信息载体的数据同样存在于两个世界中。 表示一组数据元素及其相互关系的数据结构同样也有两种不同的表现形式， 一种是数据结构的逻辑层面，即数据的逻辑结构； 一种是存在于计算机世界的物理层面，即数据的存储结构。 数据的逻辑结构按照数据元素之间相互关系的特性来分， 可以分为以下四种结构：集合、线性结构、树形结构和图状结构。 主要有线性表、栈、队列、树和图，其中线性表、栈、队列属于线性结构，树和图属于非线性结构。 抽象数据类型 抽象数据类型是描述数据结构的一种理论工具。在介绍抽象数据类型之前我们先介绍一下数据类型的基本概念。 数据类型(datatype) 是一组性质相同的数据元素的集合以及加在这个集合上的一组操作。 例如Java语言中就有许多不同的数据类型，包括数值型的数据类型、字符串、布尔型等数据类型。 以Java中的int型为例，int型的数据元素的集合是[-2147483648,2147483647]间的整数，定义在其上的操作有加、减、乘、除四则运算，还有模运算等。 定义数据类型的作用一个是隐藏计算机硬件及其特性和差别，使硬件对于用户而言是透明的，即用户可以不关心数据类型是怎么实现的而可以使用它。 定义数据类型的另一个作用是，用户能够使用数据类型定义的操作，方便的实现问题的求解。 例如，用户可以使用Java定义在int型的加法操作完成两个整数的加法运算，而不用关心两个整数的加法在计算机中到底是如何实现的。 这样不但加快了用户解决问题的速度，也使得用户可以在更高的层面上考虑问题。 与机器语言、汇编语言相比，高级语言的出现大大地简便了程序设计。 但是要将解答问题的步骤从非形式的自然语言表达到形式化的高级语言表达， 仍然是一个复杂的过程，仍然要做很多繁杂琐碎的事情，因而仍然需要抽象。 对于一个明确的问题，要解答这个问题，总是先选用该问题的一个数据模型。 接着，弄清该问题所选用的数据模型在已知条件下的初始状态和要求的结果状态，以及隐含着的两个状态之间的关系。 然后探索从数据模型的已知初始状态出发到达要求的结果状态所必需的运算步骤。 我们在探索运算步骤时，首先应该考虑顶层的运算步骤，然后再考虑底层的运算步骤。 所谓顶层的运算步骤是指定义在数据模型级上的运算步骤，或叫宏观运算。 它们组成解答问题步骤的主干部分。 其中涉及的数据是数据模型中的一个变量，暂时不关心它的数据结构； 涉及的运算以数据模型中的数据变量作为运算对象，或作为运算结果，或二者兼而为之，简称为定义在数据模型上的运算。 由于暂时不关心变量的数据结构，这些运算都带有抽象性质，不含运算的细节。 所谓底层的运算步骤是指顶层抽象的运算的具体实现。 它们依赖于数据模型的结构，依赖于数据模型结构的具体表示。 因此，底层的运算步骤包括两部分：一是数据模型的具体表示； 二是定义在该数据模型上的运算的具体实现。我们可以把它们理解为微观运算。 于是，底层运算是顶层运算的细化，底层运算为顶层运算服务。 为了将顶层算法与底层算法隔开，使二者在设计时不会互相牵制、互相影响，必须对二者的接口进行一次抽象。 让底层只通过这个接口为顶层服务，顶层也只通过这个接口调用底层的运算。 这个接口就是抽象数据类型。 抽象数据类型(abstractdatatype,简称ADT) 由一种数据模型和在该数据模型上的一组操作组成。 抽象数据类型包括定义和实现两个方面，其中定义是独立于实现的。 抽象数据类型的定义仅取决于它的逻辑特性，而与其在计算机内部的实现无关，即无论它的内部结构如何变化，只要它的逻辑特性不变，都不会影响到它的使用。 其内部的变化（抽象数据类型实现的变化）只是可能会对外部在使用它解决问题时的效率上产生影响，因此我们的一个重要任务就是如何简单、高效地实现抽象数据类型。 很明显，对于不同的运算组，为使组中所有运算的效率都尽可能地高，其相应的数据模型具体表示的选择将是不同的。 在这个意义下，数据模型的具体表示又依赖于数据模型上定义的那些运算。 特别是，当不同运算的效率互相制约时，还必须事先将所有的运算的相应使用频度排序，让所选择的数据模型的具体表示优先保证使用频度较高的运算有较高的效率。 我们应该看到，抽象数据类型的概念并不是全新的概念。 抽象数据类型和数据类型在实质上是一个概念，只不过是对数据类型的进一步抽象，不仅限于各种不同的计算机处理器中已经实现的数据类型，还包括为解决更为复杂的问题而由用户自定义的复杂数据类型。 例如高级语言都有的“整数”类型就是一种抽象数据类型，只不过高级语言中的整型引进实现了，并且实现的细节可能不同而已。 我们没有意识到抽象数据类型的概念已经孕育在基本数据类型的概念之中，是因为我们已经习惯于在程序设计中使用基本数据类型和相关的运算，没有进一步深究而已。 抽象数据类型一方面使得使用它的人可以只关心它的逻辑特征，不需要了解它的实现方式。另一方面可以使我们更容易描述现实世界，使得我们可以在更高的层面上来考虑问题。例如可以使用树来描述行政区划，使用图来描述通信网络。 根据抽象数据类型的概念，对抽象数据类型进行定义就是约定抽象数据类型的名字，同时，约定在该类型上定义的一组运算的各个运算的名字，明确各个运算分别要有多少个参数，这些参数的含义和顺序，以及运算的功能。 一旦定义清楚，人们在使用时就可以像引用基本数据类型那样，十分简便地引用抽象数据类型； 同时，抽象数据类型的实现就有了设计的依据和目标。 抽象数据类型的使用和实现都与抽象数据类型的定义打交道，这样使用与实现没有直接的联系。因此，只要严格按照定义，抽象数据类型的使用和实现就可以互相独立，互不影响，实现对它们的隔离，达到抽象的目的。 为此抽象数据类型可以使用一个三元组来表示： ADT=(D,S,P) 其中D是数据对象，S是D上的关系集，P是加在D上的一组操作。 在定义抽象数据类型时，我们使用以下格式： ADT抽象数据类型名{数据对象：数据关系：基本操作：} 研究内容 数据结构的研究内容是构造复杂软件系统的基础，它的核心技术是分解与抽象。 通过分解可以划分出数据的3个层次； 再通过抽象，舍弃数据元素的具体内容，就得到逻辑结构。 类似地，通过分解将处理要求划分成各种功能， 再通过抽象舍弃实现细节，就得到运算的定义。 上述两个方面的结合可以将问题变换为数据结构。 这是一个从具体（即具体问题）到抽象（即数据结构）的过程。 然后，通过增加对实现细节的考虑进一步得到存储结构和实现运算， 从而完成设计任务。这是一个从抽象（即数据结构）到具体（即具体实现）的过程。 数据的逻辑结构 指反映数据元素之间的逻辑关系的数据结构，其中的逻辑关系是指数据元素之间的前后间关系，而与他们在计算机中的存储位置无关。逻辑结构包括： 集合：数据结构中的元素之间除了“同属一个集合” 的相互关系外，别无其他关系 线性结构：数据结构中的元素存在一对一的相互关系； 树形结构：数据结构中的元素存在一对多的相互关系； 图形结构：数据结构中的元素存在多对多的相互关系。 数据的物理结构 指数据的逻辑结构在计算机存储空间的存放形式。 数据的物理结构是数据结构在计算机中的表示（又称映像）， 它包括数据元素的机内表示和关系的机内表示。 由于具体实现的方法有顺序、链接、索引、散列等多种， 所以，一种数据结构可表示成一种或多种存储结构。 数据元素的机内表示（映像方法）： 用二进制位（bit）的位串表示数据元素。通常称这种位串为节点（node）。 当数据元素有若干个数据项组成时， 位串中与个数据项对应的子位串称为数据域（data field）。 因此，节点是数据元素的机内表示（或机内映像）。 关系的机内表示（映像方法）： 数据元素之间的关系的机内表示可以分为顺序映像和非顺序映像， 常用两种存储结构：顺序存储结构和链式存储结构。 顺序映像借助元素在存储器中的相对位置来表示数据元素之间的逻辑关系。 非顺序映像借助指示元素存储位置的指针（pointer）来表示数据元素之间的逻辑关系。 数据存储结构 数据的逻辑结构在计算机存储空间中的存放形式称为数据的物理结构(也称为存储结构)。 一般来说，一种数据结构的逻辑结构根据需要可以表示成多种存储结构， 常用的存储结构有顺序存储、链式存储、索引存储和哈希存储等。 数据的顺序存储结构的特点是： 借助元素在存储器中的相对位置来表示数据元素之间的逻辑关系； 非顺序存储的特点是： 借助指示元素存储地址的指针表示数据元素之间的逻辑关系 常用的数据结构(八大数据结构) 在计算机科学的发展过程中，数据结构也随之发展。程序设计中常用的数据结构包括如下几个。 数组(Array) 数组是一种聚合数据类型， 它是将具有相同类型的若干变量有序地组织在一起的集合。 数组可以说是最基本的数据结构，在各种编程语言中都有对应。 一个数组可以分解为多个数组元素， 按照数据元素的类型，数组可以分为整型数组、字符型数组、 浮点型数组、指针数组和结构数组等。数组还可以有一维、 二维以及多维等表现形式。 int[] data = new int[100]； data[0] = 1; 栈( Stack) 栈是一种特殊的线性表， 它只能在一个表的一个固定端进行数据结点的插入和删除操作。 栈按照后进先出的原则来存储数据，也就是说，先插入的数据将被压入栈底， 最后插入的数据在栈顶，读出数据时，从栈顶开始逐个读出。 栈在汇编语言程序中，经常用于重要数据的现场保护。栈中没有数据时， 称为空栈。 队列(Queue) 队列和栈类似，也是一种特殊的线性表。 和栈不同的是，队列只允许在表的一端进行插入操作， 而在另一端进行删除操作。一般来说，进行插入操作的一端称为队尾， 进行删除操作的一端称为队头。队列中没有元素时，称为空队列。 链表( Linked List) 链表是一种数据元素按照链式存储结构进行存储的数据结构， 这种存储结构具有在物理上存在非连续的特点。 链表由一系列数据结点构成，每个数据结点包括数据域和指针域两部分。 其中，指针域保存了数据结构中下一个元素存放的地址。 链表结构中数据元素的逻辑顺序是通过链表中的指针链接次序来实现的。 树( Tree) 树是典型的非线性结构，它是包括，2个结点的有穷集合K。 在树结构中，有且仅有一个根结点，该结点没有前驱结点。 在树结构中的其他结点都有且仅有一个前驱结点， 而且可以有两个后继结点，m≥0。 图(Graph) 图是另一种非线性数据结构。在图结构中， 数据结点一般称为顶点，而边是顶点的有序偶对。 如果两个顶点之间存在一条边，那么就表示这两个顶点具有相邻关系。 堆(Heap) 堆是一种特殊的树形数据结构， 一般讨论的堆都是二叉堆。 堆的特点是根结点的值是所有结点中最小的或者最大的， 并且根结点的两个子树也是一个堆结构。 散列表(Hash) 散列表源自于散列函数(Hash function)， 其思想是如果在结构中存在关键字和T相等的记录， 那么必定在F(T)的存储位置可以找到该记录， 这样就可以不用进行比较操作而直接取得所查记录 为什么我们需要数据结构 数据是计算机科学当中最关键的实体， 而数据结构则可以将数据以某种组织形式存储， 因此，数据结构的价值不言而喻。 无论你以何种方式解决何种问题， 你都需要处理数据——无论是涉及员工薪水、股票价格、 购物清单，还是只是简单的电话簿问题。 数据需要根据不同的场景，按照特定的格式进行存储。 有很多数据结构能够满足以不同格式存储数据的需求。 现有目录 数组 二维数组 向量 链表 树 二叉树 二叉查找树 平衡树 平衡二叉树 多路查找树 2-3树 2-3-4树 红黑树 B树 B+树 为什么MongoDB索引选择B树而Mysql选择B+树 B*树 LSM树 霍夫曼树 树和森林与二叉树的相互转换 串 字典树 堆 二叉堆 队列 双端队列 优先队列 栈 最小栈 映射 散列表 词典 图 图存储结构 图的遍历 图最短路径 图拓扑排序 图关键路径 最小生成树 未来添加 图论、深度优先搜索、广度优先搜索、Prim算法、 Kruskal算法、Boruvka算法、迪杰斯特拉（Dijkstra）算法、 Bellman-Ford算法、弗洛伊德（Floyd）算法、 拓扑排序、关键路径、欧拉回路、Fleury算法 "},"Chapter02/Array.html":{"url":"Chapter02/Array.html","title":"数组","keywords":"","body":"数组(array) 数组是数据结构中很基本的结构，很多编程语言都内置数组。 在Java中当创建数组时会在内存中划分出一块连续的内存， 然后当有数据进入的时候会将数据按顺序的存储在这块连续的内存中。 当需要读取数组中的数据时，需要提供数组中的索引，然后数组根据索引将内存中的数据取出来， 返回给读取程序。 在Java中并不是所有的数据都能存储到数组中，只有相同类型的数据才可以一起存储到数组中。 在Java语言中，数组是对象（An object is a class instance or an array.）， 而且是动态创建的。数组超类是Objcet，可以在数组上调用Object类的所有方法。 每个数组都有一个关联的Class对象，与具有相同组成类型的所有其他数组共享 虽然数组类型不是一个class，但每个数组的Class对象的行为如下： 每个数组类型的直接超类都是Object。 每个数组类型都实现了Cloneable和java.io.Serializable接口。 数组在java里是一种特殊类型，既不是基本数据类型（开玩笑，当然不是）也不是引用数据类型。 有别于普通的“类的实例”对象，java里数组不是类，所以也就没有对应的class文件，数组类型是由jvm从元素类型合成出来的；在jvm中获取数组的长度是用arraylength这个专门的字节码指令的； 在数组的对象头里有一个_length字段，记录数组长度，只需要去读_length字段就可以了。 所有的数据结构都支持几个基本操作：读取、插入、删除。 因为数组在存储数据时是按顺序存储的，存储数据的内存也是连续的， 所以他的特点就是寻址读取数据比较容易，插入和删除比较困难。 简单解释一下为什么 在读取数据时，只需要告诉数组要从哪个位置（索引）取数据就可以了， 数组会直接把你想要的位置的数据取出来给你。 插入和删除比较困难是因为这些存储数据的内存是连续的， 要插入和删除就需要变更整个数组中的数据的位置。 举个例子： 一个数组中编号0->1->2->3->4这五个内存地址中都存了数组的数据， 但现在你需要往4中插入一个数据，那就代表着从4开始， 后面的所有内存中的数据都要往后移一个位置。这可是很耗时的。 数组是线性表，就是数据排成像一条直线一样的结构，除了数组，链表，队列，栈都是线性结构 而非线性表就是二叉树，堆，图等，数据之间不是简单的先后关系。 数组和链表的区别？ 我们时常会回答：链表适合插入，删除，时间复杂度为O(1); 数组适合查找，查找时间复杂度是O(1) 实际上，这种表达是不准确的，数组是适合查找，但是查找的时间复杂度并不是O(1). 即便是排好序的数组，用二分查找，时间复杂度也是O(logn)。 所以，正确的表述是，数组支持随机访问，根据下表随机访问的时间复杂度是O(1) 为什么数组插入会很低效？ 你插入一个元素，其他元素就要往后面排，最坏的情况是O(N)， 因为我们在每个位置插入元素的概率是一样， 所以平均情况时间复杂度是（1+2+。。。n）/n = O(n) 如果数组中的数据是有序的，我们在某一位置插入一个新的元素， 就必须按照刚才的方法搬移后面的数据， 但是，若数组没有任何规律，如果要插入到第k个位置，为了避免大规模搬移， 还有一个简单的就是直接将第k位数据搬移到数组元素的最后，把新的元素直接放入第k个位置。 那删除操作呢？ 和插入数据类似，内存因为是连续的,也需要搬移，若删除数组末尾数据，就是O(1)， 删除开头就是O(n)，平均也是O(n)。 为什么大多数语言数组都是0开头？而不是1开头呢？ 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移(offset)”, 如果用a来表示数组的首地址， a[0]就是偏移为0的位置，也就是首地址，a[k]就表示偏移k个type_size的位置， 所以计算a[k]的内存地址只需要a[k]_address = base_address + k type_size 若是数组从1开始计数， 那么我们计算数组元素a[k]的内存地址就是 a[k]_address = base_address + (k-1) type_size 从1开始编号，每次随机访问数组元素都多了一次减法运算， 对于cpu来说，就是多了一次减法指令。 一个Characters数组不是一个String 在Java语言中，一个char数组并不是一个String，并且String和char数组都不会被'\\ u0000'（NUL字符）终止。 String对象是不可变的，它的内容永远不变，而char数组有可变元素。 String类中的toCharArray方法返回一个包含与String相同字符序列的字符数组。StringBuffer类在可变字符数组上实现有用的方法。 为什么数组有length属性，而字符串没有？或者，为什么字符串有length()方法，而数组没有？ 一旦数组被创建，他的长度就是固定的了。 数组的长度可以作为final实例变量的长度。 因此，长度可以被视为一个数组的属性。 String背后的数据结构是一个char数组, 所以没有必要来定义一个不必要的属性（因为该属性在char数值中已经提供了）。 对象数组和基本类型数组 对象数组和基本类型数组在使用上几乎是相同的， 唯一的区别就是对象数组保存的是引用，基本类型数组直接保存基本类型的值。 多维数组中构成矩阵的每个向量都可以具有任意的长度（这被称为粗糙数组）。 数组与其他种类的容器 数组与其他种类的容器之间的区别有三方面：效率、类型和保存基本类型的能力。 在Java中，数组是一种效率最高的存储和随机访问对象引用序列的方式。 数组是一个简单的线性序列，这使得元素访问非常快速。 付出的代价是数组对象的大小被固定，并且在其生命周期中不可改变。 数组之所以优于泛型之前的同期，就是因为可以创建一个数组去持有某种具体类型。 在泛型之前，其他的容器在处理对象时，都将他们视为没有任何具体类型，即将这些对象当做Java中根类的根类Object处理。 有了泛型后，容器可以指定并检查它们所持有对象的类型，并且有了自动包装机制，容器看起来还能够持有基本类型。 随着自动包装机制的出现，容器已经可以与数组几乎一样方便的用于基本类型中了。 数组硕果仅存的优点就是效率 。 然而，如果要解决更一般化的问题，数组可能会受到过多的限制，因此在这些情形下还是会使用容器。 容器能否完全替代数组？ 针对数组类型，很多语言都提供了容器类，比如 Java 中的 ArrayList、C++ STL 中的 vector。 在项目开发中，什么时候适合用数组，什么时候适合用容器呢？ 这里我拿 Java 语言来举例。如果你是 Java 工程师， 几乎天天都在用 ArrayList，对它应该非常熟悉。 那它与数组相比，到底有哪些优势呢？ 我个人觉得，ArrayList 最大的优势就是可以将很多数组操作的细节封装起来。 比如前面提到的数组插入、删除数据时需要搬移其他数据等。 另外，它还有一个优势，就是支持动态扩容。 数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。 如果我们申请了大小为 10 的数组，当第 11 个数据需要存储到数组中时， 我们就需要重新分配一块更大的空间，将原来的数据复制过去， 然后再将新的数据插入。 如果使用 ArrayList，我们就完全不需要关心底层的扩容逻辑， ArrayList 已经帮我们实现好了。每次存储空间不够的时候， 它都会将空间自动扩容为 1.5 倍大小。 不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移， 是比较耗时的。所以，如果事先能确定需要存储的数据大小， 最好在创建 ArrayList 的时候事先指定数据大小。 有些时候，用数组会更合适些，我总结了几点自己的经验。 Java ArrayList 无法存储基本类型，比如 int、long， 需要封装为 Integer、Long 类， 而 Autoboxing、Unboxing 则有一定的性能消耗， 所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。 如果数据大小事先已知，并且对数据的操作非常简单， 用不到 ArrayList 提供的大部分方法，也可以直接使用数组。 还有一个是我个人的喜好，当要表示多维数组时， 用数组往往会更加直观。比如 Object[][] array； 而用容器的话则需要这样定义：ArrayList array。 数组的算法实现 冒泡排序 直接选择排序 栈内存 在方法中定义的一些基本类型的变量和对象的引用变量都在方法的栈内存中分配， 当在一段代码中定义一个变量时，java就在栈内存中为这个变量分配内存空间， 当超出变量的作用域后，java会自动释放掉为该变量所分配的内存空间。 堆内存 堆内存用来存放由new运算符创建的对象和数组，在堆中分配的内存， 由java虚拟机的自动垃圾回收器来管理。 在堆中创建了一个数组或对象后，同时还在栈内存中定义一个特殊的变量。 让栈内存中的这个变量的取值等于数组或者对象在堆内存中的首地址， 栈中的这个变量就成了数组或对象的引用变量， 引用变量实际上保存的是数组或对象在堆内存中的地址（也称为对象的句柄）， 以后就可以在程序中使用栈的引用变量来访问堆中的数组或对象。 与结构或类中的字段的区别 数组中的所有元素都具有相同类型(这一点和结构或类中的字段不同，它们可以是不同类型)。 数组中的元素存储在一个连续性的内存块中， 并通过索引来访问(这一点也和结构和类中的字段不同，它们通过名称来访问)。 面试中关于数组的常见问题 寻找数组中第二小的元素 第一种：先排序然后找出第一个大于array[0]的数 第二种：定义两个变量分别代表第一小和第二小的数,以此和数组的数进行比较 第三种：用小顶堆，但是这个比较适合找出数组中第n小的数 找到数组中第一个不重复出现的整数 第一种：计数排序法，把数组出现的次数放到另一个数组中，然后找出全部出现一次的数字和原始数组嵌套遍历，原始数组最先出现在新数组的即为第一个。 第二种：双循环：内循环中一个也没重复的即为第一个，即刻停止，时间复杂度是O(N^2) 第三种：借助有序哈希表。每一个数字作为key，出现的次数为value，这个算法的时间复杂度是O(n) 合并两个有序数组 第一种：混合插入有序数组，由于两个数组都是有序的，所以只要按顺序比较大小即可。 将数组里的正数排在数组的后面，正数排在数组的后面。但不改变原先正数和正数的排列次序。 例：input: -5，2，-3, 4，-8，-9, 1, 3，-10；output: -5, -3, -8, -9, -10, 2, 4, 1, 3。 "},"Chapter02/TwoDimensionalArray.html":{"url":"Chapter02/TwoDimensionalArray.html","title":"二维数组","keywords":"","body":"二维数组 如果一维数组的各个元素仍然是一个数组，那么它就是一个二维数组。 二维数组常用于表示表，表中的信息以行和列的形式组织， 第一个下标代表元素所在的行，第二个下标代表所在的列。 二维数组的创建 先声明，再用 new 运算符进行内存分配 声明语法格式： 数组元素的类型 数组名字[][]; 数组元素的类型[][] 数组名字； 分配内存语法： 数组名字 = new 数组类型[][] a = new int[2][4]; // 直接给每一维分配内存空间 b = new int[2][]; //分别为每一维分配内存，这种方式列可省，行不可省 a[0] = new int[2]; a[1] = new int[3]; 声明的同时为数组分配内存 数组元素的类型 数组名字[][] = new 数组元素的类型[][] 二维数组的初始化 二维数组的初始化与一位数组初始化类似，同样使用大括号完成。 语法格式： type：数组数据类型; arrayname：数组名称，一个合法的标识符; value：数组中各元素的值。 使用二维数组 使用二维数组常见的就是遍历数组，介绍两种遍历数组的方法。 先来学习一下各种表示方法： 表示二维数组的行数： 二维数组名.length 表示一行：　　　　　 二维数组名[行下标] （行下标的范围：[0，二维数组总行数 - 1]） 表示每一行的列数： 二维数组名[行下标].length 表示具体的一个数据： 二维数组名[行下标][列下标] （列下标的范围：[0，该行总列数 - 1]） 1、使用 for 循环遍历 2、使用 foreach 循环遍历 int array[][] = new int[3][4]; //创建一个3行4列的数组，并且里面值都为默认值 // 使用 for 循环 for(int i=0;i "},"Chapter02/Vector.html":{"url":"Chapter02/Vector.html","title":"向量","keywords":"","body":"向量 知道向量指向需要先知道一个概念序列（Sequence。 所谓序列，就是依次排列的多个对象。 比如，每一计算机程序都可以看作一个序列， 它由一系列依次排列的指令组成，正是指令之间的次序决定了程序的具体功能。 因此，所谓序列，就是一组对象之间的后继与前驱关系。 在实际问题中，序列可以用来实现很多种数据结构， 因此被认为是数据结构设计的基础。 栈、队列以及双端队列，都可以看作带有附加限制的序列。 向量（Vector）和列表（List）都属于序列 什么是向量 对数组结构进行抽象与扩展之后，就可以得到向量结构， 因此向量也称作数组列表（Array list）。 向量提供一些访问方法，使得我们可以通过下标直接访问序列中的元素， 也可以将指定下标处的元素删除，或将新元素插入至指定下标。 为了与通常数组结构的下标（Index）概念区分开来， 我们通常将序列的下标称为秩（Rank）。 假定集合 S 由n 个元素组成，它们按照线性次序存放， 于是我们就可以直接访问其中的第一个元素、第二个元素、第三个元素……。 也就是说，通过[0, n-1]之间的每一个整数， 都可以直接访问到唯一的元素e， 而这个整数就等于S 中位于e 之前的元素个数——在此，我们称之为该元素的秩（Rank）。 不难看出，若元素e 的秩为r，则只要e 的直接前驱（或直接后继）存在，其秩就是r-1（或r+1）。 支持通过秩直接访问其中元素的序列， 称作向量（Vector）或数组列表（Array list）。 实际上，秩这一直观概念的功能非常强大——它可以直接指定插入或删除元素的位置。 向量ADT(AbstractDataType) 操作方法 功能描述 getSize() 报告向量中的元素数目输入：无输出：非负整数 isEmpty() 判断向量是否为空输入：无输出：布尔值 getAtRank(r) 若0 ≤ r 输入：一个整数输出：对象 replaceAtRank(r, e) 若0 ≤ r 输入：一个整数和一个对象输出：对象 insertAtRank(r, e) 若0 ≤ r ≤ getSize()，则将e 插入向量中，作为秩为r 的元素（原秩不小于r 的元素顺次后移），并返回原来的元素 ；否则，报错输入：一个整数和一个对象输出：对象 removeAtRank(r) 若0 ≤ r 输入：一个整数输出：对象 基于数组的简单实现 package datastructure.sequence; public interface FireVector { boolean isEmpty(); int size(); void add(E e); void set(int index, E e); E get(int index); void remove(int index); boolean contains(E e); } package datastructure.sequence; import java.util.Arrays; public class FireArrayVector implements FireVector { private Object[] elementData; private int elementCount; private int capacityIncrement; protected int modCount; /** * * @param initialCapacity 初始容量 * @param capacityIncrement 扩容时增长的容量 */ public FireArrayVector(int initialCapacity, int capacityIncrement) { if (initialCapacity elementData.length){ grow(minLength); } } /** * The maximum size of array to allocate. * Some VMs reserve some header words in an array. * Attempts to allocate larger arrays may result in OutOfMemoryError: Requested array size exceeds VM limit * * 要分配的最大数组大小。 一些虚拟机在数组中保留一些头字。 * 尝试分配更大的阵列可能会导致OutOfMemoryError：请求的阵列大小超出VM限制 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; private void grow(int minCapacity) { int oldCapacity = elementData.length; int newCapacity= oldCapacity + ((capacityIncrement > 0) ? capacityIncrement : oldCapacity); if(minCapacity - newCapacity > 0){ newCapacity = minCapacity; } if (newCapacity > MAX_ARRAY_SIZE){ } elementData = Arrays.copyOf(elementData, newCapacity); } @Override public void set(int index, E element) { if (index >= elementCount){ throw new ArrayIndexOutOfBoundsException(); } elementData[index] = element; } @Override public E get(int index) { if (index >= elementCount){ throw new ArrayIndexOutOfBoundsException(); } return (E) elementData[index]; } @Override public void remove(int index) { if(index elementCount){ throw new ArrayIndexOutOfBoundsException(index); } int j = elementCount - index - 1; System.arraycopy(elementData, index + 1, elementData, index, j); elementData[--elementCount] = null; } @Override public boolean contains(Object o) { return indexOf(o) >= 0; } private int indexOf(Object o) { if (o == null) { for (int i = 0; i Java本身也提供了与向量ADT功能类似的两个类：java.util.ArrayList和java.util.Vector。 "},"Chapter02/LinkedList.html":{"url":"Chapter02/LinkedList.html","title":"链表","keywords":"","body":"链表 链表这类数据结构，有点像生活中的火车，一节车厢连着下一节车厢， 在火车里面，只有到了4号车厢你才能进入5号车厢， 一般情况下，不可能直接在3号车厢绕过4号车厢进入5号车厢。 不过更准确来说，火车是双向链表， 也就是说在4号车厢也可以反向进入3号车厢。 链表是一系列的存储数据元素的单元通过指针串接起来形成的， 因此每个单元至少有两个域，一个域用于数据元素的存储， 另一个或两个域是指向其他单元的指针(Pointer)。 这里具有一个数据域和多个指针域的存储单元通常称为节点（node）。 链表的第一个节点和最后一个节点，分别称为链表的头节点和尾节点。 尾节点的特征是其 next 引用为空（null）。 链表中每个节点的 next 引用都相当于一个指针，指向另一个节点， 借助这些 next 引用，我们可以从链表的头节点移动到尾节点。 链表数据结构中主要包含单向链表、双向链表及循环链表 由于不必须按顺序存储，链表的插入和删除操作可以达到O(1)的复杂度。 这里的链表是一种抽象的数据结构，每个节点有信息和下一个节点的地址，和怎么实现半毛钱关系也没有。 链表当然也可以用数组实现，也可以用STL的vector，都不用动态allocate 单向链表 单向链表(单链表)是链表的一种，它由节点组成， 每个节点都包含下一个节点的指针，下图就是一个单链表， 表头为空，表头的后继节点是\"结点10\"(数据为10的结点)， \"节点10\"的后继结点是\"节点20\"(数据为10的结点)，... 单链表删除节点 我们看看单链表删除节点的操作，比如说下面这个单链表中我们要删除\"节点30\"。 删除之前：\"节点20\" 的后继节点为\"节点30\"，而\"节点30\" 的后继节点为\"节点40\"。 删除之后：\"节点20\" 的后继节点为\"节点40\"。 单链表添加节点 我们再来看看单链表添加节点的操作，比如说下面这个单链表中我们在\"节点10\"与\"节点20\"之间添加\"节点15\" 添加之前：\"节点10\" 的后继节点为\"节点20\"。 添加之后：\"节点10\" 的后继节点为\"节点15\"，而\"节点15\" 的后继节点为\"节点20\"。 代码实现 package datastructure.sequence; /** * 单向链表 */ public class FireSinglyLinkedList { int size = 0; Node first; Node last; public FireSinglyLinkedList() { } public boolean isEmpty() { return this.size == 0; } public int size() { return this.size; } public void add(E e) { Node p = this.last; Node newNode = new Node<>(e, null); this.last = newNode; if (p == null) { this.first = newNode; } else { last.next = newNode; } this.size++; } public void add(int index, E e) { checkPositionInde(index); if (index == size) { add(e); } else if (index == 0) { Node newNode = new Node<>(e, null); newNode.next = first; this.first = newNode; this.size++; } else { Node pre = node(index - 1); Node newNode = new Node<>(e, null); Node x = pre.next; pre.next = newNode; newNode.next = x; this.size++; } } private void checkPositionInde(int index) { if (!isPositionIndex(index)) { throw new IndexOutOfBoundsException(\"Index: \" + index + \", Size: \" + size); } } private boolean isPositionIndex(int index) { return index >= 0 && index x = node(index); x.item = e; } private Node node(int index) { Node x = first; for (int i = 0; i head = first; this.first = first.next; head.next = null; //last = null; } else { Node pre = node(index - 1); Node x = pre.next; Node next = x.next; pre.next = next; x.next = null; } size--; if (this.size == 0) { this.last = null; } } private class Node { E item; Node next; Node(E element, Node next) { this.item = element; this.next = next; } } } 循环链表 双向链表 双向链表(双链表)是链表的一种。和单链表一样，双链表也是由节点组成，它的每个数据结点中都有两个指针，分别指向直接后继和直接前驱。 所以，从双向链表中的任意一个结点开始，都可以很方便地访问它的前驱结点和后继结点。一般我们都构造双向循环链表。 表头为空，表头的后继节点为\"节点10\"(数据为10的节点)； \"节点10\"的后继节点是\"节点20\"(数据为10的节点)， \"节点20\"的前继节点是\"节点10\"；\"节点20\"的后继节点是\"节点30\"， \"节点30\"的前继节点是\"节点20\"；...；末尾节点的后继节点是表头。 双向链表删除节点 我们看看双向链表删除节点的操作，比如说下面这个单链表中我们要删除\"节点30\"。 删除之前：\"节点20\"的后继节点为\"节点30\"，\"节点30\" 的前继节点为\"节点20\"。 \"节点30\"的后继节点为\"节点40\"，\"节点40\" 的前继节点为\"节点30\"。 删除之后：\"节点20\"的后继节点为\"节点40\"，\"节点40\" 的前继节点为\"节点20\"。 双向链表添加节点 我们再来看看双向链表添加节点的操作，比如说下面这个双向链表在\"节点10\"与\"节点20\"之间添加\"节点15\" 添加之前：\"节点10\"的后继节点为\"节点20\"，\"节点20\" 的前继节点为\"节点10\"。 添加之后：\"节点10\"的后继节点为\"节点15\"，\"节点15\" 的前继节点为\"节点10\"。\"节点15\"的后继节点为\"节点20\"，\"节点20\" 的前继节点为\"节点15\"。 代码实现 用链表的目的是什么？省空间还是省时间？ 链表的优点除了「插入删除不需要移动其他元素」之外，还在于它是一个局部化结构。 就是说当你拿到链表的一个 node 之后，不需要太多其它数据，就可以完成插入，删除的操作。而其它的数据结构不行。 比如说 array，你只拿到一个 item 是断不敢做插入删除的。 当然了，局部化这个好处只有 intrusive 链表才有，就是必须 prev/next 嵌入在数据结构中。 像 STL 和 Java 那种设计是失败了。 数组（非动态）对比链表： 数组（非动态）对比链表：数组简单易用，在实现上使用的是连续的内存空间。 得益于现代处理器的缓存机制，其访问效率很高。 数组的缺点是大小固定，一经声明就要占用整块内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它。 而如果声明的数组过小，则可能出现不够用的情况。这时只能再声明一个更大的数组，然后把原数组拷贝进去，非常费时。 链表最大的特点是大小动态可调，其大小与已经而不是计划存储的元素个数成正比。在确定了操作点之后，插入和删除的操作耗时是固定的。 但是使用链表也要为其动态特性付出代价，例如每个元素都需要消耗额外的存储空间去存储一份指向下一个元素的指针类数据。 链表与数组的各种操作复杂度如下所示： 操作 ------------------- 链表 ---- 数组 查找 -------------------- O(n) ---- O(1) 在头部插入/删除 ------ O(1) ---- O(n) 在尾部插入/删除 ------ O(n) ---- O(1) 在中间插入/删除 ------ O(n) ---- O(n) 额外的存储空间 ------- O(n) ---- 0 注意虽然表面看起来复杂度都是 O(n)，但是链表和数组在插入和删除时进行的是完全不同的操作。 链表的主要耗时操作是历遍查找，删除和插入操作本身的复杂度是O(1)。 数组查找很快，主要耗时的操作是拷贝覆盖。 因为除了目标元素在尾部的特殊情况，数组进行插入和删除时需要对操作点之后的所有元素进行前后移位操作，只能通过拷贝和覆盖的方法进行。 from \"Data Structures and Algorithms Made Easy\" 关于数据结构与 ADT: Linked-list 中的 linked 描述的仍然是数据的结构而不是实现的手段。 一般意义上的实现即 implementation 是跟具体的编程语言挂钩的。 另外计算机科学中一般说 Abstract data type (ADT) 而不是 Abstract data structure。ADT 是从用户的角度出发， 关心的重点既包括数据的定义也包括数据的使用方法。 而数据结构是从设计者的角度出发，关心的重点是数据的组织和存储方式。单论链表的组织和存储方法，这时链表的概念就是数据结构。 如果再加上链表的创建、历遍、删除、插入等操作，这时链表的概念就是 ADT。而用某一种编程语言实现了链表的 ADT，这叫做链表的实现。 面试中关于链表的常见问题 反转链表 链表反转一般指的是单向链表，常见的方式是按原始顺序迭代结点，并将它们逐个移动到列表的头部 寻找中间节点 看看我们如何来理解快慢指针的：把链表看作一条赛道，运动员A速度是运动员B的两倍，当运动员A跑完全程，运动员B就刚刚好跑了一半。 我们要做的代码如下，fast的next指针为空或者fast的next的next指针为空，则退出寻址循环 slow = slow.next fast = fast.next.next 删除链表中的节点 输入: 1->2->6->3->4->5->6, val = 6 输出: 1->2->3->4->5 奇偶链表 给定一个单链表，把所有的奇数节点和偶数节点分别排在一起。请注意，这里的奇数节点和偶数节点指的是节点编号的奇偶性，而不是节点的值的奇偶性。 请尝试使用原地算法完成。你的算法的空间复杂度应为 O(1)，时间复杂度应为 O(nodes)，nodes 为节点总数。 输入: 1->2->3->4->5->NULL 输出: 1->3->5->2->4->NULL 输入: 2->1->3->5->6->4->7->NULL 输出: 2->3->6->7->1->5->4->NULL 回文链表 返回链表倒数第N个节点 删除链表中的重复项 第一种使用额外的存储空间，引入hash表 第二种不能使用额外的存储空间，直接在原始链表上进行操作 建立指针p，用于遍历链表； 建立指针q，q遍历p后面的结点，并与p数值比较； 建立指针r，r保存需要删掉的结点，再把需要删掉的结点的前后结点相接。由此去掉重复值。 合并两个有序的单链表，合并之后的链表依然有序 这个类似于归并排序，你创建一个新链表，然后把上面两个链表依次比较，插入新链表 检测链表中的循环 这里也是用到两个指针，如果一个链表有环，那么用一个指针去遍历，是永远走不到头的。 因此，我们用两个指针去遍历：first指针每次走一步，second指针每次走两步，如果first指针和second指针相遇，说明有环。 ;如果不存在环，指针fast遇到NULL退出。 取出有环链表中，环的长度 在环上相遇后，记录第一次相遇点为Pos，之后指针slow继续每次走1步，fast每次走2步。 在下次相遇的时候fast比slow正好又多走了一圈，也就是多走的距离等于环长。 设从第一次相遇到第二次相遇，设slow走了len步，则fast走了2*len步，相遇时多走了一圈： 环长=2*len-len。 单链表中，取出环的起始点 第一次碰撞点Pos到连接点Join的距离=头指针到连接点Join的距离， 因此，分别从第一次碰撞点Pos、头指针head开始走，相遇的那个点就是连接点。 在环上相遇后，记录第一次相遇点为Pos，连接点为Join，假设头结点到连接点的长度为LenA，连接点到第一次相遇点的长度为x，环长为R。 第一次相遇时，slow走的长度 S = LenA + x; 第一次相遇时，fast走的长度 2S = LenA + n*R + x; 所以可以知道，LenA + x = n*R;　　LenA = n*R -x; 求有环单链表的链表长 　上述中求出了环的长度和连接点的位置，就可以求出头结点到连接点的长度。两者相加就是链表的长度。 判断两个单链表相交的第一个交点 解这道题之前，我们需要首先明确一个概念： 如果两个单链表有共同的节点，那么从第一个共同节点开始，后面的节点都会重叠，直到链表结束。 因为两个链表中有一个共同节点，则这个节点里的指针域指向的下一个节点地址一样，所以下一个节点也会相交，依次类推。所以，若相交，则两个链表呈“Y”字形。如下图： 暴力解法。 从头开始遍历第一个链表，遍历第一个链表的每个节点时，同时从头到尾遍历第二个链表，看是否有相同的节点，第一次找到相同的节点即第一个交点。若第一个链表遍历结束后，还未找到相同的节点，即不存在交点。时间复杂度为O(n^2)。这种方法显然不是写这篇博客的重点。。。不多说了。 使用栈。 我们可以从头遍历两个链表。创建两个栈，第一个栈存储第一个链表的节点，第二个栈存储第二个链表的节点。每遍历到一个节点时，就将该节点入栈。两个链表都入栈结束后。则通过top判断栈顶的节点是否相等即可判断两个单链表是否相交。因为我们知道，若两个链表相交，则从第一个相交节点开始，后面的节点都相交。 若两链表相交，则循环出栈，直到遇到两个出栈的节点不相同，则这个节点的后一个节点就是第一个相交的节点。 遍历链表记录长度。 同时遍历两个链表到尾部，同时记录两个链表的长度。若两个链表最后的一个节点相同，则两个链表相交。 有两个链表的长度后，我们就可以知道哪个链表长，设较长的链表长度为len1,短的链表长度为len2。 则先让较长的链表向后移动(len1-len2)个长度。然后开始从当前位置同时遍历两个链表，当遍历到的链表的节点相同时，则这个节点就是第一个相交的节点。 https://xiaozhuanlan.com/topic/5021347986 "},"Chapter02/Tree.html":{"url":"Chapter02/Tree.html","title":"树","keywords":"","body":"树 树状图是一种数据结构，它是由n（n>=0）个有限结点组成一个具有层次关系的集合。 把它叫做“树”是因为它看起来像一棵倒挂的树，也就是说它是根朝上，而叶朝下的。 它具有以下的特点： 每个结点有零个或多个子结点； 没有父结点的结点称为根结点； 每一个非根结点有且只有一个父结点； 除了根结点外，每个子结点可以分为多个不相交的子树； n=0时称为空树。 在任意一颗非空树中： 有且仅有一个特定的称为根（Root）的结点； 当n>1时，其余结点可分为m(m>0)个互不相交的有限集T1、T2、......、Tn，其中每一个集合本身又是一棵树，并且称为根的子树。 此外，树的定义还需要强调以下两点： n>0时根结点是唯一的，不可能存在多个根结点，数据结构中的树只能有一个根结点。 m>0时，子树的个数没有限制，但它们一定是互不相交的。 名词理解： 结点：指树中的一个元素； 结点的度：指结点拥有的子树的个数，二叉树的度不大于2； 数的度：指树中的最大结点度数； 叶子：度为0的结点，也称为终端结点； 高度：叶子节点的高度为1，根节点高度最高； 层：根在第一层，以此类推； 面试中关于树结构的常见问题 在二叉搜索树中查找第k个最大值 查找与根节点距离k的节点 "},"Chapter02/BinaryTree.html":{"url":"Chapter02/BinaryTree.html","title":"二叉树","keywords":"","body":"二叉树(BinaryTree) 二叉树是n(n>=0)个结点的有限集合， 该集合或者为空集（称为空二叉树），或者由一个根结点和两棵互不相交的、分别称为根结点的左子树和右子树组成。 二叉树特点 由二叉树定义以及图示分析得出二叉树有以下特点： 每个结点最多有两颗子树，所以二叉树中不存在度大于2的结点。 左子树和右子树是有顺序的，次序不能任意颠倒。 即使树中某结点只有一棵子树，也要区分它是左子树还是右子树。 二叉树性质 在二叉树的第i层上最多有2i-1 个节点 。（i>=1） 二叉树中如果深度为k,那么最多有2k-1个节点。(k>=1） n0=n2+1 n0表示度数为0的节点数，n2表示度数为2的节点数。 在完全二叉树中，具有n个节点的完全二叉树的深度为[log2n]+1，其中[log2n]是向下取整。 若对含 n 个结点的完全二叉树从上到下且从左至右进行 1 至 n 的编号，则对完全二叉树中任意一个编号为 i 的结点有如下特性： (1) 若 i=1，则该结点是二叉树的根，无双亲, 否则，编号为 [i/2] 的结点为其双亲结点; (2) 若 2i>n，则该结点无左孩子， 否则，编号为 2i 的结点为其左孩子结点； (3) 若 2i+1>n，则该结点无右孩子结点， 否则，编号为2i+1 的结点为其右孩子结点。 斜树 斜树：所有的结点都只有左子树的二叉树叫左斜树。所有结点都是只有右子树的二叉树叫右斜树。这两者统称为斜树。 满二叉树 在一棵二叉树中。如果所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上，这样的二叉树称为满二叉树。 满二叉树的特点有： 叶子只能出现在最下一层。出现在其它层就不可能达成平衡。 非叶子结点的度一定是2。 在同样深度的二叉树中，满二叉树的结点个数最多，叶子数最多。 完全二叉树 对一颗具有n个结点的二叉树按层编号，如果编号为i(1 特点： 1）叶子结点只能出现在最下层和次下层。 2）最下层的叶子结点集中在树的左部。 3）倒数第二层若存在叶子结点，一定在右部连续位置。 4）如果结点度为1，则该结点只有左孩子，即没有右子树。 5）同样结点数目的二叉树，完全二叉树深度最小。 注：满二叉树一定是完全二叉树，但反过来不一定成立。 二叉树的存储结构 顺序存储 二叉树的顺序存储结构就是使用一维数组存储二叉树中的结点，并且结点的存储位置，就是数组的下标索引。 上图所示的一棵完全二叉树采用顺序存储方式，如下图表示： 由图上图可以看出，当二叉树为完全二叉树时，结点数刚好填满数组。 那么当二叉树不为完全二叉树时，采用顺序存储形式如何呢? 其中浅色结点表示结点不存在。那么上图所示的二叉树的顺序存储结构如下图所示： 其中，∧表示数组中此位置没有存储结点。 此时可以发现，顺序存储结构中已经出现了空间浪费的情况。 那么对于右斜树极端情况对应的顺序存储结构如下图所示： 由上图可以看出，对于这种右斜树极端情况，采用顺序存储的方式是十分浪费空间的。 因此，顺序存储一般适用于完全二叉树。 二叉链表 既然顺序存储不能满足二叉树的存储需求，那么考虑采用链式存储。 由二叉树定义可知，二叉树的每个结点最多有两个孩子。 因此，可以将结点数据结构定义为一个数据和两个指针域。 表示方式如下图所示： 定义节点代码 class Node{ E data; Node rchild; Node lchild; } 则完全二叉树可以采用下图表示。 上图中采用一种链表结构存储二叉树，这种链表称为二叉链表。 二叉树遍历 二叉树的遍历是指从二叉树的根结点出发， 按照某种次序依次访问二叉树中的所有结点， 使得每个结点被访问一次，且仅被访问一次。 二叉树的访问次序可以分为四种： 前序遍历: 根->左子树->右子树 中序遍历: 左子树->根->右子树 后序遍历; 左子树->右子树->根 层序遍历 前序遍历 通俗的说就是从二叉树的根结点出发， 当第一次到达结点时就输出结点数据， 按照先向左在向右的方向访问。 上图所示二叉树访问如下： 从根结点出发，则第一次到达结点A，故输出A; 继续向左访问，第一次访问结点B，故输出B； 按照同样规则，输出D，输出H； 当到达叶子结点H，返回到D，此时已经是第二次到达D，故不在输出D，进而向D右子树访问，D右子树不为空，则访问至I，第一次到达I，则输出I； I为叶子结点，则返回到D，D左右子树已经访问完毕，则返回到B，进而到B右子树，第一次到达E，故输出E； 向E左子树，故输出J； 按照同样的访问规则，继续输出C、F、G； 则上图所示二叉树的前序遍历输出为： ABDHIEJCFG 中序遍历 就是从二叉树的根结点出发，当第二次到达结点时就输出结点数据，按照先向左在向右的方向访问。 上图所示二叉树中序访问如下： 从根结点出发，则第一次到达结点A，不输出A，继续向左访问，第一次访问结点B，不输出B；继续到达D，H； 到达H，H左子树为空，则返回到H，此时第二次访问H，故输出H； H右子树为空，则返回至D，此时第二次到达D，故输出D； 由D返回至B，第二次到达B，故输出B； 按照同样规则继续访问，输出J、E、A、F、C、G； 则上图所示二叉树的中序遍历输出为： HDIBJEAFCG 后序遍历 就是从二叉树的根结点出发，当第三次到达结点时就输出结点数据，按照先向左在向右的方向访问。 上图所示二叉树后序访问如下： 从根结点出发，则第一次到达结点A，不输出A，继续向左访问，第一次访问结点B，不输出B；继续到达D，H； 到达H，H左子树为空，则返回到H，此时第二次访问H，不输出H； H右子树为空，则返回至H，此时第三次到达H，故输出H； 由H返回至D，第二次到达D，不输出D； 继续访问至I，I左右子树均为空，故第三次访问I时，输出I； 返回至D，此时第三次到达D，故输出D； 按照同样规则继续访问，输出J、E、B、F、G、C，A； 则上图所示二叉树的后序遍历输出为： HIDJEBFGCA 层次遍历 层次遍历就是按照树的层次自上而下的遍历二叉树。 针对上图所示二叉树的层次遍历结果为： ABCDEFGHIJ 虽然二叉树的遍历过程看似繁琐，但是由于二叉树是一种递归定义的结构， 故采用递归方式遍历二叉树的代码十分简单。 递归实现代码如下： package datastructure.tree; import lombok.Getter; import lombok.Setter; import java.util.Queue; import java.util.concurrent.LinkedBlockingDeque; /** * 二叉树 * * * 定义:二叉树是n(n>=0)个结点的有限集合，该集合或者为空集（称为空二叉树），或者由一个根结点和两棵互不相交的、分别称为根结点的左子树和右子树组成 * * * 二叉树特点: 由二叉树定义以及图示分析得出二叉树有以下特点： 1）每个结点最多有两颗子树，所以二叉树中不存在度大于2的结点。 * 2）左子树和右子树是有顺序的，次序不能任意颠倒。 3）即使树中某结点只有一棵子树，也要区分它是左子树还是右子树。 * * * 二叉树性质: 1）在二叉树的第i层上最多有2i-1 个节点 。（i>=1） 2）二叉树中如果深度为k,那么最多有2k-1个节点。(k>=1） * 3）n0=n2+1 n0表示度数为0的节点数，n2表示度数为2的节点数。 * 4）在完全二叉树中，具有n个节点的完全二叉树的深度为[log2n]+1，其中[log2n]是向下取整。 5）若对含 n * 个结点的完全二叉树从上到下且从左至右进行 1 至 n 的编号，则对完全二叉树中任意一个编号为 i 的结点有如下特性： * * * (1) 若 i=1，则该结点是二叉树的根，无双亲, 否则，编号为 [i/2] 的结点为其双亲结点; (2) 若 2i>n，则该结点无左孩子， 否则，编号为 * 2i 的结点为其左孩子结点； (3) 若 2i+1>n，则该结点无右孩子结点， 否则，编号为2i+1 的结点为其右孩子结点。 。 * * * @author liuyi27 * */ @Setter @Getter public class BinaryTree { private Object data; private BinaryTree lchild, rchild; /** * 前序遍历递归算法 * * 前序遍历通俗的说就是从二叉树的根结点出发，当第一次到达结点时就输出结点数据，按照先向左再向右的方向访问。 * */ public void preOrderTraverse(BinaryTree tree) { if (tree == null) { return; } System.out.print(tree.getData()); preOrderTraverse(tree.getLchild()); preOrderTraverse(tree.getRchild()); } /** * 中序遍历递归算法 * * 中序遍历就是从二叉树的根结点出发，当第二次到达结点时就输出结点数据，按照先向左再向右的方向访问 * */ public void inOrderTraverse(BinaryTree tree) { if (tree == null) { return; } inOrderTraverse(tree.getLchild()); System.out.print(tree.getData()); inOrderTraverse(tree.getRchild()); } /** * 后序遍历递归算法 * * 后序遍历就是从二叉树的根结点出发，当第三次到达结点时就输出结点数据，按照先向左再向右的方向访问 * */ public void postOrderTraverse(BinaryTree tree) { if (tree == null) { return; } postOrderTraverse(tree.getLchild()); postOrderTraverse(tree.getRchild()); System.out.print(tree.getData()); } /** * 前序建立二叉树 */ public static void preCreate() { } /** * 层序遍历 * * 层次遍历就是按照树的层次自上而下的遍历二叉树 * * * @param node * @param queue */ public void LayerOrder(BinaryTree node, Queue queue) { if (node != null && queue.isEmpty()) { // 将当前节点放入队列首指针所指位置 queue.add(node); System.out.print(queue.poll().getData()); } else { System.out.print(node.getData()); } if (node.lchild != null) { queue.add(node.lchild); } if (node.rchild != null) { queue.add(node.rchild); } BinaryTree nextNode = queue.poll(); if (nextNode != null) { this.LayerOrder(nextNode, queue); } } public static void main(String[] args) { BinaryTree biTree2L3L = new BinaryTree(); biTree2L3L.setData(\"D\"); BinaryTree biTree2L = new BinaryTree(); biTree2L.setData(\"B\"); biTree2L.setLchild(biTree2L3L); BinaryTree biTree2R = new BinaryTree(); biTree2R.setData(\"C\"); BinaryTree biTree = new BinaryTree(); biTree.setData(\"A\"); biTree.setLchild(biTree2L); biTree.setRchild(biTree2R); Queue queue = new LinkedBlockingDeque(); biTree.LayerOrder(biTree, queue); System.out.println(\"\"); biTree.preOrderTraverse(biTree); System.out.println(\"\"); biTree.inOrderTraverse(biTree); System.out.println(\"\"); biTree.postOrderTraverse(biTree); } } 面试中关于二叉树结构的常见问题 求二叉树的高度 在二叉树中查找给定节点的祖先节点 求二叉树的最低公共祖先LCA 性质:如果两条链有公共祖先,那么公共祖先往上的结点都重合.因为如果x=x',那么x->next=x'->next必然成立. 可能性一:若是二叉搜索树. 1如果x,y小于root,则在左边找 2如果x,y大于root,则在右边找 3如果x,y在root之间,则root就是LCA 可能性二:不是二叉搜索树,甚至不是二叉树,但是,每个一节点都有parent指针 那么解法有2: 1:空间换时间:从x,y到root的链表可以保存在栈中,找出最后一个相同结点即可. 2.不用空间换时间,多重扫描法,x,y到root两条链路可能一长一短,相差为n个结点,那么长链表先前移n步,然后,二者同步前移,找到第一个相同结点即可.(树不含环,这种办法有效.) 可能性三:这是一棵只有left和right的平凡二叉树. 那么需要辅助空间,空间换时间法,先调用16.中的GetNodePath()获得两条从root->x和root->y的链表路径.然后比较两条链表,找到最后一个相同的结点即可. //若是二叉搜索树,返回x和y的公共最低祖先LCA node* LowestCommonAncestor1(node* root,node* x,node* y) { if(!root || !x || !y) return NULL; if(x->value value && y->value value) return LowestCommonAncestor1(root->left,x,y); else if(x->value > root->value && y->value > root->value) return LowestCommonAncestor1(root->right,x,y); else return root; } //若不是搜索二叉树,但是,每个结点都有父结点,空间换时间法,否则需要重重复复地扫描路径 node* LowestCommonAncestor2(node* x,node* y) { stack st1,st2; while(x) { st1.push(x); x=x->p; } while(y) { st2.push(y); y=y->p; } node* pLCA=NULL; while(!st1.empty() && !st2.empty() && st1.top()==st2.top()) { pLCA=st1.top(); st1.pop(); st2.pop(); } return pLCA; } //不用空间换时间法 int GetListLength(node* x) { int Count=0; while(x) { ++Count; x=x->p; } return Count; } int Myabs(int val) { return val > 0 ? val : -val; } node* LowestCommonAncestor3(node* x,node* y) { int LengthX=GetListLength(x); int LengthY=GetListLength(y); node* pLong=x,*pShort=y; if(LengthX p; while( pLong && pShort && pLong !=pShort) { pLong=pLong->p; pShort=pShort->p; } if(pLong == pShort) return pLong; return NULL; } //既不是二叉搜索树,也不没有parent指针,只是一棵平凡的二叉树 bool GetNodePath(node* root, node* pNode, list& path); node* LowestCommonAncestor4(node* root,node* x,node* y) { list path1; list path2; GetNodePath(root,x,path1); GetNodePath(root,y,path2); node* pLCA=NULL; list::const_iterator it1=path1.begin(); list::const_iterator it2=path2.begin(); while(it1 != path1.end() && it2 != path2.end() && *it1 == * it2) { pLCA=*it1; ++it1; ++it2; } return pLCA; } 已知前序遍历序列和中序遍历序列，确定一棵二叉树。 已知后序遍历序列和中序遍历序列，确定一棵二叉树。 在二叉树中找出和为某一值的所有路径 要求所有路径,路径即root到某一结点的结点之集合.这是一个深度优先原则的搜索.我们很容易想到先序遍历. 为了跟踪路径和,我们需要一个额外的辅助栈来跟踪递归调用栈的操作过程. 在进入到下一个调用FindPath(left)和FindPath(right)时,递归栈会将root压入栈,因此我们也模仿进栈.当FindPath(left)和FindPath(right)返回,FindPath(root)运行周期到之后, 局部函数变量root会被析造,root会从递归栈中弹出,因此,我们也从辅助栈中弹出root,只需要在中间加上判断条件,将满足条件的结果输出即可.改成迭代版也很简单. void FindPath(node* root,int expectedSum,vector& Path,int currentSum);//先序遍历改装版 void FindPath(node* root,int expectedSum) { int currentSum=0; vector Path; FindPath(root,expectedSum,Path,currentSum); } void FindPath(node* root,int expectedSum,vector& Path,int currentSum)//先序遍历改装版 { if(!root) return ; //访问根结点,同时将root->value加入辅助栈 currentSum += root->value; Path.push_back(root->value); if(root->left==NULL && root->right ==NULL && currentSum == expectedSum) { for(vector::size_type i=0; i left,expectedSum,Path,currentSum); FindPath(root->right,expectedSum,Path,currentSum); //递归栈中,此时返回时,会将父结点销毁,因为,局部函数生命周期已经到了. //所以辅助栈也需要和递归栈同步,将栈顶元素弹栈,同时当前路径减去栈顶元素 Path.pop_back(); currentSum-=root->value; } 编写一个程序，把一个有序整数数组放到二叉树中 这道题做法非常多,单纯是这么要求比较奇怪,因此,我选择广度优先插入, 利用队列实现,其实插成一个链表,或者随便插入不知道可不可以?没有其它要求真不知道怎么弄. 判断整数序列是不是二叉搜索树的后序遍历结果 典型的递归思维,后序遍历,根在最后,因此用根将二叉搜索树可以分成左右子树,再递归处理左右子树即可. 求二叉树的镜像 画出一个特例,我们发现只需要交换每个结点的左右指针(注意:不是数值)即可.因此在先序遍历的时候换指针即可,没有顺序要求. 一棵排序二叉树（即二叉搜索树BST），令 f=(最大值+最小值)/2，设计一个算法，找出距离f值最近、大于f值的结点。复杂度应尽可能低。 BST中最大值是最右边的值,最小值是最左边的值,这样就容易求出f,再求f的父指针或者右指针都可以?(我是这么认为的).这里求父指针. 把二叉搜索树转变成排序的双向链表 其实就是中序遍历的迭代版本,只是将中间的访问结点代码换成了调整指针.这种办法返回的是链表的最后一个指针LastVist,因为是双链表,这也可以接受. 打印二叉树中的所有路径. 和前边的思路类似,用辅助栈记录递归栈的运行过程,先序遍历(深搜)的过程中,遇到叶子结点(!left && !right的结点)就输出辅助栈的内容. 求二叉树中从根到某一结点的一条路径. 思路一如既往还是利用辅助栈来追踪递归调用栈的运行过程,只是过程有所区别,将结点入栈,如果在结点的左边能找到一条路径,那么不需要和递归栈同步(即弹栈),直接返回true,如果左边没找到这样的一条路径,再到右边找,如果找到了,返回true.如果左右都找不到存在一条这样的路径,则说明在这个结点上不可能存在这样的路径,需要在辅助栈中弹出这个结点.再遍历其它结点.实质上也是先序遍历的改装版. 判断B子树是否是A子树的子结构 利用先序和中序结果重建二叉树 求二叉树中叶子结点的个数 求二叉树中节点的最大距离。 如果我们把二叉树看成一个图，父子节点之间的连线是双向的，我们定义距离为两个节点之间边的个数。（来自编程之美） 特点：相距最远的两个节点，一定是两个叶子节点，或者一个结点到它的根节点。（为什么？）因为如果当前结点不是叶子结点，即它还有子结点，那么它的子结点到另一个端点的距离肯定可以达到更远。如果是根结点，那么根结点只可能有一条支路。 打印二叉树中某层的结点。 判断两棵二叉树是否相等 "},"Chapter02/BinarySearchTree.html":{"url":"Chapter02/BinarySearchTree.html","title":"二叉查找树","keywords":"","body":"二叉搜索树 二叉查找树（Binary Search Tree），（又：二叉搜索树，二叉排序树） 所谓二叉搜索树（Binary Search Tree），又叫二叉排序树， 简单而言就是左子树上所有节点的值均小于根节点的值， 而右子树上所有结点的值均大于根节点的值， 左小右大，并不是乱序，因此得名二叉排序树。 一个新事物不能凭空产生，那二叉搜索树又有什么用呢？ 有了二叉搜索树，当你要查找一个值， 就不需要遍历整个序列或者说遍历整棵树了， 可以根据当前遍历到的结点的值来确定搜索方向， 这就好比你要去日本，假设你没有见过世界地图， 你不知道该往哪个方向走， 只能满地球找一遍才能保证一定能够到达日本； 而如果你见过世界地图，你知道日本在中国的东边， 你就不会往西走、往南走、往北走。 这种思维在搜索中被叫做“剪枝”， 把不必要的分枝剪掉可以提高搜索效率。 在二叉搜索树中查找值，每次都会把搜索范围缩小， 与二分搜索的思维类似。 创建二叉搜索树 现有序列：A = {61, 87, 59, 47, 35, 73, 51, 98, 37, 93}。根据此序列构造二叉搜索树过程如下： i = 0，A[0] = 61，节点61作为根节点； i = 1，A[1] = 87，87 > 61，且节点61右孩子为空，故81为61节点的右孩子； i = 2，A[2] = 59，59 i = 3，A[3] = 47，47 i = 4，A[4] = 35，35 i = 5，A[5] = 73，73 i = 6，A[6] = 51，47 i = 7，A[7] = 98，98 i = 8，A[8] = 93，93 创建完毕后如下图中的二叉搜索树： 构造复杂度 二叉搜索树的构造过程，也就是将节点不断插入到树中适当位置的过程。该操作过程，与查询节点元素的操作基本相同，不同之处在于： 查询节点过程是，比较元素值是否相等，相等则返回，不相等则判断大小情况，迭代查询左、右子树，直到找到相等的元素，或子节点为空，返回节点不存在 插入节点的过程是，比较元素值是否相等，相等则返回，表示已存在，不相等则判断大小情况，迭代查询左、右子树，直到找到相等的元素，或子节点为空，则将节点插入该空节点位置。 由此可知，单个节点的构造复杂度和查询复杂度相同，为 ~。 使用二叉搜索树可以提高查找效率，其平均时间复杂度为O(log2n)。 二叉搜索树的两种极端情况： 完全二叉树，所有节点尽量填满树的每一层，上一层填满后还有剩余节点的话，则由左向右尽量填满下一层 每一层只有一个节点的二叉树。如下图所示： 插入 插入流程： 先检测该元素是否在树中已经存在。如果已经存在，则不进行插入； 若元素不存在，则进行查找过程，并将元素插入在查找结束的位置。 图解过程 删除复杂度 二叉搜索树的节点删除包括两个过程，查找和删除。查询的过程和查询复杂度已知， 这里说明一下删除节点的过程。 节点的删除有以下三种情况： 待删除节点度为零； 待删除节点度为一； 待删除节点度为二。 第一种情况如下图 s_1 所示，待删除节点值为 “6”， 该节点无子树，删除后并不影响二叉搜索树的结构特性， 可以直接删除。即二叉搜索树中待删除节点度为零时， 该节点为叶子节点，可以直接删除； 第二种情况如下图 s_2 所示，待删除节点值为 “7”，该节点有一个左子树，删除节点后，为了维持二叉搜索树结构特性，需要将左子树“上移”到删除的节点位置上。即二叉搜索树中待删除的节点度为一时，可以将待删除节点的左子树或右子树“上移”到删除节点位置上，以此来满足二叉搜索树的结构特性。 第三种情况如下图 s_3 所示，待删除节点值为 “9”，该节点既有左子树，也有右子树，删除节点后，为了维持二叉搜索树的结构特性，需要从其左子树中选出一个最大值的节点，“上移”到删除的节点位置上。即二叉搜索树中待删除节点的度为二时，可以将待删除节点的左子树中的最大值节点“移动”到删除节点位置上，以此来满足二叉搜索树的结构特性。 其实在真实的实现代码中，该情况下的实际节点删除操作是： 1.查找出左子树中的最大值节点 2.替换待删除节点 的值为 的值 3.删除 节点 因为 作为左子树的最大值节点，所以节点的度一定是 0 或 1，所以删除节点的情况就转移为以上两种情况。 之前提到二叉搜索树中节点的删除操作，包括查询和删除两个过程，这里称删除节点后，维持二叉搜索树结构特性的操作为“稳定结构”操作，观察以上三种情况可知： 前两种情况下，删除节点后，“稳定结构”操作的复杂度都是常数级别，即整个的节点删除操作复杂度为 ~； 第三种情况下，设删除的节点为 ，“稳定结构”操作需要查找 节点左子树中的最大值，也就是左子树中最“右”的叶子结点，即“稳定结构”操作其实也是一种内部的查询操作，所以整个的节点删除操作其实就是两个层次的查询操作，复杂度同为 ~； 性能分析 由以上查询复杂度、构造复杂度和删除复杂度的分析可知，三种操作的时间复杂度皆为 ~。下面分析线性结构的三种操作复杂度，以二分法为例： 查询复杂度，时间复杂度为 ，优于二叉搜索树； 元素的插入操作包括两个步骤，查询和插入。查询的复杂度已知，插入后调整元素位置的复杂度为 ，即单个元素的构造复杂度为： 删除操作也包括两个步骤，查询和删除，查询的复杂度已知，删除后调整元素位置的复杂度为 ，即单个元素的删除复杂度为： 由此可知，二叉搜索树相对于线性结构，在构造复杂度和删除复杂度方面占优；在查询复杂度方面，二叉搜索树可能存在类似于斜树，每层上只有一个节点的情况，该情况下查询复杂度不占优势。 总结 二叉搜索树的节点查询、构造和删除性能，与树的高度相关，如果二叉搜索树能够更“平衡”一些，避免了树结构向线性结构的倾斜，则能够显著降低时间复杂度。二叉搜索树的存储方面，相对于线性结构只需要保存元素值，树中节点需要额外的空间保存节点之间的父子关系，所以在存储消耗上要高于线性结构。 package datastructure.tree; import lombok.Setter; import java.util.Queue; import java.util.concurrent.LinkedBlockingDeque; /** * 二叉排序树：二叉排序树（Binary Sort Tree），又称二叉查找树（Binary Search Tree），也称二叉搜索树。 * * @author liuyi27 * * * 二叉排序树或者是一棵空树，或者是具有下列性质的二叉树: * * * （1）若左子树不空，则左子树上所有结点的值均小于或等于它的根结点的值； * * * （2）若右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值； * * * （3）左、右子树也分别为二叉排序树； * * */ @Setter public class BinarySortTree { private int data; private BinarySortTree lchild, rchild; /** * 递归查找二叉排序树T中是否存在key 指针f指向T的双亲，其初始调用值为NULL 若查找成功，* 则指针p指向该数据元素结点，并返回TRUE * 否则指针p指向查找路径上访问的最后一个结点并返回FALSE */ // 如果树是空的，则查找结束，无匹配。 // 如果被查找的值和根结点的值相等，查找成功。否则就在子树中继续查找。如果被查找的值小于根结点的值就选择左子树，大于根结点的值就选择右子树。 public static boolean search(BinarySortTree tree, int key) { if (tree == null) { return false; } if (tree.data == key) { return true; } System.out.println(tree.data); if (key \"); /* 在右子树中继续查找 */ return search(tree.rchild, key); } } /** * 若查找的key已经有在树中，则p指向该数据结点。 若查找的key没有在树中，则p指向查找路径上最后一个结点。 * * @param tree * @param key * @return */ public static BinarySortTree add(BinarySortTree tree, int key) { if (tree == null) { tree = new BinarySortTree(); tree.data = key; return tree; } if (key key) { return remove(tree.lchild, key); } else { return remove(tree.rchild, key); } } /** * 1）删除结点为叶子结点； 2）删除的结点只有左子树； 3）删除的结点只有右子树 4）删除的结点既有左子树又有右子树。 * * @param tree * @return */ private static boolean delete(BinarySortTree tree) { if (tree.rchild == null) { // 右子树空则只需重接它的左子树（待删结点是叶子也走此分支) tree = tree.lchild; } else if (tree.lchild == null) { // 只需重接它的右子树 tree = tree.rchild; } else { // 左右子树均不空 // 转左 BinarySortTree tempP = null; BinarySortTree temp = tree.lchild; // 然后向右到尽头（找待删结点的前驱） while (temp.rchild != null) { tempP = temp; temp = temp.rchild; } // TODO 感觉非常有问题 if (tempP != null) { tempP.rchild = null; }else { temp.lchild = null; } // 将被删结点前驱的值取代被删结点的值(指向被删结点的直接前驱) tree.data = temp.data; } return true; } public static void layerOrder(BinarySortTree node, Queue queue) { if (node != null && queue.isEmpty()) { // 将当前节点放入队列首指针所指位置 queue.add(node); System.out.print(queue.poll().data + \" \"); } else { System.out.print(node.data + \" \"); } if (node.lchild != null) { queue.add(node.lchild); } if (node.rchild != null) { queue.add(node.rchild); } BinarySortTree nextNode = queue.poll(); if (nextNode != null) { layerOrder(nextNode, queue); } } public static void main(String[] args) { int[] array = { 10, 4, 6, 34, 32, 5, 2, 1, 11, 23 }; BinarySortTree tree = null; tree = create(tree, array); Queue queue = new LinkedBlockingDeque(); layerOrder(tree, queue); System.out.println(\"查找1\"); // System.out.println(search(tree, 10)); // System.out.println(search(tree, 4)); // System.out.println(search(tree, 6)); // System.out.println(search(tree, 34)); // System.out.println(search(tree, 32)); // System.out.println(search(tree, 5)); // System.out.println(search(tree, 2)); // System.out.println(search(tree, 11)); // System.out.println(search(tree, 23)); System.out.println(\"查找1\"); boolean result = remove(tree, 4); System.out.println(\"\"); System.out.println(result); layerOrder(tree, queue); System.out.println(\"\"); System.out.println(\"查找2\"); // System.out.println(search(tree, 10)); // System.out.println(search(tree, 4)); // System.out.println(search(tree, 34)); // System.out.println(search(tree, 32)); // System.out.println(search(tree, 2)); // System.out.println(search(tree, 11)); // System.out.println(search(tree, 23)); System.out.println(search(tree, 5)); System.out.println(search(tree, 6)); } } import java.util.Random; /** * 二叉搜索树 * @author 爱学习的程序员 * @version V1.0 */ public class BST{ // 根结点 public static Node root = null; /** * 二叉搜索树的结点类 */ public static class Node{ // 父结点 Node p; // 左孩子 Node left; // 右孩子 Node right; // 关键字 int key; public Node(Node p, Node left, Node right, int key){ this.p = p; this.left = left; this.right = right; this.key = key; } } /** * 插入结点 * @param z 待插入结点 * @return 根结点 */ public static void insert(Node z){ // 树为空，直接作为根结点 if(root == null) root = z; else{ Node y = null; Node x = root; // 寻求树中结点z的合适位置 while(x != null){ y = x; if(z.key "},"Chapter02/BalanceTree.html":{"url":"Chapter02/BalanceTree.html","title":"平衡树","keywords":"","body":"平衡树 平衡树(Balance Tree，BT) 指的是，任意节点的子树的高度差都小于等于1。常见的符合平衡树的有，B树（多路平衡搜索树）、AVL树（二叉平衡搜索树）等。 平衡树可以完成集合的一系列操作, 时间复杂度和空间复杂度相对于“2-3树”要低，在完成集合的一系列操作中始终保持平衡，为大型数据库的组织、索引提供了一条新的途径。 "},"Chapter02/AVLTree.html":{"url":"Chapter02/AVLTree.html","title":"平衡二叉树","keywords":"","body":"平衡二叉树 平衡二叉树，又称AVL树，指的是左子树上的所有节点的值都比根节点的值小， 而右子树上的所有节点的值都比根节点的值大，且左子树与右子树的高度差最大为1。 因此，平衡二叉树满足所有二叉排序（搜索）树的性质。 至于AVL，则是取自两个发明平衡二叉树的科学家的名字：G. M. Adelson-Velsky和E. M. Landis。 有了二叉排序树就可以使插入、搜索效率大大提高了，为什么还要引入平衡二叉树？ 二叉搜索树的结构与值的插入顺序有关，同一组数，若其元素的插入顺序不同，二叉搜索树的结构是千差万别的。举个例子，给出一组数[1,3,5,8,9,13]。 若按照[5,1,3,9,13,8]这样的顺序插入，其流程是这样的： 若按照[1,3,5,8,9,13]这样的顺序插入，其流程是这样的： 依据此序列构造的二叉搜索树为右斜树，同时二叉树退化成单链表，搜索效率降低为O(n)。 为了避免二叉搜索树变成“链表”，我们引入了平衡二叉树，即让树的结构看起来尽量“均匀”，左右子树的节点数尽量一样多。 生成平衡二叉树 那给定插入序列，如何生成一棵平衡二叉树呢？ 先按照生成二叉搜索树的方法构造二叉树，直至二叉树变得不平衡， 即出现这样的节点：左子树与右子树的高度差大于1。 至于如何调整，要看插入的导致二叉树不平衡的节点的位置。 主要有四种调整方式：LL（左旋）、RR（右旋）、LR（先左旋再右旋）、RL（先右旋再左旋）。 平衡因子 某节点的左子树与右子树的高度(深度)差即为该节点的平衡因子（BF,Balance Factor）， 平衡二叉树中不存在平衡因子大于1的节点。 在一棵平衡二叉树中，节点的平衡因子只能取-1、1或者0。 对于给定结点数为n的AVL树，最大高度为O(log2n). 定义节点 private class Node{ E data; Node lchild; Node rchild; Node(E element){ this.data = element; } } 左旋 如下图所示的平衡二叉树 如在此平衡二叉树插入节点62，树结构变为： 可以得出40节点的左子树高度为1，右子树高度为3，此时平衡因子为-2，树失去平衡。 为保证树的平衡，此时需要对节点40做出旋转，因为右子树高度高于左子树，对节点进行左旋操作，流程如下： 节点的右孩子替代此节点位置 右孩子的左子树变为该节点的右子树 节点本身变为右孩子的左子树 图解过程： 然而更多时候根节点并不是只有一个子树，下图为复杂的LL（左旋，插入13导致值为4的节点不平衡）： 红色节点为插入后不平衡的节点， 黄色部分为需要改变父节点的分支，左旋后， 原红色节点的右孩子节点变成了根节点， 红色节点变成了它的左孩子， 而它原本的左孩子（黄色部分）不能丢， 而此时红色节点的右孩子是空的， 于是就把黄色部分放到了红色节点的右孩子的位置上。 调整后该二叉树还是一棵二叉排序（搜索）树， 因为黄色部分的值大于原来的根节点的值， 而小于后来的根节点的值，调整后， 黄色部分还是位于原来的根节点（红色节点）和后来的根节点之间。 /** * 左旋 * * @param e * @return 返回的是左旋后的根节点，左旋后的根节点是原来根节点的右孩子，左旋后的根节点的左孩子需要嫁接到原来根节点的右孩子上，原来的根节点嫁接到左旋后根节点的左孩子上。temp对应上图中值为8的节点，root对应上图中值为4的节点。 */ private Node leftRotate(Node e){ Node temp = e.rchild; e.rchild = temp.lchild; temp.lchild = e; return temp; } 右旋 右旋操作与左旋类似，操作流程为： 节点的左孩子代表此节点 节点的左孩子的右子树变为节点的左子树 将此节点作为左孩子节点的右子树。 图解过程： 然而更多时候根节点并不是只有一个子树，下图为复杂的RR（右旋，插入1导致值为9的节点不平衡）： 红色节点为插入后不平衡的节点，黄色部分为需要改变父节点的分支，右旋后，原红色节点的左孩子节点变成了根节点，红色节点变成了它的右孩子， 而它原本的右孩子（黄色部分）不能丢，而此时红色节点的左孩子是空的，于是就把黄色部分放到了红色节点的左孩子的位置上。调整后该二叉树还是一棵二叉排序（搜索）树，因为黄色部分的值小于原来的根节点的值，而大于后来的根节点的值，调整后，黄色部分还是位于后来的根节点和原来的根节点（红色节点）之间。 右旋代码如下： /** * 右旋 返回的是右旋后的根节点，右旋后的根节点是原来根节点的左孩子，右旋后的根节点的右孩子需要嫁接到原来根节点的左孩子上，原来的根节点嫁接到右旋后根节点的右孩子上。temp对应上图中值为5的节点，root对应上图中值为9的节点。 * @param e * @return */ private Node rightRotate(Node e){ Node temp = e.lchild; e.lchild = temp.rchild; temp.rchild = e; return temp; } 先左旋再右旋 所谓LR（先左旋再右旋）就是先将左子树左旋，再整体右旋， 下图为最简洁的LR旋转（插入2导致值为3的节点不平衡）： 然而更多时候根节点并不是只有一个子树， 下图为复杂的LR旋转（插入8导致值为9的节点不平衡）： 先将红色节点的左子树左旋，红色节点的左子树的根原本是值为4的节点， 左旋后变为值为6的节点，原来的根节点变成了左旋后根节点的左孩子， 左旋后根节点原本的左孩子（蓝色节点）变成了原来的根节点的右孩子； 再整体右旋，原来的根节点（红色节点）变成了右旋后的根节点的右孩子， 右旋后的根节点原本的右孩子（黄色节点）变成了原来的根节点（红色节点）的左孩子。旋转完成后， 仍然是一棵二叉排序（搜索）树。 LR旋转代码如下 /** * 先左旋再右旋 * @param element * @return 返回的是LR旋转后的根节点，先对根节点的左子树左旋，再整体右旋。root对应上图中值为9的节点。 */ private Node leftRightRotate(Node element){ //先对element的左子树左旋 element.lchild = this.leftRotate(element.lchild); //再对element右旋 return rightRotate(element); } 先右旋再左旋 所谓RL（先右旋再左旋）就是先将右子树右旋，再整体左旋，下图为最简洁的RL旋转（插入2导致值为1的节点不平衡）： 然而更多时候根节点并不是只有一个子树，下图为复杂的RL旋转（插入8导致值为4的节点不平衡）： 先将红色节点的右子树右旋，红色节点的右子树的根原本是值为9的节点，右旋后变为值为6的节点， 原来的根节点变成了右旋后根节点的右孩子， 右旋后根节点原本的右孩子（蓝色节点）变成了原来的根节点的左孩子； 再整体左旋，原来的根节点（红色节点）变成了左旋后的根节点的左孩子， 左旋后的根节点原本的左孩子（黄色节点）变成了原来的根节点（红色节点）的右孩子。 旋转完成后，仍然是一棵二叉排序（搜索）树。 RL旋转代码如下： /** * * @param element * @return 返回的是RL旋转后的根节点，先对根节点的右子树右旋，再整体左旋。root对应上图中值为4的节点。 */ private Node rightLeftRotate(Node element){ //先对element的左子树左旋 element.rchild = this.rightRotate(element.rchild); //再对element右旋 return leftRotate(element); } 插入 假设一颗 AVL 树的某个节点为A，有四种操作会使 A 的左右子树高度差大于 1， 从而破坏了原有 AVL 树的平衡性。平衡二叉树插入节点的情况分为以下四种： A的左孩子的左子树插入节点(LL) 出现不平衡时到底是执行LL、RR、LR、RL中的哪一种旋转，取决于插入的位置。 可以根据值的大小关系来判断插入的位置。插入到不平衡节点的右子树的右子树上， 自然是要执行LL旋转；插入到不平衡节点的左子树的左子树上，自然是要执行RR旋转； 插入到不平衡节点的左子树的右子树上，自然是要执行LR旋转；插入到不平衡节点的右子树的左子树上， 自然是要执行RL旋转。 若向平衡二叉树中插入一个新结点后破坏了平衡二叉树的平衡性。 首先要找出插入新结点后失去平衡的最小子树根结点的指针。 然后再调整这个子树中有关结点之间的 链接关系，使之成为新的平衡子树。 当失去平衡的最小子树被调整为平衡子树后，原有其他所有不平衡子树无需调整， 整个二叉排序树就又成为一棵平衡二叉树。 失去平衡的最小子树是指以离插入结点最近，且平衡因子绝对值大于1的结点作为根的子树。 假设用A表示失去平衡的最小子树的根结点，则调整该子树的操作可归纳为下列四种情况。 LL型平衡旋转法 由于在A的左孩子B的左子树上插入结点F，使A的平衡因子由1增至2而失去平衡。故需进行一次顺时针旋转操作。 即将A的左孩子B向右上旋转代替A作为根结点，A向右下旋转成为B的右子树的根结点。而原来B的右子树则变成A的左子树。 RR型平衡旋转法 由于在A的右孩子C 的右子树上插入结点F，使A的平衡因子由-1减至-2而失去平衡。故需进行一次逆时针旋转操作。即将A的右孩子C向左上旋转代替A作为根结点，A向左下旋转成为C的左子树的根结点。而原来C的左子树则变成A的右子树。 LR型平衡旋转法 由于在A的左孩子B的右子数上插入结点F，使A的平衡因子由1增至2而失去平衡。故需进行两次旋转操作（先逆时针，后顺时针）。即先将A结点的左孩子B的右子树的根结点D向左上旋转提升到B结点的位置，然后再把该D结点向右上旋转提升到A结点的位置。即先使之成为LL型，再按LL型处理。 RL型平衡旋转法 由于在A的右孩子C的左子树上插入结点F，使A的平衡因子由-1减至-2而失去平衡。故需进行两次旋转操作（先顺时针，后逆时针），即先将A结点的右孩子C的左子树的根结点D向右上旋转提升到C结点的位置，然后再把该D结点向左上旋转提升到A结点的位置。即先使之成为RR型，再按RR型处理。 public void add(E element) { Node newNode = new Node<>(element); root = this.insert(root, newNode); } private Node insert(Node origin, Node newNode) { if (origin == null) { origin = newNode; } else if (origin.data.compareTo(newNode.data) adjustment(Node origin, Node newNode) { if (origin.lchild != null && origin.lchild.data.compareTo(newNode.data) > 0 ){ //新节点小于不平衡节点的左节点则是插入了左子树的左子树 //RR型 return rightRotate(origin); } if (origin.rchild.data.compareTo(newNode.data) 0){ //新节点小于不平衡节点的右节点则是插入了右子树的左子树 //RL型 return rightRotate(origin); } return origin; } private boolean isBalance(Node origin) { return isBalancedTreeHelper(origin).balanced; } private TreeInfo isBalancedTreeHelper(Node origin) { if (origin == null){ return new TreeInfo(-1, true); } // Check subtrees to see if they are balanced. TreeInfo left = isBalancedTreeHelper(origin.lchild); if (!left.balanced) { return new TreeInfo(-1, false); } TreeInfo right = isBalancedTreeHelper(origin.rchild); if (!right.balanced) { return new TreeInfo(-1, false); } // Use the height obtained from the recursive calls to // determine if the current node is also balanced. if (Math.abs(left.height - right.height) 删除 比如要下从下图中删除节点20 首先找到替换20被删除的节点15，并将二者内容替换，如下图所示： 然后删除节点20得到下图： 在删除节点20后，节点10违反了平衡二叉树的性质，对以10为根节点的子树进行调整（类似于插入时，需要先做一次左旋再做一次右旋）可得下图： 另一种情形如下图所示： 另一种情形如下图所示： 另一种情形如下图所示： 如何判断一棵二叉树是否是平衡二叉树. 根据定义，一棵二叉树 T 存在节点 p∈T，满足 height(p.left)−height(p.right)>1 时，它是不平衡的。 下图中每个节点的高度都被标记出来，高亮区域是一棵不平衡子树。 平衡子树暗示了一个事实，每棵子树也是一个子问题。 现在的问题是：按照什么顺序处理这些子问题？ 方法一：自顶向下的递归 定义方法 height，用于计算任意一个节点 p∈T 的高度： 接下来就是比较每个节点左右子树的高度。 在一棵以 rr 为根节点的树 TT 中，只有每个节点左右子树高度差不大于 1 时，该树才是平衡的。因此可以比较每个节点左右两棵子树的高度差，然后向上递归。 class Solution { // Recursively obtain the height of a tree. An empty tree has -1 height private int height(TreeNode root) { // An empty tree has height -1 if (root == null) { return -1; } return 1 + Math.max(height(root.left), height(root.right)); } public boolean isBalanced(TreeNode root) { // An empty tree satisfies the definition of a balanced tree if (root == null) { return true; } // Check if subtrees have height within 1. If they do, check if the // subtrees are balanced return Math.abs(height(root.left) - height(root.right)) 复杂度分析 时间复杂度：O(nlogn)。 对于每个深度为 d 的节点 p，height(p) 被调用 p 次。 首先，需要知道一棵平衡二叉树可以拥有的节点数量。令 f(h) 表示一棵高度为 h 的平衡二叉树需要的最少节点数量。 方法二：自底向上的递归 方法一计算 height 存在大量冗余。 每次调用 height 时，要同时计算其子树高度。 但是自底向上计算，每个子树的高度只会计算一次。 可以递归先计算当前节点的子节点高度， 然后再通过子节点高度判断当前节点是否平衡，从而消除冗余。 算法 使用与方法一中定义的 height 方法。 自底向上与自顶向下的逻辑相反，首先判断子树是否平衡， 然后比较子树高度判断父节点是否平衡。算法如下： // Utility class to store information from recursive calls final class TreeInfo { public final int height; public final boolean balanced; public TreeInfo(int height, boolean balanced) { this.height = height; this.balanced = balanced; } } class Solution { // Return whether or not the tree at root is balanced while also storing // the tree's height in a reference variable. private TreeInfo isBalancedTreeHelper(TreeNode root) { // An empty tree is balanced and has height = -1 if (root == null) { return new TreeInfo(-1, true); } // Check subtrees to see if they are balanced. TreeInfo left = isBalancedTreeHelper(root.left); if (!left.balanced) { return new TreeInfo(-1, false); } TreeInfo right = isBalancedTreeHelper(root.right); if (!right.balanced) { return new TreeInfo(-1, false); } // Use the height obtained from the recursive calls to // determine if the current node is also balanced. if (Math.abs(left.height - right.height) 复杂度分析 时间复杂度：O(n)，计算每棵子树的高度和判断平衡操作都在恒定时间内完成。 空间复杂度：O(n)，如果树不平衡，递归栈可能达到 O(n)。 https://leetcode-cn.com/problems/balanced-binary-tree/ 拓展：判断一棵树是否平衡 实现一个函数检查一棵树是否平衡。对于这个问题而言， 平衡指的是这棵树任意两个叶子结点到根结点的距离之差不大于1。 对于这道题，要审清题意。它并不是让你判断一棵树是否为平衡二叉树。 平衡二叉树的定义为：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 而本题的平衡指的是这棵树任意两个叶子结点到根结点的距离之差不大于1。 这两个概念是不一样的。例如下图， 它是一棵平衡二叉树，但不满足本题的平衡条件。 (叶子结点f和l到根结点的距离之差等于2，不满足题目条件) 对于本题，只需要求出离根结点最近和最远的叶子结点， 然后看它们到根结点的距离之差是否大于1即可。 假设只考虑二叉树，我们可以通过遍历一遍二叉树求出每个叶子结点到根结点的距离。 使用中序遍历，依次求出从左到右的叶子结点到根结点的距离，递归实现。 /*判断树是否平衡，并不是判断是否是平衡二叉树*/ int Max=INT_MIN,Min=INT_MAX,curLen=0; void FindDepth(node* root) { if( root==NULL) { return ; } ++curLen; FindDepth(root->left); if( root->left ==NULL && root->right ==NULL) { if( curLen > Max ) Max=curLen; else if( curLen right); --curLen; } bool isBalance(node* root) { FindDepth(root); return ((Max-Min) "},"Chapter02/MultipathSearchTree.html":{"url":"Chapter02/MultipathSearchTree.html","title":"多路查找树","keywords":"","body":"多路查找树(N叉树) 树家族是为了实现方便快捷的查找而存在的。 树的高度是命中查找的一个不可抗拒的时间下限。 在一定的数据条件下，树的高度和宽度是互相制约的。 （就像一定面积下，矩形的长和宽是互相制约的）而树家族中最简单的二叉树，尽管易于实现，却不能有实际的价值。 其最最令人发指的是二叉树的高度太高。 n叉树的提出和实现解决了二叉树的不足， 典型的多路查找树有：2-3-4树、红黑树和B树。 定义 二叉树中每个结点有一个数据项,最多有两个子节点,如果允许树的每个节点可以有两个以上的子节点, 那么这个树就称为n阶的多叉树，或者称为n叉树。 性质 每个节点有m个子节点和m-1个键值。 每个节点中的键值按升序排列。 前i个子节点中的键值都小于第i个键值。 后m-1个子节点中的键值都大于第i个键值。 "},"Chapter02/2-3Tree.html":{"url":"Chapter02/2-3Tree.html","title":"2-3树","keywords":"","body":"2-3 树 前面讲到了二叉搜索树(BST)和二叉平衡树(AVL)，二叉搜索树在最好的情况下搜索的时间复杂度为O(logn)，但如果插入节点时，插入元素序列本身就是有序的，那么BST树就退化成一个线性表了，搜索的时间复杂度为O(n)。 如果想要减少比较次数，就需要降低树的高度。在插入和删除节点时，要保证插入节点后不能使叶子节点之间的深度之差大于1，这样就能保证整棵树的深度最小，这就是AVL树解决BST搜索性能降低的策略。但由于每次插入或删除节点后，都可能会破坏AVL的平衡，而要动态保证AVL的平衡需要很多操作，这些操作会影响整个数据结构的性能，除非是在树的结构变化特别少的情形下，否则AVL树平衡带来的搜索性能提升有可能还不足为了平衡树所带来的性能损耗。 因此，引入了2-3树来提升效率。2-3树本质也是一种平衡搜索树，但2-3树已经不是一棵二叉树了，因为2-3树允许存在3-这种节点，3-节点中可以存放两个元素，并且可以有三个子节点。 定义 对于2-节点，和普通的BST节点一样，有一个数据域和两个子节点指针，两个子节点要么为空，要么也是一个2-3树，当前节点的数据的值要大于左子树中所有节点的数据，要小于右子树中所有节点的数据。 对于3-节点，有两个数据域a和b和和三个子节点指针，左子树中所有的节点数据要小于a，中子树中所有节点数据要大于a而小于b，右子树中所有节点数据要大于b。 下图所示的树为一棵2-3树，树中共有5个2-节点和3个3-节点。 性质 对于每一个结点有1或者2个关键码。 当节点有1个关键码的时，节点有2个子树。 当节点有2个关键码时，节点有3个子树。 所有叶子点都在树的同一层。 查找 2-3树的查找类似二叉搜索树的查找过程，根据键值的比较来决定查找的方向。 例如：在下图所示的2-3树中查找键为H的节点： 例如：在下图所示的2-3树中查找键为B的节点： 节点分裂与合并 在将插入之前，先介绍一下节点分裂与合并。 2-3树只能存在2节点和3节点，由于插入的时候会引入4节点，所以我们需要将其分裂。 节点分裂 比如单个4节点，只需将中间节点往上提，左边值作为其左子树，右边值作为其右子树即可。 节点合并 比如有父节点的4节点，节点分裂后，需与父节点进行合并。若合并后父节点还是4节点，则继续分裂，直至满足定义为止。下图中6与3合并后，满足条件，无需再进行操作。 插入 插入一个节点后，也要满足2-3树的定义。 我们需要找到一个适合的位置来插入新的值，但是和二叉树不同的是， 它不会生成新的叶子节点来存储，而是找到合适的叶子节点来进行合并。 但是注意，插入的原则是尽量保持树的高度，也就是尽量不要增加树的高度。 因为树的高度越小，查找效率会更高。 下面分几种情况来说明不同的处理情况。其关键字是往上分裂，从下往上生长。 空树 生成新节点，则其为根节点。 待插入节点为2节点 如果不能直接放到空的子节点，则放到父节点中，此时成为3节点，仍然满足定义。比如我们在这棵树中插入12。 首先找到待插入节点9。 节点9为2节点，可直接插入。 待插入节点为3节点 这种情况下，稍微会复杂一些，因为涉及到分裂，且跟待插入节点的父节点有关。 假定待插入节点为p，待插入节点的父节点为pp。 将节点强插到p中，此时p中会有三个值，我们暂且称之为4节点。 4节点是不满足2-3树的定义的，因此需要将4节点中的某个节点往上抽离， 与pp进行合并。这时需要考虑pp的类型了。 若pp为二节点 将分裂的节点放到pp中，则pp成为3节点，满足定义。比如我们在这棵树中插入13。 找到待插入叶子节点[9,12] 插入13，变成4节点 进行分裂，合并到父节点，插入完成 若pp为三节点 将分裂的节点放到pp中，则pp成为了4节点，不满足定义，那么4-节点需要提出一个值，并向上合并，这时需要重新设置新旧节点的关系。往上合并的过程就是继续套用这几种情况。好的情况是往上的过程中遇到了2节点，且平衡，则结束；坏的情况是一直到根节点，并且根节点是3树，那么只好继续往上分裂出新的根节点，然后处理新节点与其他节点的关系，此时树高增加了1。 比如在这颗树中插入18。 插入18，找到叶子节点插入，成为4节点 向上分裂，将18插入父节点，变成4节点，需继续分裂 根节点成为3节点，插入结束 以上的插入，树的高度都没有变化。下面说一种树的高度会+1的情况。 比如在这棵树中插入32。 找到待插入的节点直接插入32 分裂节点，此时父节点变成4节点，还需分裂 此时根节点为4节点，需分裂 生成新的根节点，树的高度加1 删除 删除的情况会复杂一些，下面分几种情况来说。 待删除的值在叶子节点 叶子节点为3节点 直接删除即可。如下图12可直接删除。 删除后，3节点成2节点。 叶子节点为2节点 这里需要区分临近兄弟节点的类型。先将节点删除。 兄弟节点为3节点 此时被删除后，节点会为空。通过向兄弟节点借一个最近的键值，然后再调整该节点与父节点的值。 比如在这颗树中删除7。 向兄弟节点借最近节点，此时大小关系不满足 调整大小，8，9交换，满足定义 兄弟节点为2节点，这时需要判断父节点类型 父节点为3节点，此时兄弟节点不够借，父节点降元，从3节点变成2节点，与兄弟节点合并。 比如从这棵树中删除36。 30与18合并，3节点变成2节点，删除完成 父节点为2节点，将父节点和兄弟节点合并，形成新的节点，这是把新节点当做当前节点，不断套用上述几种情况进行调整，直至平衡。 这种情况下，若根节点是2节点，树的高度会减1。 从这颗树中删除12 兄弟节点和父节点都为2节点，进行合并。此时新节点为[8,9]。 当前节点([8,9])的父节点和兄弟节点都为2节点，还需进行合并（即节点2,5合并）合并完成如下图。 从这棵树中删除30 节点30的父节点和兄弟节点为2节点，进行合并，此时[22,25]为新节点。 把[22,25]看作当前节点，由于其兄弟节点为2节点， 父节点为3节点，父节点降元，调整完成。 待删除的值在父节点 将该节点与其前驱或后继节点交换，然后删除交换后的叶子节点，此时转换成上一种情况的处理。 使用中序遍历的顺序，前驱就是指其前一个节点，后继是指其后面的一个节点。最直接的定位如下： 前驱节点：该节点左子树最右节点 后继节点：该节点右子树最左节点 比如下图，5的前驱是3，后继是7。 那为什么要用前驱/后继节点交换呢？ 因为，用前驱/后继节点交换后，才能保持大小顺序。 后继节点是右子树中最小的节点，与父节点交换后，排除待删除的叶节点， 仍保持左子树 这里我们使用后继节点来进行替换。 从这颗树中删除节点5。 由于5的后继节点是7，先将值进行交换。 这时候目的就是删除叶子节点5，于是可以转换成其父节点和兄弟节点都为2节点的情况进行调整。 总结 先找插入结点，若结点有空(即2-结点)，则直接插入。如结点没空(即3-结点)， 则插入使其临时容纳这个元素，然后分裂此结点，把中间元素移到其父结点中。对父结点亦如此处理。 （中键一直往上移，直到找到空位，在此过程中没有空位就先搞个临时的，再分裂。） 2-3树插入算法的根本在于这些变换都是局部的： 除了相关的结点和链接之外不必修改或者检查树的其他部分。每次变换中， 变更的链接数量不会超过一个很小的常数。所有局部变换都不会影响整棵树的有序性和平衡性。 同时，通过上面树的深度增加的例子，可以看出2-3树和标准二叉树不同， 标准的二叉树的的深度是由上到下的增加的，而2-3树的深度生长是由下至上的。 "},"Chapter02/2-3-4Tree.html":{"url":"Chapter02/2-3-4Tree.html","title":"2-3-4树","keywords":"","body":"2-3—4 树  在上一篇文章中介绍了2-3树的定义以及插入删除操作。 本篇文章将在2-3树的基础上更进一步， 介绍比2-3树更为复杂的数据结构2-3-4树。 之所以介绍2-3-4树是因为2-3-4树与极为重要的红黑树有着等价关系， 通过先学习2-3-4树为后面学习红黑树打下基础， 增进对于红黑树的理解。 2-3树不再是单纯的二叉树了，因为2-3树中除了2-节点之外还存在3-节点。 在2-3树的基础上进一步扩展，2-3-4树在2-3树的基础上添加4-节点。 4-节点可以存储3个键值，最多可以拥有4棵子树。 定义 每个节点每个节点有1、2或3个key，分别称为2-节点，3-节点，4-节点。 所有叶子节点到根节点的长度一致（也就是说叶子节点都在同一层）。 每个节点的key从左到右保持了从小到大的顺序，两个key之间的子树中所有的 key一定大于它的父节点的左key，小于父节点的右key。 下图所示既是一棵2-3-4树。其中，有5个2-节点，2个3-节点和1个4-节点。 查找 2-3-4树的查找类似了二叉树的查找过程，通过键值的比较来决定遍历方向。 在下图所示树中查找key为22的节点。 在下图所示树中查找key为15的节点。 插入 2-3-4树插入节点跟删除节点的处理，实际上跟2-3树很像，特别是插入节点，基本上跟2-3树是一模一样，只是分裂的条件由2个key变成了3个key而已，即， 如果待插入的节点不是3个key，则直接插入即可； 如果待插入的节点有3个key，则对节点进行分裂， 即3个key加上待插入的key，这4个key分裂成1个key跟2个子节点， 然后将分裂之后的4个key中的父节点看作向上层插入的key， 然后重复1、2步骤，直到满足2-3-4树的定义性质。 如下图所示，插入“125”，而此时待插入节点有3个key，需要对节点进行分裂， “100 125 130”节点分裂之后，如下图所示，分裂成父节点“120”与两个子节点“100”与“125 130”，此时将父节点“120”看作向上层插入的key， 而又由于“120”的上层节点是“60 70 80”是3个key的节点，则需要对3个key节点进行分裂，如下图所示，分裂成父节点”70”与子节点“60”与“80 120”， 将父节点“70”看作向上层插入的key，此时上层节点“22 50”是2个key，则直接插入即可，结果如下图所示，此时满足2-3-4树，完成调整。 2-3-4树节点的插入就差不多这样了，也比较简单的， 其实从前面到这里可以看出一些规律，就是不管是二叉查找树也好， 平衡二叉树，以及2-3树的节点插入，相对来说都算简单， 但是对于一棵树节点的删除却比较复杂， 有的甚至需要不断的回溯到根节点才能把树调整平衡。 所以，关于2-3-4树节点的删除也不简单，至少比节点的插入要复杂麻烦的多， 但这里就讲个大概，类比2-3树节点删除去推就可以推出来，思路是一致的。 删除 2-3-4树节点的删除，首先，如果删除的key不存在，则删除失败。 类比2-3树总结也是两个判断： 删除的是什么节点？ 删除了节点之后是否符合满足2-3-4树的性质？ 2-3-4树有4种节点，1个key与非1个key的节点 和 是否为叶子节点 的组合，即： 非1个key的叶子节点； 仅1个key的叶子节点；3.非1个key的非叶子节点；4.仅1个key的非叶子节点。 2-3-4节点删除操作： 当删除的节点是非1个key的叶子节点，则将要删除的目标key删除即可； 当删除的节点是非叶子节点，无论待删除节点的key是多少个，先使用中序遍历找到待删除节点的后继节点，然后将后继节点与待删除节点位置互换，此时就将问题转化为删除节点为叶子节点（平衡树的非叶子节点中序遍历后继节点肯定叶子节点），如果该叶子是非1个key，则跟情况（1）一样，如果该节点是只有1个key，则跟后面的情况（3）一样； 当删除的节点是1个key的叶子节点，则将节点删除，此时树肯定需要调整，即： 当父节点是1个key（即此时仅有一个兄弟节点），兄弟节点是非1个key，则将兄弟节点的一个key上移成父节点，而父节点下移成子节点，此时树满足2-3-4树，完成调整。 当父节点是1个key，兄弟节点也是1个key，则此时将父节点与兄弟节点合并，将合并后的节点看成当前节点，然后重复（3）的判断，即判断合并后的当前节点的兄弟节点与父节点的情况，然后走对应的a.b.c处理，直到满足2-3-4树，完成调整。 当父节点是非1个key，即此时有两个或三个兄弟节点，此时看相邻兄弟节点是否“丰满”，也即是否为3个key，如下， i. 若删除节点的相邻兄弟节点为非3个key，则父节点的一个key下移，与相邻兄弟节点合并，此时树满足2-3树，完成调整； ii. 若删除节点的相邻兄弟节点为3个key，则父节点的一个key下移成1个key的节点，相邻兄弟节点的一个key上移与父节点合并，此时树满足2-3树，完成调整； 下面画几个图演示一下吧，如下图所示，符合（3）中的 b 情况， 即对兄弟节点“18”与父节点“13”合并， 合并之后，如下图所示，此时符合（3）中 c 的 ii 情况，即对节点“22”做左旋操作（参考2-3树文章最后的备注部分）， 左旋结果如下图所示，此时2-3-4树调整完成。 最后再重复一点，关于2-3-4树节点删除情况（3）中的 b ： “将合并后的节点看成当前节点，然后重复（3）的判断， 即判断合并后的当前节点的兄弟节点与父节点的情况” 这句话， 由于此时合并后的当前节点，其兄弟节点，是带有子节点的， 所以此时重复（3）的判断之后，如果是 c 中的 i 或 ii 情况， 对于（兄弟）key的上移与（父）key的下移，对应的子节点是需要出让的， 即此时的变换，实际上为左旋或右旋，具体是左旋还是右旋，看对应的场景。 "},"Chapter02/RedBlackTree.html":{"url":"Chapter02/RedBlackTree.html","title":"红黑树","keywords":"","body":"红黑树 红黑树是树的数据结构中最为重要的一种。Java的容器TreeSet、TreeMap均使用红黑树实现。 JDK1.8中HashMap中也加入了红黑树。C++ STL中的map和set同样使用红黑树实现。 红黑树和平衡二叉树类似，本质上都是为了解决排序二叉树在极端情况下退化成链表导致检索效率大大降低的问题。 红黑树最早是由 Rudolf Bayer 于 1972 年发明的。 红黑树首先肯定是一个排序二叉树，它在每个节点上增加了一个存储位来表示节点的颜色， 可以是 RED 或 BLACK 。 下图所示的一棵红黑树： 平衡二叉树真的很不错，在查找时既有着二叉查找树的优越性， 在插入时还能通过调整继续保持着。那么为什么还要使用到红黑树呢？我觉得可以从以下两个方面来考虑： 删除：对于平衡二叉树来说，在最坏情况下，需要维护从被删节点到根节点这条路径上所有节点的平衡性， 旋转的量级是O(logN)。但是红黑树就不一样了，最多只需3次旋转就会重新平衡，旋转的量级是O(1)。 保持平衡：平衡二叉树高度平衡，这也就意味着在大量插入和删除节点的场景下， 平衡二叉树为了保持平衡需要调整的频率会更高。 注意：在大量查找的情况下，平衡二叉树的效率更高，也是首要选择。在大量增删的情况下，红黑树是首选。 鉴于以上原因，因此我们才使用到了红黑树这种更好的结构。 定义 红黑树是每个节点都带有颜色属性的平衡二叉查找树 ，颜色为红色或黑色。 除了二叉查找树一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求(红黑树的特性): 每个节点要么是红色，要么是黑色。 根节点永远是黑色的。 所有的叶子节点都是空节点（即null），并且是黑色的。 每个红色节点的两个子节点都是黑色。（从每个叶子到根的路径上不会有两个连续的红色节点。） 从任一节点到其子树中每个叶子节点的路径都包含相同数量的黑色节点。 针对上面的 5 种性质，我们简单理解下， 对于性质 1 和性质 2 ，相当于是对红黑树每个节点的约束，根节点是黑色，其他的节点要么是红色，要么是黑色。 对于性质 3 中指定红黑树的每个叶子节点都是空节点，而且叶子节点都是黑色，但 Java 实现的红黑树会使用 null 来代表空节点，因此我们在遍历 Java里的红黑树的时候会看不到叶子节点，而看到的是每个叶子节点都是红色的，这一点需要注意。 对于性质 5，这里我们需要注意的是，这里的描述是从任一节点，从任一节点到它的子树的每个叶子节点黑色节点的数量都是相同的，这个数量被称为这个节点的黑高。如果我们从根节点出发到每个叶子节点的路径都包含相同数量的黑色节点，这个黑色节点的数量被称为树的黑色高度。树的黑色高度和节点的黑色高度是不一样的，这里要注意区分。 其实到这里有人可能会问了，红黑树的性质说了一大堆， 那是不是说只要保证红黑树的节点是红黑交替就能保证树是平衡的呢？ 其实不是这样的，我们可以看来看下面这张图： 左边的子树都是黑色节点，但是这个红黑树依然是平衡的，5 条性质它都满足。 这个树的黑色高度为 3，从根节点到叶子节点的最短路径长度是 2，该路径上全是黑色节点，包括叶子节点， 从根节点到叶子节点最长路径为 4，每个黑色节点之间会插入红色节点。 通过上面的性质 4 和性质 5，其实上保证了没有任何一条路径会比其他路径长出两倍，所以这样的红黑树是平衡的。 其实这算是一个推论，红黑树在最差情况下，最长的路径都不会比最短的路径长出两倍。 其实红黑树并不是真正的平衡二叉树，它只能保证大致是平衡的，因为红黑树的高度不会无限增高， 在实际应用用，红黑树的统计性能要高于平衡二叉树，但极端性能略差。 定义节点名称： 父节点——P(Parent) 祖父节点——G(GrandParent) 叔叔节点——U(Uncle) 当前节点——C(Current) 兄弟节点——B(Brother) 左孩子——L(Left) 右孩子——R(Right) 性质 根节点到任意叶子节点的路径长度，最多相差一半。若树存在最短路径，则最短路径上均为黑色节点， 那么第五条性质保证根节点到达最长路径与最短路径所包含的黑色节点数目相同，若最短路径长为N， 则最长路径M=N+红色节点数目，性质4要求红色节点必定不连续，因此红色节点数目最多为N， 则最长路径与最短路径最多相差N。 红黑树与AVL AVL是一种极度平衡的二叉树，那为什么不用AVL呢？因为AVL插入删除要保持平衡， 相比红黑树要慢一些，需要左旋右旋等等。但实际上它的旋转也只是几个场景的套用 ，哪些场景需要怎么旋转，理解就行了。 而红黑树是近似平衡的（黑平衡），也就是说它不像AVL那样绝对的平衡， 所以添加/删除节点后的平衡操作没那么多。 所以对于插入和删除操作较多的场景，用红黑树效率会高一些。 将2-3树转换成红黑树 主要思想：3节点分裂成2节点。 将3节点的第一个元素，作为第二个元素的左节点，并用红色的线连接，此时红色线连接的节点就相当于红色。 将2-3树按照以上思想转换后，就得到了一颗红黑树。用这种方式理解是不是简单多了呢？ 同时也有几个问题值得我们思考： 为什么红链规定在左边呢？ 我觉得是前人的一个约定，为了保持统一，简化处理，都放在左边。那都放右边是不是也可以呢？ 没有任何一个节点同时与两个红链接相连 因为一个红链表示一个3节点，如果有2个红链相连，则表示为4节点，不符合2-3树定义。 根节点为黑色 只有3节点的左链才为红色。根节点没有父节点，不可能为红色。 根节点到叶子节点经过的黑色节点数目相同 因为2-3树是完美平衡的。红黑树中经过的黑节点数=其层数。 2-3-4树和红黑树的等价关系 如果一棵树满足红黑树，把红色节点收缩到其父节点，就变成了2-3-4树， 所有红色节点都与其父节点构成3或4节点，其它节点为2节点。 一颗红黑树对应唯一形态的2-3-4树，但是一颗2-3-4树可以对应多种形态的红黑树 （主要是3节点可以对应两种不同的红黑树形态）。 之前的文章已经详细介绍了2-3-4树的性质与操作。 2-3-4树和红黑树是完全等价的，但是2-3-4树的编程实现相对复杂， 所以一般是通过实现红黑树来实现替代2-3-4树， 而红黑树也同样保证在O(logN)的时间内完成查找、插入和删除操作。 查找 红黑树的查找操作与二叉搜索树查找方式一致，这里不再赘述。 插入 想要彻底理解红黑树，除了上面说到的理解红黑树的性质以外，就是理解红黑树的插入操作了。 红黑树的插入和普通排序二叉树的插入基本一致，排序二叉树的要求是左子树上的所有节点都要比根节点小， 右子树上的所有节点都要比跟节点大，当插入一个新的节点的时候， 首先要找到当前要插入的节点适合放在排序二叉树哪个位置，然后插入当前节点即可。 红黑树和排序二叉树不同的是，红黑树需要在插入节点调整树的结构来让树保持平衡。 一般情况下，红黑树中新插入的节点都是红色的，那么，为什么说新加入到红黑树中的节点要是红色的呢？ 这个问题可以这样理解，我们从性质5中知道，当前红黑树中从根节点到每个叶子节点的黑色节点数量是一样的， 此时假如新的黑色节点的话，必然破坏规则，但加入红色节点却不一定， 除非其父节点就是红色节点，因此加入红色节点，破坏规则的可能性小一些。 接下来我们重点来讲红黑树插入新节点后是如何保持平衡的。 给定下面这样一颗红黑树： 当我们插入值为66的节点的时候，示意图如下： 很明显，这个时候结构依然遵循着上述5大特性，无需启动自动平衡机制调整节点平衡状态。 如果再向里面插入值为51的节点呢，这个时候红黑树变成了这样。 这样的结构实际上是不满足性质4的，红色两个子节点必须是黑色的，而这里49这个红色节点现在有个51的红色节点与其相连。 这个时候我们需要调整这个树的结构来保证红黑树的平衡。 首先尝试将49这个节点设置为黑色，如下示意图。 这个时候我们发现黑高是不对的，其中 60-56-45-49-51-null 这条路径有 4 个黑节点，其他路径的黑色节点是 3 个。 接着调整红黑树，我们再次尝试把45这个节点设置为红色的，如下图所示： 这个时候我们发现问题又来了，56-45-43 都是红色节点的，出现了红色节点相连的问题。 于是我们需要再把 56 和 43 设置为黑色的，如下图所示。 于是我们把 68 这个红色节点设置为黑色的。 对于这种红黑树插入节点的情况下，我们可以只需要通过变色就可以保持树的平衡了。但是并不是每次都是这么幸运的，当变色行不通的时候，我们需要考虑另一个手段就是旋转了。 例如下面这种情况，同样还是拿这颗红黑树举例。 现在这颗红黑树，我们现在插入节点65。 我们尝试把 66 这个节点设置为黑色，如下图所示。 这样操作之后黑高又出现不一致的情况了，60-68-64-null 有 3 个黑色节点，而60-68-64-66-null 这条路径有 4 个黑色节点，这样的结构是不平衡的。 或者我们把 68 设置为黑色，把 64 设置为红色，如下图所示： 但是，同样的问题，上面这颗红黑树的黑色高度还是不一致，60-68-64-null 和 60-68-64-66-null 这两条路径黑色高度还是不一致。 这种情况如果只通过变色的情况是不能保持红黑树的平衡的。 红黑树的旋转 接下来我们讲讲红黑树的旋转，旋转分为左旋和右旋。 左旋 文字描述：逆时针旋转两个节点，让一个节点被其右子节点取代，而该节点成为右子节点的左子节点。 文字描述太抽象，接下来看下图片展示。 首先断开节点PL与右子节点G的关系，同时将其右子节点的引用指向节点C2；然后断开节点G与左子节点C2的关系，同时将G的左子节点的应用指向节点PL。 接下来再放下 gif 图，希望能帮助大家更好地理解左旋，图片来自网络。 右旋 文字描述：顺时针旋转两个节点，让一个节点被其左子节点取代，而该节点成为左子节点的右子节点。 右旋的图片展示： 首先断开节点G与左子节点PL的关系，同时将其左子节点的引用指向节点C2；然后断开节点PL与右子节点C2的关系，同时将PL的右子节点的应用指向节点G。 右旋的gif展示（图片来自网络）: 介绍完了左旋和右旋基本操作，我们来详细介绍下红黑树的几种旋转场景。 左左节点旋转（插入节点的父节点是左节点，插入节点也是左节点） 如下图所示的红黑树，我们插入节点是65。 操作步骤如下可以围绕祖父节点 69 右旋，再结合变色，步骤如下所示： 左右节点旋转（插入节点的父节点是左节点，插入节点是右节点） 还是上面这颗红黑树，我们再插入节点 67。 这种情况我们可以这样操作，先围绕父节点 66 左旋， 然后再围绕祖父节点 69 右旋，最后再将 67 设置为黑色，把 69 设置为红色， 如下图所示。 右左节点旋转（插入节点的父节点是右节点，插入节点左节点） 如下图这种情况，我们要插入节点68。 这种情况，我们可以先围绕父节点 69 右旋，接着再围绕祖父节点 66 左旋， 最后把 68 节点设置为黑色，把 66 设置为红色，我们的具体操作步骤如下所示。 右右节点旋转（插入节点的父节点是右节点，插入节点也是右节点） 还是来上面的图来举例，我们在这颗红黑树上插入节点 70 。 我们可以这样操作围绕祖父节点 66 左旋，再把旋转后的根节点 69 设置为黑色， 把 66 这个节点设置为红色。具体可以参看下图： 红黑树的删除 删除相对插入来说，相对更复杂一点， 但是复杂的地方是因为在删除节点的这个操作情况有很多种， 但是插入不一样，插入节点的时候实际上这个节点的位置是确定的， 在节点插入成功后只需要调整红黑树的平衡就可以了。 但是删除不一样的是，删除节点的时候我们不能简单地把这个节点设置为null， 因为如果这个节点有子节点的情况下，不能简单地把当前删除的节点设置为null， 这个被删除的节点的位置需要有新的节点来填补。这样一来，需要分多种情况来处理了。 删除节点是根节点 直接删除根节点即可。 删掉节点的左子节点和右子节点都是为空 直接删除当前节点即可。 删除节点有一个子节点不为空 这个时候需要使用子节点来代替当前需要删除的节点，然后再把子节点删除即可。 给定下面这棵树，当我们需要删除节点69的时候。 首先用子节点代替当前待删除节点，然后再把子节点删除。 最终的红黑树结构如下面所示，这个结构的红黑树我们是不需要通过变色+旋转来保持红黑树的平衡了，因为将子节点删除后树已经是平衡的了。 还有一种场景是当我们待删除节点是黑色的，黑色的节点被删除后，树的黑高就会出现不一致的情况，这个时候就需要重新调整结构。 还是拿上面这颗删除节点后的红黑树举例，我们现在需要删除节点67。 因为67 这个节点的两个子节点都是null，所以直接删除,得到如下图所示结构： 这个时候我们树的黑高是不一致的，左边黑高是3，右边是2，所以我们需要把64节点设置为红色来保持平衡。 删除节点两个子节点都不为空 删除节点两个子节点都不为空的情况下，跟上面有一个节点不为空的情况下也是有点类似， 同样是需要找能替代当前节点的节点，找到后，把能替代删除节点值复制过来，然后再把替代节点删除掉。 先找到替代节点，也就是前驱节点或者后继节点 然后把前驱节点或者后继节点复制到当前待删除节点的位置，然后在删除前驱节点或者后继节点。 那么什么叫做前驱，什么叫做后继呢？ 前驱是左子树中最大的节点，后继则是右子树中最小的节点。 前驱或者后继都是最接近当前节点的节点，当我们需要删除当前节点的时候，也就是找到能替代当前节点的节点，能够替代当前节点肯定是最接近当前节点。 在当前删除节点两个子节点不为空的场景下，我们需要再进行细分，主要分为以下三种情况。 第一种，前驱节点为黑色节点，同时有一个非空节点 如下面这样一棵树，我们需要删除节点64： 首先找到前驱节点，把前驱节点复制到当前节点： 接着删除前驱节点。 这个时候63和60这个节点都是红色的，我们尝试把60这个节点设置为黑色即可使整个红黑树达到平衡。 第二种，前驱节点为黑色节点，同时子节点都为空 前驱节点是黑色的，子节点都为空，这个时候操作步骤与上面基本类似。 如下操作步骤： 因为要删除节点64，接着找到前驱节点63，把63节点复制到当前位置，然后将前驱节点63删除掉，变色后出现黑高不一致的情况下，最后把63节点设置为黑色，把65节点设置为红色，这样就能保证红黑树的平衡。 第三种，前驱节点为红色节点，同时子节点都为空 给定下面这颗红黑树，我们需要删除节点64的时候。 同样地，我们找到64的前驱节点63，接着把63赋值到64这个位置。 然后删除前驱节点。 删除节点后不需要变色也不需要旋转即可保持树的平衡。 红黑树在 Java 中的实现 Java 中的红黑树实现类是 TreeMap ， 接下来我们尝试从源码角度来逐行解释 TreeMap 这一套机制是如何运作的。 思考题 请画出下图的插入自平衡处理过程。 黑结点可以同时包含一个红子结点和一个黑子结点吗？ 答：可以。如下图的F结点： 请画出下图的删除自平衡处理过程。 "},"Chapter02/BTree.html":{"url":"Chapter02/BTree.html","title":"B树","keywords":"","body":"B 树 网上有些博客说B树和B-树是两种树，这是纯属扯淡。 B树也叫平衡多叉查找树 定义 B 树可以看作是对2-3查找树的一种扩展，一棵N阶的B树 (N叉树)的特性如下： B树中所有节点的孩子节点数中的最大值称为B树的阶，记为N（重点） 树中的每个节点至多有N棵子树 ---即：如果定了N，则这个B树中任何节点的子节点数量都不能超过N 若根节点不是终端节点，则至少有两棵子树 除根节点和叶节点外，所有点至少有m/2棵子树（上溢） 所有的叶子结点都位于同一层。 下图是一个N=4 阶的B树: 可以看到B树是2-3树的一种扩展，他允许一个节点有多于2个的元素。 B树的插入及平衡化操作和2-3树很相似，这里就不介绍了。下面动画演示往B树中依次插入如下数据 6 10 4 14 5 11 15 3 2 12 1 7 8 8 6 3 6 21 5 15 15 6 32 23 45 65 7 8 6 5 4 为什么我们要引入B树呢？有二叉树不是很好吗？ 存在即合理。所以还是有存在的理由的。 B树的时间复杂度与二叉树一样，均为O(logN)。 然而Ｂ树出现是因为磁盘ＩＯ。ＩＯ操作的效率很低， 那么，当在大量数据存储中， 查询时我们不能一下子将所有数据加载到内存中，只能逐一加载磁盘页， 每个磁盘页对应树的节点。 造成大量磁盘ＩＯ操作（最坏情况下为树的高度）。 平衡二叉树由于树深度过大而造成磁盘IO读写过于频繁， 进而导致效率低下。 所以，我们为了减少磁盘ＩＯ的次数，就你必须降低树的深度，将“瘦高”的树变得“矮胖”。 PS：这里我找到了一个讲述很好的二叉树与B树的比较链接。大家可以阅读以下。 http://m.elecfans.com/article/662237.html B树的查找操作 查找操作的理解不难。我直接用几张图来阐述一下。 在B树上进行查找和二叉树的查找很相似，二叉树是每个节点上有一个关键子和两个分支。B树上每个节点有K个关键字和K+1个分支。二叉树的查找只考虑向左还是向右走，而B树中需要由多个分支决定。 B树的查找分两步，首先查找节点，由于B树通常是在磁盘上存储的所以这步需要进行磁盘IO操作。第二步是查找关键字，当找到某个节点后将该节点读入内存中然后通过顺序或者折半查找来查找关键字。若没有找到关键字，则需要判断大小来找到合适的分支继续查找。 如下有一个3阶的B树，观察查找元素21的过程： 第一次磁盘IO： 第二次磁盘IO： 这里有一次内存比对：分别跟3与12比对。 第三次磁盘IO: 这里有一次内存比对，分别跟14与21比对 由于B树相对于二叉树来说矮胖了许多， 所以它所涉及的IO操作也相对少了许多。不过根据我们上面的分析， 其在查找数据的时候并没有减少比较次数。 但是我们知道，我们在比较数据的时候是在内存中进行的， 所以相对来说时间上会更加迅速，几乎可以忽略。 而相同数量的key在B树中生成的节点要远远少于二叉树中的节点， 相差的节点数量就等同于磁盘IO的次数。 这样到达一定数量后，性能的差异就显现出来了。 这里先放下一个问题，既然我的介数M是自行设定的。那么我们的M如果很小那么会变成类似于二叉树，那么当M很大的时候呢？ 我认为当M很大会导致比较次数过多，在当前磁盘空间能容纳的范围下也有可能导致比较次数过多而导致效率降低的情况。 代码实现 首先是 关键字的实现 /** * * 私有内部类 代表关键字以及数据域 */ class Entry { private K key; private V value; public void setKey(K key) { this.key = key; } public K getKey() { return this.key; } public void setValue(V value) { this.value = value; } public V getValue() { return this.value; } public Entry(K key, V value) { this.key = key; this.value = value; } public Entry() { } @Override public String toString() { return \"key: \" + this.key + \" , \"; } } 返回值的封装实现 class SearchResult { private boolean isExist; private V value; private int index; //索引 //构造方法，将查询结果封装入对象 public SearchResult(boolean isExist, int index, V value) { this.isExist = isExist; this.index = index; this.value = value; } public boolean isExist() { return isExist; } public V getValue() { return value; } public int getIndex() { return index; } } 节点的代码实现 /** * description: 节点列表 * @params entries关键字列表 children儿子节点列表 索引为2的关键字的左子树为2右子树为2+1 */ class Node { //关键字列表 private List> entries; //子树列表 private List> children; //是否为叶子节点 private boolean leaf; //键值比较函数对象，如果采用倒序或者其它排序方式，传入该对象 private Comparator kComparator; //比较两个key，如果没有传入自定义排序方式则采用默认的升序 /** * description: 默认情况下是返回key2.comPareto(key1)也就是key2>key1 返回大于0的值 == 等于0 ) key2).compareTo(key1) : kComparator.compare(key1, key2); } Node() { this.entries = new LinkedList>(); this.children = new LinkedList>(); this.leaf = false; } Node(Comparator kComparator) { this(); this.kComparator = kComparator; } //返回本节点的关键字字数 public int nodeSize() { return this.entries.size(); } public List> getEntries() { return entries; } public void setEntries(List> entries) { this.entries = entries; } public List> getChildren() { return children; } public void setChildren(List> children) { this.children = children; } public boolean isLeaf() { return leaf; } public void setLeaf(boolean leaf) { this.leaf = leaf; } public Comparator getkComparator() { return kComparator; } public void setkComparator(Comparator kComparator) { this.kComparator = kComparator; } /** * description: 查找当前节点有没有这个key本质上就是一个二分搜索 index是给插入关键字用的判断是要插入到mid的哪个位置 * 关于返回值的index要明白 索引为0的关键字的左子树为0 右子树为0+1 经过推断可以一直延续下去 * 所以如果没找到key就会返回合适的子树索引 * 索引可以直接使用 如果是叶子节点 * @params [key] * @return BTreeNode.SearchResult */ public SearchResult search(K key){ int left = 0; //左指针 int right = this.nodeSize()-1;//右指针 V res = null;//返回值 int mid = (left+right)/2;//中间的位置 boolean isExist = false; //判断是否找到了 int index = 0; //要找的关键字的下标 while(left midEntry = this.entries.get(mid);//拿到中间的关键字 int comparator = this.compare(key,midEntry.getKey()); if(comparator == 0){//找到了 break; }else { if(comparator==1){//midKey大于key 在左侧 right = mid-1; }else{//在右侧 left = mid+1; } } } //二分查找结束了 if(left0){ //key大 那么就是关键字的右子树 isExist = false; index = left + 1; res = null; }else { //key小 返回当前left的左子树 isExist = false; index = left ; res = null; } } }else {//走到这里也是因为right(isExist, index, res); } //删除给定索引位置的项 public Entry removeEntry(int index) { Entry re = this.entries.get(index); this.entries.remove(index); return re; } //得到index处的项 public Entry entryAt(int index) { return this.entries.get(index); } //将新项插入指定位置 这里设置成private封装 // 会使用到这里只有两种情况一个是叶子节点的插入关键字以及确定好了位置 //节点分裂但是也已经确定好了位置 private void insertEntry(Entry entry, int index) { this.entries.add(index, entry); } //将新关键字插入 如果存在相同的关键字key不允许插入 public boolean insertEntry(Entry entry){ SearchResult result = search(entry.getKey()); //这里会返回适合的节点 if (result.isExist()) { //如果有了相同的key就不然插入 return false; } else { insertEntry(entry, result.getIndex()); return true; } } //更新项，如果项存在，更新其值并返回原值，否则直接插入 public V putEntry(Entry entry) { SearchResult re = search(entry.getKey()); if (re.isExist) { Entry oldEntry = this.entries.get(re.getIndex()); V oldValue = (V) oldEntry.getValue(); oldEntry.setValue(entry.getValue()); return oldValue; } else { insertEntry(entry); return null; } } //获得指定索引的子节点 public Node childAt(int index) { return this.children.get(index); } //删除给定索引的子节点 public void removeChild(int index) { this.children.remove(index); } //将新的子节点插入到指定位置 public void insertChild(Node child, int index) { this.children.add(index, child); } } 上面最关键的方法是SearchResult 作用是： 查找在某个节点查找传入关键字 有可能会面临的情况 left left==right，如果left下标的关键字大于要找的关键字那么就返回 left作为索引，如果是小于要找的关键字就返回 left+1作为索引 left>right 要查找的key比left要小所以返回left作为索引 关于没有找到的情况下那个索引是指向子树的，比如用下图做例子0012关键字的索引是1他的左子树的索引是就是这个关键字的索引 右子树的索引就是关键字索引加1 public class BTreeNode { //默认是5阶树 private Integer DEFAULT_T = 5; //根节点 private Node root; private int m = DEFAULT_T; //非根节点的最小项数，体现的是除了根节点，其余节点都是分裂而来的！ //这里体现了除根节点和叶子节点外每个节点的最小儿子数量 private int nodeMinSize = m/2; //节点的最大儿子数 private int nodeMaxSize = m; //节点的最大关键字数 private int keyMaxSize = m-1; //节点的最小关键字数 private int keyMinSize = m/2; //比较函数对象 private Comparator kComparator; //查找 public BTreeNode(Integer DEFAULT_T) { this.root = new Node<>(); this.root.setLeaf(true); m = DEFAULT_T; nodeMinSize = m/2; nodeMaxSize = m; keyMaxSize = m-1; keyMinSize = m/2; this.DEFAULT_T = DEFAULT_T; } } B树的插入 对高度为ｋ的m阶B树，新结点一般是插在叶子层。通过检索可以确定关键码应插入的结点位置。然后分两种情况讨论： 若该结点中关键码个数小于m-1，则直接插入即可。 若该结点中关键码个数等于m-1，则将引起结点的分裂。以中间关键码为界将结点一分为二，产生一个新结点，并把中间关键码插入到父结点(ｋ-1层)中重复上述工作，最坏情况一直分裂到根结点，建立一个新的根结点，整个B树增加一层。 例如：在下面的B树中插入key：4 第一步：检索key插入的节点位置如上图所示，在3,5之间； 第二步：判断节点中的关键码个数：节点3，5已经是两元素节点，无法再增加（已经 = 3-1）。父亲节点 2， 6 也是两元素节点，也无法再增加。根节点9是单元素节点，可以升级为两元素节点。； 第三步：结点分裂：拆分节点3，5与节点2，6，让根节点9升级为两元素节点4，9。节点6独立为根节点的第二个孩子。 最终结果如下图：虽然插入比较麻烦，但是这也能确保Ｂ树是一个自平衡的树。 另一个例子： 当插入一个关键字60后,节点内的关键字个数超过出了m-1=2，此时必须进行节点分裂，分裂的过程如上图所示。 B树的删除 B树的删除操作相对较为复杂， 首先，根据key删除记录，如果B树中的记录中不存对应key的记录，则删除失败。 之后我们需要分一下四种情况来考虑。 （下面的例子中以5阶B树为例，介绍B树的删除操作，5阶B树中， 结点最多有4个key,最少有2个key）对于删除key的过程来说， 对于叶结点和非叶结点其实差别在一个地方， 那就是如果当前我们操作的是非叶结点， 那么后继key（这里的后继key指最接近当前删除值，且大于当前删除值的值）覆盖要删除的key， 然后在后继key所在的子支中删除该后继key。 原始状态 在上述情况下接着删除27。从上图可知27位于非叶子结点中，所以用27的后继替换它。 从图中可以看出，27的后继为28，我们用28替换27，然后在28（原27）的右孩子结点中删除28。 删除后的结果如下图所示。 之后我们就把非叶结点的情况转换为了处理叶结点的情况。 此时我们就要分一下几种情况来考虑如何处理叶结点。 ① 该结点key个数>=Ceil(m/2)-1（上界，例如m=5，那么最后为2），结束删除操作，否则执行下一步。 ② 该结点key的个数Ceil(m/2)-1，则父结点中的key下移到该结点，兄弟结点中的一个key上移，删除操作结束。 紧接上面的例子。 删除后发现，当前叶子结点的记录的个数小于2（Ceil(m/2)-1 当前结点还有一个右兄弟， 选择右兄弟就会出现合并结点的情况，不论选哪一个都行，只是最后B树的形态会不一样而已）， 我们可以从兄弟结点中借取一个key。 所以父结点中的28下移，兄弟结点中的26上移，删除结束。 结果如下图所示。 ③ 如果该结点key的个数 原图为 在上述情况下删除32。 当删除后，当前结点中只有一个key（个数 结果如下图所示。 由于我们移动了父结点中的数据，所以其key值减少，那么我们就要对当前父结点进行考虑。 下面看一个考虑父结点的例子 上述情况下，我们接着删除key为40的记录，删除后结果如下图所示。 同理，当前结点的记录数小于2（根据情况③）， 兄弟结点中没有多余key，所以父结点中的key下移， 和兄弟（这里我们选择左兄弟，选择右兄弟也可以）结点合并， 合并后的指向当前结点的指针就指向了父结点。 同理，对于当前结点而言只能继续合并了，最后结果如下所示。（根据情况③） 合并后结点当前结点满足条件，删除结束。 B-Tree与二叉查找树的对比 我们知道二叉查找树查询的时间复杂度是O（logN）， 查找速度最快和比较次数最少，既然性能已经如此优秀，但为什么实现索引是使用B-Tree而不是二叉查找树，关键因素是磁盘IO的次数。 数据库索引是存储在磁盘上，当表中的数据量比较大时，索引的大小也跟着增长，达到几个G甚至更多。 当我们利用索引进行查询的时候，不可能把索引全部加载到内存中，只能逐一加载每个磁盘页，这里的磁盘页就对应索引树的节点。 二叉树 我们先来看二叉树查找时磁盘IO的次：定义一个树高为4的二叉树，查找值为10： "},"Chapter02/BPlusTree.html":{"url":"Chapter02/BPlusTree.html","title":"B+树","keywords":"","body":"B+ 树 B+树是对B树的一种变形树，它与B树的差异在于： 有k个子结点的结点必然有k个关键码； 非叶结点仅具有索引作用，跟记录有关的信息均存放在叶结点中。 树的所有叶结点构成一个有序链表，可以按照关键码排序的次序遍历全部记录。 如下图，是一个B+树: 下图是B+树的插入动画： B和B+树的区别 B+树的非叶子结点只包含导航信息，不包含实际的值，所有的叶子结点和相连的节点使用链表相连， 便于区间查找和遍历。 B+树的优点： 由于B+树在内部节点上不包含数据信息，因此在内存页中能够存放更多的key。 数据存放的更加紧密， 具有更好的空间局部性。因此访问叶子节点上关联的数据也具有更好的缓存命中率。 B+树的叶子结点都是相链的，因此对整棵树的便利只需要一次线性遍历叶子结点即可。 而且由于数据顺序排列并且相连，所以便于区间查找和搜索。而B树则需要进行每一层的递归遍历。 相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。 B树得优点 由于B树的每一个节点都包含key和value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。 下面是B 树和B+树的区别图： 下面我们看一个B+树的例子，感受感受它吧！ 插入操作 对于插入操作很简单，只需要记住一个技巧即可： 当节点元素数量大于m-1的时候，按中间元素分裂成左右两部分，中间元素分裂到父节点当做索引存储， 但是，本身中间元素还是分裂右边这一部分的。 下面以一颗5阶B+树的插入过程为例，5阶B+树的节点最少2个元素，最多4个元素。 插入5，10，15，20 插入25，此时元素数量大于4个了，分裂 接着插入26，30，继续分裂 有了这几个例子，相信插入操作没什么问题了，下面接着看看删除操作。 删除操作 对于删除操作是比B树简单一些的，因为叶子节点有指针的存在，向兄弟节点借元素时，不需要通过父节点了， 而是可以直接通过兄弟节移动即可（前提是兄弟节点的元素大于m/2）， 然后更新父节点的索引；如果兄弟节点的元素不大于m/2（兄弟节点也没有多余的元素）， 则将当前节点和兄弟节点合并，并且删除父节点中的key， 下面我们看看具体的实例。 初始状态 删除10，删除后，不满足要求，发现左边兄弟节点有多余的元素，所以去借元素，最后，修改父节点索引 删除元素5，发现不满足要求，并且发现左右兄弟节点都没有多余的元素， 所以，可以选择和兄弟节点合并，最后修改父节点索引 发现父节点索引也不满足条件，所以，需要做跟上面一步一样的操作 这样，B+树的删除操作也就完成了，是不是看完之后，觉得非常简单！ 为什么说B+树比B树更适合做操作系统的数据库索引和文件索引？ B+树的磁盘读写的代价更低 B+树内部结点没有指向关键字具体信息的指针，这样内部结点相对B树更小。 B+树的查询更加的稳定 因为非终端结点并不是最终指向文件内容的结点，仅仅是作为叶子结点中关键字的索引。这样所有的关键字的查找都会走一条从根结点到叶子结点的路径。所有的关键字查询长度都是相同的，查询效率相当。 "},"Chapter02/B&BPlusTree.html":{"url":"Chapter02/B&BPlusTree.html","title":"为什么MongoDB索引选择B树而Mysql选择B+树","keywords":"","body":"为什么 MongoDB 索引选择B-树，而 Mysql 选择B+树 B-树和B+树的区别 B+树查询时间复杂度固定是logn，B-树查询复杂度最好是 O(1)。 B+树相邻接点的指针可以大大增加区间访问性，可使用在范围查询等，而B-树每个节点 key 和 data 在一起，则无法区间查找。 B+树更适合外部存储，也就是磁盘存储。由于内节点无 data 域，每个节点能索引的范围更大更精确 注意这个区别相当重要，是基于（1）（2）（3）的，B-树每个节点即保存数据又保存索引，所以磁盘IO的次数很少，B+树只有叶子节点保存，磁盘IO多，但是区间访问比较好。 原因解释 想要解释原因，我们还必须要了解一下MongoDB和Mysql的基本概念。 MongoDB MongoDB: 是文档型的数据库，是一种 nosql，它使用类 Json 格式保存数据。 比如之前我们的表可能有用户表、订单表、购物篮表等等，还要建立他们之间的外键关联关系。 但是类Json就不一样了。 我们可以看到这种形式更简单，通俗易懂。那为什么 MongoDB 使用B-树呢？ MongoDB使用B-树，所有节点都有Data域，只要找到指定索引就可以进行访问，无疑单次查询平均快于Mysql。 Mysql： 作为一个关系型数据库，数据的关联性是非常强的， 区间访问是常见的一种情况，B+树由于数据全部存储在叶子节点， 并且通过指针串在一起，这样就很容易的进行区间遍历甚至全部遍历。 这俩区别的核心如果你能看懂B-树和B+树的区别就很容易理解。 "},"Chapter02/B++Tree.html":{"url":"Chapter02/B++Tree.html","title":"B*树","keywords":"","body":"B*tree 是B+Tree的变体，在B+Tree的非根和非叶子结点再增加指向兄弟的指针； B树定义了非叶子结点关键字个数至少为(2/3)M，即块的最低使用率为2/3（代替B+树的1/2）； B+树的分裂：当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点， 最后在父结点中增加新结点的指针； B+树的分裂只影响原结点和父结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针； B*树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中， 再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）； 如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点， 最后在父结点增加新结点的指针； 所以，B*树分配新结点的概率比B+树要低，空间使用率更高； 下图是B+树 下图是B*树 "},"Chapter02/LSMTree.html":{"url":"Chapter02/LSMTree.html","title":"LSM树","keywords":"","body":"LSM树 LSM树，即日志结构合并树(Log-Structured Merge-Tree)。 其实它并不属于一个具体的数据结构，它更多是一种数据结构的设计思想。 大多NoSQL数据库核心思想都是基于LSM来做的， 只是具体的实现不同。 它的核心思路其实非常简单，就是假定内存足够大，因此不需要每次有数据更新就必须将数据写入到磁盘中， 而可以先将最新的数据驻留在内存中，等到积累到最后多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾 (因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起)。 LSM具有批量特性，存储延迟。当写读比例很大的时候（写比读多），LSM树相比于B树有更好的性能。因为随着insert操作，为了维护B树结构，节点分裂。 读磁盘的随机读写概率会变大，性能会逐渐减弱。 多次单页随机写，变成一次多页随机写,复用了磁盘寻道时间，极大提升效率。 LSM Tree弄了很多个小的有序结构，比如每m个数据，在内存里排序一次，下面100个数据，再排序一次……这样依次做下去，我就可以获得N/m个有序的小的有序结构。 在查询的时候，因为不知道这个数据到底是在哪里，所以就从最新的一个小的有序结构里做二分查找，找得到就返回，找不到就继续找下一个小有序结构，一直到找到为止。 很容易可以看出，这样的模式，读取的时间复杂度是(N/m)*log2N 。读取效率是会下降的。 当然也可以做一些优化 Bloom filter: 就是个带随即概率的bitmap,可以快速的告诉你，某一个小的有序结构里有没有指定的那个数据的。于是就可以不用二分查找，而只需简单的计算几次就能知道数据是否在某个小集合里啦。效率得到了提升，但付出的是空间代价。 compact:小树合并为大树:因为小树他性能有问题，所以要有个进程不断地将小树合并到大树上，这样大部分的老数据查询也可以直接使用log2N的方式找到，不需要再进行(N/m)*log2n的查询了 LSM树诞生背景 传统关系型数据库使用btree或一些变体作为存储结构，能高效进行查找。 但保存在磁盘中时它也有一个明显的缺陷，那就是逻辑上相离很近但物理却可能相隔很远，这就可能造成大量的磁盘随机读写。 随机读写比顺序读写慢很多，为了提升IO性能，我们需要一种能将随机操作变为顺序操作的机制，于是便有了LSM树。 LSM树能让我们进行顺序写磁盘，从而大幅提升写操作，作为代价的是牺牲了一些读性能。 关于磁盘IO 磁盘读写时涉及到磁盘上数据查找，地址一般由柱面号、盘面号和块号三者构成。也就是说移动臂先根据柱面号移动到指定柱面，然后根据盘面号确定盘面的磁道，最后根据块号将指定的磁道段移动到磁头下，便可开始读写。 整个过程主要有三部分时间消耗，查找时间(seek time) +等待时间(latency time)+传输时间(transmission time) 。分别表示定位柱面的耗时、将块号指定磁道段移到磁头的耗时、将数据传到内存的耗时。 整个磁盘IO最耗时的地方在查找时间，所以减少查找时间能大幅提升性能。 LSM树原理 LSM树由两个或以上的存储结构组成，比如在论文中为了方便说明使用了最简单的两个存储结构。一个存储结构常驻内存中，称为C0 tree，具体可以是任何方便健值查找的数据结构，比如红黑树、map之类，甚至可以是跳表。 另外一个存储结构常驻在硬盘中，称为C1 tree，具体结构类似B树。C1所有节点都是100%满的，节点的大小为磁盘块大小。 插入步骤 大体思路是：插入一条新纪录时，首先在日志文件中插入操作日志，以便后面恢复使用，日志是以append形式插入，所以速度非常快；将新纪录的索引插入到C0中，这里在内存中完成，不涉及磁盘IO操作；当C0大小达到某一阈值时或者每隔一段时间，将C0中记录滚动合并到磁盘C1中；对于多个存储结构的情况，当C1体量越来越大就向C2合并，以此类推，一直往上合并Ck。 合并步骤 合并过程中会使用两个块：emptying block和filling block。 从C1中读取未合并叶子节点，放置内存中的emptying block中。从小到大找C0中的节点，与emptying block进行合并排序，合并结果保存到filling block中，并将C0对应的节点删除。不断执行第2步操作，合并排序结果不断填入filling block中，当其满了则将其追加到磁盘的新位置上，注意是追加而不是改变原来的节点。合并期间如故宫emptying block使用完了则再从C1中读取未合并的叶子节点。C0和C1所有叶子节点都按以上合并完成后即完成一次合并。 关于优化措施 本文用图阐述LSM的基本原理，但实际项目中其实有很多优化策略，而且有很多针对LSM树优化的paper。比如使用布隆过滤器快速判断key是否存在，还有做一些额外的索引以帮助更快找到记录等等。 插入操作 向LSM树中插入 A E L R U 首先会插入到内存中的C0树上，这里使用AVL树，插入“A”，先向磁盘日志文件追加记录，然后再插入C0 插入“E”，同样先追加日志再写内存， 继续插入“L”，旋转后如下， 插入“R”“U”，旋转后最终如下。 假设此时触发合并，则因为C1还没有树，所以emptying block为空，直接从C0树中依次找最小的节点。filling block长度为4，这里假设磁盘块大小为4。 开始找最小的节点，并放到filling block中， 继续找第二个节点， 以此类推，填满filling block 开始写入磁盘，C1树， 继续插入 B F N T 先分别写日志，然后插入到内存的C0树中 假如此时进行合并，先加载C1的最左边叶子节点到emptying block， 接着对C0树的节点和emptying block进行合并排序，首先是“A”进入filling block， 然后是“B”， 合并排序最终结果为， 将filling block追加到磁盘的新位置，将原来的节点删除掉， 继续合并排序，再次填满filling block， 将filling block追加到磁盘的新位置，上一层的节点也要以磁盘块（或多个磁盘块）大小写入，尽量避开随机写。另外由于合并过程可能会导致上层节点的更新，可以暂时保存在内存，后面在适当时机写入。 查找操作 查找总体思想是先找内存的C0树，找不到则找磁盘的C1树，然后是C2树，以此类推。 假如要找“B”，先找C0树，没找到。 接着找C1树，从根节点开始， 找到“B”。 删除操作 删除操作为了能快速执行，主要是通过标记来实现，在内存中将要删除的记录标记一下，后面异步执行合并时将相应记录删除。 比如要删除“U”，假设标为#的表示删除，则C0树的“U”节点变为， 而如果C0树不存在的记录，则在C0树中生成一个节点，并标为#，查找时就能在内存中得知该记录已被删除，无需去磁盘找了。比如要删除“B”，那么没有必要去磁盘执行删除操作，直接在C0树中插入一个“B”节点，并标为#。 假如对写操作的吞吐量比较敏感，可采用日志策略（顺序读写，只追加不修改）来提升写性能。存在问题：数据查找需要倒序扫描，花费很多时间。比如，预写日志WAL，WAL的中心概念是数据文件（存储着表和索引）的修改必须在这些动作被日志记录之后才被写入，即在描述这些改变的日志记录被刷到持久存储以后。如果我们遵循这种过程，我们不需要在每个事务提交时刷写数据页面到磁盘，因为我们知道在发生崩溃时可以使用日志来恢复数据库：任何还没有被应用到数据页面的改变可以根据其日志记录重做（这是前滚恢复，也被称为REDO）。使用WAL可以显著降低磁盘的写次数，因为只有日志文件需要被刷出到磁盘以保证事务被提交，而被事务改变的每一个数据文件则不必被刷出。 其只是提高了写的性能，对于更为复杂的读性能，需要寻找其他的方法，其中有四种方法来提升读性能： 二分查找: 将文件数据有序保存，使用二分查找来完成特定key的查找。 哈希：用哈希将数据分割为不同的bucket B+树：使用B+树 或者 ISAM 等方法，可以减少外部文件的读取 外部文件： 将数据保存为日志，并创建一个hash或者查找树映射相应的文件。 所有的四种方法都可以有效的提高了读操作的性能（最少提供了O(log(n)) )，但是，却丢失了日志文件超好的写性能，上面这些方法，都强加了总体的结构信息在数据上，数据被按照特定的方式放置，所以可以很快的找到特定的数据，但是却对写操作不友善，让写操作性能下降。更糟糕的是，当需要更新hash或者B+树的结构时，需要同时更新文件系统中特定的部分，这就是造成了比较慢的随机读写操作，这种随机的操作要尽量减少。 既要保证日志文件好的写性能，又要在一定程度上保证读性能，所以LSM-Tree应运而生。 讲LSM树之前，需要提下三种基本的存储引擎，这样才能清楚LSM树的由来： 哈希存储引擎 是哈希表的持久化实现，支持增、删、改以及随机读取操作，但不支持顺序扫描，对应的存储系统为key-value存储系统。对于key-value的插入以及查询，哈希表的复杂度都是O(1)，明显比树的操作O(n)快,如果不需要有序的遍历数据，哈希表就是your Mr.Right B树存储引擎是B树（关于B树的由来，数据结构以及应用场景可以看之前一篇博文）的持久化实现，不仅支持单条记录的增、删、读、改操作，还支持顺序扫描（B+树的叶子节点之间的指针），对应的存储系统就是关系数据库（Mysql等）。 LSM树（Log-Structured Merge Tree）存储引擎和B树存储引擎一样，同样支持增、删、读、改、顺序扫描操作。而且通过批量存储技术规避磁盘随机写入问题。当然凡事有利有弊，LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能。 LSM树（Log Structured Merge Tree，结构化合并树）的思想非常朴素，就是将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘（由此提升了写性能），是一种基于硬盘的数据结构，与B-tree相比，能显著地减少硬盘磁盘臂的开销。当然凡事有利有弊，LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能。 读取时需要合并磁盘中的历史数据和内存中最近的修改操作,读取时可能需要先看是否命中内存，否则需要访问较多的磁盘文件（存储在磁盘中的是许多小批量数据，由此降低了部分读性能。但是磁盘中会定期做merge操作，合并成一棵大树，以优化读性能）。LSM树的优势在于有效地规避了磁盘随机写入问题，但读取时可能需要访问较多的磁盘文件。 代表数据库：nessDB、leveldb、hbase等 核心思想的核心就是放弃部分读能力，换取写入的最大化能力，放弃磁盘读性能来换取写的顺序性。极端的说，基于LSM树实现的HBase的写性能比Mysql高了一个数量级，读性能低了一个数量级。 LSM操作 LSM树 插入数据可以看作是一个N阶合并树。数据写操作（包括插入、修改、删除也是写）都在内存中进行， 数据首先会插入内存中的树。当内存树的数据量超过设定阈值后，会进行合并操作。合并操作会从左至右便利内存中树的子节点 与 磁盘中树的子节点并进行合并，会用最新更新的数据覆盖旧的数据（或者记录为不同版本）。当被合并合并数据量达到磁盘的存储页大小时。会将合并后的数据持久化到磁盘，同时更新父节点对子节点的指针。 LSM树 读数据 磁盘中书的非子节点数据也被缓存到内存中。在需要进行读操作时，总是从内存中的排序树开始搜索，如果没有找到，就从磁盘上的排序树顺序查找。 在LSM树上进行一次数据更新不需要磁盘访问，在内存即可完成，速度远快于B+树。当数据访问以写操作为主，而读操作则集中在最近写入的数据上时，使用LSM树可以极大程度地减少磁盘的访问次数，加快访问速度。 LSM树 删除数据 前面讲了。LSM树所有操作都是在内存中进行的，那么删除并不是物理删除。而是一个逻辑删除，会在被删除的数据上打上一个标签，当内存中的数据达到阈值的时候，会与内存中的其他数据一起顺序写入磁盘。 这种操作会占用一定空间，但是LSM-Tree 提供了一些机制回收这些空间。 LSM VS B树 B树被广泛应用于各种传统数据库。采用了B树的存储系统，所有数据都是排序的，并将这些数据分成一个个page。而B树就是指向这些page的索引组成的m阶树。每次读写数据的过程就是顺着B树查找或更新各个page的过程。B树相对于AVL、红黑树等的优点在于可以减少文件读写次数。 对比LSM和B树之前，我们先来考虑一下它们为什么会设计成这样。要设计一个系统，我们可以从最简单的设计出发。对于存储系统，最简单的就是把记录直接写到记录文件的末尾，这样的做法写效率是最高的。然而要查询某一条记录，需要遍历整个文件，这是无法接受的。为了快速查询，一个办法是建立hash索引，但是hash索引有其自己的问题，比如数据量大的时候，索引在内存中就放不下了。另一个办法就是事先对数据进行排序。从排好序的文件中查找记录有一箱的数据结构可以用，平衡二叉树、堆、红黑树等等，还有今天的主角B树（啊，不，B树只是被来出来陪衬的，今天的主角是LSM）。 这里的关键是“事先”是什么时候。首先会想到的思路是在写入的时候。在计算机系统中真正foundmental的创新是很不容易的。大多数的优化其实都是tradeoff，也就是牺牲一点A，得到一点B。在这里，一共两种操作，写入或者读取，为了提高读取效率，我们就要在写入的时候多做一些事情。对于B树，这多做的事情首先是找到正确的位置，其次还会有page的分裂等。 大多数时候，B树的表现是很优秀的，他也一直很努力的提高自己，不断增加新技能，进化出了B+\\B*树等进化体。然而当系统同时服务的客户越来越来多，对吞吐量的要求越来越高。B树表示在大并发写操作的时候，压力有点大，因为要做的事情有点多。那怎么办，为了读取数据的时候轻松一点，这些事情不得不做啊。 当B树不堪重负的时候，主角LSM树登场了。他说，想要有高的写吞吐，就给我减负，我可管不了那么多，我可是主角。作者也很无奈，想想也是，哪个主角没几个挂呢，给他开挂吧。本来都是写入的时候要做的事，就少做一点吧，给你几个后台线程，剩下的事情用它们做吧。有了这几个后台线程帮忙，LSM树处理大量写入的能力一下就上来了。LSM由此直接拿下Hbase、Cassandra、kudu等大量地盘。老大哥B树表示，他有挂，我很慌。 到这里就比较清楚了，B树把所有的压力都放到了写操作的时候，从根节点索引到数据存储的位置，可能需要多次读文件；真正插入的时候，又可能会引起page的分裂，多次写文件。而LSM树在插入的时候，直接写入内存，只要利用红黑树保持内存中的数据有序即可，所以可以提供更高的写吞吐。不过，把compaction交给后台线程来做，意味着有时间差，读取的时候，通常不止一个SSTable，要么逐个扫描，要么先merge，所以会影响到读效率。另外，当后台线程做compaction的时候，占用了IO带宽，这时也会影响到写吞吐。所以B树还不会被LSM取代。 Hbase VS kudu Hbase 的存储实现是LSM的典型应用，适合大规模在线读写。然而，除了这种OLTP的访问模式，正如我在大数据场景与挑战中提到的，还有一种OLAP的数据访问模式，Hbase其实是不合适的。对此，最常见的做法是定期把数据导出到专门针对OLAP场景的存储系统。这个做法一点都不优雅，因为一份同样的数据同时存在两个不同的地方，而且还会有一个不一致的时间窗口。Kudu就是为了解决这个问题而诞生的。我最早看到kudu就很有兴趣，也很好奇，一个存储系统能同时满足OLTP和OLAP两种场景，那是厉害的。不过现在kudu由于运维成本等其他问题还没有被广泛采用，挺可惜的。 扯远了，我们来看为了更好的支持OLAP，kudu对LSM做了哪些优化。OLAP经常会做列选择，所有的OLAP存储引擎都是以列式存储的。kudu也想到了这一点。kudu的memtable(在kudu中叫MemRowSet)还是同之前一样，只是SSTable(在kudu中叫DiskRowSet)改成了列式存储。对于列式存储，读取一个记录需要分别读每个字段，因此kudu精心设计了RowSet中的索引(针对并发访问等改进过的B树)，加速这个过程。 除了列式存储，kudu保证一个key只可能出现在一个RowSet中，并记录了每个RowSet中key的最大值和最小值，加速数据的范围查找。这也意味着，对于数据更新，不能再像之前一样直接插入memtable即可。需要找到对应的RowSet去更新，为了保持写吞吐，kudu并不直接更新RowSet，而是又新建一个DeltaStore，专门记录数据的更新。所以，后台除了RowSet的compaction线程，还要对DeltaStore进行merge和apply。从权衡的角度考虑，kudu其实是牺牲了一点写效率，单记录查询效率，换取了批量查询效率。 这样看来，从B树到LSM，到kudu对LSM的优化，其实都是针对不同场景不同的访问需求做出的各种权衡而已。了解了这些，我们在选择这些技术的时候心里就有底了。另外，权衡并不是那么容易的事。怎么样牺牲A去补偿B，可能有不同的策略。研究现有系统的一些思想，有助于当我们自己面临问题的时候，有更多思路。 "},"Chapter02/HuffmanTree.html":{"url":"Chapter02/HuffmanTree.html","title":"霍夫曼树","keywords":"","body":"Huffman树 又称为最优二叉数，是一种带权路径最短的树。 哈夫曼编码就是哈夫曼树的应用,可以用来进行编码压缩. 关于霍夫曼编码（Huffman编码） Huffman是一种前缀编码；Huffman编码是建立在Huffman树的基础上进行的，因此为了进行Huffman编码，必须先构建Huffman树；树的路径长度是每个叶节点到根节点的路径之和；带权路径长度是（每个叶节点的路径长度*wi）之和；Huffman树是最小带权路径长度的二叉树； 构造Huffman树的过程： 将各个节点按照权重从小到大排序； 取最小权重的两个节点，并新建一个父节点作为这两个节点的双亲，双亲节点的权重为子节点权重之和，再将此父节点放入原来的队列； 重复(2)的步骤，直到队列中只有一个节点，此节点为根节点； 构造完Huffman树之后，就可以进行Huffman编码了，编码规则：左分支填0，右分支填1； Huffman解码过程：给定一个01串，将01串进行Huffman树，到叶子节点了就表明已经解码一个节点，然后再次遍历Huffman树； "},"Chapter02/forest.html":{"url":"Chapter02/forest.html","title":"树和森林与二叉树的相互转换","keywords":"","body":"树和森林与二叉树的相互转换 什么是森林？ 很容易想到，由树组成森林。 专业一点的定义是：若干棵互不相交的树的集合。 下面我们要用的是左孩子右兄弟的方法， 简单三步就能将树和二叉树相互转换。 树 -> 二叉树 加线。在所有的兄弟结点之间加一条线。 去线。树中的每个结点，只保留它与第一个孩子结点的连线， 删除其他孩子结点之间的连线。 调整。以树的根结点为轴心，将整个树调节一下（第一个孩子是结点的左孩子，兄弟转过来的孩子是结点的右孩子） 所以最终结果为： 二叉树 -> 树 知道了树转换为二叉树，那么二叉树转换为树就是个逆过程呗。 调整。将二叉树从左上到右下分为若干层。然后调整成水平方向。 加线。找到每一层节点在其上一层的父节点，加线。 去线。去除兄弟节点之间的连线。 所以最终结果为： 二叉树 -> 森林 在此我需要再次强调的是，根据孩子兄弟表示法，根节点是没有兄弟的。 前提：加入一棵二叉树的根节点有右孩子，则这棵二叉树能够转换为森林，否则转换为一棵树。 删除右孩子连线。 从根节点开始，若右孩子存在，则把与右孩子结点的连线删除。再查看分离后的二叉树，若其根节点的右孩子存在，则连续删除。直到所有这些根结点与右孩子的连线都删除为止。 将每棵分离后的二叉树转换为树。 所以最终结果为： "},"Chapter02/String.html":{"url":"Chapter02/String.html","title":"串","keywords":"","body":"串 （String） 串是由有限个字符组成的一种线性结构，其中每个字符都来自某个字符表（Alphabet）Σ，比如 ASCII 字符集或 Unicode 字符集。 串具有两个突出的特点：结构简单，规模庞大。 结构简单，一方面是线性结构，另一方面是指字符表规模不大，在某些应用问题中，字符表的规模甚至可能极小。以生物信息序列为例， 组成蛋白质（文本）的氨基酸（字符）只有约 20 种，而组成DNA序列（文本）的碱基（字符）则只有 4 种。 然而，这类文本的规模往往很大，其中每个字符都大量重复地出现，串中字符的重复率一般非常高。 这里我们将直接采用Java本身提供的String类 串模式匹配（String pattern matching） 在串文本的众多应用问题中，会反复涉及到一项非常基本的判断性操作： 给定串 T（称作主串）和串 P（称作模式串），T 中是否存在的某个子串与 P 相同？如果存在，找到该子串在 T 中的起始位置。 实际上，根据具体应用的不同，串匹配问题有多种形式： 有些场合属于串匹配检测（Pattern detection）问题：我们只关心是否存在匹配，而不关心具体的匹配位置。 有些场合则属于定位（Pattern location）问题：若经判断的确存在匹配，则还需要确定具体的匹配位置。 有些场合属于计数（Pattern counting）问题：倘若有多处匹配，统计出这些匹配子串的总数。 有些场合则属于枚举（Pattern enumeration）问题：在有多处匹配时，报告出所有匹配的具体位置。 比如，以上邮件过滤器的例子就属于检测型问题：一旦特征匹配，即可判定为垃圾邮件，从而直接删除，或者将其隔离以待用户确认，此时我们并不关心特征串的具体位置。 然而，反病毒系统的任务则属于枚举型问题：不仅必须在二进制代码中找出所有的病毒特征串，还需要报告它们的具体位置，以便修复。 蛮力算法 蛮力串匹配算法是最直接、直观的方法。 我们想象着将主串和模式串分别写在两条印有等间距方格的纸带上，主串对应的纸带固定，模式串的首字符与主串的首字符对齐，沿水平方向放好。主串的前m个字符将与模式串的m个字符两两对齐。 接下来，自左向右检查对齐的每一对字符：如果匹配，则转向下一对字符；如果失配，则说明在这个位置主串与模式串无法匹配，于是将模式串对应的纸带右移一个字符，然后从首字符开始重新对比。 若经过检查，当前的m个字符对都是匹配的，则匹配成功，并返回匹配子串的位置。 蛮力算法的具体实现 package other; public class PM_BruteForce { /* * 串模式匹配：蛮力算法 若返回位置i > length(T) - length(P)，则说明失配 否则，i为匹配位置 */ ////////////////////////////////////////////////////////////////////////// // T: 0 1 . . . i i+1 . . . i+j . . n-1 // --------------------|-------------------|------------ // P: 0 1 . . . j . . // |-------------------| ////////////////////////////////////////////////////////////////////////// public static int PM(String T, String P) { int i;// 模式串相对于主串的起始位置 int j;// 模式串当前字符的地址 for (i = 0; i = P.length()) break;// 找到匹配子串 } return (i); } } 在最坏情况下蛮力算法的运行时间为主串、模式串长度的乘积，因此只适用于小规模的串匹配应用。 如何实现文本编辑器中的查找功能？ 文本编辑器中的查找替换功能，我想你应该不陌生吧？比如，我们在 Word 中把一个单词统一替换成另一个，用的就是这个功能。 你有没有想过，它是怎么实现的呢？ 当然，你用上一节讲的 BF 算法和 RK 算法，也可以实现这个功能，但是在某些极端情况下，BF 算法性能会退化的比较严重， 而 RK 算法需要用到哈希算法，而设计一个可以应对各种类型字符的哈希算法并不简单。 对于工业级的软件开发来说，我们希望算法尽可能的高效，并且在极端情况下，性能也不要退化的太严重。 那么，对于查找功能是重要功能的软件来说，比如一些文本编辑器，它们的查找功能都是用哪种算法来实现的呢？有没有比 BF 算法和 RK 算法更加高效的字符串匹配算法呢？ 今天，我们就来学习 BM（Boyer-Moore）算法。它是一种非常高效的字符串匹配算法，有实验统计，它的性能是著名的KMP 算法的 3 到 4 倍。BM 算法的原理很复杂，比较难懂，学起来会比较烧脑，我会尽量给你讲清楚，同时也希望你做好打硬仗的准备。好，现在我们正式开始！ BF算法: 即暴力匹配算法，循环遍历匹配。 RK算法: 即根据哈希值进行匹配。假设主串长度为 m ，模式串长度为 n ，则只需计算主串中 m-n+1 个子串的哈希值，然后与模式串的哈希值相比即可。 哈希算法可以自定义。 比如使用字符集的个数作为几进制，然后将其转换成整数。 如果字符中只包含 a-z， 那么字符集的个数为 26 。则转换成 26 进制。 \"aab\" => ('a' - 'a') * 26 * 26 + ('a' - 'a') * 26 + ('b' - 'a') * 1 当有哈希冲突时，即当遇到不同的子串有相同哈希值时，再次与模式串的字符逐个比较是否相等。 BM算法: 从后向前匹配，效率比经典的 KMP 算法还要快 3~4 倍。 坏字符规则 当遇到不匹配的字符（称之为坏字符），在模式串中的下标为 si 。 如果模式串中该字符不存在，则直接向后移动一个模式串的长度。 如果存在，下标为 xi , 则移动 (si-xi) 位，使其跟主串的坏字符对应。 移动后，继续从模式串末尾开始匹配。 记录模式串中每个字符对应的 index ，重复的会被靠后的位置替代。 好后缀规则 当遇到不匹配的字符，将已经匹配过的字符（称之为好后缀）。 在模式串中查找是否能匹配整个好后缀，如有，则移动至对齐。 若没有，则在模式串中查找是否有前缀子串跟好后缀的后缀子串匹配，若有，则移动最长的前缀子串与其对应。若没有，则直接移动整个模式串。 后缀子串，最后一个字符跟其对齐，不包括首字符。abc，后缀子串为c，bc。 前缀子串，第一个字符跟其对齐，不包括末尾。abc，前缀子串为a，ab。 求好后缀的匹配串的位置 记录 suffix[k] = i ，k 表示后缀长度。subffix[1] = 1，表示最后一个字符在i=1开始是匹配的。如果不存在，则 suffix[k] = -1 。 比如字符串 \"dacda\", suffix 数组如下。 suffix[1] = 1 suffix[2] = 0 suffix[3] = -1 suffix[4] = -1 由于还需要判断是否有前缀子串与后缀的后缀子串匹配，所以还需记录是否有前缀子串，prefix[k] = 0，表示末尾 k 位数，有匹配的前缀子串。若为 -1 ，则没有。 根据 suffix 数组，如果其值为 0，则表示有前缀子串。 prefix[1] = false prefix[2] = true prefix[3] = false prefix[4] = false suffix 数组计算方法： 模式串中的 0-i(0 代码如下： // pattern-模式串，m-模式串长度。 var j = 0 while j = 0, pattern[m - k - 1] == pattern[i] { k += 1 suffix[k] = i i -= 1 } if i == -1 { prefix[k] = true } j += 1 } 坏字符规则与好后缀规则结合 取移动步数最大的。 完整算法： // p-模式串，m-模式串长度，s-主串，n-主串长度 function bm() { var i = 0 while i = 0 { if s[i+j] != p[j] { break; } j -= 1 } if j Int { // 坏字符后面一个j+1，后缀长度为m-() let k = m - 1 - j if suffix[k] != -1 { return j + 1 - suffix[k] } // 遍历所有后缀子串 var r = m + 2 while r KMP算法 从前往后匹配。 当遇到不匹配的字符时，下标为j，查找前面匹配的字符串的前缀与后缀匹配的最大长度值 k = next[j - 1] ，然后模式串 j = k 。 即前 m 个字符，前缀与后缀匹配的最大长度为 k 。记为 next[m-1] = k 。 计算 next 数组，采用动态规划。 假设 next[i] = k, 若 pattern[i+1] = patter[k], 则 next[i+1] = k+1 ; 若不相等，则从 next[k-1] 再开始计算。 // 模式串：pattern[]，n：模式串长度 var i = 0 var j = 1 var k = 0 next=[0] while j 0 { i = l k = l } else { next[j] = 0 j += 1 } } } 代码 // m-主串长度，n-模式串长度 var i = 0 var j = 0 while i = 1 { j = next[j - 1] } else { i += 1 } } } } return -1 "},"Chapter02/Trie.html":{"url":"Chapter02/Trie.html","title":"字典树","keywords":"","body":"字典树 字典树，也称为“前缀树”，是一种特殊的树状数据结构， 对于解决字符串相关问题非常有效。它能够提供快速检索， 主要用于搜索字典中的单词，在搜索引擎中自动提供建议， 甚至被用于IP的路由。 Trie 树，也叫字典树，专门做字符串匹配的数据结构，也可快速的做字符串前缀匹配。 它是一种多叉树，即把前缀相同的字符串合并在一起，根节点默认不存储字符。如下图所示： 以下是在字典树中存储三个单词“top”，“so”和“their”的例子： 这些单词以顶部到底部的方式存储，其中绿色节点“p”，“s”和“r”分别表示“top”，“thus”和“theirs”的底部。 插入 hi 、her、how 的插入过程如下： 查找 查找 hi 过程 从根节点 root 开始，查找其子节点有无 h，结果是有 查找 h 的子节点有无 i，结果也有，同时 i 是末尾节点，表示是个完整的字符串，匹配完成。 查找 he 过程 从根节点 root 开始，查找其子节点有无 h，结果是有 查找 h 的子节点有无 e，结果也有，但 e 不是末尾节点，表示其只是匹配到了前缀，并不能完全匹配到 he。 实现 前面提到过，Trie 树是多叉树，节点的取值范围在字符串的所有可能出现的字符集内。如字符串包含的只是 a-z，那么最多只需要 26 个子节点；如果字符串包含 a-zA-Z，那么最多需要 52 个自己点；以此类推。 当然，不是每个节点的子节点都会包含所有字符集，不存在的子节点设置为 null。 所以，假设字符集为 a-z ，总共 26 个小写字母，其数据结构如下： class TrieNode { var data: Character // 字符集 26 个小写字母 var children: [TrieNode?] var isEnd: Bool = false init(data: Character) { self.data = data children = [TrieNode]() // 初始化 26 个 var i = 0 while i 需要取子节点时，只需要算出 index = ch - 'a' ，然后根据 p.children[index] 取出即可，为 null 则表示不存在该 ch 字符。 Trie 树定义如下，包含插入和查找两个方法。 class Trie { let root = TrieNode(data: Character(\"/\")) // 插入 func insert(text: String) {} // 查找 func match(text: String) -> Bool {} // 计算index func indexOfChar(_ ch: Character) -> Int { let index = ch.toInt() - Character(\"a\").toInt() return index } } 插入 逐个遍历字符串，若当前节点不存在该字符对应的子节点，则生成新的节点插入；若存在，则沿着树的分支继续往下走。当遍历完成，将最后一个节点结束符 isEnd 置为 true。 func insert(text: String) { var p = root var i = 0 while i = 26 { assert(false, \"包含非法字符\") } if (index >= 0 && index 查找 逐个遍历字符串，若当前节点不存在该字符对应的子节点，则说明不匹配；若存在，则沿着树的分支继续往下走。当遍历完成，若最后一个节点是结束符，则完全匹配；否则只是前缀匹配。 func match(text: String) -> Bool { var p = root var i = 0 while i = 0 && index 效率 Trie 树构建时，需要遍历所有的字符串，因此时间复杂度为所有字符串长度总和 n，时间复杂度为 O(n)。 但是 Trie 树的查找效率很高，如果字符串长度为 k，那么时间复杂度 为 O(k)。 Trie 是一种以空间换时间的结构，当字符集较大时，会占用很多空间，同时如果前缀重合较少，空间会占用更多。所以其比较适合查找前缀匹配。 Tire树的应用： 串的快速检索 给出N个单词组成的熟词表，以及一篇全用小写英文书写的文章， 请你按最早出现的顺序写出所有不在熟词表中的生词。 在这道题中，我们可以用数组枚举，用哈希，用字典树，先把熟词建一棵树，然后读入文章进行比较， 这种方法效率是比较高的。 “串”排序 给定N个互不相同的仅由一个单词构成的英文名，让你将他们按字典序从小到大输出。用字典树进行排序， 采用数组的方式创建字典树，这棵树的每个结点的所有儿子很显然地按照其字母大小排序。 对这棵树进行先序遍历即可。 最长公共前缀 对所有串建立字典树，对于两个串的最长公共前缀的长度即他们所在的结点的公共祖先个数， 于是，问题就转化为求公共祖先的问题。 为什么 360 面试官说 Trie 树没用？ 说句实话，工程上来说，和hash比起来，trie就是没有用的东西。 这个可以说很深刻地表现出了一个学生和一个软件工程师的思维差异。 你可能很清楚，hash类的算法都有可能因为碰撞而退化， 而trie是稳定的复杂度，简直太招人喜欢了。 但是工程师99.9%的情况下都会用hash。因为，hash函数、map、set， 都有内置库，所有语言都有。这意味着一个功能用hash就是五行代码， 甚至体现不出用了hash，而用trie， 你要么先交一份trie开源库的分析选型比较报告来，要么自己撸一个， 附带着先写一千行的单元测试用例，再跑个压测。万一将来换个语言， 请从头再来。是的，就是这么简单，工程师才不会考虑碰撞， 他们甚至不关心rehash、hash实现这些细节， 许多语言内置的hash实现已经考虑了防止恶意碰撞了， 而随机碰撞，没有那么巧的事情。 写出简单可用能快速上线的代码要更重要。你看出来了， 学术关心理论最优，工程关心实践最优。你可能愤愤不平， 为啥标准库不把trie加进去？那你有没有想过这些问题呢？ 如果字符串不是常见的英文小写字母，而是unicode呢？ 如果这些字符串超级长，甚至有傻子拿来了一千个文本文件， 每个有100KB呢？ 字符串现在多得不能忍了，需要分布式处理， 你要怎么设计一个分布式的trie （要记得trie的节点分布可能是高度不均衡的）？所以， 工程看重什么也是有道理的。当然trie自然不是真的没用， 它支持前缀匹配，支持范围查找，这些都有独特的应用， 比如数据库里字符串类型的索引就经常实现为前缀树 （另一种常见实现自然就是hash）。 但说实话，不会有工程师认为这是两种可以相互竞争的技术。 如果面试官问的问题就如同题主描述的一样的话， 那可以直接告诉面试官scan一遍就结束了。 毕竟一共才10万个字符串，只是需要找给定的字符串是否有匹配。 从性能方面考虑， 给了这10万个字符串无论如何都要扫描一遍才知道是否存在或存在多少个匹配的字符串。 上来先排序的话我觉得都是浪费，qsort复杂度nlog(n)，也是得把所有字符串至少读一遍。 为了帮一个给定字符串找匹配而做一遍sort实在划不来。 当然了，我觉得面试官想问的肯定没这么简单， 面试官希望得到的回答一定是对这10万个字符串建立某种数据结构从而使得以后每次做这种字符串匹配查询都很快。 对于10万这个量级的数据，从工程角度而言naive一点的话还是直接扫一遍就完了，完全不用排序。 为什么呢？假设每个字符串是100个byte，10万个字符串也不过就是1000KB就是10MB。 现在稍微一般的处理器L3 cache size都是25MB了， 把这么多字符串存在L3上扫一遍又能慢到哪里去呢？ 要记住，字符串比较这种问题肯定是memory bound而不是cpu bound。 当然你要觉得scan一遍太简单粗暴想更fancy一点的， 那么可以把这10万个字符串sort一遍然后用个二分查找。 性能一定不差。如果你要觉得sort还不够fancy， 那我们就可以考虑一下hash或者trie了。 到底是hash好还是trie好呢？trie完全没用？ 我想说，脱离实际的workload讨论这种问题完全是在耍流氓。 首先一个问题就是这是静态的数据集还是动态的数据集？ 也就是说是否用户可以插入新的字符串到这个现有的字符串集合中？ 如果是静态的数据集，那么用hash吧。 数据集大小固定， 用perfect hashing或者cuckoohashing可以达到collision概率最小， 构建完hash table之后对给定字符串进行查询几乎肯定是一次搞定。 然而如果是个动态的数据集，而且新的字符串被不断的插入呢？ 这个时候用perfect/cuckoo hashing就不合适了。 这是因为perfect/cuckoo hashing的一个假设是workload size是提前预知的， 在这种假设前提下才能开出足够的空间在设定好hash function的情况下达到collision最小。 然而当有大量插入时，perfect/cuckoo hash table的性能就变得不稳定了， 因为每次insert都可能需要多次调整内部数据存放的位置。为了降低collision，只能增大hash table， 然而resize是perfect/cuckoo hashing最不擅长的，因为必须重新构建hash table，性能非常之差。 这时的解决方法是考虑extentiable hashing或者linear hashing。 这两种hash table能够很好的动态改变数据结构大小。当然相比于perfect/cuckoo hashing， collision也会相对较多，同时也会带来linear probing的代价。在动态数据集的情况下， 我们可以尝试考虑trie作为索引结构。 trie的好处在于能够方便的插入新的数据，且相比于btree-style来讲comparison的代价也更小， 毕竟每次只要compare一个byte就完了。而相比于insert，trie的性能也可能很有可能更高， 这是因为trie更加cache-friendly。 对于这种memory bound的问题中合理利用cache是非常重要的。 我也看到有些答主说trie会占用大量内存。如果出现这种情况， 那99%的可能是你只实现了leetcode上刷题需要用到的那种trie。 trie的内部结构可以进行高效的压缩， 比如有如果“a”->\"b\"->\"c\"这三个连续的节点 （a是b的父节点，b是c的父节点）， \"a\"与\"b\"这两个节点均只有唯一的子节点，那么就可以很轻易的把\"abc\"合并为一个节点。 在有insert的情况下，trie树也可以合理的split merge内部节点来节约空间。 相比于btree-style数据结构，trie的内存使用率高了一大截， 因为btree中多数节点也就是半满的状态，很难达到全满。 其实面试官问的这种开放式的问题是十分好回答的， 毕竟这个直接凭平时写代码用数据结构的经验就能回答了。 就说这道题吧， 扩展一下的话我们还可以跟面试官扯扯如何用SIMD（甚至GPU）做加速， 如何支持concurrent insert， 在数据量极大的情况下如何合理使用SSD/HDD，如何分布式存储等等。 问题实在太多，扯一个小时都没问题。 对了，那个KMP好像是做substring matching的，不是很适合。 对我来讲上次碰KMP估计都是本科上算法课的时候了， 面试中关于字典树的常见问题 计算字典树中的总单词数 打印存储在字典树中的所有单词 使用字典树对数组的元素进行排序 使用字典树从字典中形成单词 构建T9字典（字典树+ DFS ） "},"Chapter02/Heap.html":{"url":"Chapter02/Heap.html","title":"堆","keywords":"","body":"堆 堆就是用数组实现的二叉树，所有它没有使用父指针或者子指针。 堆根据堆属性来排序，堆属性决定了树中节点的位置。 堆是利用完全二叉树的结构来维护一组数据，然后进行相关操作， 一般的操作进行一次的时间复杂度在O(1)~O(logn)之间。 堆的常用方法： 构建优先队列 支持堆排序 快速找出一个集合中的最小值（或者最大值） 在朋友面前装逼 堆和普通树的区别 堆并不能取代二叉搜索树，它们之间有相似之处也有一些不同。我们来看一下两者的主要差别： 节点的顺序: 在二叉搜索树中，左子节点必须比父节点小，右子节点必须必比父节点大。但是在堆中并非如此。在最大堆中两个子节点都必须比父节点小，而在最小堆中，它们都必须比父节点大。 内存占用: 普通树占用的内存空间比它们存储的数据要多。你必须为节点对象以及左/右子节点指针分配额为是我内存。堆仅仅使用一个数据来村塾数组，且不使用指针。 平衡: 二叉搜索树必须是“平衡”的情况下，其大部分操作的复杂度才能达到O(log n)。你可以按任意顺序位置插入/删除数据，或者使用 AVL 树或者红黑树，但是在堆中实际上不需要整棵树都是有序的。我们只需要满足对属性即可，所以在堆中平衡不是问题。因为堆中数据的组织方式可以保证O(log n) 的性能。 搜索: 在二叉树中搜索会很快，但是在堆中搜索会很慢。在堆中搜索不是第一优先级，因为使用堆的目的是将最大（或者最小）的节点放在最前面，从而快速的进行相关插入、删除操作。 来自数组的树 用数组来实现树相关的数据结构也许看起来有点古怪，但是它在时间和空间山都是很高效的。 我们准备将上面的例子中的树这样存储： [ 10, 7, 2, 5, 1 ] 就这多！我们除了一个简单的数组以外，不需要任何额外的空间。 如果我们不允许使用指针，那么我们怎么知道哪一个节点是父节点，哪一个节点是它的子节点呢？问得好！节点在数组中的位置index 和它的父节点已经子节点的索引之间有一个映射关系。 如果 i 是节点的索引，那么下面的公式就给出了它的父节点和子节点在数组中的位置： parent(i) = floor((i - 1)/2) left(i) = 2i + 1 right(i) = 2i + 2 注意 right(i) 就是简单的 left(i) + 1。左右节点总是处于相邻的位置。 "},"Chapter02/BinaryHeap.html":{"url":"Chapter02/BinaryHeap.html","title":"二叉堆","keywords":"","body":"二叉堆 二叉堆是一种特殊的堆，二叉堆是完全二元树（二叉树）或者是近似完全二元树（二叉树）。 二叉堆有两种 最大堆：父结点的键值总是大于或等于任何一个子节点的键值； 最小堆：父结点的键值总是小于或等于任何一个子节点的键值。 二叉堆的根节点叫做堆顶 最大堆和最小堆的特点，决定了在最大堆的堆顶是整个堆中的最大元素；最小堆的堆顶是整个堆中的最小元素。 堆的自我调整 对于二叉堆，如下有几种操作： 插入节点 删除节点 构建二叉堆 这几种操作都是基于堆的自我调整。 下面让我们以最小堆为例，看一看二叉堆是如何进行自我调整的。 插入节点 二叉堆的节点插入，插入位置是完全二叉树的最后一个位置。比如我们插入一个新节点，值是 0。 这时候，我们让节点0的它的父节点5做比较，如果0小于5，则让新节点“上浮”和父节点交换位置。 继续用节点0和父节点3做比较，如果0小于3，则让新节点继续“上浮”。 继续比较，最终让新节点0上浮到了堆顶位置。 删除节点 二叉堆的节点删除过程和插入过程正好相反，所删除的是处于堆顶的节点。 比如我们删除最小堆的堆顶节点1。 这时候，为了维持完全二叉树的结构，我们把堆的最后一个节点10补到原本堆顶的位置。 接下来我们让移动到堆顶的节点10和它的左右孩子进行比较，如果左右孩子中最小的一个（显然是节点2）比节点10小，那么让节点10“下沉”。 继续让节点10和它的左右孩子做比较，左右孩子中最小的是节点7，由于10大于7，让节点10继续“下沉”。 这样一来，二叉堆重新得到了调整。 构建二叉堆 构建二叉堆，也就是把一个无序的完全二叉树调整为二叉堆，本质上就是让所有非叶子节点依次下沉。 我们举一个无序完全二叉树的例子： 首先，我们从最后一个非叶子节点开始，也就是从节点10开始。如果节点10大于它左右孩子中最小的一个，则节点10下沉。 接下来轮到节点3，如果节点3大于它左右孩子中最小的一个，则节点3下沉。 接下来轮到节点1，如果节点1大于它左右孩子中最小的一个，则节点1下沉。事实上节点1小于它的左右孩子，所以不用改变。 接下来轮到节点7，如果节点7大于它左右孩子中最小的一个，则节点7下沉。 节点7继续比较，继续下沉。 这样一来，一颗无序的完全二叉树就构建成了一个最小堆。 堆的代码实现 在撸代码之前，我们还需要明确一点： 二叉堆虽然是一颗完全二叉树,但它的存储方式并不是链式存储，而是顺序存储。换句话说，二叉堆的所有节点都存储在数组当中。 数组中，在没有左右指针的情况下，如何定位到一个父节点的左孩子和右孩子呢？ 像图中那样，我们可以依靠数组下标来计算。 假设父节点的下标是parent，那么它的左孩子下标就是 2 parent + 1；它的右孩子下标就是 2 parent + 2 。 比如上面例子中，节点6包含9和10两个孩子，节点6在数组中的下标是3，节点9在数组中的下标是7，节点10在数组中的下标是8。 7 = 3*2+1 8 = 3*2+2 刚好符合规律。 还有一个规律就是完全二叉树的的叶子节点占总长度的一半;比如一个长度为10的完全二叉树那么它的非叶子节点占5个,叶子节点就是总下表5开始的。 有了这个前提，下面的代码就更好理解了： 代码中有一个优化的点，就是父节点和孩子节点做连续交换时，并不一定要真的交换，只需要先把交换一方的值存入temp变量，做单向覆盖，循环结束后，再把temp的值存入交换后的最终位置。 堆的好处 二叉堆正是实现堆排序以及优先级队列的基础。 "},"Chapter02/Queue.html":{"url":"Chapter02/Queue.html","title":"队列","keywords":"","body":"队列 与栈一样，队列也是最基本的数据结构之一。 队列也是对象的一种容器，其中对象的插入和删除遵循“先进先出”（First-In-First-Out, FIFO）的原则--也就是说，每次删除的只能是最先插入的对象。 因此，我们可以想象成将对象追加在队列的后端，而从其前端摘除对象。 就这一性质而言，队列与栈堪称“孪生兄弟”。 比如，在银行等待接受服务时，顾客们就会排成一个队列--最先到达者优先得到服务。 再如，用球桶来乘放羽毛球时，所有的球也排成一个队列--你总是从一端将球取出，而从另一端把球放入。 队列ADT(AbstractDataType) 队列的抽象数据类型就是一个容器，其中的对象排成一个序列， 我们只能访问和取出排在最前端（Front）的对象， 只能在队列的尾部（Rear）插入新对象。正是按照这一规则， 才能保证最先被插入的对象首先被删除（FIFO）。 队列ADT 首先支持下面的两个基本方法： 操作方法 功能描述 enqueue(x) 将元素x 加到队列末端输入：一个对象输出：无 dequeue() 若队列非空，则将队首元素移除，并将其返回否则，报错输入：无输出：对象 size() 返回队列中当前包含的元素数目输入：无输出：非负整数 isEmpty() 检查队列是否为空输入：无输出：布尔标志 front() 若队列非空，则返回队首元素（但并不移除）否则，报错输入：无输出：队头对象 队列的应用十分广泛，无论是商店、剧院、机场还是银行、医院，凡是按照“先到的客户优先接受服务”原则的场合，都可以利用队列这一数据结构来编排相应的服务。 基于定长循环数组的队列的实现 面试中关于队列的常见问题 使用队列表示栈 对队列的前k个元素倒序 使用队列生成从1到n的二进制数 循环分配器 队列结构很适于用来实现循环分配器：按照环形次序反复循环， 为共享某一资源的一群客户（比如共享一个CPU 的多个应用程序） 做资源分配。 算法：RoundRobin { e = Q.dequeue(); Serve(e); Q.enqueue(e); } osephus 环 (约瑟夫环) 孩提时的你是否玩过“烫手山芋”游戏：一群小孩围成一圈， 有一个刚出锅的山芋在他们之间传递。其中一个孩子负责数数， 每数一次，拿着山芋的孩子就把山芋转交给右边的邻居。 一旦数到某个特定的数，拿着山芋的孩子就必须退出，然后重新数数。 如此不断，最后剩下的那个孩子就是幸运者。 通常，数数的规则总是从1 开始，数到k 时让拿着山芋的孩子出列， 然后重新从1 开始。Josephus问题可以表述为：n 个孩子玩这个游戏， 最后的幸运者是谁？ 为了解答这个问题，我们可以利用队列结构来表示围成一圈的n个孩子。 一开始，假定对应于队列首节点的那个孩子拿着山芋。 然后，按照游戏的规则，把“土豆”向后传递到第k个孩子 （交替进行k次dequeue()和k次enqueue()操作）， 并让她出队（dequeue()）。如此不断迭代，直到队长 （getSize()）为1。 /* * Josephus环 */ import dsa.*; import java.io.*; //模拟Josephus环 public class Josephus { // 利用队列结构模拟Josophus环 public static Object Josephus(Queue Q, int k) { if (Q.isEmpty()) return null; while (Q.getSize() > 1) {// 不断迭代 Q.Traversal();// 显示当前的环 for (int i = 0; i "},"Chapter02/PriorityQueue.html":{"url":"Chapter02/PriorityQueue.html","title":"优先队列","keywords":"","body":"优先队列 听这个名字就能知道，优先队列也是一种队列，只不过不同的是，优先队列的出队顺序是按照优先级来的；在有些情况下，可能需要找到元素集合中的最小或者最大元素，可以利用优先队列ADT来完成操作，优先队列ADT是一种数据结构，它支持插入和删除最小值操作（返回并删除最小元素）或删除最大值操作（返回并删除最大元素）； 这些操作等价于队列的enQueue和deQueue操作，区别在于，对于优先队列，元素进入队列的顺序可能与其被操作的顺序不同，作业调度是优先队列的一个应用实例，它根据优先级的高低而不是先到先服务的方式来进行调度； 如果最小键值元素拥有最高的优先级，那么这种优先队列叫作升序优先队列（即总是先删除最小的元素），类似的，如果最大键值元素拥有最高的优先级，那么这种优先队列叫作降序优先队列（即总是先删除最大的元素）；由于这两种类型时对称的，所以只需要关注其中一种，如升序优先队列； 优先队列的应用 数据压缩：赫夫曼编码算法； 最短路径算法：Dijkstra算法； 最小生成树算法：Prim算法； 事件驱动仿真：顾客排队算法； 选择问题：查找第k个最小元素； 等等等等.... 优先队列的实现比较 二叉堆 二叉堆） 基于二叉堆实现的简单降序优先队列 package datastructure.queue; import java.util.Arrays; /** * 降序优先队列 */ public class MaxPriorityQueue { private int size = 0; private int[] array; public MaxPriorityQueue() { array = new int[32]; } public MaxPriorityQueue(int size) { array = new int[size]; } public MaxPriorityQueue(int[] array) { this.size = array.length; this.array = array; synchronized (this.array) { for (int parentIndex = 0; parentIndex array[parentIndex]) { int temp = array[parentIndex]; array[parentIndex] = array[childIndex]; array[childIndex] = temp; } } } } public void add(int element) { if (size >= array.length) { System.out.println(\"resize\"); resize(); System.out.println(\"resize over\"); } array[size++] = element; upAdjust(); } public int pop() { if (size temp) { array[parentIndex] = array[childIndex]; parentIndex = childIndex; childIndex = 2 * parentIndex + 1; } else { break; } } array[parentIndex] = temp; } private void upAdjust() { int childIndex = this.size - 1; int parentIndex = (childIndex - 1) / 2; int temp = this.array[childIndex]; while (childIndex > 0 && temp > array[parentIndex]) { array[childIndex] = array[parentIndex]; childIndex = parentIndex; parentIndex = (childIndex - 1) / 2; } this.array[childIndex] = temp; } private void resize() { int newSize = this.size * 2; this.array = Arrays.copyOf(this.array, newSize); } public int size() { return this.size; } public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); MaxPriorityQueue queue = new MaxPriorityQueue(array); System.out.println(\"准备添加\"); queue.add(14); queue.add(11); queue.add(20); queue.add(12); System.out.println(\"添加完\"); int size = queue.size(); for (int i = 0; i java 中的优先队列 在Java中也实现了自己的优先队列java.util.PriorityQueue，与我们自己写的不同之处在于，Java中内置的为最小堆，然后就是一些函数名不一样，底层还是维护了一个Object类型的数组，大家可以戳戳看有什么不同，另外如果想要把最小堆变成最大堆可以给PriorityQueue传入自己的比较器 LeetCode相关题目整理 23. 合并K个排序链表 215. 数组中的第K个最大元素 239. 滑动窗口最大值 264. 丑数 II 295.数据流的中位数 347. 前K个高频元素 692. 前K个高频单词 "},"Chapter02/Stack.html":{"url":"Chapter02/Stack.html","title":"栈","keywords":"","body":"栈 栈是存放对象的一种特殊容器，在插入与删除对象时，这种结构遵循后进先出（Last-in-first-out，LIFO）的原则--也就是说，对象可以任意插入栈中，但每次取出的都是此前插入的最后一个对象。 比如一摞椅子，只能将最顶端的椅子移出，也只能将新椅子放到最顶端--这两种操作分别称作入栈（Push）和退栈（Pop）。 栈是最基本的数据结构之一，在实际应用中几乎无所不在。 例如，网络浏览器会将用户最近访问过的地址组织为一个栈： 用户每访问一个新页面，其地址就会被存放至栈顶； 而用户每次按下“Back”按钮，最后一个被记录下的地址就会被清除掉。 再如，当今主流的文本编辑器大都支持编辑操作的历史记录功能： 用户的编辑操作会被依次记录在一个栈中； 一旦出现误操作，用户只需按下“Undo”按钮， 即可撤销最近一次操作并回到此前的编辑状态。 由于栈的重要性，在Java 的java.util 包中已经专门为栈结构内建了一个类--java.util.Stack 栈ADT(AbstractDataType) 作为一种抽象数据类型，栈必须支持以下方法： 操作方法 功能描述 push(x) 将对象x 压至栈顶输入：一个对象输出：无 pop() 若栈非空，则将栈顶对象移除，并将其返回否则，报错输入：无输出：对象 getSize() 返回栈内当前对象的数目输入：无输出：非负整数 isEmpty() 检查栈是否为空输入：无输出：布尔标志 top() 若栈非空，则返回栈顶对象（但并不移除）否则，报错输入：无输出：栈顶对象 基于数组的简单实现 package datastructure.stack; public class FireStack { private int size; private Integer[] data; public FireStack(){ data = new Integer[10]; } public int getSize(){ return size; } public boolean isEmpty(){ return this.size == 0; } public void push(Integer element){ //考虑扩容 data[size++] = element; } public Integer pop(){ if (isEmpty()){ throw new IndexOutOfBoundsException(\"-1\"); } int result = data[--size]; data[size] = null; return result; } public Integer top(){ if (isEmpty()){ throw new IndexOutOfBoundsException(\"-1\"); } return data[size-1]; } public static void main(String[] args) { FireStack fireStack = new FireStack(); fireStack.push(0); fireStack.push(1); fireStack.push(2); fireStack.push(3); int length = fireStack.size; for (int i = 0; i 测试结果 3 2 1 0 面试中关于栈的常见问题 使用栈计算后缀表达式 对栈的元素进行排序 判断表达式是否括号平衡 括号匹配算法 Tip借助栈进行数组倒置 "},"Chapter02/MinStack.html":{"url":"Chapter02/MinStack.html","title":"最小栈","keywords":"","body":"最小栈 设计一个支持 push，pop，top 操作，并能在常数时间内检索到最小元素的栈。 push(x) -- 将元素 x 推入栈中。 pop() -- 删除栈顶的元素。 top() -- 获取栈顶元素。 getMin() -- 检索栈中的最小元素。 示例: MinStack minStack = new MinStack(); minStack.push(-2); minStack.push(0); minStack.push(-3); minStack.getMin(); --> 返回 -3. minStack.pop(); minStack.top(); --> 返回 0. minStack.getMin(); --> 返回 -2. package datastructure.stack; public class FireMinStack { private FireStack data; private FireStack minIndex; public FireMinStack(){ data = new FireStack(); minIndex = new FireStack(); } public int Size(){ return data.getSize(); } public boolean isEmpty(){ return data.isEmpty(); } public void push(Integer element){ if (data.isEmpty()){ minIndex.push(element); }else if(element 可能得变种 ginMin() 取出最小的元素而不是检索最小的元素 "},"Chapter02/Map.html":{"url":"Chapter02/Map.html","title":"映射","keywords":"","body":"映射（Map） 实际上，借助关键码直接查找数据元素并对其进行操作的这一形式，已经为越来越多的数据结构所采用，也成为现代数据结构的一个重要特征。 本文将要讨论的映射（Map）及后面要介绍的词典（Dictionary）结构， 就是其中最典型的例子， 它们对优先队列中利用关键码的思想做了进一步发挥和推广——不再只是可以读取或修改最小元素， 而是能够对任意给定的关键码进行查找，并修改相应的元素。 与优先队列一样，映射和词典中存放的元素也是一组由关键码和数据合成的条目。 映射要求不同条目的关键码互异 词典则允许多个条目拥有相同的关键码。 映射（Map）也是一种存放一组条目的容器。与优先队列一样，映射中的条目也是形如(key, value)的组合对象， 其中 key 为关键码对象，value 为具体的数据对象。 需要特别指出的是，在映射中，各条目的关键码不允许重复冗余。比如，若准备将某个学校所有学生的记录组织为一个映射结构， 则不能以年龄或班级作为关键码，因为不同记录的这些信息都有可能重复；反过来，通常学号都是学生的唯一标识，故可以将学号作为关键码。 映射的实现 映射的接口 package dsa.Map; import dsa.Iterator.Iterator; public interface Map { /* * 映射结构接口 */ // 查询映射结构当前的规模 public int getSize(); // 判断映射结构是否为空 public boolean isEmpty(); // 若映射中存在以key为关键码的条目，则返回该条目的数据对象；否则，返回null public Object get(Object key); // 若映射中不存在以key为关键码的条目，则插入条目(key, value)并返回null // 否则，将已有条目的数据对象替换为value，并返回原先的数据对象 public Object put(Object key, Object value); // 若映射中存在以key为关键码的条目，则删除之并返回其数据对象；否则，返回null public Object remove(Object key); // 返回映射中所有条目的一个迭代器 public Iterator entries(); } 判等器 由其 ADT 描述可知，映射结构必须能够比较任意一对关键码是否相等， 每个映射结构在被创建的时候，都需要指定某一具体标准， 以便进行关键码的比较。因此，为了实现映射结构， 首先必须实现这样的一个判等器（Equality tester）： package dsa.Map; public interface EqualityTester { /* * 判等器接口 */ public boolean isEqualTo(Object a, Object b);// 若a与b相等，则返回true；否则，返回false } package dsa.Map; public class EqualityTesterDefault implements EqualityTester { /* * 默认判等器 */ public EqualityTesterDefault() { } public boolean isEqualTo(Object a, Object b) { return (a.equals(b)); }// 使用Java提供的判等器 } 尽管上面的默认判等器也是通过标准的equals()方法实现的，但重要的是，利用这种模式，程序员完全可以编写出独立的通用判等器而无需触及对象内部的结构。 基于列表实现映射类 实现映射结构的最简单办法，就是直接将映射 M 中的条目组织成双向链表形式的一个列表 L。 这样，getSize()和 isEmpty()方法可以直接套用 List 接口中对应的方法。而在 get(key)、put(key, value)和 remove(key)方法中为了确定操作条目的位置，可以将 S 中的元素逐一与给定的 key 做对比。 package dsa.Map; import dsa.Iterator.Iterator; import dsa.Iterator.IteratorElement; import dsa.List.List; import dsa.List.List_DLNode; import dsa.PriorityQueue.Entry; import dsa.PriorityQueue.EntryDefault; import other.Position; public class Map_DLNode implements Map { /* * 基于列表实现映射结构 */ private List L;// 存放条目的列表 private EqualityTester T;// 判等器 // 构造方法 public Map_DLNode() { this(new EqualityTesterDefault()); } // 默认构造方法 public Map_DLNode(EqualityTester t) { L = new List_DLNode(); T = t; } /***************************** ADT方法 *****************************/ // 查询映射结构当前的规模 public int getSize() { return L.getSize(); } // 判断映射结构是否为空 public boolean isEmpty() { return L.isEmpty(); } // 若M中存在以key为关键码的条目，则返回该条目的数据对象；否则，返回null public Object get(Object key) { Iterator P = L.positions(); while (P.hasNext()) { Position pos = (Position) P.getNext(); Entry entry = (EntryDefault) pos.getElem(); if (T.isEqualTo(entry.getKey(), key)) return entry.getValue(); } return null; } // 若M中不存在以key为关键码的条目，则将条目(key, value)加入到M中并返回null // 否则，将已有条目的数据对象替换为value，并返回原先的数据对象 public Object put(Object key, Object value) { Iterator P = L.positions(); while (P.hasNext()) {// 逐一对比 Position pos = (Position) P.getNext();// 各个位置 Entry entry = (EntryDefault) pos.getElem();// 处的条目 if (T.isEqualTo(entry.getKey(), key)) {// 若发现key已出现在某个条目中，则 Object oldValue = entry.getValue();// 先保留该条目原先的数据对象 L.replace(pos, new EntryDefault(key, value));// 再替之以新数据对象 return oldValue;// 最后返回原先的数据对象。注意：返回null时的歧义 } } // 若此循环结束，说明key尚未在M中出现，因此 L.insertFirst(new EntryDefault(key, value));// 将新条目插至表首，并 return null;// 返回null标志 } // 若M中存在以key为关键码的条目，则删除之并返回其数据对象；否则，返回null public Object remove(Object key) { Iterator P = L.positions(); while (P.hasNext()) {// 逐一对比 Position pos = (Position) P.getNext();// 各个位置 Entry entry = (EntryDefault) pos.getElem();// 处的条目 if (T.isEqualTo(entry.getKey(), key)) {// 若发现key已出现在某个条目中，则 Object oldValue = entry.getValue();// 先保留该条目原先的数据对象 L.remove(pos);// 删除该条目 return oldValue;// 最后返回原先的数据对象。注意：返回null时的歧义 } } // 若此循环结束，说明key尚未在映射中出现，因此 return null;// 返回null标志 } // 返回M中所有条目的一个迭代器 public Iterator entries() { return new IteratorElement(L); }// 直接利用List接口的方法生成元素迭代器 } 上述实现虽然简单，但效率不高。为了执行其中的 get(key)、put(key, value)或 remove(key)方法，都需要扫描整个列表。 "},"Chapter02/Hash.html":{"url":"Chapter02/Hash.html","title":"散列表","keywords":"","body":"哈希表(Hash table) 散列表（Hash table）——将条目的关键码视作其在映射结构中的存放位置 散列表由两个要素构成：桶数组与散列函数 桶数组 散列表使用的桶数组（Bucket array ），其实就是一个容量为 N 的普通数组 A[0..N-1]，只不过在这里，我们将其中的每个单元都想象为一个“桶”（Bucket），每个桶单元里都可以存放一个条目。 另外我们还需要某一函数，将任意关键码转换为介于 0 与 N-1 之间的整数⎯⎯这个函数就是所谓的散列函数（Hash function）。 散列函数 为了将散列技术推广至一般类型的关键码，我们需要借助散列函数 h，将关键码 key映射为一个整数 h(key) ∈ [0..N-1]，并将对应的条目存放至第 h(key)号桶内，其中 N 为桶数组的容量。 如果将桶数组记作 A[ ]，这一技术就可以总结为“将条目 e = (key, value)存放至 A [ h ( key ) ] 中”。 反过来，为了查找关键码为 key 的条目，只需取出桶单元 A[h(key)]中存放的对象。因此，h(key)也被称作 e 的散列地址。 不过，若要兑现上述构思，还需要满足另一个条件—— h( )是一个单射，即不同的关键码key1 ≠ key2 必然对应于不同的散列地址h(key1) ≠ h(key2)。不幸的是，在绝大多数应用问题中，这一条件都很难满足。如果不同关键码的散列地址相同，我们就说该散列发生了冲（Collision）。 一个好的散列函数 h()必须兼顾以下两条基本要求： h( )应该尽可能接近单射； 对于任何关键码 key，h(key)的计算必须能够在 O(1)时间内完成。 关于散列函数的计算，Java有其特有的习惯。Java将h(key)的计算划分为两步： 将一般性的关键码key转换为一个称作“散列码”的整数； 然后再通过所谓的“压缩函数”将该整数映射至区间[0..N-1]内。 散列码 Java 可以帮助我们将任意类型的关键码 key 转换为一个整数，称作 key 的散列码（Hash code）。请注意，散列码距离我们最终所需的散列地址还有很大距离⎯⎯它不见得落在区间[0..N-1]内，甚至不见得是正的整数。 不过这并不要紧，在这一阶段我们最关心的是：各关键码的散列码之间，应尽可能地减少冲突。显然，要是在这一阶段就发生冲突，后面的冲突就无法避免。 此外，从判等器的判等效果来看，散列码必须与关键码对象相互一致：被判等器 EqualityTester 判定为相等的两个关键码，对应的散列码也应该相等。 Java 的通用类 Object 提供了一个默认的散列码转换方法 hashCode()，利用它可以将任意对象实例映射为“代表”该对象的某个整数。具体来说，hashCode()方法的返回值是一个 32 位 int 型整数（实际上，这一默认 hashCode()方法所返回的不过就是对象在内存中的存储地址）。 遗憾的是，这一看似再自然不过的方法，实际上存在着严重的缺 陷，因此我们在使用时需格外小心。 比如，这种散列码的转换办法对字符串型关键码就极不适用。若两个字符串对象完全相等，本应该将它们转换为同一散列码，但由于它们的内存地址不同，由 hashCode()得到的散列码将绝对不会一样。 实际上，在实现 String 类时，Java 已经将 Object 类的 hashCode()方法改写为一种更加适宜于字符串关键码的方法。 压缩函数 模余法 最简单的压缩办法，就是取 N 为素数，并将散列码 i 映射为： ** | i | mod N ** 之所以将 N 选取为素数，是为了最大程度地将散列码均匀地映射至[0..N-1]区间内。 比如，对于散列码集合{200, 205, 210, 215, …, 690, 695, 700}，若选取 N = 100，则其中的每个散列码都会与另外的至少四个关键码相冲突；而若改用 N = 101，则不会有任何冲突。 MAD 法 采用一种将乘法（Mutiply）、加法（Add）和除法（Divide）结合起来的方法，该方法也因此得名。具体来说，对于散列码 i，MAD 法会将 i 映射为： ** | a×i + b | mod N ** 其中 N 仍为素数，a>0，b>0，a mod N ≠ 0，它们都是在确定压缩函数时随机选取的常数。 基于散列表实现映射类 我们给出基于散列表实现的映射结构： package dsa.Map; import dsa.Iterator.Iterator; import dsa.Iterator.IteratorElement; import dsa.List.List; import dsa.List.List_DLNode; import dsa.PriorityQueue.Entry; public class Map_HashTable implements Map { /* * 基于散列表实现的映射结构 采用分离链策略解决冲突 */ private Map[] A;// 桶数组，每个桶本身也是一个（基于列表实现的）映射结构 private int N;// 散列表长 private final double maxLemda = 0.75;// 装填因子上限 private int size;// 映射结构的规模 private EqualityTester T;// 判等器 // 默认构造方法 public Map_HashTable() { this(0, new EqualityTesterDefault()); } // 构造方法 public Map_HashTable(int n, EqualityTester t) { T = t; N = p(n);// 桶数组容量取为不小于n的最小素数 A = new Map[N]; for (int i = 0; i n) n = 3; n = n | 1;// 奇数化 while (!prime(n)) n += 2; return n; } /***************************** ADT方法 *****************************/ // 查询映射结构当前的规模 public int getSize() { return size; } // 判断映射结构是否为空 public boolean isEmpty() { return 0 == size; } // 若M中存在以key为关键码的条目，则返回该条目的数据对象；否则，返回null public Object get(Object key) { return A[h(key)].get(key); } // 若M中不存在以key为关键码的条目，则将条目(key, value)加入到M中并返回null // 否则，将已有条目的数据对象替换为value，并返回原先的数据对象 public Object put(Object key, Object value) { Object oldValue = A[h(key)].put(key, value); if (null == oldValue) {// 若插入的条目未出现于原散列表中，则 size++;// 更新规模记录 if (size > N * maxLemda) rehash();// 若装填因子过大，则重散列 } return oldValue; } // 若M中存在以key为关键码的条目，则删除之并返回其数据对象；否则，返回null public Object remove(Object key) { Object oldValue = A[h(key)].remove(key); if (null != oldValue) size--; return oldValue; } // 返回M中所有条目的一个迭代器 // 将各桶对应的映射结构的迭代器串接起来，构成整体的迭代器 public Iterator entries() { List L = new List_DLNode(); for (int i = 0; i 装填因子 对于散列表的性能而言，装填因子λ = n/N是最重要的影响因素。 如果λ > 1，则冲突在所难免。 实际上，关于散列表平均复杂度的分析结果指出： 采取分离链策略时应该保持λ 采取开放定址策略时则应该保持λ 这些都得到了实验统计的证明。 若采用分离链策略，则在发生冲突的桶中，对条目的查找将退化为对链表的查找。因此，随着λ不断接近于 1，发生冲突的概率也将不断接近于 100%，从而导致更多的时间消耗于对链表的查找，使得各种操作的效率下降。 当采用开放定址策略时，随着λ超过 0.5 并不断提高，条目在桶数组中聚集的程度将急速加剧，于是，需要经过越来越多次的探测才能完成一次查找。 重散列 通常都采用重散列的方法⎯⎯将所有条目全部取出，将桶数组的规模 加倍，然后将各条目重新插入其中。 例如，Java 内建的 java.util.HashMap 类实现了映射结构的 ADT，在创建该类的对象时，程序员可以指定装填因子的上限（默认设置为 0.75）。一旦装填因子超出这一范围，java.util.HashMap会自动进行重散列（Rehashing）。 面试中关于哈希结构的常见问题 在数组中查找对称键值对 追踪遍历的完整路径 查找数组是否是另一个数组的子集 检查给定的数组是否不相交 "},"Chapter02/Dictionary.html":{"url":"Chapter02/Dictionary.html","title":"词典","keywords":"","body":"词典 与前面介绍的映射结构一样，词典结构也是用来存放条目对象的一种容器，不过，词典与映射之间有一个非常重要的差别——词典不再要求其中各条目的关键码互异。 这一点与我们日常使用的纸介质词典类似，不少单次都具有多种解释，每一种解释分别对应于一个词条。因此，我们往往将词典中的条目直接称作“词条”。 总体而言，词典可以分为两大类：无序词典和有序词典。 顾名思义，前一类词典中存放的条目无所谓次序，我们只能（利用某一判等器）比较一对条目（的关键码）是否相等；而在后一类词典所存放的条目之间，则（根据某一比较器）定义了某种全序关系，因此也相应地能够支持 first()、last()、prev()和 succ()之类的方法。 无序词典 操作方法 功能描述 find(key) 若词典中存在以 key 为关键码的条目，则返回该条目的数据对象 ；否则，返回 null输入：一个关键码对象输出：条目对象 findAll(key) 若词典中存在以 key 为关键码的条目，则返回这些条目组成的迭代器 ；否则，返回 null输入：一个关键码对象输出：条目对象的迭代器 insert(key, value) 插入条目(key, value)，并返回该条目输入：一个关键码对象和一个数据对象输出：条目对象 remove(key) 若词典中存在以 key 为关键码的条目，则将摘除其中的一个并返回；否则，返回 null输入：一个关键码对象输出：条目对象 entries() 回词典中所有关键码对象的一个迭代器输入：无输出：条目对象的迭代器 getSize() 报告词典的规模，即其中元素的数目输入：无输出：非负整数 isEmpty() 判断词典是否为空输入：无输出：布尔标志 无序词典的接口 package dsa.Dictionary; import dsa.Iterator.Iterator; import dsa.PriorityQueue.Entry; public interface Dictionary { /* * （无序）词典结构接口 */ // 查询词典结构当前的规模 public int getSize(); // 判断词典结构是否为空 public boolean isEmpty(); // 若词典中存在以key为关键码的条目，则返回其中的一个条目；否则，返回null public Entry find(Object key); // 返回由关键码为key的条目组成的迭代器 public Iterator findAll(Object key); // 插入条目(key, value)，并返回该条目 public Entry insert(Object key, Object value); // 若词典中存在以key为关键码的条目，则将摘除其中的一个并返回；否则，返回null public Entry remove(Object key); // 返回词典中所有条目的一个迭代器 public Iterator entries(); } 基于列表实现的无序词典 package dsa.Dictionary; import dsa.Iterator.Iterator; import dsa.Iterator.IteratorElement; import dsa.List.List; import dsa.List.List_DLNode; import dsa.Map.EqualityTester; import dsa.Map.EqualityTesterDefault; import dsa.PriorityQueue.Entry; import dsa.PriorityQueue.EntryDefault; import other.Position; public class Dictionary_DLNode implements Dictionary { /* * 基于列表实现（无序）词典结构 */ private List L;// 存放条目的列表 private EqualityTester T;// 判等器 // 构造方法 public Dictionary_DLNode() { this(new EqualityTesterDefault()); } // 默认构造方法 public Dictionary_DLNode(EqualityTester t) { L = new List_DLNode(); T = t; } /***************************** ADT方法 *****************************/ // 查询词典结构当前的规模 public int getSize() { return L.getSize(); } // 判断词典结构是否为空 public boolean isEmpty() { return L.isEmpty(); } // 若词典中存在以key为关键码的条目，则返回其中的一个条目；否则，返回null public Entry find(Object key) { Iterator P = L.positions(); while (P.hasNext()) { Position pos = (Position) P.getNext(); Entry entry = (EntryDefault) pos.getElem(); if (T.isEqualTo(entry.getKey(), key)) return entry; } return null; } // 返回由关键码为key的条目组成的迭代器 public Iterator findAll(Object key) { List list = new List_DLNode(); Iterator P = L.positions(); while (P.hasNext()) { Position pos = (Position) P.getNext(); Entry entry = (EntryDefault) pos.getElem(); if (T.isEqualTo(entry.getKey(), key)) list.insertLast(entry); } return new IteratorElement(list); } // 插入条目(key, value)，并返回该条目 public Entry insert(Object key, Object value) { Entry entry = new EntryDefault(key, value);// 创建新条目 L.insertFirst(entry);// 将新条目插至表首，并 return entry;// 返回null标志 } // 若词典中存在以key为关键码的条目，则将摘除其中的一个并返回；否则，返回null public Entry remove(Object key) { Iterator P = L.positions(); while (P.hasNext()) {// 逐一对比 Position pos = (Position) P.getNext();// 各个位置 Entry entry = (EntryDefault) pos.getElem();// 处的条目 if (T.isEqualTo(entry.getKey(), key)) {// 若发现key已出现在某个条目中，则 Entry oldEntry = entry;// 先保留该条目 L.remove(pos);// 删除该条目 return oldEntry;// 最后返回原先的条目 } } // 若此循环结束，说明key尚未在词典中出现，因此 return null;// 返回null标志 } // 返回词典中所有条目的一个迭代器 public Iterator entries() { return new IteratorElement(L); }// 直接利用List接口的方法生成元素迭代器 } 有序词典 上面所说的，基于无序列表实现的词典结构非常适用于解决网络访问日志之类的应用问题，这类问题的共同特点是：插入操作频繁，查找、删除操作却极少进行。 另外一些问题则正好相反，它们要求频繁地进行查询，但插入、删除操作相对更少，这方面的例子包括在线电话簿、订票系统等。 有序词典的 ADT 从 ADT 的角度，有序词典可以看作是无序词典的扩充，也就是说， 只需在无序词典 ADT 的基础上再增加以下操作： 操作方法 功能描述 find(key) 若词典中存在以 key 为关键码的条目，则返回该条目的数据对象 ；否则，返回 null输入：一个关键码对象输出：条目对象 last( ) 若词典非空，则返回其中关键码最大的条目；否则，返回 null输入：无输出：条目对象 successors(key) 返回由关键码不小于 key 的条目依非降序组成的迭代器输入：一个关键码对象输出：条目对象的迭代器 predecessors(key) 返回由关键码不大于 key 的条目依非升序组成的迭代器输入：一个关键码对象输出：条目对象的迭代器 有序词典接口 package dsa.Dictionary; import dsa.Iterator.Iterator; import dsa.PriorityQueue.Entry; public interface SortedDictionary extends Dictionary { /* * 有序词典接口 */ // 若词典非空，则返回其中关键码最小的条目；否则，返回null public Entry first(); // 若词典非空，则返回其中关键码最大的条目；否则，返回null public Entry last(); // 返回由关键码不小于key的条目依非降序组成的迭代器 public Iterator successors(Object key); // 返回由关键码不大于key的条目依非升序组成的迭代器 public Iterator predecessors(Object key); } 这里利用可扩充数组实现有序查找表，并在此基础上利用二分查找算法 package dsa.Dictionary; import dsa.Iterator.Iterator; import dsa.Iterator.IteratorElement; import dsa.List.List; import dsa.List.List_DLNode; import dsa.PriorityQueue.Comparator; import dsa.PriorityQueue.ComparatorDefault; import dsa.PriorityQueue.Entry; import dsa.PriorityQueue.EntryDefault; import dsa.Vector.Vector; import dsa.Vector.Vector_ExtArray; public class SortedDictionary_ExtArray implements SortedDictionary { /* * 基于有序查找表实现的有序词典 */ Vector S;// 有序查找表 Comparator C;// 比较器 // 默认构造方法 public SortedDictionary_ExtArray() { this(new ComparatorDefault()); } // 构造方法 public SortedDictionary_ExtArray(Comparator comp) { S = new Vector_ExtArray(); C = comp; } /**************************** 辅助方法 ****************************/ // 二分查找 // 返回值可能是命中元素的秩，也可能是key可以插入的秩 // 具体如何，需要进一步检查 // 不变性：若将key按照返回的秩插入有序向量，向量依然有序 private static int binSearch(Vector s, Comparator c, Object key, int lo, int hi) { if (lo > hi) return lo;// 递归基，查找失败 int mi = (lo + hi) >> 1;// 取中值 Entry e = (Entry) s.getAtRank(mi);// 居中的条目 int flag = c.compare(key, e.getKey());// 比较关键码 if (flag 0) return binSearch(s, c, key, mi + 1, hi);// 转向右半区间 else return mi;// 命中 } /**************************** 无序词典ADT方法 ****************************/ // 查询词典结构当前的规模 public int getSize() { return S.getSize(); } // 判断词典结构是否为空 public boolean isEmpty() { return S.isEmpty(); } // 若词典中存在以key为关键码的条目，则返回其中的一个条目；否则，返回null public Entry find(Object key) { int k = binSearch(S, C, key, 0, S.getSize() - 1);// 查找关键码为key的条目 if (0 > k || k >= S.getSize() || (0 != C.compare(key, ((Entry) S.getAtRank(k)).getKey()))) return null;// 若这样的条目不存在，则返回失败标志 return (Entry) S.getAtRank(k); } // 返回由关键码为key的条目组成的迭代器 public Iterator findAll(Object key) { List L = new List_DLNode();// 创建一个链表L int k = binSearch(S, C, key, 0, S.getSize() - 1);// 查找关键码为key的条目 if (0 > k || k >= S.getSize() || (0 != C.compare(key, ((Entry) S.getAtRank(k)).getKey()))) return new IteratorElement(L);// 若这样的条目不存在，则返回空迭代器 L.insertFirst(S.getAtRank(k));// 将e插入L中 int lo = k;// 从S[k-1]开始 while (0 k || k >= S.getSize() || (0 != C.compare(key, ((Entry) S.getAtRank(k)).getKey()))) return null;// 若这样的条目不存在，则返回失败标志 return (Entry) S.removeAtRank(k); } // 返回词典中所有条目的一个迭代器 public Iterator entries() { List L = new List_DLNode(); for (int i = 0; i k || k >= S.getSize() || (0 != C.compare(key, ((Entry) S.getAtRank(k)).getKey()))) return new IteratorElement(L);// 若这样的条目不存在，则返回空迭代器 while (0 ++k)// 将后继的所有元素依次 L.insertLast(S.getAtRank(k));// 插入L中 return new IteratorElement(L);// 由L创建迭代器，返回之 } // 返回由关键码不大于key的条目依非升序组成的迭代器 public Iterator predecessors(Object key) { List L = new List_DLNode();// 创建一个链表L int k = binSearch(S, C, key, 0, S.getSize() - 1);// 查找关键码为key的条目 if (0 > k || k >= S.getSize() || (0 != C.compare(key, ((Entry) S.getAtRank(k)).getKey()))) return new IteratorElement(L);// 若这样的条目不存在，则返回空迭代器 while (S.getSize() > ++k)// 从S[k-1]开始向后搜索，直至符合要求的、秩最大的元素 if (0 != C.compare(key, ((Entry) S.getAtRank(k)).getKey())) break; while (0 "},"Chapter02/Graph.html":{"url":"Chapter02/Graph.html","title":"图","keywords":"","body":"图 弥诺陶洛斯（Minotaur）是希腊神话中半人半牛的怪物，它藏身于一个精心设计的迷宫之中。 这个迷宫的结构极其复杂，一般人一旦进入其中，都休想走出来。 不过，在公主阿里阿德涅（Ariadne）的帮助下，古希腊英雄特修斯（Theseus）还是想出了一个走出迷宫的方法。 特修斯带上一团线去闯迷宫，在入口处，他将绳线的一头绑在门上。 然后，他不断查找迷宫的各个角落，而绳线的另一头则始终抓在他的手里，跟随他穿梭于蜿蜒曲折的迷宫之中。 借助如此简单的工具，特修斯不仅终于找到了怪物并将其杀死，而且还带着公主轻松地走出了迷宫。 特修斯之所以能够成功，关键在于他借助绳线来掌握迷宫内各个通道之间的联接关系， 而在很多的问题中，掌握类似的信息都是至关重要的。 通常，这类信息所反映的都是一组对象之间的二元关系， 比如城市交通图中联接于不同点之间的街道，或 Internet 中联接于两个 IP 之间的路由等。 在某种程度上，我们前面所讨论过的树结构也可以携带和表示这种二元关系，只不过树结构中的这类关系仅限于父、子节点之间。 然而在一般情况下，我们所需要的二元关系往往是定义在任意一对对象之间。 实际上，这样的二元关系恰恰正是图论（Graph theory ） 图是数据结构中重要内容。相比于线性表与树，图的结构更为复杂。 在线性表的存储结构中，数据直接按照前驱后继的线性组织形式排列。 在树的结构中，数据节点以层的方式排列，节点与节点之间是一种层次关系。 但是，在图的结构中数据之间可以有任意关系，这就使得图的数据结构相对复杂。 定义 图（Graph）是由顶点的有穷非空集合和顶点之间边的集合组成，通常表示为：G（V，E），其中， G表示一个图，V是图G中顶点的集合，E是图G中边的集合。 例如：下图所示图 上图中，共有V0，V1，V2，V3 4个顶点，4个顶点之间共有5条边。 当线性表没有数据节点时，线性表为空表。 树中没有节点时，树为空树。 但是，在图中不允许没有顶点，但是可以没有边。 图（Graph）可以表示为 G = (V, E) 其中集合 V 中的对象称作顶点（Vertex） 而集合 E 中的每一元素都对应于 V 中某一对顶点⎯⎯说明这两个顶点之间存在某种关系⎯⎯称作边（Edge） 这里还约定 V 和 E 都是有限集，通常记 n = |V|、m = |E|。 对于图的定义，我们需要明确几个注意的地方： 线性表中我们把数据元素叫元素，树中叫结点，在图中数据元素我们则称之为顶点(Vertex)。 线性表可以没有数据元素，称为空表，树中可以没有结点，叫做空树，而图结构在咱国内大部分的教材中强调顶点集合V要有穷非空。 线性表中，相邻的数据元素之间具有线性关系，树结构中，相邻两层的结点具有层次关系，而图结构中，任意两个顶点之间都可能有关系，顶点之间的逻辑关系用边来表示，边集可以是空的。 无向边 若顶点 x 和 y 之间的边没有方向，则称该边为无向边(x,y)，(x,y) 与 (y,x) 意义相同，表示x和y之间有连接。 下图所示图中的边均为无向边。 有向边 有向边：若顶点 x 和 y 之间的边有方向，则称该边为有向边， 与 表示的意义是不同的，表示从x连接到y，x称为尾，y称为头。表示从y连接到x，y称为尾，x称为头。 下图所示图中的边为有向边。 无向图 若图中任意两个顶点之间的边均是无向边，则称该图为无向图。 有向图 若图中任意两个顶点之间的边均是有向边，则称该图为有向图。 混合图 顶点与顶点的度 顶点的度：顶点V的度是和V相关联的边的数目，记为TD(V)。 上图中，V0顶点的度为3。 入度：以顶点v为头的边的数目，记为ID(V)。 上图中，V0的入度为1。 出度：以顶点v为尾的边的数目，记为OD(V)。 上图中，V0的出度为2。 顶点的度 = 入度 + 出度。 即TD(V) = ID(V) + OD(V)。 邻接 邻接是两个顶点之间的一种关系。如果图包含（u,v），则称顶点 v 与顶点 u 邻接。 在无向图中，这也暗示了顶点 u 也与顶点 v 邻接。换句话说，在无向图中邻接关系是对称的。 路径 在图中，依次遍历顶点序列之间的边所形成的轨迹。 例如：在下图中依次访问顶点 V0 、V3 和 V2 ，则构成一条路径。 完全图 每个顶点都与其他顶点相邻接的图。 无向完全图 在无向图中，如果任意两个顶点之间都存在边，则称该图为无向完全图。 （含有n个顶点的无向完全图有(n×(n-1))/2条边） 下图为无向完全图 有向完全图 在有向图中，如果任意两个顶点之间都存在方向互为相反的两条边，则称该图为有向完全图。 （含有 n 个顶点的有向完全图有 n×(n-1) 条边） 下图为有向完全图。 通路、环路及可达分量 通路——所谓图中的一条通路或路径（Path），就是由（不一定互异的）m+1 个顶点与 m 条边交替构成的一个序列ρ = {v 0 , e 1 , v 1 , e 2 , v 2 , …, e m , v m }，m ≥ 0，而且 e i = (v i-1 , v i )，1 ≤ i ≤ m。 环路—— 长度 m ≥ 1 的路径，若第一个顶点与最后一个顶点相同，则称之为环路（Cycle）。 可达分量——对于指定的顶点 s，从 s 可达的所有顶点所组成的集合，称作 s 在 G 中对应的可达分量，记作 V r (G, s)。 连通图 在无向图 G 中，如果从顶点 v 到顶点 v' 有路径，则称 v 和 v' 是连通的。 如果对于图中任意两个顶点 vi 、vj ∈E， vi，和vj都是连通的，则称 G 是连通图，否则图为非连通图。 例如：图4.1所示图，图中顶点A、B、C、D是连通的，但是其中任一顶点与顶点E或者顶点F之间没有路径， 因此图4.1中所示的图为非连通图。 若添加顶点B与顶点F之间的邻接边，则图变为连通图，如下图所示： 树、森林和连通图 无向图 G = (V, E)： 若 G 中不含任何环路，则称之为森林（Forest）。 连通的森林称作树（Tree）。 不难看出，森林中的每一连通分量都是一棵树，反之亦然。 设 G 为由 n 个顶点与 m 条边组成一幅无向图： 若 G 是连通的，则 m ≥ n-1； 若 G 是一棵树，则 m = n-1； 若 G 是森林，则 m ≤ n-1。 面试中关于图的常见问题 实现广度和深度优先搜索 检查图是否为树 计算图的边数 找到两个顶点之间的最短路径 "},"Chapter02/GraphStorageStructure.html":{"url":"Chapter02/GraphStorageStructure.html","title":"图存储结构","keywords":"","body":"图存储结构 数组存储 图的数组存储方式也称为邻接矩阵存储。 图中的数据信息包括：顶点信息和描述顶点之间关系的边的信息，将这两种信息存储在数组中即为图的数组存储。 首先，创建顶点数组，顶点数组中存储的是图的顶点信息，采用一维数组的方式即可存储所有的顶点信息。存储图中边的信息时，由于边是描述顶点与顶点之间关系的信息，因此需要采用二维数组进行存储。 定义 设图G有n个顶点，则邻接矩阵是一个n X n的方阵A，定义为： 其中，或者(Vi , Vj,)表示顶点Vi与顶点Vj邻接。wi,j表示边的权重值。 例如：下图所示的无向图，采用数组存储形式如下。 注：图中的数组存储方式简化了边的权值为1。 无向图的数组存储主要有以下特性 （1）顶点数组长度为图的顶点数目n。边数组为n X n的二维数组。 （2）边数组中，A[i][j] =1代表顶点i与顶点j邻接，A[i][j] = 0代表顶点i与顶点j不邻接。 （3）在无向图中。由于边是无向边，因此顶点的邻接关系是对称的，边数组为对称二维数组。 （4）顶点与自身之间并未邻接关系，因此边数组的对角线上的元素均为0。 （5）顶点的度即为顶点所在的行或者列1的数目。例如：顶点V2的度为3，则V2所在行和列中的1的数目为3。 当图为有向图时，图的数组存储方式要发生变化。 例如：下图所示的有向图，采用数组存储形式如下。 有向图的数组存储主要有以下特性 （1）顶点数组长度为图的顶点数目n。边数组为n X n的二维数组。 （2）边数组中，数组元素为1，即A[i][j] = 1,代表第i个顶点与第j个顶点邻接，且i为尾，j为头。 A[i][j] = 0代表顶点与顶点不邻接。 （3）在有向图中，由于边存在方向性，因此数组不一定为对称数组。 （4）对角线上元素为0。 （5）第i行中，1的数目代表第i个顶点的出度。例如：顶点V1的出度为2，则顶点V1所在行的1的数目为2。 （6）第j列中，1的数目代表第j个顶点的入度。例如：V3的入度为1，则V3所在列中1的数目为1。 数组存储方式优点 数组存储方式容易实现图的操作。例如：求某顶点的度、判断顶点之间是否有边（弧）、找顶点的邻接点等等。 数组存储方式缺点 采用数组存储方式，图若有n个顶点则需要n2个单元存储边(弧)，空间存储效率为O(n2)。 当顶点数目较多，边数目较少时，此时图为稀疏图，这时尤其浪费空间。 例如：下图中有9个顶点，边数为10，需要9X9的二维数组， 而实际存储边信息空间只有10，造成空间浪费。 上图所示无向图的存储数组： 邻接表 当使用数组存储时，主要有以下三个问题： （1）对于一个图，若图中的顶点数目过大，则无法使用邻接矩阵进行存储。因为在分配数组内存时可能会导致内存分配失败。 （2）对于某些稀疏图（即顶点数目多，边数目少），创建的数组大小很大，而真正存储的有用信息又很少，这就造成了空间上的浪费。 （3）有时两个点之间不止存在有一条边，这是用邻接矩阵就无法同时表示两条以上的边。 针对以上情况，提出了一种特殊的图存储方式，让每个节点拥有的数组大小刚好就等于它所连接的边数， 由此建立一种邻接表的存储方式。 邻接表存储方法是一种数组存储和链式存储相结合的存储方法。 在邻接表中，对图中的每个顶点建立一个单链表，第i个单链表中的结点依附于顶点Vi的边（对有向图是以顶点Vi为尾的弧）。 链表中的节点称为表节点，共有3个域，具体结构见下图： 表结点由三个域组成，adjvex存储与Vi邻接的点在图中的位置，nextarc存储下一条边或弧的结点，data存储与边或弧相关的信息如权值。 除表节点外，需要在数组中存储头节点，头结点由两个域组成，分别指向链表中第一个顶点和存储Vi的名或其他信息。具体结构如下图： 其中，data域中存储顶点相关信息，firstarc指向链表的第一个节点。 无向图采用邻接表方式存储 例如：下图所示的无向图采用邻接表存储。 采用邻接表方式存储上图中的无向图，绘图过程中忽略边节点的info信息，头结点中的data域存储顶点名称。 以V1顶点为例，V1顶点的邻接顶点为V2、V3、V4，则可以创建3个表节点，表节点中adjvex分别存储V2、V3、V4的索引1、2、3，按照此方式，得到的邻接表为： 无向图的邻接表存储特性： （1）数组中头节点的数目为图的顶点数目。 （2）链表的长度即为顶点的度。例如：V1顶点的度为3，则以V1为头节点的链表中表节点的数目为3。 有向图采用邻接表方式存储 例如：图 6.3 所示的有向图采用邻接表存储。 采用邻接表方式存储图6.3中的有向图，绘图过程中忽略边节点的info信息， 头结点中的data域存储顶点名称。以V1顶点为例， V1顶点的邻接顶点为V2、V3、V4，但是以V1顶点为尾的边只有两条， 即和因此，创建2个表节点。表节点中adjvex分别存储V3、V4的索引2、3， 按照此方式，得到的邻接表为： 有向图的邻接表存储特性： （1）数组中表节点的数目为图的顶点数目。 （2）链表的长度即为顶点的出度。例如V1的出度为2，V1为头节点的链表中，表节点的数目为2。 （3）顶点Vi的入度为邻接表中所有adjvex值域为i的表结点数目。例如：顶点V3的入度为4，则链表中所有adjvex值域为2的表结点数目为4。 注：图采用邻接表的方式表示时，其表示方式是不唯一的。这是因为在每个顶点对应的单链表中，各边节点的链接次序可以是任意的，取决于建立邻接表的算法以及边的输入次序。 逆邻接表 在邻接表中，可以轻易的得出顶点的出度，但是想要得到顶点的入度，则需要遍历整个链表。为了便于确定顶点的入度，可以建立有向图的逆邻接表。逆邻接表的建立与邻接表相反。 采用逆邻接表的方式存储图3.2所示的无向图。以V3顶点为例，V3顶点的邻接顶点为V1、V2、V4、V5，以V3顶点为头的边有4条，即、、、因此，创建4个表节点。表节点中adjvex分别存储V0、V1、V3、V4的索引0、1、3、4，按照此方式，得到的逆邻接表为： 十字链表 对于有向图而言，邻接链表的缺陷是要查询某个顶点的入度时需要遍历整个链表，而逆邻接链表在查询某个顶点的出度时要遍历整个链表。为了解决这些问题，十字链表将邻接链表和逆邻接链表综合了起来，而得到的一种十字链表。在十字链表中，每一条边对应一种边节点，每一个顶点对应为顶点节点。 顶点节点 顶点节点即为头节点，由3个域构成，具体形式如下： 其中，data域存储与顶点相关的信息，firstin和firstout分别指向以此顶点为头或尾的第一个边节点。 边节点 在边节点为链表节点，共有 5 个域，具体形式如下： 其中，尾域tailvex和头域headvex分别指向尾和头的顶点在图中的位置。链域hlink指向头相同的下一条边，链域tlink指向尾相同的下一条边。info 存储此条边的相关信息。 例如：图8.1所示的有向图，采用十字链表存储图方式。 采用十字链表的方式存储图8.1中的有向图，绘图过程忽略边节点中的info信息，表头节点中的data域存储顶点名称。 以V1顶点为例，顶点节点的data域存储V1顶点名，firstin存储以V1顶点为头第一个边节点，以V1顶点为头边为，firstout存储以以V1顶点为尾第一个边节点，对应边为。 按照此规则，得到的十字链表存储为： 注：采用十字链表存储时，表头节点仍然使用数组存储，采用下标索引方式获取。 邻接多重表 对于无向图而言，其每条边在邻接链表中都需要两个结点来表示，而邻接多重表正是对其进行优化，让同一条边只用一个结点表示即可。邻接多重表仿照了十字链表的思想，对邻接链表的边表结点进行了改进。 重新定义的边结点结构如下图： 其中，ivex和jvex是指某条边依附的两个顶点在顶点表中的下标。 ilink指向依附顶点ivex的下一条边，jlink指向依附顶点jvex的下一条边。info存储边的相关信息。 重新定义的顶点结构如下图： 其中，data存储顶点的相关信息，firstedge指向第一条依附于该顶点的边。 例如：下图所示的无向图，采用邻接多重表存储图。 上图所示的无向图，采用邻接多重表存储，以 V0 为例，顶点节点的data域存储V0名称，firstedge 指向(V0 , V1)边，边节点中的ilink指向依附V0顶点的下一条边(V0 , V3)，jlink指向依附V1顶点的下一条边(V1 , V2)，按照此方式建立邻接多重表： 基于列表实现的顶点与边的结构： （有向）图的顶点结构接口 package dsa.Graph; import dsa.Iterator.Iterator; import other.Position; public interface Vertex { /* * （有向）图的顶点结构接口 */ // 常量 final static int UNDISCOVERED = 0;// 尚未被发现的顶点 final static int DISCOVERED = 1;// 已被发现的顶点 final static int VISITED = 2;// 已访问过的顶点 // 返回当前顶点的信息 public Object getInfo(); // 将当前顶点的信息更新为x，并返回原先的信息 public Object setInfo(Object x); // 返回当前顶点的出、入度 public int outDeg(); public int inDeg(); // 返回当前顶点所有关联边、关联边位置的迭代器 public Iterator inEdges(); public Iterator inEdgePositions(); public Iterator outEdges(); public Iterator outEdgePositions(); // 取当前顶点在所属的图的顶点集V中的位置 public Position getVPosInV(); // 读取、设置顶点的状态（DFS + BFS） public int getStatus(); public int setStatus(int s); // 读取、设置顶点的时间标签（DFS） public int getDStamp(); public int setDStamp(int s); public int getFStamp(); public int setFStamp(int s); // 读取、设置顶点至起点的最短距离（BFS或BestFS） public int getDistance(); public int setDistance(int s); // 读取、设置顶点在的DFS、BFS、BestFS或MST树中的父亲 public Vertex getBFSParent(); public Vertex setBFSParent(Vertex s); } （有向）图的边结构接口 package dsa.Graph; import other.Position; public interface Edge { /* * （有向）图的边结构接口 */ // 常量 final static int UNKNOWN = 0;// 未知边 final static int TREE = 1;// 树边 final static int CROSS = 2;// 横跨边 final static int FORWARD = 3;// 前向跨边 final static int BACKWARD = 4;// 后向跨边 // 返回当前边的信息（对于带权图，也就是各边的权重） public Object getInfo(); // 将当前边的信息更新为x，并返回原先的信息 public Object setInfo(Object x); // 取当前边在所属的图的边集E中的位置 public Position getEPosInE(); // 取v[i]在顶点集V中的位置（i=0或1，分别对应于起点、终点） public Position getVPosInV(int i); // 当前边在其两个端点的关联边集I(v[i])中的位置 public Position getEPosInI(int i); // 读取、设置边的类别（针对遍历） public int getType(); public int setType(int t); } （有向）图结构接口 package dsa.Graph; import dsa.Iterator.Iterator; import other.Position; public interface Graph { /* * （有向）图结构接口 */ // 取图中顶点、边的数目 public int vNumber(); public int eNumber(); // 取图中所有顶点、顶点位置的迭代器 public Iterator vertices(); public Iterator vPositions(); // 返回图中所有边、边位置的迭代器 public Iterator edges(); public Iterator ePositions(); // 检测是否有某条边从顶点u指向v public boolean areAdjacent(Vertex u, Vertex v); // 取从顶点u指向v的边，若不存在，则返回null public Edge edgeFromTo(Vertex u, Vertex v); // 将顶点v从顶点集中删除，并返回之 public Vertex remove(Vertex v); // 将边e从边集中删除，并返回之 public Edge remove(Edge e); // 在顶点集V中插入新顶点v，并返回其位置 public Position insert(Vertex v); // 在边集E中插入新边e，并返回其位置 public Position insert(Edge e); } 基于邻接边表实现图的顶点结构 package dsa.Graph; import dsa.Iterator.Iterator; import dsa.List.List; import dsa.List.List_DLNode; import other.Position; public class Vertex_List implements Vertex { /* * 基于邻接边表实现图的顶点结构 */ // 变量 protected Object info;// 当前顶点中存放的数据元素 protected Position vPosInV;// 当前顶点在所属的图的顶点表V中的位置 protected List outEdges;// 关联边表：存放以当前顶点为尾的所有边（的位置） protected List inEdges;// 关联边表：存放以当前顶点为头的所有边（的位置） protected int status;// （在遍历图等操作过程中）顶点的状态 protected int dStamp;// 时间标签：DFS过程中该顶点被发现时的时刻 protected int fStamp;// 时间标签：DFS过程中该顶点被访问结束时的时刻 protected int distance;// 到指定起点的距离：BFS、Dijkstra等算法所确定该顶点到起点的距离 protected Vertex bfsParent;// 在最短距离树（BFS或BestFS）中的父亲 // 构造方法：在图G中引入一个属性为x的新顶点 public Vertex_List(Graph G, Object x) { info = x;// 数据元素 vPosInV = G.insert(this);// 当前顶点在所属的图的顶点表V中的位置 outEdges = new List_DLNode();// 出边表 inEdges = new List_DLNode();// 入边表 status = UNDISCOVERED; dStamp = fStamp = Integer.MAX_VALUE; distance = Integer.MAX_VALUE; bfsParent = null; } // 返回当前顶点的信息 public Object getInfo() { return info; } // 将当前顶点的信息更新为x，并返回原先的信息 public Object setInfo(Object x) { Object e = info; info = x; return e; } // 返回当前顶点的出、入度 public int outDeg() { return outEdges.getSize(); } public int inDeg() { return inEdges.getSize(); } // 返回当前顶点所有关联边、关联边位置的迭代器 public Iterator inEdges() { return inEdges.elements(); } public Iterator inEdgePositions() { return inEdges.positions(); } public Iterator outEdges() { return outEdges.elements(); } public Iterator outEdgePositions() { return outEdges.positions(); } // 取当前顶点在所属的图的顶点集V中的位置 public Position getVPosInV() { return vPosInV; } // 读取、设置顶点的状态（DFS + BFS） public int getStatus() { return status; } public int setStatus(int s) { int ss = status; status = s; return ss; } // 读取、设置顶点的时间标签（DFS） public int getDStamp() { return dStamp; } public int setDStamp(int s) { int ss = dStamp; dStamp = s; return ss; } public int getFStamp() { return fStamp; } public int setFStamp(int s) { int ss = fStamp; fStamp = s; return ss; } // 读取、设置顶点至起点的最短距离（BFS） public int getDistance() { return distance; } public int setDistance(int s) { int ss = distance; distance = s; return ss; } // 读取、设置顶点在的DFS、BFS、BestFS或MST树中的父亲 public Vertex getBFSParent() { return bfsParent; } public Vertex setBFSParent(Vertex s) { Vertex ss = bfsParent; bfsParent = s; return ss; } } 基于邻接边表实现图的边结构 package dsa.Graph; import dsa.Deque.DLNode; import other.Position; public class Edge_List implements Edge { /* * 基于邻接边表实现图的边结构 */ // 变量 protected Object info;// 当前边中存放的数据元素 protected Position ePosInE;// 当前边在所属的图的边表中的位置 protected Position vPosInV[];// 当前边两个端点在顶点表中的位置 protected Position ePosInI[];// 当前边在其两个端点的关联边表中的位置 // 约定：第0（1）个顶点分别为尾（头）顶点 // 禁止头、尾顶点相同的边 protected int type;// （经过遍历之后）边被归入的类别 // 构造方法：在图G中，生成一条从顶点u到v的新边（假定该边尚不存在） public Edge_List(Graph G, Vertex_List u, Vertex_List v, Object x) { info = x;// 数据元素 ePosInE = G.insert(this);// 当前边在所属的图的边表中的位置 vPosInV = new DLNode[2];// 当前边两个端点在顶点表中的位置 vPosInV[0] = u.getVPosInV(); vPosInV[1] = v.getVPosInV(); ePosInI = new DLNode[2];// 当前边在其两个端点的关联边表中的位置 ePosInI[0] = u.outEdges.insertLast(this);// 当前边加入u的邻接（出）边表 ePosInI[1] = v.inEdges.insertLast(this);// 当前边加入v的邻接（入）边表 type = UNKNOWN; } // 返回当前边的信息 public Object getInfo() { return info; } // 将当前边的信息更新为x，并返回原先的信息 public Object setInfo(Object x) { Object e = info; info = x; return e; } // 取当前边在所属的图的边集E中的位置 public Position getEPosInE() { return ePosInE; } // 取v[i]在顶点集V中的位置（i=0或1，分别对应于起点、终点） public Position getVPosInV(int i) { return vPosInV[i]; } // 当前边在其两个端点的关联边集I(v[i])中的位置 public Position getEPosInI(int i) { return ePosInI[i]; } // 读取、设置边的类别（针对遍历） public int getType() { return type; } public int setType(int t) { int tt = type; type = t; return tt; } } 基于邻接边表实现图结构 package dsa.Graph; import dsa.Iterator.Iterator; import dsa.List.List; import dsa.List.List_DLNode; import other.Position; public class Graph_List implements Graph { /* * 基于邻接边表实现图结构 */ // 变量 protected List E;// 容器：存放图中所有边 protected List V;// 容器：存放图中所有顶点 // 构造方法 public Graph_List() { E = new List_DLNode(); V = new List_DLNode(); } // 取图的边表、顶点表 protected List getE() { return E; } protected List getV() { return V; } // 取图中顶点、边的数目 public int vNumber() { return V.getSize(); } public int eNumber() { return E.getSize(); } // 取图中所有顶点、顶点位置的迭代器 public Iterator vertices() { return V.elements(); } public Iterator vPositions() { return V.positions(); } // 返回图中所有边、边位置的迭代器 public Iterator edges() { return E.elements(); } public Iterator ePositions() { return E.positions(); } // 检测是否有某条边从顶点u指向v public boolean areAdjacent(Vertex u, Vertex v) { return (null != edgeFromTo(u, v)); } // 取从顶点u指向v的边，若不存在，则返回null public Edge edgeFromTo(Vertex u, Vertex v) { for (Iterator it = u.outEdges(); it.hasNext();) {// 逐一检查 Edge e = (Edge) it.getNext();// 以u为尾的每一条边e if (v == e.getVPosInV(1).getElem())// 若e是(u, v)，则 return e;// 返回该边 } return null;// 若不存在这样的(u, v)，则返回null } // 将顶点v从顶点集中删除，并返回之 public Vertex remove(Vertex v) { while (0 这里主要涉及三个算法，具体分析如下： 判断任意一对顶点是否相邻 算法：areAdjacent(u, v) 输入：一对顶点u和v 输出：判断是否有某条边从顶点u指向v { 取顶点u的出边迭代器it; 通过it逐一检查u的每一条出边e; 一旦e的终点为v，则报告true; 若e的所有出边都已检查过，则返回false; } 删除边 算法：RemoveEdge(e) 输入：边e = (u, v) 输出：将边e从边集E中删除 { 从起点u的出边邻接表中删除e; 从终点v的入边邻接表中删除e; 从边表E中删除e; } 删除顶点 算法：removeVertex(v) 输入：顶点v 输出：将顶点v从顶点集V中删除 { 扫描v的出边邻接表，（调用removeEdge()算法）将所有边逐一删除; 扫描v的入边邻接表，（调用removeEdge()算法）将所有边逐一删除; 在顶点表V中删除v; } 边集数组 边集数组是由两个一维数组构成，一个是存储顶点的信息， 另一个是存储边的信息，这个边数组每个数据元素由一条边的起点下标(begin)、终点下标(end)和权(weight)组成。 "},"Chapter02/TraversalOfGraph.html":{"url":"Chapter02/TraversalOfGraph.html","title":"图的遍历","keywords":"","body":"图的遍历 遍历是指从某个节点出发，按照一定的的搜索路线，依次访问对数据结构中的全部节点， 且每个节点仅访问一次。 在二叉树基础中，介绍了对于树的遍历。 树的遍历是指从根节点出发，按照一定的访问规则，依次访问树的每个节点信息。 树的遍历过程，根据访问规则的不同主要分为四种遍历方式： 先序遍历 中序遍历 后序遍历 层次遍历 类似的，图的遍历是指，从给定图中任意指定的顶点（称为初始点）出发， 按照某种搜索方法沿着图的边访问图中的所有顶点，使每个顶点仅被访问一次，这个过程称为图的遍历。 遍历过程中得到的顶点序列称为图遍历序列。 图的遍历过程中，根据搜索方法的不同，又可以划分为两种搜索策略： 深度优先搜索（DFS，Depth First Search） 广度优先搜索（BFS，Breadth First Search） 深度优先搜索 算法思想 假设初始状态是图中所有顶点均未被访问，则从某个顶点v出发，首先访问该顶点， 然后依次从它的各个未被访问的邻接点出发深度优先搜索遍历图， 直至图中所有和v有路径相通的顶点都被访问到。若此时尚有其他顶点未被访问到， 则另选一个未被访问的顶点作起始点，重复上述过程，直至图中所有顶点都被访问到为止。 算法特点 深度优先搜索是一个递归的过程。首先，选定一个出发点后进行遍历， 如果有邻接的未被访问过的节点则继续前进。若不能继续前进，则回退一步再前进， 若回退一步仍然不能前进，则连续回退至可以前进的位置为止。 重复此过程，直到所有与选定点相通的所有顶点都被遍历。 深度优先搜索是递归过程，带有回退操作，因此需要使用栈存储访问的路径信息。 当访问到的当前顶点没有可以前进的邻接顶点时，需要进行出栈操作，将当前位置回退至出栈元素位置。 图解过程 无向图深度优先搜索 以下图中所示无向图说明深度优先搜索遍历过程。 首先选取顶点A为起始点，输出A顶点信息，且将A入栈，并标记A为已访问顶点。 A的邻接顶点有C、D、F，从中任意选取一个顶点前进。这里我们选取C顶点为前进位置顶点。 输出C顶点信息，将C入栈，并标记C为已访问顶点。当前位置指向顶点C。 顶点C的邻接顶点有A、D和B，此时A已经标记为已访问顶点，因此不能继续访问。 从B或者D中选取一个顶点前进，这里我们选取B顶点为前进位置顶点。 输出B顶点信息，将B入栈，标记B顶点为已访问顶点。当前位置指向顶点B。 顶点B的邻接顶点只有C、E，C已被标记，不能继续访问，因此选取E为前进位置顶点 ，输出E顶点信息，将E入栈，标记E顶点，当前位置指向E。 顶点E的邻接顶点均已被标记，此时无法继续前进，则需要进行回退。 将当前位置回退至顶点B，回退的同时将E出栈。 顶点B的邻接顶点也均被标记，需要继续回退，当前位置回退至C，回退同时将B出栈。 顶点C可以前进的顶点位置为D，则输出D顶点信息，将D入栈，并标记D顶点。当前位置指向顶点D。 顶点D没有前进的顶点位置，因此需要回退操作。将当前位置回退至顶点C，回退同时将D出栈。 顶点C没有前进的顶点位置，继续回退，将当前位置回退至顶点A，回退同时将C出栈。 顶点A前进的顶点位置为F，输出F顶点信息，将F入栈，并标记F。将当前位置指向顶点F。 顶点F的前进顶点位置为G，输出G顶点信息，将G入栈，并标记G。将当前位置指向顶点G。 顶点G没有前进顶点位置，回退至F。当前位置指向F，回退同时将G出栈。 顶点F没有前进顶点位置，回退至A，当前位置指向A，回退同时将F出栈。 顶点A没有前进顶点位置，继续回退，栈为空，则以A为起始的遍历结束。 若图中仍有未被访问的顶点，则选取未访问的顶点为起始点，继续执行此过程。直至所有顶点均被访问。 采用深度优先搜索遍历顺序为A->C->B->E->D->F->G。 有向图深度优先搜索 以下图中所示有向图说明深度优先搜索遍历过程。 以顶点A为起始点，输出A，将A入栈，并标记A。当前位置指向A。 以A为尾的边只有1条，且边的头为顶点B，则前进位置为顶点B，输出B，将B入栈，标记B。当前位置指向B。 顶点B可以前进的位置有C与F，选取F为前进位置，输出F，将F入栈，并标记F。当前位置指向F。 顶点F的前进位置为G，输出G，将G入栈，并标记G。当前位置指向G。 顶点G没有可以前进的位置，则回退至F，将F出栈。当前位置指向F。 顶点F没有可以前进的位置，继续回退至B，将F出栈。当前位置指向B。 顶点B可以前进位置为C和E，选取E，输出E，将E入栈，并标记E。当前位置指向E。 顶点E的前进位置为D，输出D，将D入栈，并标记D。当前位置指向D。 顶点D的前进位置为C，输出C，将C入栈，并标记C。当前位置指向C。 顶点C没有前进位置，进行回退至D，回退同时将C出栈。 继续执行此过程，直至栈为空，以A为起始点的遍历过程结束。若图中仍有未被访问的顶点，则选取未访问的顶点为起始点，继续执行此过程。 直至所有顶点均被访问 算法分析 当图采用邻接矩阵存储时，由于矩阵元素个数为n^2，因此时间复杂度就是O(n^2)。 当图采用邻接表存储时，邻接表中只是存储了边结点(e条边，无向图也只是2e个结点)，加上表头结点为n（也就是顶点个数），因此时间复杂度为O(n+e)。 广度优先搜索 算法思想 从图中某顶点v出发，在访问了v之后依次访问v的各个未曾访问过的邻接点，然后分别从这些邻接点出发依次访问它们的邻接点， 并使得“先被访问的顶点的邻接点先于后被访问的顶点的邻接点被访问，直至图中所有已被访问的顶点的邻接点都被访问到。 如果此时图中尚有顶点未被访问，则需要另选一个未曾被访问过的顶点作为新的起始点， 重复上述过程，直至图中所有顶点都被访问到为止。 算法特点 广度优先搜索类似于树的层次遍历，是按照一种由近及远的方式访问图的顶点。在进行广度优先搜索时需要使用队列存储顶点信息。 图解过程 无向图的广度优先搜索 例如：下图所示的无向图，采用广度优先搜索过程。 选取A为起始点，输出A，A入队列，标记A，当前位置指向A。 队列头为A，A出队列。A的邻接顶点有B、E，输出B和E，将B和E入队，并标记B、E。当前位置指向A。 队列头为B，B出队列。B的邻接顶点有C、D，输出C、D，将C、D入队列，并标记C、D。当前位置指向B。 队列头为E，E出队列。E的邻接顶点有D、F，但是D已经被标记，因此输出F，将F入队列，并标记F。当前位置指向E。 队列头为C，C出队列。C的邻接顶点有B、D，但B、D均被标记。无元素入队列。当前位置指向C。 队列头为D，D出队列。D的邻接顶点有B、C、E，但是B、C、E均被标记，无元素入队列。当前位置指向D。 队列头为F，F出队列。F的邻接顶点有G、H，输出G、H，将G、H入队列，并标记G、H。当前位置指向F。 队列头为G，G出队列。G的邻接顶点有F，但F已被标记，无元素入队列。当前位置指向G。 队列头为H，H出队列。H的邻接顶点有F，但F已被标记，无元素入队列。当前位置指向H。 队列空，则以A为起始点的遍历结束。若图中仍有未被访问的顶点，则选取未访问的顶点为起始点，继续执行此过程。直至所有顶点均被访问。 有向图的广度优先搜索 以下图所示的有向图为例进行广度优先搜索。 选取A为起始点，输出A，将A入队列，标记A。 队列头为A，A出队列。以A为尾的边有两条，对应的头分别为B、C，则A的邻接顶点有B、C。输出B、C，将B、C入队列，并标记B、C。 队列头为B，B出队列。B的邻接顶点为C，C已经被标记，因此无新元素入队列。 队列头为C，C出队列。C的邻接顶点有E、F。输出E、F，将E、F入队列，并标记E、F。 队列头为E，E出队列。E的邻接顶点有G、H。输出G、H，将G、H入队列，并标记G、H。 队列头为F，F出队列。F无邻接顶点。 队列头为G，G出队列。G无邻接顶点。 队列头为H，H出队列。H邻接顶点为E，但是E已被标记，无新元素入队列。 队列为空，以A为起始点的遍历过程结束，此时图中仍有D未被访问，则以D为起始点继续遍历。选取D为起始点，输出D，将D入队列，标记D。 队列头为D，D出队列，D的邻接顶点为B，B已被标记，无新元素入队列。 队列为空，且所有元素均被访问，广度优先搜索遍历过程结束。广度优先搜索的输出序列为：A->B->E->C->D->F->G->H。 算法分析 假设图有V个顶点，E条边，广度优先搜索算法需要搜索V个节点，时间消耗是O(V)，在搜索过程中， 又需要根据边来增加队列的长度，于是这里需要消耗O(E)，总得来说，效率大约是O(V+E)。 总结 图的遍历主要就是这两种遍历思想，深度优先搜索使用递归方式，需要栈结构辅助实现。 广度优先搜索需要使用队列结构辅助实现。在遍历过程中可以看出，对于连通图， 从图的任意一个顶点开始深度或广度优先遍历一定可以访问图中的所有顶点，但对于非连通图， 从图的任意一个顶点开始深度或广度优先遍历并不能访问图中的所有顶点。 "},"Chapter02/GraphShortestPath.html":{"url":"Chapter02/GraphShortestPath.html","title":"图最短路径","keywords":"","body":"图最短路径  最短路径问题一直是图论研究的热点问题。例如在实际生活中的路径规划、地图导航等领域有重要的应用。 关于求解图的最短路径方法也层出不穷，本篇文章将详细讲解图的最短路径经典算法。 重要概念 图的路径：图G = 中，从任一顶点开始，由边或弧的邻接至关系构成的有限长顶点序列称为路径。 注意：有向图的路径必须沿弧的方向构成顶点序列；构成路径的顶点可能重复出现(即允许反复绕圈)。 路径长度：路径中边或弧的数目。 简单路径：除第一个和最后一个顶点外，路径中无其它重复出现的顶点，称为简单路径。 回路或环：路径中的第一个顶点和最后一个顶点相同时，称为回路或环。 图的最短路径：如果从有向图中某一顶点(称为源点)到达另一顶点(称为终点)的路径可能不止一条，如何找到一条路径使得沿此路径上各边上的权值总和达到最小。 深度或广度优先搜索算法 算法概述 从起点开始访问所有深度遍历路径或广度优先路径，则到达终点节点的路径有多条，取其中路径权值最短的一条则为最短路径。 算法流程 选择单源的起点作为遍历的起始点。 采用深度优先搜索或者广度优先搜索的方式遍历图，在遍历同时记录可以到达终点的路径。 在所有路径中选择距离最短的路径。 实例图解 例如：下图所示的有向图中，选取A为源点，D为终点，采用遍历的方式获取最短路径。 选择A为遍历起始点，D为终点。 采用遍历的方式获取A到D路径。通过遍历方式得到的路径共有5条。 从中选择距离最短的路径为A->B->D，长度为9。 算法分析 采用遍历的方式获取单源最短路径，是一种暴力破解的方式。算法的性能与遍历过程性能相关。采用深度优先搜索遍历时时间复杂度为O(n+e)。 迪杰斯特拉（Dijkstra）算法 算法概述 Dijkstra（迪杰斯特拉）算法是典型的单源最短路径算法，用于计算某个顶点到其他所有顶点的最短路径。Dijkstra（迪杰斯特拉）算法要求图中不存在负权边，即保证图中每条边的权重值为正。 算法的基本思想是：从源点出发，每次选择离源点最近的一个顶点前进，然后以该顶点为中心进行扩展，最终得到源点到其余所有点的最短路径。 算法流程 将所有的顶点分为两部分：已知最短路程的顶点集合P和未知最短路径的顶点集合Q。最开始，已知最短路径的顶点集合P中只有源点s一个顶点。我们这里用一个book[i]数组来记录哪些点在集合P中。例如对于某个顶点i，如果book[i] = 1则表示这个顶点在集合P中，如果book[i] = 0则表示这个顶点在集合Q中。 设置源点s到自己的最短路径为0即dist = 0。若存在源点有能直接到达的顶点i，则把dist[i]设为e[s][i]。同时把所有其它（即源点不能直接到达的）顶点的最短路径为设为∞。 在Q中选择一个离源点s最近的顶点u（即dist[u]最小）加入到P中。并考察所有以点u为起点的边，对每一条边进行松弛操作。 重复第3步，如果集合Q为空，算法结束。最终dist数组中的值就是源点到所有顶点的最短路径。 实例图解 例如：下图所示的有向图，以顶点1为源点，运用Dijkstra算法，获得最短路径。 初始状态下，集合P中只有顶点1， book[1]=1。book数组以及dist数组如图： 从dist数组中可以看出，距离顶点1最近的顶点为2，不存在可以中转的顶点使得顶点1到顶点2的距离更短，且顶点2不在集合P中。 因此，选择顶点2加入集合P中，令book[2]=1。顶点2加入后，需要考虑经过顶点2进行中转，使得顶点1到达其余顶点的距离发生改变。 顶点2的出边有和。 则需重新计算dist[3]和dist[4]。dist[3] = dis[2]+e[2][3] = 10 从剩余顶点3、4、5、6中选择dist中最近顶点为顶点4（因为顶点2已经在集合P中不能再次选择）。 将顶点4加入集合P中，令book[4]=1。按照相同的方式更新dist数组。顶点4的所有出边（dist[3] = dis[4]+e[4][3]），（dist[5] = dis[4]+e[4][5]）和（dist[6] = dis[4]+e[4][6]）用同样的方法进行松弛。 松弛完毕之后book数组和dist数组为： 继续在剩余的顶点3、顶点5顶点和6中，选出离顶点1最近的顶点。选择3号顶点。 此时，dis[3]的值已对3号顶点的所有出边（3->5）（dist[5] = dis[3]+e[3][5]）进行松弛。松弛完毕之后dist数组为： 继续在剩余的顶点5和顶点6，选出离顶点1最近的顶点，选择5号顶点。对5号顶点的所有出边（5->4）（dist[4] = dis[5]+e[5][4]）进行松弛。 松弛完毕之后dist数组为： 最后选择顶点6加入集合P，令book[6]=1。由于6号顶点没有出边，因此不用进行松弛处理。 最终得到的dist数组如下： 算法分析 复杂度 迪杰斯特拉（Dijkstra）算法适用于权值为非负的图的单源最短路径， 使用最小堆时间复杂度是O(VLogV)，用斐波那契堆的复杂度O(E+VlgV)。 为什么不能有负权边 Dijkstra算法当中将节点分为已求得最短路径的集合（记为P）和未确定最短路径的个集合（记为Q）， 归入P集合的节点的最短路径及其长度不再变更，如果边上的权值允许为负值，那么有可能出现当与P内某点（记为a）以负边相连的点（记为b）确定其最短路径时， 它的最短路径长度加上这条负边的权值结果小于a原先确定的最短路径长度(意思是原先从a0---a已经确定一个最短路径，而此时的边权值为负， 则此步骤中的边权计算结果必定小于已经确定了的路径长度)，但是a在Dijkstra算法下是无法更新的，由此便可能得不到正确的结果。 Bellman-Ford算法 算法概述 Bellman-Ford算法是从Dijkstra算法算法引申出来的，它可以解决带有负权边的最短路径问题。值得注意的是，Dijkstra算法和下面的Floyd算法是基于邻接矩阵的，而Bellman-Ford算法是基于邻接表，从边的角度考量的。 用一句话概括就是：对所有的边进行n-1次松弛操作。如果图中存在最短路径（即不存在负权回路），那么最短路径所包含的边最多为n-1条，也就是不可能包含回路。因为如果存在正回路，该路径就不是最短的，而如果存在负回路，就压根就不存在所谓的最短路径。 算法流程 从源点到任意一点u的最短路径的长度，初始化数组dist[u]为0，其余dist[i]为无穷大。 以下操作循环执行至多n-1次，n为顶点数：对于每一条边edge(u,v)，如果dist[u] + weight(u,v) 检测图中是否存在负环路，即权值之和小于0的环路。对于每一条边edge(u,v)，如果存在dist[u] + weight(u,v) 实例图解 以下图所示的有向图为例，以顶点1为源点，采用Bellman-Ford算法计算最短路径。 选取顶点1为源点，令dist[1]=1，dist[2]-dist[6]=INF。 图中共有9条边，分别为，，，，，，，，。对于每一条边执行松弛操作。此过程至多执行5次。 第一次松弛操作： 对于边，dist[2]=dist[1]+weight[1,2]=1 ，dist[3]=dist[1]+weight[1,3]=12，dist[3]=dist[2]+weight[2,3]=10，dist[4]=dist[2]+weight[2,4]=4，dist[5]=dist[3]+weight[3,5]=15，dist[3]=dist[4]+weight[4,3]=8，dist[5]=dist[4]+weight[4,5]=17>15。则dist[5]=15; 对于边，dist[6]=dist[4]+weight[4,6]=19，dist[6]=dist[5]+weight[5,6]=19得到的dist数组为： 第二次松弛操作： 对于边，dist[2]=dist[1]+weight[1,2]=1=1。则dist[2]=1; 对于边，dist[3]=dist[1]+weight[1,3]=12=12。则dist[3]=12; 对于边，dist[3]=dist[2]+weight[2,3]=10>8。则dist[3]=8; 对于边，dist[4]=dist[2]+weight[2,4]=4=4。则dist[4]=4; 对于边，dist[5]=dist[3]+weight[3,5]=13，dist[3]=dist[4]+weight[4,3]=8，dist[5]=dist[4]+weight[4,5]=17>13。则dist[5]=13; 对于边，dist[6]=dist[4]+weight[4,6]=19=19。则dist[6]=19; 对于边，dist[6]=dist[5]+weight[5,6]=17得到的dist数组为： 5.第三次松弛操作： 对于边，dist[2]=dist[1]+weight[1,2]=1=1。则dist[2]=1; 对于边，dist[3]=dist[1]+weight[1,3]=12=12。则dist[3]=12; 对于边，dist[3]=dist[2]+weight[2,3]=10>8。则dist[3]=8; 对于边，dist[4]=dist[2]+weight[2,4]=4=4。则dist[4]=4; 对于边，dist[5]=dist[3]+weight[3,5]=13，dist[3]=dist[4]+weight[4,3]=8，dist[5]=dist[4]+weight[4,5]=17>13。则dist[5]=13; 对于边，dist[6]=dist[4]+weight[4,6]=19=19。则dist[6]=19; 对于边，dist[6]=dist[5]+weight[5,6]=17得到的dist数组为： 6.第三次松弛操作没有对dist进行更新，说明最短路径已经查找完毕。最终结果为： 算法分析 Bellman-Ford算法初始化过程时间复杂度为O(V)，对边进行了V-1趟操作，每趟操作的运行时间为O(E)。整体的时间复杂度为O(V*E) SPFA算法 SPFA(Shortest Path Faster Algorithm)算法是求单源最短路径的一种算法，它是Bellman-ford的队列优化。 算法流程 初始化：选取顶点u为源点，令dist[u]=0，其余赋值为INF。并将源点入队列。 读取队列头的顶点，并将头顶点u出队列，将与u邻接的所有顶点v进行松弛，若v没有在队列中，则将邻接顶点v入队列。如果已经在队列中，则不再入队。 队列为空时，单源最短路径查找完毕。 实例图解 例如：下图所示有向图，以顶点1为源点，采用SPFA算法求解最短路径。 执行初始化操作，并将顶点1入队列。 顶点1出队列，邻接顶点有2、3。进行松弛操作： dist[2] = dist[1]+weight[1,2] = 1; dist[3] = dist[1]+weight[1,3] = 12; 更新dist数组，并将顶点2、3入队列。 顶点2出队列，邻接顶点有3、4，进行松弛操作： dist[3] = dist[2]+weight[2,3] = 10; dist[4] = dist[2]+weight[2,3] = 4; 更新dist数组，并将顶点4入队列（顶点3已在队列中）。 顶点3出队列，邻接顶点有5。进行松弛操作： dist[5] = dist[3]+weight[3,5] = 17; 更新dist数组，并将顶点5入队列。 顶点4出队列，邻接顶点有3、5、6。进行松弛操作： dist[3] = dist[4]+weight[4,3] = 8; dist[5] = dist[4]+weight[4,5] = 17; dist[6] = dist[4]+weight[4,6] = 19; 更新dist数组，并将顶点3、6入队列。 顶点5出队列，邻接顶点有6。进行松弛操作： dist[6] = dist[5]+weight[5,6] = 19 = 19; 无需更新dist中数组值，同时无元素入队列。 顶点3出队列，邻接顶点有5。进行松弛操作： dist[5] = dist[3]+weight[3,5] = 13; 更新dist数组，并将顶点5入队列。 顶点6出队列，无邻接顶点，无需松弛操作。无元素入队列。 顶点5出队列，邻接顶点有6。进行松弛操作：dist[6] = dist[5]+weight[5,6] = 17;更新dist中数组值，顶点6入队列。 顶点6出队列，无邻接顶点，无需松弛操作。无元素入队列。此时队列为空，求解过程结束。得到最终结果为： 算法分析 SPFA算法是Bellman-Ford算法的一种优化。Bellman-Ford算法的复杂度是O(ev)，由于Bellman-Ford算法依次对每一条边进行松弛操作，重复n-1次后得到最短路径。SPFA算法中，如果一个顶点上没有被松弛过，那么下次就不会从这个点开始松弛。只将松弛过的点加入队列，减少了每个点的更新次数。 弗洛伊德（Floyd）算法 Floyd算法是一个经典的动态规划算法。其主要思想为：从任意顶点u到任意顶点v的最短路径不外乎2种可能，一是直接从u到v，二是从u经过若干个顶点k到v。所以，我们假设dist(u,v)为顶点u到顶点v的最短路径的距离，对于每一个顶点k，我们检查dist(u,k) + dist(k,v) 算法流程 从任意一条单边路径开始。所有两点之间的距离是边的权，如果两点之间没有边相连，则权为无穷大。 　　 对于每一对顶点u和v，看看是否存在一个顶点w使得从u到w再到v比己知的路径更短。如果是更新它。 实例图解 例如：下图所示的有向图采用Floyd算法求解最短路径。选取顶点1为源点，顶点3为终点。 选取单边路径为，由于顶点1与顶点3距离为12。 遍历剩余顶点2、4、5、6，寻找是否有可以选做中间站的顶点，使得顶点1到顶点3路径小于12。 遍历完毕后，找到中间顶点2。选择顶点2位中间顶点，使得因此顶点1到顶点3最短路径为10。此时需要走的路径为（1,2），（2,3）。 （1,2）边路径已经为最短路径，不存在中转顶点。遍历剩余顶点寻找（2,3）之间的中转顶点，发现通过顶点4可以使得1->3路径更短，路径长度为7。以此类推，逐逐步寻找最短路径。   例如：上图所示的有向图采用Floyd算法求解最短路径。选取顶点2为源点，顶点5为终点。 顶点2与顶点5不邻接，因此距离为INF 顶点2与顶点5可以通过顶点3中转，经过中转后距离为14。此时路径为2->3->5。 顶点2到顶点3又可以通过顶点4中转，经过转后顶点2至顶点5距离为12。此时路径为2->4->3->5。 算法分析 弗洛伊德（Floyd）算法的核心代码如下： for(int i = 1; i dist[j][i] + dist[i][k]) { dist[j][k] = dist[j][i] + dist[i][k]; } } } } 可以看出Floyd算法是一种暴力破解的方式获取最短路径。Floyd算法的时间复杂度为O(n^3)，空间复杂度为O(n^2)。Floyd算法可以获得任意顶点对之间的最短路径。 结语 最短路径问题是图论研究中的一个经典算法问题。因此针对图最短路径问题先后提出了许多算法。各类算法的应用场景不尽相同。Dijkstra算法和Bellman-Ford算法用于解决单源最短路径，而Floyd算法可以解决多源最短路径。 Dijkstra算法适用稠密图（邻接矩阵），因为稠密图问题与顶点关系密切。Bellman-Ford算法算法适用稀疏图（邻接表），因为稀疏图问题与边关系密切。 Floyd算法在稠密图（邻接矩阵）和稀疏图（邻接表）中都可以使用。 "},"Chapter02/GraphTopologicalSorting.html":{"url":"Chapter02/GraphTopologicalSorting.html","title":"图拓扑排序","keywords":"","body":"图拓扑排序 在工程实践中,一个工程项目往往由若干个子项目组成。这些子项目间往往有两种关系: 先后关系，即必须在某个项完成后才能开始实施另一个子项目。 子项目间无关系，即两个子项目可以同时进行,互不影响。 例如：在工厂里产品的生产线上，一个产品由若干个零部件组成。零部件生产时，也存在这两种关系: 先后关系，即一个部件必须在完成后才能生产另一个部件; 部件间无先后关系，即这两个部件可以同时生产。 那么如何合理的分配资源才能保证工程能够按时完成呢？将任务作为图的顶点，将任务之间的依赖关系作为图的边，这样就可以将实际问题抽象为数据结构图论中的典型问题——图的拓扑排序。 重要概念 有向无环图（Directed Acyclic Graph, DAG）是有向图的一种，字面意思的理解就是图中没有环。常常被用来表示事件之间的驱动依赖关系，管理任务之间的调度。 AOV网：在每一个工程中，可以将工程分为若干个子工程，这些子工程称为活动。如果用图中的顶点表示活动，以有向图的弧表示活动之间的优先关系，这样的有向图称为AOV网，即顶点表示活动的网。 在AOV网中，如果从顶点vi到顶点j之间存在一条路径，则顶点vi是顶点vj的前驱，顶点vj是顶点vi的后继。活动中的制约关系可以通过AOV网中的表示。 在AOV网中，不允许出现环，如果出现环就表示某个活动是自己的先决条件。 因此需要对AOV网判断是否存在环，可以利用有向图的拓扑排序进行判断。 拓扑序列：设G=(V,E)是一个具有n个顶点的有向图，V中的顶点序列v1,v2,…,vn,满足若从顶点vi到vj有一条路径，则在顶点序列中顶点vi必在vj之前，则我们称这样的顶点序列为一个拓扑序列。 拓扑排序：拓扑排序是对一个有向图构造拓扑序列的过程。 拓扑排序 拓扑排序（Topological Sorting）是一个有向无环图（DAG, Directed Acyclic Graph）的所有顶点的线性序列。且该序列必须满足下面两个条件： 每个顶点出现且只出现一次。 A在B前面，则不存在B在A前面的路径。( 不能成环！！！！) 顶点的顺序是保证所有指向它的下个节点在被指节点前面！(例如A—>B—>C那么A一定在B前面，B一定在C前面)。所以，这个核心规则下只要满足即可，所以拓扑排序序列不一定唯一！ 注：有向无环图（DAG）才有拓扑排序，非DAG图没有拓扑排序一说。 拓扑排序算法分析 正常步骤为(方法不一定唯一)： 从DGA图中找到一个 没有前驱的顶点输出。(可以遍历，也可以用优先队列维护) 删除以这个点为起点的边。(它的指向的边删除，为了找到下个没有前驱的顶点) 重复上述，直到最后一个顶点被输出。如果还有顶点未被输出，则说明有环！ 对于上图的简单序列，可以简单描述步骤为： 删除1或2输出 删除2或3以及对应边 删除3或者4以及对应边 重复以上规则步骤 这样就完成一次拓扑排序，得到一个拓扑序列，但是这个序列并不唯一！从过程中也看到 有很多选择方案， 具体得到结果看你算法的设计了。但只要满足即是拓扑排序序列。 另外观察 12436579这个序列满足我们所说的有关系的节点指向的在前面， 被指向的在后面。如果完全没关系那不一定前后(例如1,2) 拓扑排序代码实现 对于拓扑排序，如何用代码实现呢？对于拓扑排序，虽然在上面详细介绍了思路和流程，也很通俗易懂。但是实际上代码的实现还是很需要斟酌的， 如何在空间和时间上能够得到较好的平衡且取得较好的效率？ 首先要考虑 存储。对于节点，首先他有联通点这么多属性。遇到稀疏矩阵还是用邻接表比较好。 因为一个节点的指向节点较少，用 邻接矩阵较浪费资源。 另外，如果是1，2，3，4，5，6这样的序列求拓扑排序，我们可以考虑用数组，但是如果遇到1，2，88，9999类似数据，可以考虑用map中转一下。那么， 我们具体的代码思想为： 新建node类，包含节点数值和它的指向(这里直接用list集合替代链表了) 一个数组包含node(这里默认编号较集中)。初始化，添加每个节点指向的时候同时被指的节点入度+1！(A—>C)那么C的入度+1； 扫描一遍所有node。将所有入度为0的点加入一个 栈(队列)。 当栈(队列)不空的时候，抛出其中任意一个node(栈就是尾，队就是头，顺序无所谓，上面分析了只要同时入度为零可以随便选择顺序)。将node输出，并且 node指向的所有元素入度减一。如果某个点的入度被减为0，那么就将它加入栈(队列)。 重复上述操作，直到栈为空。 这里主要是利用栈或者队列储存入度只为0的节点，只需要初次扫描表将入度为0的放入栈(队列)中。 这里你或许会问为什么。 因为节点之间是有相关性的，一个节点若想入度为零，那么它的父节点(指向节点)肯定在它为0前入度为0，拆除关联箭头。从父节点角度，它的这次拆除联系，可能导致被指向的入读为0，也可能不为0(还有其他节点指向儿子) 至于具体demo： package Graph; import java.util.ArrayDeque; import java.util.ArrayList; import java.util.List; import java.util.Queue; import java.util.Stack; public class tuopu { static class node { int value; List next; public node(int value) { this.value=value; next=new ArrayList(); } public void setnext(Listlist) { this.next=list; } } public static void main(String[] args) { // TODO Auto-generated method stub node []nodes=new node[9];//储存节点 int a[]=new int[9];//储存入度 Listlist[]=new ArrayList[10];//临时空间，为了存储指向的集合 for(int i=1;i(); } initmap(nodes,list,a); //主要流程 //Queueq1=new ArrayDeque(); Stacks1=new Stack(); for(int i=1;inext=n1.next; for(int i=0;i[] list, int[] a) { list[1].add(3); nodes[1].setnext(list[1]); a[3]++; list[2].add(4);list[2].add(6); nodes[2].setnext(list[2]); a[4]++;a[6]++; list[3].add(5); nodes[3].setnext(list[3]); a[5]++; list[4].add(5);list[4].add(6); nodes[4].setnext(list[4]); a[5]++;a[6]++; list[5].add(7); nodes[5].setnext(list[5]); a[7]++; list[6].add(8); nodes[6].setnext(list[6]); a[8]++; list[7].add(8); nodes[7].setnext(list[7]); a[8]++; } } 输出结果 2 4 6 1 3 5 7 8 当然，上面说过用栈和队列都可以！如果使用队列就会得到 12345678的拓扑序列 至于图的构造，因为没有条件可能效率并不高，算法也可能不太完美. 入度表法 入度表法是根据顶点的入度来判断是否存在依赖关系。若顶点入度不为0。则必然此顶点的事件有前驱依赖事件，因此每次选取入度为0的顶点输出，则符合拓扑排序的性质。 算法流程 从图中选择一个入度为0的顶点，输出该顶点。 从图中删除该节点及其所有出边（即与之邻接的所有顶点入度-1） 反复执行这两个步骤，直至所有节点都输出，即整个拓扑排序完成；或者直至剩下的图中再没有入度为0的节点，这就说明此图中有回路，不可能进行拓扑排序。 实例图解 例如：下图所示的有向无环图，采用入度表的方法获取拓扑排序过程。 选择图中入度为0的顶点1，输出顶点1。删除顶点1，并删除以顶点1为尾的边。删除后图为： 继续选择入度为0的顶点。现在，图中入度为0的顶点有2和4，这里我们选择顶点2，输出顶点2。删除顶点2，并删除以顶点2为尾的边。删除后图为： 选择入度为0的顶点4，输出顶点4.删除顶点4，并删除以顶点4为尾的边。删除后图为： 选择入度为0的顶点3，输出顶点3.删除顶点3，并删除以顶点3为尾的边。删除后图为： 最后剩余顶点5，输出顶点5，拓扑排序过程结束。最终的输出结果为： 性能分析 算法时间复杂度分析：统计所有节点入度的时间复杂性为（VE）；接下来删边花费的时间也是（VE），总花费时间为O（VE）。若使用队列保存入度为0的顶点，则可以将这个算法复杂度将为O（V+E）。 DFS方法 深度优先搜索过程中，当到达出度为0的顶点时，需要进行回退。在执行回退时记录出度为0的顶点，将其入栈。则最终出栈顺序的逆序即为拓扑排序序列。 算法流程 对图执行深度优先搜索。 在执行深度优先搜索时，若某个顶点不能继续前进，即顶点的出度为0，则将此顶点入栈。 最后得到栈中顺序的逆序即为拓扑排序顺序。 实例图解 例如下图所示的有向无环图，采用DFS的方法获取拓扑排序过程。 选择起点为顶点1,，开始执行深度优先搜索。顺序为1->2->3->5。 深度优先搜索到达顶点5时，顶点5出度为0。将顶点5入栈。 深度优先搜索执行回退，回退至顶点3。此时顶点3的出度为0，将顶点3入栈。 回退至顶点2，顶点2出度为0，顶点2入栈。 回退至顶点1，顶点1可以前进位置为顶点4，顺序为1->4。 顶点4出度为0，顶点4入栈。 回退至顶点1，顶点1出度为0，顶点1入栈。 栈的逆序为1->4->2->3->5。此顺序为拓扑排序结果。 性能分析 时间复杂度分析：首先深度优先搜索的时间复杂度为O(V+E)，而每次只需将完成访问的顶点存入数组中，需要O(1)，因而总复杂度为O(V+E)。 "},"Chapter02/FigureCriticalPath.html":{"url":"Chapter02/FigureCriticalPath.html","title":"图关键路径","keywords":"","body":"图关键路径 关键路径是图中一个比较重要的知识点，它的用处也很大。例如：利用关键路径可以帮助企业哪些生产步骤是整个生产进度的关键，提高这些生产步骤的效率就能提高整个生产过程的效率。 在进一步学习求解关键路径前首先需要明确一些重要概念。 重要概念 AOV网：用顶点表示活动，用弧表示活动间的优先关系的有向图。称为顶点表示活动的网（Activity On Vertex Network），简称为AOV网。在图的拓扑排序中已经详细讲解了AOV网的定义和性质。下面我们来看与AOV网十分类似的数据结构——AOE网。 AOE网：AOE网是一个带权的有向无环图。AOE网中只有一个入度为零的点（称为源点）和一个出度为零的点（称为汇点）。在AOV网顶点表示事件（Event），弧表示活动，权表示活动持续的时间。 在AOE网中，顶点之间不仅只有事件的前后驱动关系，又加入了活动的持续时间制约。在AOE网中只有在某顶点所代表的事件发生后，从该顶点出发的各活动才能开始。只有在进入某顶点的各活动都结束，该顶点所代表的事件才能发生。 例如：图2.1以制造汽车为例简易说明AOV网和AOE网的区别： AOV AOE 由实例可以看出在AOV网中，顶点代表的是活动，弧代表各个活动之间的制约关系。 而在AOE网中，顶点代表的是事件，弧代表的是活动，且弧的权值代表完成此活动需要的时间。 关键路径 事件的最早发生时间ve[k]：事件的最早发生时间ve[k]是指从源点开始到顶点vk的最大路径长度。这个长度决定了从顶点vk发出的活动能够开工的最早时间。 事件的最晚发生时间vl[k]：,事件的最晚发生时间vl[k]是指在不推迟整个工期的前提下，事件vk允许的最晚发生时间。 活动的最早开始时间e[i]：若活动ai是由弧表示，则活动ai的最早开始时间应等于事件vk的最早发生时间。因此，有：e[i]=ve[k] 活动的最晚开始时间l[i]：活动ai的最晚开始时间是指，在不推迟整个工期的前提下， ai必须开始的最晚时间。若ai由弧表示，则ai的最晚开始时间要保证事件vj的最晚发生时间不拖后。因此，有：l[i]=vl[j]-weight 关键路径：在AOE网中，从始点到终点具有最大路径长度（该路径上的各个活动所持续的时间之和）的路径称为关键路径。 关键活动：关键路径上的活动称为关键活动。关键活动：e[ ai]=l[ ai]的活动 对AOE网有待研究的问题是: 完成整个工程至少需要多少时间？   完成整个工程的时间即求解出AOE网由源点到汇点的最长路径长度。 那些活动是影响工程进度的关键？   关键活动的延期必然导致整个工期的推迟。也就是说处于关键路径上的关键活动是没有松弛时间的。而处于非关键路径上的活动可以有适当的松弛时间。 求解过程 输入e条弧，建立AOE网的存储结构。 从源点v1出发，令ve[1]=0。按拓扑有序序列次序求其余各顶点的最早发生时间ve[k]（2)}。如果得到的拓扑有序序列中顶点个数小于网中顶点的个数n，说明网中存在环路，不能求关键路径算法终止，否则执行步骤（3）。   （3）从汇点vn出发，令vl[n]=ve[n]，按逆拓扑有序序列求其余各顶点的最晚发生时间vl[k]（n-1>=k>=1），vl[k]=min{vl[vj]-weight()}。 根据各顶点的ve值和vl值，求每条弧的最早开始时间e[ai]。e[ai]等于弧ai的弧尾顶点vk的最早发生时间ve[k]。 根据各顶点的ve值和vl值计算每条弧的最晚开始时间l[ai]。l[ai]等于弧头顶点vk的最晚发生时间减去弧ai的权值。 若某条弧ai满足e[ai]=l[ai]则为关键活动，由所有关键活动构成的网的一条或几条关键路径。 实例图解 例如：下图所示的AOE网的关键路径求解过程。 首先，根据求出图中各顶点代表的事件最早开始时间。顶点v1为源点，则ve[1]=0。完成活动a1的时间需要3天，顶点v1到顶点v2的最长路径长度为3，则v2的最早开始时间为3，则ve[2]=3。同理得出ve[3]=2，ve[4]=6。 由顶点v1到顶点v5有4条路径，分别为1->2->5，1->4->5，1->2->4->5，1->3->4->5。选择四条路径中最长的一条1->2->5或者1->4->5，对应的路径长度为7，则ve[5]=7。按照相同的方式，分别求出ve[6]=5，ve[7]=10。得到各事件的最早开始时间如下表： "},"Chapter02/MinimumSpanningTree.html":{"url":"Chapter02/MinimumSpanningTree.html","title":"最小生成树","keywords":"","body":"最小生成树 生成树 在无向图G中，如果从顶点V1到顶点V2有路径，则称V1和V2是连通的， 如果对于图中任意两个顶点Vi和Vj都是连通的，则称G是连通图(Connected Graph)。 无向图中的极大连通子图称为连通分量。 注意以下概念： 首先要是子图，并且子图是要连通的； 连通子图含有极大顶点数； 具有极大顶点数的连通子图包含依附于这些顶点的所有边 在有向图G中，如果对于每一对Vi到Vj都存在路径，则称G是强连通图。 有向图中的极大强连通子图称为有向图的强连通分量。 下图左侧并不是强连通图（不满足每一对Vi到Vj都存在路径），右侧是。并且右侧是左侧的极大强连通子图，也是左侧的强连通分量。 最后我们再来看连通图的生成树定义。 所谓的一个连通图的生成树是一个极小的连通子图，它含有图中全部的n个顶点，但只有构成一棵树的n-1条边。 如果一个有向图恰有一个顶点入度为0，其余顶点的入度均为1，则是一棵有向树。 生成树的定义 一个连通图的生成树是一个极小的连通子图，它包含图中全部的n个顶点，但只有构成一棵树的n-1条边。 可以看到一个包含3个顶点的完全图可以产生3颗生成树。对于包含n个顶点的无向完全图最多包含 颗生成树。比如上图中包含3个顶点的无向完全图，生成树的个数为： . 生成树的属性 一个连通图可以有多个生成树； 一个连通图的所有生成树都包含相同的顶点个数和边数； 生成树当中不存在环； 移除生成树中的任意一条边都会导致图的不连通， 生成树的边最少特性； 在生成树中添加一条边会构成环。 对于包含n个顶点的连通图，生成树包含n个顶点和n-1条边； 对于包含n个顶点的无向完全图最多包含 [公式] 颗生成树。 最小生成树 所谓一个 带权图 的最小生成树，就是原图中边的权值最小的生成树 ，所谓最小是指边的权值之和小于或者等于其它生成树的边的权值之和。 首先你明白最小生成树是和带权图联系在一起的；如果仅仅只是非带权的图，只存在生成树。其他的，我们看栗子解决就好了。 上图中，原来的带权图可以生成左侧的两个最小生成树，这两颗最小生成树的权值之和最小，且包含原图中的所有顶点。 看图就是清晰，一下子理解了，但我们又如何从原图得到最小生成树呢？ 最小生成树算法有很多，其中最经典的就是克鲁斯卡尔（Kruskal）算法和 普里姆（Prim）算法，也是我们考试、面试当中经常遇到的两个算法。 Kruskal算法 克鲁斯卡尔算法（Kruskal）是一种使用贪婪方法的最小生成树算法。 该算法初始将图视为森林，图中的每一个顶点视为一棵单独的树。 一棵树只与它的邻接顶点中权值最小且不违反最小生成树属性（不构成环）的树之间建立连边。 第一步：将图中所有的边按照权值进行非降序排列； 第二步：从图中所有的边中选择可以构成最小生成树的边。 选择权值最小的边 ：没有环形成，则添加： 选择边 ：没有形成环，则添加： 选择边 ：没有形成环，则添加： 选择边 ：没有形成环，则添加： 选择边 ：没有形成环，则添加： 选择边 ：没有形成环，则添加： 选择边 ：没有形成环，则添加： 选择边 ：添加这条边将导致形成环，舍弃，不添加； 选择边 ：添加这条边将导致形成环，舍弃，不添加； 选择边 ：没有形成环，则添加： 此时已经包含了图中顶点个数9减1条边，算法停止。 我们该如何判断添加一条边后是否形成环呢？ 要判断添加一条边 X-Y 是否形成环，我们可以判断顶点X在最小生成树中的终点与顶点Y在最小生成树中的终点是否相同，如果相同则说明存在环路，否则不存环路，从而决定是否添加一条边。 所谓终点，就是将所有顶点按照从小到大的顺序排列好之后；某个顶点的终点就是\"与它连通的最大顶点\"。看下图，我们可以对图中顶点进行排序，排序后的顶点存放在一个数组中，每一个顶点则对应一个下标，同样的我们针对排序后的数组创建一个顶点的终点数组，初始时图中的每一个顶点是一棵树，每一个顶点的终点初始化为自身，我们用0来表示。 回到之前的算法执行过程，我们配合这个终点数组再来一次。 选择权值最小的边 ：没有环形成（ 的终点为4， 的终点为7），则添加，并更新终点数组： 此时发现4的终点更新为7； 选择边 ：没有形成环（ 的终点为2， 的终点为8），则添加，并更新终点数组： 2的终点更新为8； 选择边 ：没有形成环（ 的终点为0， 的终点为1），则添加，并更新终点数组： 0的终点更新为1； 选择边：没有形成环（ 的终点为1，的终点为5），则添加，并更新终点数组： 将 1的终点更新为5； 选择边 ：没有形成环（ 的 终点为5， 的 终点为8），则添加，并更新数组： 将 5的终点更新为8； 选择边：没有形成环（的 终点为3， 的 终点为7 ），则添加，并更新数组： 将 3的终点更新为7； 选择边 ：没有形成环 （ 的 终点为8， 的 终点为6 ），则添加，并更新终点数组： 将 8的终点更新为6； 选择边：添加这条边将导致形成环 （的 终点为6， 的 终点为6 ，两个顶点的终点相同则说明添加后会形成环），舍弃，不添加； 选择边：添加这条边将导致形成环（的 终点为6， 的 终点为6 ，两个顶点的终点相同则说明添加后会形成环），舍弃，不添加； 选择边 ：没有形成环（ 的 终点为6， 的 终点为7 ），则添加： 将 6的终点更新为7；此时已经包含了图中顶点个数9减1条边，算法停止。 实现 int Find(int *parent, int f) { while( parent[f] > 0 ) { f = parent[f]; } return f; } // Kruskal算法生成最小生成树 void MiniSpanTree_Kruskal(MGraph G) { int i, n, m; Edge edges[MAGEDGE]; // 定义边集数组 int parent[MAXVEX]; // 定义parent数组用来判断边与边是否形成环路 int eCount = 0; for( i=0; i 时间复杂度分析 O(ElogE)或者O(ElogV)，其中E代表图中的边的数目，V代表图中的顶点数目。对图中的边按照非降序排列需要O(ElogE)的时间。排序后遍历所有的边并判断添加边是否构成环，判断添加一条边是否构成环最坏情况下需要O(logV)，关于这个复杂度等到景禹给你们谈并查集的时候再分析；因此，总的时间复杂度为O(ElogE + ElogV)，其中E的值最大为V(V-1)/2，因此O(logV) 等于 O(logE)。因此，总的时间复杂度为O(ElogE) 或者O(ElogV)。 Prim算法 普里姆算法在找最小生成树时，将顶点分为两类，一类是在查找的过程中已经包含在生成树中的顶点（假设为 A 类），剩下的为另一类（假设为 B 类）。 对于给定的连通网，起始状态全部顶点都归为 B 类。在找最小生成树时，选定任意一个顶点作为起始点，并将之从 B 类移至 A 类；然后找出 B 类中到 A 类中的顶点之间权值最小的顶点，将之从 B 类移至 A 类，如此重复，直到 B 类中没有顶点为止。所走过的顶点和边就是该连通图的最小生成树。 我们同样以下图为栗子进行说明： 假如从顶点 出发，顶点 、的权值分别为3、4，所以对于顶点 来说，到顶点 的权值最小，将顶点 加入到生成树中： 继续分析与顶点 和 相邻的所有顶点（包括 、、、），其中权值最小的为 ， 将 添加到生成树当中： 继续分析与顶点 和 、相邻的所有顶点中权值最小的顶点，发现为 ，则添加到生成树当中。 继续分析与生成树中已添加顶点相邻的顶点中权值最小的顶点，发现为 ，则添加到生成树中： 重复上面的过程，直到生成树中包含图中的所有顶点，我们直接看接下来的添加过程： 此时算法结束，我们找出了图中的最小生成树。 时间复杂度分析 上面的代码中，当 i == 1的时候，内层的 while 与 for 循环中 j 的取值范围是从 1 到 n-1，内循环一共计算了 2(n-1) 次，其中n为图中的顶点个数； 当 i == 2 的时候，内层循环还是一共计算 2(n-1) 次； 以此类推...... i 取最大值 n -1，内层循环还是一共计算 2(n-1) 次； 所以，整体的执行次数就是 (n-1) * 2(n-1)，Prim算法的复杂度是 [公式] 级别的。 应用实例 某公司规模不断扩大，在全国各地设立了多个分公司，为了提高公司的工作效率，使各分公司之间的信息可以更快、更准确的进行交流，该公司决定要在各分公司之间架设网络，由于地理位置和距离等其他因素，使各个分公司之间架设网线的费用不同，公司想各分公司之间架设网线的费用降到最低，那么应该怎样来设计各分公司及总公司之间的线路？该公司的所有分公司及总公司的所在位置如下图所示，顶点代表位置及公司名称，边表示可以架设网线的路线，边上的数字代表架设该网线所需要的各种花费的总和。这样就构成了一个带权的连通图，从而问题就转化为求所得到的带权连通图的最小生成树。 总结 最小生成树的问题，简单得理解就是给定一个带有权值的连通图（连通网），从众多的生成树中筛选出权值总和最小的生成树，即为该图的最小生成树。 最经典的两个最小生成树算法： Kruskal 算法与 Prim 算法。两者分别从不同的角度构造最小生成树，Kruskal 算法从边的角度出发，使用贪心的方式选择出图中的最小生成树，而 Prim 算法从顶点的角度出发，逐步找各个顶点上最小权值的边来构建最小生成树的。 最小生成树问题应用广泛，最直接的应用就是网线架设、道路铺设。还可以间接应用于纠错的LDPC码、Renyi 熵图像配准、学习用于实时脸部验证的显著特征、减少蛋白质氨基酸序列测序中的数据存储，在湍流（turbulent）中模拟粒子交互的局部性，以及用于以太网桥接的自动配置，以避免在网络中形成环路。除此之外，最小生成树在聚类算法中也是应用广泛。 所以一定要搞懂最小生成树、Kruskal 算法及Prim 算法奥！！！ "},"Chapter03/algorithm.html":{"url":"Chapter03/algorithm.html","title":"Part III 算法篇","keywords":"","body":"第三章 算法 算法分类 基础 排序算法、查找算法、缓存算法、递归、分治、动态规划、贪心、回溯、分支界限 常见面试算法 集合求子集问题、数字全排列问题、八皇后问题、迷宫问题、约瑟夫环、汉诺塔问题 智能算法 爬山算法、禁忌搜索、模拟退火、蚁群算法、粒子群算法、遗传算法、神经网络 计算机基础 死锁、银行家算法、作业调度算法、页面置换算法、进程调度算法、磁盘调度算法 其他算法 通信校验算法：奇偶校验、海明码、循环冗余码、数据加密算法：DES加密、RSA加密、Base64编码、数据压缩算法：霍夫曼编码、LZ77压缩、滤波算法：九种常用滤波算法、傅里叶变换、快速傅里叶变换、 已有篇章 排序算法 冒泡排序 选择排序 插入排序 希尔排序 归并排序 快速排序 堆排序 计数排序 桶排序 基数排序 排序算法总结 查找算法 顺序查找 二分查找 插值查找 斐波那契查找 树表查找 分块查找 哈希查找 查找算法总结 缓存算法 LRU LFU 加密算法 "},"Chapter03/sortingAlgorithm.html":{"url":"Chapter03/sortingAlgorithm.html","title":"排序算法","keywords":"","body":"十大经典排序算法 排序的定义 对一序列对象根据某个关键字进行排序。 术语说明 稳定 ：如果a原本在b前面，而a=b，排序之后a仍然在b的前面； 不稳定 ：如果a原本在b的前面，而a=b，排序之后a可能会出现在b的后面； 内排序 ：所有排序操作都在内存中完成； 外排序 ：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行； 时间复杂度 ： 一个算法执行所耗费的时间。 空间复杂度 ：运行完一个程序所需内存的大小。 排序算法分类 十种常见排序算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序， 由于其时间复杂度不能突破O(nlogn)， 因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序， 它可以突破基于比较排序的时间下界，以线性时间运行， 因此也称为线性时间非比较类排序。 算法复杂度 图片名词解释： n: 数据规模 k: “桶”的个数 In-place: 占用常数内存，不占用额外内存 Out-place: 占用额外内存 比较和非比较的区别 常见的快速排序、归并排序、堆排序、冒泡排序 等属于比较排序 。在排序的最终结果里，元素之间的次序依赖于它们之间的比较。每个数都必须和其他数进行比较，才能确定自己的位置 。 在冒泡排序之类的排序中，问题规模为n，又因为需要比较n次，所以平均时间复杂度为O(n²)。在归并排序、快速排序之类的排序中，问题规模通过分治法消减为logN次，所以时间复杂度平均O(nlogn)。 比较排序的优势是，适用于各种规模的数据，也不在乎数据的分布，都能进行排序。可以说，比较排序适用于一切需要排序的情况。 计数排序、基数排序、桶排序则属于非比较排序 。非比较排序是通过确定每个元素之前，应该有多少个元素来排序。针对数组arr，计算arr[i]之前有多少个元素，则唯一确定了arr[i]在排序后数组中的位置 。 非比较排序只要确定每个元素之前的已有的元素个数即可，所有一次遍历即可解决。算法时间复杂度O(n)。 非比较排序时间复杂度底，但由于非比较排序需要占用空间来确定唯一位置。所以对数据规模和数据分布有一定的要求。 "},"Chapter03/BubbleSort.html":{"url":"Chapter03/BubbleSort.html","title":"冒泡排序","keywords":"","body":"冒泡排序 冒泡排序是一种简单的排序算法。 它重复地走访过要排序的数列，一次比较两个元素， 如果它们的顺序错误就把它们交换过来。 走访数列的工作是重复地进行直到没有再需要交换， 也就是说该数列已经排序完成。 这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 动图演示 代码实现 package algorithm.sort; import java.util.Arrays; /** * 冒泡排序 */ public class Bubble { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(Bubble.bubble(array))); } public static int[] bubble(int[] origin) { if (origin.length origin[j + 1]) { item = origin[j + 1]; origin[j + 1] = origin[j]; origin[j] = item; } } } return origin; } } 执行结果: 原始数组：[1, 2, 9, 4, 6, 7, 8, 3, 0, 5] 排序后数组：[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 算法分析 最佳情况：T(n) = O(n) 最差情况：T(n) = O(n2) 平均情况：T(n) = O(n2) 时间复杂度 　　若文件的初始状态是正序的，一趟扫描即可完成排序。 所需的关键字比较次数 和记录移动次数 均达到最小值：C_min = n-1, M_min = 0。 　　所以，冒泡排序最好的时间复杂度为 O(n)。 　　若初始文件是反序的，需要进行n-1 趟排序。 每趟排序要进行 n-i 次关键字的比较(1≤i≤n-1)， 且每次比较都必须移动记录三次来达到交换记录位置。 在这种情况下，比较和移动次数均达到最大值： C_max= \\frac{n(n-1)}{2} = O(n^2) M_max = \\frac{3n(n-1)}{2} = O(n^2) 冒泡排序的最坏时间复杂度为 O(n^2) 综上，因此冒泡排序总的平均时间复杂度为 O(n^2) 算法稳定性 冒泡排序就是把小的元素往前调或者把大的元素往后调。 比较是相邻的两个元素比较，交换也发生在这两个元素之间。 所以，如果两个元素相等，是不会再交换的； 如果两个相等的元素没有相邻， 那么即使通过前面的两两交换把两个相邻起来， 这时候也不会交换，所以相同元素的前后顺序并没有改变， 所以冒泡排序是一种稳定排序算法。 "},"Chapter03/SelectionSort.html":{"url":"Chapter03/SelectionSort.html","title":"选择排序","keywords":"","body":"选择排序 　　选择排序 是表现最稳定的排序算法之一 ， 因为无论什么数据进去都是O(n2)的时间复杂度 ， 所以用到它的时候，数据规模越小越好。 唯一的好处可能就是不占用额外的内存空间了吧。 理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 　　选择排序(Selection-sort) 是一种简单直观的排序算法。 它的工作原理：首先在未排序序列中找到最小（大）元素， 存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小 （大）元素，然后放到已排序序列的末尾。 以此类推，直到所有元素均排序完毕。 算法描述 n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。 具体算法描述如下： 步骤1：初始状态：无序区为R[1…n]，有序区为空； 步骤2：第i趟排序(i=1,2,3…n-1)开始时， 当前有序区和无序区分别为R[1…i-1]和R(i…n）。 该趟排序从当前无序区中-选出关键字最小的记录 R[k]， 将它与无序区的第1个记录R交换， 使R[1…i]和R[i+1…n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区； 步骤3：n-1趟结束，数组有序化了。 动图演示 代码实现 package algorithm.sort; import java.util.Arrays; public class Selection { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(Selection.selection(array))); } private static int[] selection(int[] array) { if(array.length == 0){ return array; } int arrayLength = array.length; int item = 0; int minIndex = 0; for (int i = 0; i } ``` 算法分析 表现最稳定的排序算法之一，因为无论什么数据进去都是O(n2)的时间复杂度， 所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。 理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 最佳情况：T(n) = O(n2) 最差情况：T(n) = O(n2) 平均情况：T(n) = O(n2) 选择排序的交换操作介于 0 和 (n - 1） 次之间。 选择排序的比较操作为 n (n - 1） / 2 次之间。 选择排序的赋值操作介于 0 和 3 (n - 1） 次之间。 比较次数O(n^2），比较次数与关键字的初始状态无关 ，总的比较次数N=(n-1）+(n-2）+...+1=n*(n-1）/2。 交换次数O(n），最好情况是，已经有序，交换0次； 最坏情况交换n-1次，逆序交换n/2次。交换次数比冒泡排序少多了 ，由于交换所需CPU时间比比较所需的CPU时间多， n值较小时，选择排序比冒泡排序快。 稳定性 选择排序是给每个位置选择当前元素最小的，比如给第一个位置选择最小的， 在剩余元素里面给第二个元素选择第二小的，依次类推， 直到第n-1个元素，第n个元素不用选择了，因为只剩下它一个最大的元素了。 那么，在一趟选择，如果一个元素比当前元素小， 而该小的元素又出现在一个和当前元素相等的元素后面，那么交换后稳定性就被破坏了。 比较拗口，举个例子，序列5 8 5 2 9，我们知道第一遍选择第1个元素5会和2交换， 那么原序列中两个5的相对前后顺序就被破坏了， 所以选择排序是一个不稳定的排序算法 "},"Chapter03/InsertionSort.html":{"url":"Chapter03/InsertionSort.html","title":"插入排序","keywords":"","body":"插入排序 (直接插入排序) 插入排序（Insertion-Sort） 的算法描述是一种简单直观的排序算法。 它的工作原理是通过构建有序序列，对于未排序数据， 在已排序序列中从后向前扫描，找到相应位置并插入。 插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序）， 因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位， 为最新元素提供插入空间。 算法描述 一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 步骤1: 从第一个元素开始，该元素可以认为已经被排序； 步骤2: 取出下一个元素，在已经排序的元素序列中从后向前扫描； 步骤3: 如果该元素（已排序）大于新元素，将该元素移到下一位置； 步骤4: 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 步骤5: 将新元素插入到该位置后； 步骤6: 重复步骤2~5。 动图演示 代码实现 方式一 package algorithm.sort; import java.util.Arrays; public class Insertion { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(Insertion.insertion(array))); } private static int[] insertion(int[] array) { if (array.length == 0) { return array; } int arrayLength = array.length; int current; int preIndex; for (int i = 1; i = 0 && current 方式二 package algorithm.sort; import java.util.Arrays; public class Insertion { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(Insertion.insertion(array))); } private static int[] insertion(int[] array) { if (array.length == 0) { return array; } int arrayLength = array.length; int item; for (int i = 1; i 0 && array[j-1] > array[j] ; j--) { item = array[j]; array[j] = array[j-1]; array[j-1] = item; } } return array; } } 显然方式二比方式一的赋值操作多，所以方式一更好 算法分析 最佳情况：T(n) = O(n) 最坏情况：T(n) = O(n2) 平均情况：T(n) = O(n2) 插入排序在实现上，通常采用in-place排序 （即只需用到O(1)的额外空间的排序）， 因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位， 为最新元素提供插入空间。 时间复杂度 在插入排序中，当待排序数组是有序时，是最优的情况， 只需当前数跟前一个数比较一下就可以了，这时一共需要比较N- 1次， 时间复杂度为 O(N) 。 最坏的情况是待排序数组是逆序的， 此时需要比较次数最多，总次数记为：1+2+3+…+N-1， 所以，插入排序最坏情况下的时间复杂度为 O(N2) 。 空间复杂度 插入排序的空间复杂度为常数阶 稳定性分析 关键词相同的数据元素将保持原有位置不变，所以该算法是稳定的 "},"Chapter03/ShellSort.html":{"url":"Chapter03/ShellSort.html","title":"希尔排序","keywords":"","body":"希尔排序 希尔排序是希尔（Donald Shell） 于1959年提出的一种排序算法。 希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破O(n2）的第一批算法之一。 它与插入排序的不同之处在于，它会优先比较距离较远的元素。 希尔排序又叫缩小增量排序。 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序； 随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。 算法描述 我们来看下希尔排序的基本步骤，在此我们选择增量gap=length/2，缩小增量继续以gap = gap/2的方式，这种增量选择我们可以用一个序列来表示，{n/2,(n/2)/2…1}，称为增量序列。 希尔排序的增量序列的选择与证明是个数学难题，我们选择的这个增量序列是比较常用的，也是希尔建议的增量，称为希尔增量，但其实这个增量序列不是最优的。 此处我们做示例使用希尔增量。 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 步骤1：选择一个增量序列t1，t2，…，tk，其中ti>tj，tk=1； 步骤2：按增量序列个数k，对序列进行k 趟排序； 步骤3：每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。 仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 过程演示 代码实现 package algorithm.sort; import java.util.Arrays; public class ShellSort { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(ShellSort.shell(array))); } private static int[] shell(int[] array) { int length = array.length; if (length == 0) { return array; } int tmp, preIndex, gap = length / 2; while (gap > 0) { for (int i = gap; i = 0 && array[preIndex] > array[preIndex + gap]) { array[preIndex + gap] = array[preIndex]; preIndex -= gap; } array[preIndex + gap] = tmp; } gap = gap / 2; } return array; } } 算法分析 最佳情况：T(n) = O(nlog2 n) 最坏情况：T(n) = O(nlog2 n) 平均情况：T(n) =O(nlog2n) 希尔排序的核心在于间隔序列的设定。 既可以提前设定好间隔序列，也可以动态的定义间隔序列。 动态定义间隔序列的算法是《算法（第4版）》的合著者Robert Sedgewick提出的 时间性能 1．增量序列的选择 Shell排序的执行时间依赖于增量序列。 好的增量序列的共同特征： ① 最后一个增量必须为1； ② 应该尽量避免序列中的值(尤其是相邻的值)互为倍数的情况。 有人通过大量的实验，给出了较好的结果：当n较大时，比较和移动的次数约在n^1.25到(1.6n)^1.25之间。 2．Shell排序的时间性能优于直接插入排序 希尔排序的时间性能优于直接插入排序的原因： ①当文件初态基本有序时直接插入排序所需的比较和移动次数均较少。 ②当n值较小时，n和 的差别也较小，即直接插入排序的最好时间复杂度O(n)和最坏时间复杂度0( )差别不大。 ③在希尔排序开始时增量较大，分组较多，每组的记录数目少，故各组内直接插入较快，后来增量di逐渐缩小，分组数逐渐减少，而各组的记录数目逐渐增多，但由于已经按di-1作为距离排过序，使文件较接近于有序状态，所以新的一趟排序过程也较快。 因此，希尔排序在效率上较直接插入排序有较大的改进。 稳定性 由于多次插入排序，我们知道一次插入排序是稳定的， 不会改变相同元素的相对顺序，但在不同的插入排序过程中， 相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱， 所以shell排序是不稳定的。 "},"Chapter03/MergeSort.html":{"url":"Chapter03/MergeSort.html","title":"归并排序","keywords":"","body":"归并排序 和选择排序一样，归并排序的性能不受输入数据的影响， 但表现比选择排序好的多， 因为始终都是O(n log n）的时间复杂度。代价是需要额外的内存空间。 归并排序 是建立在归并操作上的一种有效的排序算法。 该算法是采用分治法 （Divide and Conquer: 分治法将问题分(divide)成一些小的问题然后递归求解， 而治(conquer)的阶段则将分的阶段得到的各答案\"修补\"在一起，即分而治之） 的一个非常典型的应用。 归并排序是一种稳定的排序方法。 将已有序的子序列合并，得到完全有序的序列； 即先使每个子序列有序，再使子序列段间有序。 若将两个有序表合并成一个有序表，称为2-路归并。 算法描述 步骤1：把长度为n的输入序列分成两个长度为n/2的子序列； 步骤2：对这两个子序列分别采用归并排序； 步骤3：将两个排序好的子序列合并成一个最终的排序序列。 过程演示 分而治之 可以看到这种结构很像一棵完全二叉树， 本文的归并排序我们采用递归去实现（也可采用迭代的方式去实现）。 分阶段可以理解为就是递归拆分子序列的过程，递归深度为log2n 合并相邻有序子序列 再来看看治阶段，我们需要将两个已经有序的子序列合并成一个有序序列，比如上图中的最后一次合并，要将[4,5,7,8]和[1,2,3,6]两个已经有序的子序列，合并为最终序列[1,2,3,4,5,6,7,8]，来看下实现步骤。 归并排序动图 代码实现 递归法 package algorithm.sort; import java.util.Arrays; public class MergeSort { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(MergeSort.mergeSort(array))); } private static int[] mergeSort(int[] array) { int length = array.length; if (length = left.length) { //左数组已经遍历完 item[index] = right[r++]; } else if (r >= right.length) { //右侧数组已经遍历完 item[index] = left[l++]; } else if (left[l] > right[r]) { //这样是为了保证稳定性 item[index] = right[r++]; } else { item[index] = left[l++]; } } return item; } TimSort 归并排序是稳定排序，它也是一种十分高效的排序，能利用完全二叉树特性的排序一般性能都不会太差。java中Arrays.sort()采用了一种名为TimSort的排序算法，就是归并排序的优化版本。 从上文的图中可看出，每次合并操作的平均时间复杂度为O(n)，而完全二叉树的深度为|log2n|。总的平均时间复杂度为O(nlogn)。而且，归并排序的最好，最坏，平均时间复杂度均为O(nlogn) 算法分析 归并排序是一种稳定的排序方法。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(nlogn）的时间复杂度。代价是需要额外的内存空间。 最佳情况：T(n) = O(n) 最差情况：T(n) = O(nlogn) 平均情况：T(n) = O(nlogn) 复杂度 归并排序比较占用内存，但却是一种效率高且稳定的算法。 改进归并排序在归并时先判断前段序列的最大值与后段序列最小值的关系再确定是否进行复制比较。如果前段序列的最大值小于等于后段序列最小值，则说明序列可以直接形成一段有序序列不需要再归并，反之则需要。所以在序列本身有序的情况下时间复杂度可以降至O(n) TimSort可以说是归并排序的终极优化版本，主要思想就是检测序列中的天然有序子段（若检测到严格降序子段则翻转序列为升序子段）。在最好情况下无论升序还是降序都可以使时间复杂度降至为O(n)，具有很强的自适应性。 名称 最好时间复杂度 平均时间复杂度 最坏时间复杂度 空间复杂度 稳定性 传统归并排序 O(nlogn) O(nlogn) O(nlogn) T(n) 稳定 改进归并排序 O(n) O(nlogn) O(nlogn) T(n) 稳定 TimSort O(n) O(nlogn) O(nlogn) T(n) 稳定 注：改进归并排序是一种改进的原地归并算法，空间复杂度为O(1)。在表格里的改进归并排序只是引入其预先判断的这一步，这样便可使传统归并排序时间复杂度降至O(n) "},"Chapter03/QuickSort.html":{"url":"Chapter03/QuickSort.html","title":"快速排序","keywords":"","body":"快速排序 快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分， 其中一部分记录的关键字均比另一部分的关键字小， 则可分别对这两部分记录继续进行排序，以达到整个序列有序。 算法描述 快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 步骤1：从数列中挑出一个元素，称为 “基准”（pivot ）； 步骤2：重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 步骤3：递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序 动图演示 代码实现 package algorithm.sort; import java.util.Arrays; import java.util.HashMap; import java.util.Map; import java.util.Stack; public class QuickSort { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(QuickSort.quickSort2(array))); } //填坑法 private static int[] quickSort1(int[] array) { int length = array.length; if (length = right) { return; } //pivot pivət 枢纽 int pivot = array[left]; int index = left; int rIndex = right; int lIndex = left; for (; lIndex = right) { return; } //pivot pivət 枢纽 int pivot = array[left]; int index = left; int rIndex = right; int lIndex = left; while (lIndex pivot) { array[rIndex] = array[lIndex]; index = lIndex; rIndex--; break; } lIndex++; } } array[index] = pivot; sort11(array, left, index - 1); sort11(array, index + 1, right); } //指针法 private static int[] quickSort2(int[] array) { int length = array.length; if (length = right) { return; } int pivod = array[left]; int lIndex = left, rIndex = right; while (lIndex pivod && lIndex > quickSortStack = new Stack>(); Map rootParam = new HashMap(); rootParam.put(\"startIndex\", left); rootParam.put(\"endIndex\", right); quickSortStack.push(rootParam); while (!quickSortStack.isEmpty()) { // 栈顶元素出栈，得到起止下标 Map param = quickSortStack.pop(); // 得到基准元素位置 int pivotIndex = partition(array, param.get(\"startIndex\"), param.get(\"endIndex\")); // 根据基准元素分成两部分, 把每一部分的起止下标入栈 if (param.get(\"startIndex\") leftParam = new HashMap(); leftParam.put(\"startIndex\", param.get(\"startIndex\")); leftParam.put(\"endIndex\", pivotIndex - 1); quickSortStack.push(leftParam); } if (pivotIndex + 1 rightParam = new HashMap(); rightParam.put(\"startIndex\", pivotIndex + 1); rightParam.put(\"endIndex\", param.get(\"endIndex\")); quickSortStack.push(rightParam); } } } private static int partition(int[] arr, int startIndex, int endIndex) { // 取第一个位置的元素作为基准元素 int pivot = arr[startIndex]; int left = startIndex; int right = endIndex; while (left pivot) { right--; } //控制left指针比较并右移 while (left 算法分析 最佳情况：T(n) = O(nlogn) 最差情况：T(n) = O(n2) 平均情况：T(n) = O(nlogn) 参考文档 https://www.sohu.com/a/246785807_684445 "},"Chapter03/HeapSort.html":{"url":"Chapter03/HeapSort.html","title":"堆排序","keywords":"","body":"堆排序 堆排序（Heapsort） 是指利用堆这种数据结构所设计的一种排序算法。 堆积是一个近似完全二叉树的结构， 并同时满足堆积的性质： 即子结点的键值或索引总是小于（或者大于）它的父节点 算法描述 步骤1：将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 步骤2：将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1] 步骤3：由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。 动图演示 代码实现 package algorithm.sort; import java.util.Arrays; public class HeapSort { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(HeapSort.heapSort(array))); } private static int[] heapSort(int[] array) { int length = array.length; if (length 1) { int temp = array[0]; array[0] = array[length - 1]; array[length - 1] = temp; length--; buildMaxHeap(array, length); } return array; } /** * 建立最大堆 * * @param array * @param length */ private static void buildMaxHeap(int[] array, int length) { for (int i = length / 2 - 1; i >= 0; i--) { adjustHeap(array, i, length); } } private static void adjustHeap(int[] array, int i, int length) { int temp = array[i]; int parentIndex = i; int chaildIndex = 2 * parentIndex + 1; while (chaildIndex package algorithm.sort; import java.util.Arrays; public class HeapSort { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(HeapSort.heapSort(array))); } private static int[] heapSort(int[] array) { int length = array.length; if (length 1) { int temp = array[0]; array[0] = array[length - 1]; array[length - 1] = temp; length--; popBuildMaxHeap(array, length); } return array; } private static void popBuildMaxHeap(int[] array, int length) { int parentIndex = 0; int temp = array[parentIndex]; int childIndex = 2 * parentIndex + 1; while (childIndex temp) { array[parentIndex] = array[childIndex]; parentIndex = childIndex; childIndex = 2 * parentIndex + 1; } else { break; } } array[parentIndex] = temp; } /** * 建立最大堆 * * @param array * @param length */ private static void buildMaxHeap(int[] array, int length) { for (int i = 0; i array[parentIndex]) { int temp = array[parentIndex]; array[parentIndex] = array[childIndex]; array[childIndex] = temp; } } } 算法分析 最佳情况：T(n) = O(nlogn) 最差情况：T(n) = O(nlogn) 平均情况：T(n) = O(nlogn) 堆的应用 堆排序 其实就是用要排序的元素建一个堆（视情况而定是大根堆还是小根堆），然后依次弹出堆顶元素，最后得到的就是排序后的结果了 但是裸的并没有什么用，我们有sort而且sort还比堆排快，所以堆排一般都没有这种模板题，一般是利用堆排的思想，然后来搞一些奇奇怪怪的操作，第2个应用就有涉及到一点堆排的思想 用两个堆来维护一些查询第k小/大的操作 洛谷P1801 黑匣子 利用一个大根堆一个小根堆来维护第k小，并没有强制在线 不强制在线，所以我们直接读入所有元素，枚举询问，因为要询问第k小，所以把前面的第k个元素都放进大根堆里面，然后如果元素数量大于k，就把堆顶弹掉放到小根堆里面，使大根堆的元素严格等于k，这样这次询问的结果就是小根堆的堆顶了（前面k-1小的元素都在大根堆里面了） 记得在完成这次询问后重新把小根堆的堆顶放到大根堆里面就好 中位数 中位数也是这种操作可以解决的一种经典问题，但是实际应用不大（这种操作的复杂度为，然而求解中位数有做法） Luogu中也有此类例题，题解内也讲的比较清楚了，此处不再赘述，读者可当做拓展练习进行食用 提示：设序列长度为，则中位数其实等价于序列中大的元素 例题：Luogu P1168 中位数 事实上堆在难度较高的题目方面更多的用于维护一些贪心操作， 以降低复杂度，很少会有题目是以堆为正解来出的了，更多的，堆在这些题目中处于“工具”的位置 利用堆来维护可以“反悔的贪心” 题目：Luogu P2949 [USACO09OPEN]工作调度Work Scheduling 这道题的话算是这种类型应用的经典题了 首先只要有贪心基础就不难想出一个解题思路：因为所有工作的花费时间都一样，我们只要尽量的选获得利润高的工作，以及对于每个所选的工作，我们尽量让它在更靠近它的结束时间的地方再来工作 但是两种条件我们并不好维护，这种两个限制条件的题目也是有一种挺经典的做法的：对一个限制条件进行排序，对于另一个限制条件使用某些数据结构来维护（如treap，线段树，树状数组之类），但是这并不在我们今天的讨论范畴QAQ 考虑怎么将这两个条件“有机统一”。 排序的思路是没有问题的，我们可以对每个工作按照它的结束时间进行排序，从而来维护我们的第二个贪心的想法。 那么对于这样做所带来的一个冲突：对于一个截止时间在d的工作，我们有可能把0~d秒全都安排满了（可能会有多个任务的截止时间相同） 怎么解决这种冲突并保证答案的最有性呢？ 一个直观的想法就是把我们目前已选的工作全部都比较一下，然后选出一个创造的利润最低的工作（假设当前正在决策的这个工作价值很高），然后舍弃掉利润最低的工作，把这个工作放进去原来的那个位置。（因为我们已经按照结束时间排序了，所以舍弃的那个任务的截止完成时间一定在当前决策的工作的之前） 但是对于大小高达的n，的复杂度显然是无法接受的，结合上面的内容，读者们应该也不难想出，可以使用堆来优化这个操作 我们可以在选用了这个工作之后，将当前工作放入小根堆中，如果堆内元素大于等于当前工作的截止时间了（因为这道题中，一个工作的执行时间是一个单位时间），我们就可以把当前工作跟堆顶工作的价值比较，如果当前工作的价值较大，就可以将堆顶弹出，然后将新的工作放入堆中，给答案加上当前工作减去堆顶元素的价值（因为堆顶元素在放入堆中的时候价值已经累加进入答案了）。如果堆内元素小于截止时间那么直接放入堆中就好 至此，我们已经可以以的效率通过本题 而通过这道题我们也可以发现，只有在优化我们思考出来的贪心操作的时间复杂度时，我们才用到了堆。正如我们先前所说到的，在大部分有一定难度的题目里，堆都是以一个“工具”的身份出现，用于优化算法（大多时候是贪心）的时间复杂度等 "},"Chapter03/CountingSort.html":{"url":"Chapter03/CountingSort.html","title":"计数排序","keywords":"","body":"计数排序 ​ 计数排序 的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 计数排序(Counting sort) 是一种稳定的排序算法。计数排序使用一个额外的数组C，其中第i个元素是待排序数组A中值等于i的元素的个数。然后根据数组C来将A中的元素排到正确的位置。它只能对整数进行排序。 算法描述 步骤1：找出待排序的数组中最大和最小的元素； 步骤2：统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 步骤3：对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 步骤4：反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 动图演示 代码实现 package algorithm.sort; import java.util.Arrays; public class CountingSort { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5, 7, 6}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(CountingSort.countingSort(array))); } private static int[] countingSort(int[] array) { if (array.length max) { max = a; } } //根据数列最大值和最小值的差值确定统计数组的长度 int[] buket = new int[max - min + 1]; //偏移量 int bias = 0 - min; for (int i = 0; i 算法分析 当输入的元素是n 个0到k之间的整数时，它的运行时间是 O(n + k)。计数排序不是比较排序，排序的速度快于任何比较排序算法。由于用来计数的数组C的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。 最佳情况：T(n) = O(n+k) 最差情况：T(n) = O(n+k) 平均情况：T(n) = O(n+k) 计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 适用范围 假定20个随机整数的值如下： 9，3，5，4，9，1，2，7，8，1，3，6，5，3，4，0，10，9 ，7，9 如何给这些无序的随机整数排序呢？ 非常简单，让我们遍历这个无序的随机数列，每一个整数按照其值对号入座，对应数组下标的元素进行加1操作。 比如第一个整数是9，那么数组下标为9的元素加1： 第二个整数是3，那么数组下标为3的元素加1： 继续遍历数列并修改数组...... 最终，数列遍历完毕时，数组的状态如下： 数组每一个下标位置的值，代表了数列中对应整数出现的次数。 有了这个“统计结果”，排序就很简单了。直接遍历数组，输出数组元素的下标值，元素的值是几，就输出几次： 0，1，1，2，3，3，3，4，4，5，5，6，7，7，8，9，9，9，9，10 显然，这个输出的数列已经是有序的了。 这就是计数排序的基本过程，它适用于一定范围的整数排序。在取值范围不是很大的情况下，它的性能甚至快过那些O（nlogn）的排序 但是对于这样的数据应该怎么考虑呢 95，94，91，98，99，90，99，93，91，92 很简单，我们不再以（输入数列的最大值+1）作为统计数组的长度，而是以（数列最大值和最小值的差+1）作为统计数组的长度。 同时，数列的最小值作为一个偏移量，用于统计数组的对号入座。 以刚才的数列为例，统计数组的长度为 99-90+1 = 10 ，偏移量等于数列的最小值 90 。 对于第一个整数95，对应的统计数组下标是 95-90 = 5，如图所示： 朴素版的计数排序只是简单地按照统计计数组地下标输入了元素值，并没有真正给原始数列进行排序 如果是单纯的给整数排序，这样并没有问题。但如果放到现实业务里，比如给学生的考试分数排序，遇到相同的分数就会分不清谁是谁。 什么意思？ 举个例子： 给定一个学生的成绩表，要求按成绩从低到高排序，如果成绩相同，则遵循原表固有顺序。 那么，当我们填充统计数组以后，我们只知道有两个成绩并列95分的小伙伴，却不知道哪一个是小红，哪一个是小绿： 下面的讲解会有一些烧脑，请大家扶稳坐好。我们仍然以刚才的学生成绩表为例，把之前的统计数组变形成下面的样子： 这是如何变形的呢？统计数组从第二个元素开始，每一个元素都加上前面所有元素之和。 为什么要相加呢？初次看到的小伙伴可能会觉得莫名其妙。 这样相加的目的，是让统计数组存储的元素值，等于相应整数的最终排序位置。比如下标是9的元素值为5，代表原始数列的整数9，最终的排序是在第5位。 接下来，我们创建输出数组sortedArray，长度和输入数列一致。然后从后向前遍历输入数列： 第一步，我们遍历成绩表最后一行的小绿： 小绿是95分，我们找到countArray下标是5的元素，值是4，代表小绿的成绩排名位置在第4位。 同时，我们给countArray下标是5的元素值减1，从4变成3,，代表着下次再遇到95分的成绩时，最终排名是第3。 第三步，我们遍历成绩表倒数第三行的小红： 小红是95分，我们找到countArray下标是5的元素，值是3（最初是4，减1变成了3），代表小红的成绩排名位置在第3位。 同时，我们给countArray下标是5的元素值减1，从3变成2,，代表着下次再遇到95分的成绩时（实际上已经遇不到了），最终排名是第2。 这样一来，同样是95分的小红和小绿就能够清楚地排出顺序了，也正因此，优化版本的计数排序属于稳定排序。 后面的遍历过程以此类推，这里就不再详细描述了。 package algorithm.sort; import java.util.Arrays; public class CS { public static int[] countSort(int[] array) { //1.得到数列的最大值和最小值，并算出差值d int max = array[0]; int min = array[0]; for (int i = 1; i max) { max = array[i]; } if (array[i] = 0; i--) { sortedArray[countArray[array[i] - min] - 1] = array[i]; countArray[array[i] - min]--; } return sortedArray; } public static void main(String[] args) { int[] array = new int[]{95, 94, 91, 98, 99, 90, 99, 93, 91, 92}; int[] sortedArray = countSort(array); System.out.println(Arrays.toString(sortedArray)); } } 局限性 当数列最大最小值差距过大时，并不适用计数排序。 比如给定20个随机整数，范围在0到1亿之间，这时候如果使用计数排序，需要创建长度1亿的数组。不但严重浪费空间，而且时间复杂度也随之升高。 当数列元素不是整数，并不适用计数排序。 如果数列中的元素都是小数，比如25.213，或是0.00000001这样子，则无法创建对应的统计数组。这样显然无法进行计数排序。 "},"Chapter03/BucketSort.html":{"url":"Chapter03/BucketSort.html","title":"桶排序","keywords":"","body":"桶排序 桶排序 是计数排序的升级版。 它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。 桶排序 (Bucket sort)的工作的原理： 假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排 算法描述 步骤1：人为设置一个BucketSize，作为每个桶所能放置多少个不同数值（例如当BucketSize==5时，该桶可以存放｛1,2,3,4,5｝这几种数字，但是容量不限，即可以存放100个3）； 步骤2：遍历输入数据，并且把数据一个一个放到对应的桶里去； 步骤3：对每个不是空的桶进行排序，可以使用其它排序方法，也可以递归使用桶排序； 步骤4：从不是空的桶里把排好序的数据拼接起来。 注意，如果递归使用桶排序为各个桶排序，则当桶数量为1时要手动减小BucketSize增加下一循环桶的数量，否则会陷入死循环，导致内存溢出。 图片演示 代码实现 package algorithm.sort; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import java.util.stream.Collectors; public class BucketSort { public static void main(String[] args) { int[] array = {1, 2, 9, 4, 6, 7, 8, 3, 0, 5, 7, 6}; System.out.println(\"原始数组：\" + Arrays.toString(array)); System.out.println(\"排序后数组：\" + Arrays.toString(BucketSort.bucketSort(array, 3))); } private static int[] bucketSort(int[] array, int buketSize) { if (array.length max) { max = a; } } //求出桶的数量 int bucketCount = (max - min) / buketSize + 1; List originList = Arrays.stream(array).boxed().collect(Collectors.toList()); ArrayList> bucketArr = new ArrayList>(bucketCount); for (int i = 0; i ()); } for (int i = 0; i bucket : bucketArr) { //这里应该桶内排序 for (int b : bucket) { array[index++] = b; } } return array; } } 算法分析 桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 最佳情况：T(n) = O(n+k) 最差情况：T(n) = O(n+k) 平均情况：T(n) = O(n2) 假设原始数列有n个元素，分成m个桶（我们采用的分桶方式 m=n），平均每个桶的元素个数为n/m。 下面我们来逐步分析算法复杂度： 第一步求数列最大最小值，运算量为n。 第二步创建空桶，运算量为m。 第三步遍历原始数列，运算量为n。 第四步在每个桶内部做排序，由于使用了O（nlogn）的排序算法，所以运算量为 n/m log(n/m ) m。 第五步输出排序数列，运算量为n。 加起来，总的运算量为 3n+m+ n/m log(n/m ) m = 3n+m+n(logn-logm) 。 去掉系数，时间复杂度为： O(n+m+n(logn-logm)） 至于空间复杂度就很明显了： 空桶占用的空间 + 数列在桶中占用的空间 = O（m+n）。 桶排序在性能上并非绝对稳定。理想情况下，桶中的元素分布均匀，当 n = m时，时间复杂度可以达到O(n). 但是，如果桶内元素的分布极不均衡，极端情况下第一个桶中有n-1个元素，最后一个桶中有1个元素。此时的时间复杂度退化到O(nlogn)，还白白创建了许多空桶。 桶的概念 每一个桶（bucket）代表一个区间范围，里面可以承载一个或多个元素。桶排序的第一步，就是创建这些桶，确定每一个桶的区间范围： 具体建立多少个桶，如何确定桶的区间范围，有很多不同的方式。我们这里创建的桶数量等于原始数列的元素数量，除了最后一个桶只包含数列最大值，前面各个桶的区间按照比例确定。 区间跨度 = （最大值-最小值）/ （桶的数量 - 1） 第二步，遍历原始数列，把元素对号入座放入各个桶中： 第三步，每个桶内部的元素分别排序（显然，只有第一个桶需要排序） 第四步，遍历所有的桶，输出所有元素： 0.5，0.84，2.18，3.25，4.5 到此为止，排序结束。 "},"Chapter03/RadixSort.html":{"url":"Chapter03/RadixSort.html","title":"基数排序","keywords":"","body":"基数排序 ​ 基数排序也是非比较的排序算法，对每一位进行排序，从最低位开始排序，复杂度为O(kn),为数组长度，k为数组中的数的最大的位数； 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以是稳定的。 算法描述 步骤1：取得数组中的最大数，并取得位数； 步骤2：arr为原始数组，从最低位开始取每个位组成radix数组； 步骤3：对radix进行计数排序（利用计数排序适用于小范围数的特点）； 动图演示 算法分析 最佳情况：T(n) = O(n k) 最差情况：T(n) = O(n k) 平均情况：T(n) = O(n * k) 基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n>>k，因此额外空间需要大概n个左右。 基数排序有两种方法： MSD 从高位开始进行排序 LSD 从低位开始进行排序 基数排序 vs 计数排序 vs 桶排序 这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异： 基数排序： 根据键值的每位数字来分配桶 计数排序： 每个桶只存储单一键值 桶排序： 每个桶存储一定范围的数值 计数排序有什么局限呢？让我们看两个特殊的需求： 需求A，为一组给定的手机号排序： 18914021920 13223132981 13566632981 13660891039 13361323035 ........ ........ 按照计数排序的思路，我们要根据手机号的取值范围，创建一个空数组。 可是，11位手机号有多少种组合？恐怕要建立一个大得不可想象的数组，才能装下所有可能出现的11位手机号！ 需求B，为一组英文单词排序： banana apple orange peach cherry ........ ........ 计数排序适合的场景是对整数做排序，如果遇到英文单词，就无能为力了。 如何有效处理诸如手机号、英文单词等复杂元素的排序呢？仅仅靠一次计数排序很难实现。 这时候，我们不妨把排序工作拆分成多个阶段，每一个阶段只根据一个字符进行计数排序，一共排序k轮（k是元素长度）。 或许这样的描述有些抽象，我们来举一个例子。 数组中有若干个字符串元素，每个字符串元素都是由三个英文字母组成： bda，cfd，qwe，yui，abc，rrr，uee 如何将这些字符串按照字母顺序排序呢？ 由于每个字符串的长度是3个字符，我们可以把排序工作拆分成3轮： 第一轮：按照最低位字符排序。排序过程使用计数排序，把字母的ascii码对应到数组下标，第一轮排序结果如下： 第二轮：在第一轮排序结果的基础上，按照第二位字符排序。 需要注意的是，这里使用的计数排序必须是稳定排序，这样才能保证第一轮排出的先后顺序在第二轮还能继续保持。 比如在第一轮排序后，元素uue在元素yui之前。那么第二轮排序时，两者的第二位字符虽然同样是u，但先后顺序万万不能变，否则第一轮排序就白做了。 第三轮：在第二轮排序结果的基础上，按照最高位字符排序。 如此一来，这些字符串的顺序就排好了。 像这样把字符串元素按位拆分，每一位进行一次计数排序的算法，就是基数排序（Radix Sort）。 基数排序既可以从高位优先进行排序（Most Significant Digit first，简称MSD），也可以从低位优先进行排序（Least Significant Digit first，简称LSD）。 刚才我们所举的例子，就是典型的LSD方式的基数排序。 "},"Chapter03/SortSummary.html":{"url":"Chapter03/SortSummary.html","title":"排序算法总结","keywords":"","body":"排序算法总结 https://zhuanlan.zhihu.com/p/99273811 "},"Chapter03/SearchAlgorithm.html":{"url":"Chapter03/SearchAlgorithm.html","title":"查找算法","keywords":"","body":"查找算法 查找是在大量的信息中寻找一个特定的信息元素，在计算机应用中， 查找是常用的基本运算，例如编译程序中符号表的查找。 查找定义 根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。 查找算法分类 静态查找和动态查找； 静态或者动态都是针对查找表而言的。 动态表指查找表中有删除和插入操作的表。 无序查找和有序查找。 无序查找：被查找数列有序无序均可； 有序查找：被查找数列必须为有序数列。 平均查找长度（Average Search Length，ASL） 需和指定key进行比较的关键字的个数的期望值，称为查找算法在查找成功时的平均查找长度。 对于含有n个数据元素的查找表，查找成功的平均查找长度为：ASL = Pi*Ci的和。 Pi：查找表中第i个数据元素的概率。 Ci：找到第i个数据元素时已经比较过的次数。 七大查找算法 说是七种，其实二分查找、插值查找以及斐波那契查找都可以归为一类——插值查找。 插值查找和斐波那契查找是在二分查找的基础上的优化查找算法。 树表查找和哈希查找。 顺序查找 二分查找 插值查找 斐波那契查找 树表查找 分块查找 哈希查找 "},"Chapter03/SequentialSearch.html":{"url":"Chapter03/SequentialSearch.html","title":"顺序查找","keywords":"","body":"顺序查找 顺序查找适合于存储结构为顺序存储或链接存储的线性表。 基本思想 顺序查找也称为线形查找，属于无序查找算法。 从数据结构线形表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较， 若相等则表示查找成功；若扫描结束仍没有找到关键字等于k的结点，表示查找失败。 在程序中初始化创建查找表时，由于是顺序存储，所以将所有的数据元素存储在数组中， 但是把第一个位置留给了用户用于查找的关键字。 例如，在顺序表{1,2,3,4,5,6}中查找数据元素值为 7 的元素， 则添加后的顺序表为： 复杂度分析 查找成功时的平均查找长度为：（假设每个数据元素的概率相等） ASL = 1/n(1+2+3+…+n) = (n+1)/2 ; 当查找不成功时，需要n+1次比较，时间复杂度为O(n); 所以，顺序查找的时间复杂度为O(n)。 代码实现 package algorithm.seek; /** * 顺序查找 */ public class SequentialSearch { /** * @param array 数集合 * @param target 目标数 */ private static int sequential(int[] array, int target) { int length = array.length; for (int i = 0; i "},"Chapter03/BinarySearch.html":{"url":"Chapter03/BinarySearch.html","title":"二分查找","keywords":"","body":"二分查找 适用于数据量较大时，但是数据需要先排好顺序 说明 元素必须是有序的，如果是无序的则要先进行排序操作。 基本思想 也称为是折半查找，属于有序查找算法。 用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功； 若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行， 直到查找到或查找结束发现表中没有这样的结点。 例如，在{53，14，99，38，45，87，10，81，47，21}这个查找表使用折半查找算法查找数据之前，需要首先对该表中的数据按照所查的关键字进行排序：{10，14，21，38，45，47，53，81，87，99}，采用折半查找算法查找关键字为 47 的过程为： 复杂度分析 最坏情况下，关键词比较次数为log2(n+1)， 且期望时间复杂度为O(log2n)； 折半查找的前提条件是需要有序表顺序存储，对于静态查找表， 一次排序后不再变化，折半查找能得到不错的效率。 但对于需要频繁执行插入或删除操作的数据集来说， 维护有序的排序会带来不小的工作量，那就不建议使用。——《大话数据结构》 代码实现 package algorithm.seek; /** * 二分法查找: 适用于数据量较大时，但是数据需要先排好顺序 */ public class BinarySearch { /** * @param array 数集合 * @param target 目标数 */ private static int binary(int[] array, int target) { int low = 0; int high = array.length - 1; while (low array[middle]) { low = middle + 1; } if (target "},"Chapter03/InterpolationLookup.html":{"url":"Chapter03/InterpolationLookup.html","title":"插值查找","keywords":"","body":"插值查找 在介绍插值查找之前，首先考虑一个新问题， 为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？ 打个比方，在英文字典里面查“apple”， 你下意识翻开字典是翻前面的书页还是后面的书页呢？ 如果再让你查“zoo”，你又怎么查？ 很显然，这里你绝对不会是从中间开始查起， 而是有一定目的的往前或往后翻。 同样的，比如要在取值范围1 ~ 10000 之间 100 个元素从小到大均匀分布的数组中查找5， 我们自然会考虑从数组下标较小的开始查找。 经过以上分析，折半查找这种查找方式， 不是自适应的（也就是说是傻瓜式的）。 二分查找中查找点计算如下： mid=(low+high)/2, 即mid=low+1/2*(high-low); 通过类比，我们可以将查找的点改进为如下： mid=low+(key-a[low])/(a[high]-a[low])*(high-low)， 也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置， 让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。 基本思想 基于二分查找算法，将查找点的选择改进为自适应选择， 可以提高查找效率。当然，差值查找也属于有序查找。 注 对于表长较大，而关键字分布又比较均匀的查找表来说， 插值查找算法的平均性能比折半查找要好的多。 反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 复杂度分析 查找成功或者失败的时间复杂度均为O(log2(log2n))。 代码实现 "},"Chapter03/FibonacciLookup.html":{"url":"Chapter03/FibonacciLookup.html","title":"斐波那契查找","keywords":"","body":"斐波那契查找 在介绍斐波那契查找算法之前， 我们先介绍一下很它紧密相连并且大家都熟知的一个概念——黄金分割。 黄金比例又称黄金分割，是指事物各部分间一定的数学比例关系， 即将整体一分为二，较大部分与较小部分之比等于整体与较大部分之比， 其比值约为1:0.618或1.618:1。 0.618被公认为最具有审美意义的比例数字， 这个数值的作用不仅仅体现在诸如绘画、雕塑、音乐、建筑等艺术领域， 而且在管理、工程设计等方面也有着不可忽视的作用。 因此被称为黄金分割。 大家记不记得斐波那契数列：1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89……. （从第三个数开始，后边每一个数都是前两个数的和）。 然后我们会发现，随着斐波那契数列的递增， 前后两个数的比值会越来越接近0.618， 利用这个特性，我们就可以将黄金比例运用到查找技术中。 基本思想 也是二分查找的一种提升算法， 通过运用黄金比例的概念在数列中选择查找点进行查找， 提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。 相对于折半查找，一般将待比较的key值与第mid=（low+high）/2位置的元素比较， 比较结果分三种情况： 相等，mid位置的元素即为所求 大于，low=mid+1; 小于，high=mid-1。 斐波那契查找与折半查找很相似， 他是根据斐波那契序列的特点对有序表进行分割的。 他要求开始表中记录的个数为某个斐波那契数小1，及n=F(k)-1; 开始将k值与第F(k-1)位置的记录进行比较(及mid=low+F(k-1)-1), 比较结果也分为三种 相等，mid位置的元素即为所求 大于，low=mid+1,k-=2; 说明：low=mid+1说明待查找的元素在[mid+1,high]范围内，k-=2 说明范围[mid+1,high]内的元素个数为n-(F(k-1))= Fk-1-F(k-1)=Fk-F(k-1)-1=F(k-2)-1个，所以可以递归的应用斐波那契查找。 小于，high=mid-1,k-=1。 说明：low=mid+1说明待查找的元素在[low,mid-1]范围内，k-=1 说明范围[low,mid-1]内的元素个数为F(k-1)-1个，所以可以递归 的应用斐波那契查找。 复杂度分析 最坏情况下，时间复杂度为O(log2n)，且其期望复杂度也为O(log2n)。 代码实现 "},"Chapter03/TreeTableLookup.html":{"url":"Chapter03/TreeTableLookup.html","title":"树表查找","keywords":"","body":"树表查找 最简单的树表查找算法——二叉树查找算法。 基本思想 二叉查找树是先对待查找的数据进行生成树，确保树的左分支的值小于右分支的值， 然后在就行和每个节点的父节点比较大小，查找最适合的范围。 这个算法的查找效率很高，但是如果使用这种查找方法要首先创建树。 二叉查找树 BinarySearch Tree，也叫二叉搜索树，或称二叉排序树Binary Sort Tree。 或者是一棵空树，或者是具有下列性质的二叉树： 若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树。 二叉查找树性质 对二叉查找树进行中序遍历，即可得到有序的数列。 不同形态的二叉查找树如下图所示： 复杂度分析 它和二分查找一样，插入和查找的时间复杂度均为O(logn)， 但是在最坏的情况下仍然会有O(n)的时间复杂度。 原因在于插入和删除元素的时候， 树没有保持平衡（比如，我们查找上图（b）中的“93”，我们需要进行n次查找操作）。 我们追求的是在最坏的情况下仍然有较好的时间复杂度， 这就是平衡查找树设计的初衷。 下图为二叉树查找和顺序查找以及二分查找性能的对比图： 基于二叉查找树进行优化，进而可以得到其他的树表查找算法，如平衡树、红黑树等高效算法 平衡查找树之2-3查找树（2-3 Tree） 2-3查找树定义 和二叉树不一样，2-3树运行每个节点保存1个或者两个的值。 对于普通的2节点(2-node)，他保存1个key和左右两个自己点。 对应3节点(3-node)，保存两个Key，2-3查找树的定义如下： 要么为空 对于2节点，该节点保存一个key及对应value，以及两个指向左右节点的节点，左节点也是一个2-3节点，所有的值都比key要小，右节点也是一个2-3节点，所有的值比key要大。 对于3节点，该节点保存两个key及对应value，以及三个指向左中右的节点。左节点也是一个2-3节点，所有的值均比两个key中的最小的key还要小；中间节点也是一个2-3节点， 中间节点的key值在两个跟节点key值之间；右节点也是一个2-3节点，节点的所有key值比两个key中的最大的key还要大。 2-3查找树的性质 如果中序遍历2-3查找树，就可以得到排好序的序列； 在一个完全平衡的2-3查找树中，根节点到每一个为空节点的距离都相同。（这也是平衡树中“平衡”一词的概念，根节点到叶节点的最长距离对应于查找算法的最坏情况，而平衡树中根节点到叶节点的距离都一样，最坏情况也具有对数复杂度。） 复杂度分析 2-3树的查找效率与树的高度是息息相关的。 在最坏的情况下，也就是所有的节点都是2-node节点，查找效率为lgN 在最好的情况下，所有的节点都是3-node节点，查找效率为log3N约等于0.631lgN 距离来说，对于1百万个节点的2-3树，树的高度为12-20之间，对于10亿个节点的2-3树，树的高度为18-30之间。 对于插入来说，只需要常数次操作即可完成，因为他只需要修改与该节点关联的节点即可，不需要检查其他节点，所以效率和查找类似。 下面是2-3查找树的效率 平衡查找树之红黑树（Red-Black Tree） 2-3查找树能保证在插入元素之后能保持树的平衡状态，最坏情况下即所有的子节点都是2-node，树的高度为lgn，从而保证了最坏情况下的时间复杂度。但是2-3树实现起来比较复杂，于是就有了一种简单实现2-3树的数据结构，即红黑树（Red-Black Tree）。 基本思想 红黑树的思想就是对2-3查找树进行编码， 尤其是对2-3查找树中的3-nodes节点添加额外的信息。 红黑树中将节点之间的链接分为两种不同类型，红色链接， 他用来链接两个2-nodes节点来表示一个3-nodes节点。 黑色链接用来链接普通的2-3节点。特别的， 使用红色链接的两个2-nodes来表示一个3-nodes节点，并且向左倾斜， 即一个2-node是另一个2-node的左子节点。 这种做法的好处是查找的时候不用做任何修改， 和普通的二叉查找树相同。 下图是红黑树在各种情况下的时间复杂度，可以看出红黑树是2-3查找树的一种实现，它能保证最坏情况下仍然具有对数的时间复杂度。 B树和B+树（B Tree/B+ Tree） 平衡查找树中的2-3树以及其实现红黑树。 2-3树种，一个节点最多有2个key，而红黑树则使用染色的方式来标识这两个key。 维基百科对B树的定义为“在计算机科学中，B树（B-tree）是一种树状数据结构， 它能够存储数据、对其进行排序并允许以O(log n)的时间复杂度运行进行查找、顺序读取、插入和删除的数据结构。 B树，概括来说是一个节点可以拥有多于2个子节点的二叉查找树。 与自平衡二叉查找树不同，B树为系统最优化大块数据的读和写操作。 B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。 普遍运用在数据库和文件系统。 树表查找总结 二叉查找树平均查找性能不错，为O(logn)，但是最坏情况会退化为O(n)。在二叉查找树的基础上进行优化，我们可以使用平衡查找树。平衡查找树中的2-3查找树，这种数据结构在插入之后能够进行自平衡操作，从而保证了树的高度在一定的范围内进而能够保证最坏情况下的时间复杂度。但是2-3查找树实现起来比较困难，红黑树是2-3树的一种简单高效的实现，他巧妙地使用颜色标记来替代2-3树中比较难处理的3-node节点问题。红黑树是一种比较高效的平衡查找树，应用非常广泛，很多编程语言的内部实现都或多或少的采用了红黑树。 除此之外，2-3查找树的另一个扩展——B/B+平衡树，在文件系统和数据库系统中有着广泛的应用。 "},"Chapter03/BlockSearch.html":{"url":"Chapter03/BlockSearch.html","title":"分块查找","keywords":"","body":"分块查找 思路：它是顺序查找的一种改进方法。 具体来说是二分查找和顺序查找的结合产物， 即首先在构建的索引表上面进行二分查找， 而后在锁定的区间内进行顺序查找 将n个数据元素\"按块有序\"划分为m块（m ≤ n）。 每一块中的结点不必有序，但块与块之间必须\"按块有序\"； 即第1块中任一元素的关键字都必须小于第2块中任一元素的关键字； step1 先选取各块中的最大关键字构成一个索引表； step2 查找分两个部分：先对索引表进行二分查找或顺序查找， 以确定待查记录在哪一块中；然后，在已确定的块中用顺序法进行查找。 代码实现 "},"Chapter03/HashLookup.html":{"url":"Chapter03/HashLookup.html","title":"哈希查找","keywords":"","body":"哈希查找 什么是哈希表（Hash）？ 我们使用一个下标范围比较大的数组来存储元素。 可以设计一个函数（哈希函数， 也叫做散列函数）， 使得每个元素的关键字都与一个函数值（即数组下标）相对应， 于是用这个数组单元来存储这个元素；也可以简单的理解为， 按照关键字为每一个元素\"分类\"，然后将这个元素存储在相应\"类\"所对应的地方。 但是，不能够保证每个元素的关键字与函数值是一一对应的，因此极有可能出现对于不同的元素， 却计算出了相同的函数值，这样就产生了\"冲突\"，换句话说，就是把不同的元素分在了相同的\"类\"之中。 后面我们将看到一种解决\"冲突\"的简便做法。 总的来说，\"直接定址\"与\"解决冲突\"是哈希表的两大特点。 什么是哈希函数？ 哈希函数的规则是：通过某种转换关系，使关键字适度的分散到指定大小的的顺序结构中，越分散， 则以后查找的时间复杂度越小，空间复杂度越高。 算法思想 哈希的思路很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引， 值即为其对应的值，这样就可以快速访问任意键的值。这是对于简单的键的情况，我们将其扩展到可以处理更加复杂的类型的键。 思路 用给定的哈希函数构造哈希表； 根据选择的冲突处理方法解决地址冲突；常见的解决冲突的方法：拉链法和线性探测法。 在哈希表的基础上执行哈希查找。 哈希表是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。 那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找， 这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。 只需要调整哈希函数算法即可在时间和空间上做出取舍。 复杂度分析： 单纯论查找复杂度：对于无冲突的Hash表而言，查找复杂度为O(1)（注意，在查找之前我们需要构建相应的Hash表）。 使用Hash，我们付出了什么？ 我们在实际编程中存储一个大规模的数据，最先想到的存储结构可能就是map， 也就是我们常说的KV pair，经常使用Python的博友可能更有这种体会。 使用map的好处就是，我们在后续处理数据处理时，可以根据数据的key快速的查找到对应的value值。 map的本质就是Hash表，那我们在获取了超高查找效率的基础上，我们付出了什么？ Hash是一种典型以空间换时间的算法，比如原来一个长度为100的数组，对其查找， 只需要遍历且匹配相应记录即可，从空间复杂度上来看，假如数组存储的是byte类型数据， 那么该数组占用100byte空间。现在我们采用Hash算法，我们前面说的Hash必须有一个规则， 约束键与存储位置的关系，那么就需要一个固定长度的hash表，此时，仍然是100byte的数组， 假设我们需要的100byte用来记录键与位置的关系，那么总的空间为200byte, 而且用于记录规则的表大小会根据规则，大小可能是不定的。 代码实现 "},"Chapter03/FindingAlgorithmSummary.html":{"url":"Chapter03/FindingAlgorithmSummary.html","title":"查找算法总结","keywords":"","body":"查找算法总结 "},"Chapter03/CachingAlgorithm.html":{"url":"Chapter03/CachingAlgorithm.html","title":"缓存算法","keywords":"","body":"缓存算法 "},"Chapter03/LRU.html":{"url":"Chapter03/LRU.html","title":"LRU","keywords":"","body":"LRU LRU，最近最少使用，把数据加入一个链表中，按访问时间排序，发生淘汰的时候，把访问时间最旧的淘汰掉。 比如有数据 1，2，1，3，2 此时缓存中已有（1，2） 当3加入的时候，得把后面的2淘汰，变成（3，1） 显然 LRU对于循环出现的数据，缓存命中不高 比如，这样的数据，1，1，1，2，2，2，3，4，1，1，1，2，2，2..... 当走到3，4的时候，1，2会被淘汰掉，但是后面还有很多1，2 LRU的一个实现方法： 用一个双向链表记录访问时间，因为链表插入删除高效，时间新的在前面，旧的在后面。 用一个哈希表记录缓存(key, value)，哈希查找近似O(1)，发生哈希冲突时最坏O(n)， 同时哈希表中得记录 (key, Node(key, value)) package algorithm.cache; import java.util.HashMap; import java.util.Map; public class LRUCache { private Map> data; //容量 private int capacity; private DoublyLinkedNode head; private DoublyLinkedNode tail; public LRUCache() { this(10); } public LRUCache(int capacity) { data = new HashMap<>(capacity, 1); this.capacity = capacity; } public int Size() { return this.data.size(); } public void put(Object key, E element) { DoublyLinkedNode newNode = new DoublyLinkedNode(); newNode.key = key; newNode.element = element; if (this.data.containsKey(key)) { DoublyLinkedNode oldNode = this.data.get(key); this.remove(key, oldNode); } if (data.size() == this.capacity) { removeFail(); } this.add(key, newNode); if (data.size() == 1) { this.head = newNode; this.tail = newNode; this.head.next = this.tail; this.tail.pre = this.head; } } private void removeFail() { DoublyLinkedNode item = this.tail; this.tail = item.pre; data.remove(item.key); } private void add(Object key, DoublyLinkedNode item) { item.next = this.head; if (this.head != null) { this.head.pre = item; } this.head = item; data.put(key, item); } private void remove(Object key, DoublyLinkedNode item) { if (item.equals(this.tail)){ this.tail = item.pre; } if (item.pre != null) { item.pre.next = item.next; } if (item.next != null) { item.next.pre = item.pre; } data.remove(key); } public E get(Object key) { if (!data.containsKey(key)) { return null; } DoublyLinkedNode item = data.get(key); this.remove(key, item); this.add(key, item); return item.element; } private class DoublyLinkedNode { Object key; E element; DoublyLinkedNode pre; DoublyLinkedNode next; } public static void main(String[] args) { LRUCache cache = new LRUCache<>(2); cache.put(1, 1); cache.put(2, 2); System.out.println(\"get 1 = \" + cache.get(1)); System.out.println(\"add 3\"); cache.put(3, 3); // 该操作会使得关键字 2 作废 System.out.println(\"get 2 = \" + cache.get(2)); System.out.println(\"add 4\"); cache.put(4, 4); // 该操作会使得关键字 1 作废 System.out.println(\"get 3 = \" + cache.get(3)); System.out.println(\"get 4 = \" + cache.get(4)); System.out.println(\"get 1 = \" + cache.get(1)); } } get 1 = 1 add 3 get 2 = null add 4 get 3 = 3 get 4 = 4 get 1 = null java官方实现 LinkedHashMap LinkedHashMap底层就是用的HashMap加双链表实现的，而且本身已经实现了按照访问顺序的存储。 此外，LinkedHashMap中本身就实现了一个方法removeEldestEntry用于判断是否需要移除最不常读取的数， 方法默认是直接返回false，不会移除元素，所以需要重写该方法。即当缓存满后就移除最不常用的数。 public class LRU { private static final float hashLoadFactory = 0.75f; private LinkedHashMap map; private int cacheSize; public LRU(int cacheSize) { this.cacheSize = cacheSize; int capacity = (int)Math.ceil(cacheSize / hashLoadFactory) + 1; map = new LinkedHashMap(capacity, hashLoadFactory, true){ private static final long serialVersionUID = 1; @Override protected boolean removeEldestEntry(Map.Entry eldest) { return size() > LRU.this.cacheSize; } }; } public synchronized V get(K key) { return map.get(key); } public synchronized void put(K key, V value) { map.put(key, value); } public synchronized void clear() { map.clear(); } public synchronized int usedSize() { return map.size(); } public void print() { for (Map.Entry entry : map.entrySet()) { System.out.print(entry.getValue() + \"--\"); } System.out.println(); } } 当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致LRU命中率急剧下降，缓存污染情况比较严重。 扩展 LRU-K LRU-K中的K代表最近使用的次数，因此LRU可以认为是LRU-1。LRU-K的主要目的是为了解决LRU算法“缓存污染”的问题，其核心思想是将“最近使用过1次”的判断标准扩展为“最近使用过K次”。 相比LRU，LRU-K需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到K次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K会淘汰第K次访问时间距当前时间最大的数据。 数据第一次被访问时，加入到历史访问列表，如果书籍在访问历史列表中没有达到K次访问，则按照一定的规则（FIFO,LRU）淘汰；当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列中删除，将数据移到缓存队列中，并缓存数据，缓存队列重新按照时间排序；缓存数据队列中被再次访问后，重新排序，需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即“淘汰倒数K次访问离现在最久的数据”。 LRU-K具有LRU的优点，同时还能避免LRU的缺点，实际应用中LRU-2是综合最优的选择。由于LRU-K还需要记录那些被访问过、但还没有放入缓存的对象，因此内存消耗会比LRU要多。 two queue Two queues（以下使用2Q代替）算法类似于LRU-2，不同点在于2Q将LRU-2算法中的访问历史队列（注意这不是缓存数据的）改为一个FIFO缓存队列，即：2Q算法有两个缓存队列，一个是FIFO队列，一个是LRU队列。当数据第一次访问时，2Q算法将数据缓存在FIFO队列里面，当数据第二次被访问时，则将数据从FIFO队列移到LRU队列里面，两个队列各自按照自己的方法淘汰数据。 新访问的数据插入到FIFO队列中，如果数据在FIFO队列中一直没有被再次访问，则最终按照FIFO规则淘汰；如果数据在FIFO队列中再次被访问到，则将数据移到LRU队列头部，如果数据在LRU队列中再次被访问，则将数据移动LRU队列头部，LRU队列淘汰末尾的数据。 Multi Queue(MQ) MQ算法根据访问频率将数据划分为多个队列，不同的队列具有不同的访问优先级，其核心思想是：优先缓存访问次数多的数据。详细的算法结构图如下，Q0，Q1....Qk代表不同的优先级队列，Q-history代表从缓存中淘汰数据，但记录了数据的索引和引用次数的队列： 新插入的数据放入Q0，每个队列按照LRU进行管理，当数据的访问次数达到一定次数，需要提升优先级时，将数据从当前队列中删除，加入到高一级队列的头部；为了防止高优先级数据永远不会被淘汰，当数据在指定的时间里没有被访问时，需要降低优先级，将数据从当前队列删除，加入到低一级的队列头部；需要淘汰数据时，从最低一级队列开始按照LRU淘汰，每个队列淘汰数据时，将数据从缓存中删除，将数据索引加入Q-history头部。如果数据在Q-history中被重新访问，则重新计算其优先级，移到目标队列头部。Q-history按照LRU淘汰数据的索引。 MQ需要维护多个队列，且需要维护每个数据的访问时间，复杂度比LRU高。 LRU算法对比 对比点 对比 命中率 LRU-2 > MQ(2) > 2Q > LRU 复杂度 LRU-2 > MQ(2) > 2Q > LRU 代价 LRU-2 > MQ(2) > 2Q > LRU 实现带有过期时间的LRU // 构造方法：只要有缓存了，过期清除线程就开始工作 public LRU() { swapExpiredPool.scheduleWithFixedDelay(new ExpiredNode(), 3,3,TimeUnit.SECONDS); } public class ExpiredNode implements Runnable { @Override public void run() { // 第一步：获取当前的时间 long now = System.currentTimeMillis(); while (true) { // 第二步：从过期队列弹出队首元素，如果不存在，或者不过期就返回 Node node = expireQueue.peek(); if (node == null || node.expireTime > now)return; // 第三步：过期了那就从缓存中删除，并且还要从队列弹出 cache.remove(node.key); expireQueue.poll(); }// 此过程为while(true)，一直进行判断和删除操作 } } "},"Chapter03/LFU.html":{"url":"Chapter03/LFU.html","title":"LFU","keywords":"","body":"LFU LFU，最近不经常使用，把数据加入到链表中，按频次排序，一个数据被访问过，把它的频次+1，发生淘汰的时候，把频次低的淘汰掉。 比如有数据 1，1，1，2，2，3 缓存中有（1(3次)，2(2次)） 当3加入的时候，得把后面的2淘汰，变成（1(3次)，3(1次)） 区别：LRU 是得把 1 淘汰。 LFU对于交替出现的数据，缓存命中不高 比如，1，1，1，2，2，3，4，3，4，3，4，3，4，3，4，3，4...... 由于前面被（1(3次)，2(2次)） 3加入把2淘汰，4加入把3淘汰，3加入把4淘汰，然而3，4才是最需要缓存的，1去到了3次，谁也淘汰不了它了。 要求是缓存的加入put()，缓存读取get()，都要在O(1)内实现。 LFU的一个实现方法： 用一个主双向链表记录（访问次数，从链表头），从链表中按时间顺序记录着（key） 用一个哈希表记录（key，(value, 主链表ptr，从链表ptr)）ptr表示该key在链表中的地址 然后，get，put都在哈希表中操作，近似O(1)，哈希表中有个节点在链表中的地址，能O(1)找到，并把节点提搞访问频次，链表插入删除也都是O(1)。 "},"Chapter03/encryption.html":{"url":"Chapter03/encryption.html","title":"加密算法","keywords":"","body":"加密算法 使用JDK自带MessageDigest package com.bj58.bic.touchms.util; import lombok.extern.slf4j.Slf4j; import java.math.BigInteger; import java.security.MessageDigest; import java.security.NoSuchAlgorithmException; @Slf4j public class SignUtil { public static final String sign(String converge) { try { MessageDigest md = MessageDigest.getInstance(\"MD5\"); md.reset(); // 计算md5函数 md.update(converge.getBytes()); // digest()最后确定返回md5 hash值，返回值为8位字符串。因为md5 hash值是16位的hex值，实际上就是8位的字符 // BigInteger函数则将8位的字符串转换成16位hex值，用字符串来表示；得到字符串形式的hash值 String singStr = new BigInteger(1, md.digest()).toString(16); //BigInteger会把首位0去掉所以这里要补零 return lowZeroPadding(singStr); } catch (NoSuchAlgorithmException e) { log.error(\"签名加密出错\", e); } return \"\"; } /** * 低位补零，最后必须是32位 * @param singStr * @return */ private static String lowZeroPadding(String singStr) { if (singStr.length() 使用Spring自带的DigestUtils String md5Str = DigestUtils.md5DigestAsHex(\"原串\".getBytes()); 自己定义 /** * md5加密 * @param data * @return * @throws NoSuchAlgorithmException */ public static String md5(String data) throws NoSuchAlgorithmException { MessageDigest md = MessageDigest.getInstance(\"MD5\"); md.update(data.getBytes()); StringBuffer buf = new StringBuffer(); byte[] bits = md.digest(); for(int i=0;i "},"Chapter03/BigDataAlgorithm.html":{"url":"Chapter03/BigDataAlgorithm.html","title":"大数据算法","keywords":"","body":"大数据算法 何谓海量数据处理？ 所谓海量数据处理，无非就是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。 那解决办法呢?针对时间，我们可以采用巧妙的算法搭配合适的数据结构，如Bloom filter/Hash/bit-map/堆/数据库或倒排索引/trie树，针对空间，无非就一个办法：大而化小，分而治之（hash映射），你不是说规模太大嘛，那简单啊，就把规模大化为规模小的，各个击破不就完了嘛。 至于所谓的单机及集群问题，通俗点来讲，单机就是处理装载数据的机器有限(只要考虑cpu，内存，硬盘的数据交互)，而集群，机器有多辆，适合分布式处理，并行计算(更多考虑节点和节点间的数据交互)。 处理海量数据问题，无非就是： 分而治之/hash映射 + hash统计 + 堆/快速/归并排序； 双层桶划分 Bloom filter/Bitmap； Trie树/数据库/倒排索引； 外排序； 分布式处理之Hadoop/Mapreduce。 处理海量数据问题之六把密匙 分而治之/Hash映射 + Hash_map统计 + 堆/快速/归并排序 # 海量日志数据，提取出某日访问百度次数最多的那个IP 既然是海量数据处理，那么可想而知，给我们的数据那就一定是海量的。针对这个数据的海量，我们如何着手呢?对的，无非就是分而治之/hash映射 + hash统计 + 堆/快速/归并排序，说白了，就是先映射，而后统计，最后排序： 分而治之/hash映射：针对数据太大，内存受限，只能是：把大文件化成(取模映射)小文件，即16字方针：大而化小，各个击破，缩小规模，逐个解决 hash_map统计：当大文件转化了小文件，那么我们便可以采用常规的hash_map(ip，value)来进行频率统计。 堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，得到次数最多的IP。 具体而论，则是： “首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如%1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map对那1000个文件中的所有IP进行频率统计，然后依次找出各个文件中频率最大的那个IP）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。”--十道海量数据处理面试题与十个方法大总结。 关于本题，还有几个问题，如下： Hash取模是一种等价映射，不会存在同一个元素分散到不同小文件中的情况，即这里采用的是mod1000算法，那么相同的IP在hash取模后，只可能落在同一个文件中，不可能被分散的。因为如果两个IP相等，那么经过Hash(IP)之后的哈希值是相同的，将此哈希值取模（如模1000），必定仍然相等。 那到底什么是hash映射呢？简单来说，就是为了便于计算机在有限的内存中处理big数据，从而通过一种映射散列的方式让数据均匀分布在对应的内存位置(如大数据通过取余的方式映射成小树存放在内存中，或大文件映射成多个小文件)，而这个映射散列方式便是我们通常所说的hash函数，设计的好的hash函数能让数据均匀分布而减少冲突。尽管数据映射到了另外一些不同的位置，但数据还是原来的数据，只是代替和表示这些原始数据的形式发生了变化而已 寻找热门查询，300万个查询字符串中统计最热门的10个查询 原题：搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 解答：由上面第1题，我们知道，数据大则划为小的，如如一亿个Ip求Top 10，可先%1000将ip分到1000个小文件中去，并保证一种ip只出现在一个文件中，再对每个小文件中的ip进行hashmap计数统计并按数量排序，最后归并或者最小堆依次处理每个小文件的top10以得到最后的结。 但如果数据规模比较小，能一次性装入内存呢?比如这第2题，虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把他们都放进内存中去（300万个字符串假设没有重复，都是最大长度，那么最多占用内存3M*1K/4=0.75G。所以可以将所有字符串都存放在内存中进行处理），而现在只是需要一个合适的数据结构，在这里，HashTable绝对是我们优先的选择。 所以我们放弃分而治之/hash映射的步骤，直接上hash统计，然后排序。So，针对此类典型的TOP K问题，采取的对策往往是：hashmap + 堆。如下所示： hash_map统计：先对这批海量数据预处理。具体方法是：维护一个Key为Query字串，Value为该Query出现次数的HashTable，即hash_map(Query，Value)，每次读取一个Query，如果该字串不在Table中，那么加入该字串，并且将Value值设为1；如果该字串在Table中，那么将该字串的计数加一即可。最终我们在O(N)的时间复杂度内用Hash表完成了统计； 堆排序：第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。所以，我们最终的时间复杂度是：O（N） + N' * O（logK），（N为1000万，N’为300万）。 别忘了这篇文章中所述的堆排序思路：“维护k个元素的最小堆，即用容量为k的最小堆存储最先遍历到的k个数，并假设它们即是最大的k个数，建堆费时O（k），并调整堆(费时O（logk）)后，有k1>k2>...kmin（kmin设为小顶堆中最小元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，若x>kmin，则更新堆（x入堆，用时logk），否则不更新堆。这样下来，总费时O（klogk+（n-k）logk）=O（n*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk。”--第三章续、Top K算法问题的实现。 当然，你也可以采用trie树，关键字域存该查询串出现的次数，没有出现为0。最后用10个元素的最小推来对出现频率进行排序。 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。 由上面那两个例题，分而治之 + hash统计 + 堆/快速排序这个套路，我们已经开始有了屡试不爽的感觉。下面，再拿几道再多多验证下。请看此第3题：又是文件很大，又是内存受限，咋办?还能怎么办呢?无非还是： 分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。 hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。 堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10 如果每个数据元素只出现一次，而且只出现在某一台机器中，那么可以采取以下步骤统计出现次数TOP10的数据元素： 堆排序：在每台电脑上求出TOP10，可以采用包含10个元素的堆完成（TOP10小，用最大堆，TOP10大，用最小堆，比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。最后堆中的元素就是TOP10大）。 求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据，再利用上面类似的方法求出TOP10就可以了。 但如果同一个元素重复出现在不同的电脑中呢，如下例子所述： 这个时候，你可以有两种方法： 遍历一遍所有数据，重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中，然后采用上面所说的方法，统计每台电脑中各个元素的出现次数找出TOP10，继而组合100台电脑上的TOP10，找出最终的TOP10。 或者，暴力求解：直接统计统计每台电脑中各个元素的出现次数，然后把同一个元素在不同机器中的出现次数相加，最终从所有数据中找出TOP10。 10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。 方案1：直接上： hash映射：顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件（记为a0,a1,..a9）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。 hash_map统计：找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。注：hash_map(query,query_count)是用来统计每个query的出现次数，不是存储他们的值，出现一次，则count+1。 堆/快速/归并排序：利用快速/堆/归并排序按照出现次数进行排序，将排序好的query和对应的query_cout输出到文件中，这样得到了10个排好序的文件（记为）。最后，对这10个文件进行归并排序（内排序与外排序相结合 除此之外，此题还有以下两个方法： 方案2：一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。 方案3：与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url 可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 分而治之/hash映射：遍历文件a，对每个url求取，然后根据所取得的值将url分别存储到1000个小文件（记为，这里漏写个了a1）中。这样每个小文件的大约为300M。遍历文件b，采取和a相同的方式将url分别存储到1000小文件中（记为）。这样处理后，所有可能相同的url都在对应的小文件（）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 hash_set统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。 怎么在海量数据中找出重复次数最多的一个 方案：先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求 上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据 案：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用hash_map/搜索二叉树/红黑树等来进行统计次数。然后利用堆取出前N个出现次数最多的数据。 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。 方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归并处理，找出最终的10个最常出现的词。 方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，也可以用trie树统计每个词出现的次数，时间复杂度是O(nle)（le表示单词的平准长度），最终同样找出出现最频繁的前10个词（可用堆来实现），时间复杂度是O(nlg10)。 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？ 方案1：这题用trie树比较合适，hash_map也行。 方案2：from xjbzju:，1000w的数据规模插入操作完全不现实，以前试过在stl下100w元素插入set中已经慢得不能忍受，觉得基于hash的实现不会比红黑树好太多，使用vector+sort+unique都要可行许多，建议还是先hash成小文件分开处理再综合。 一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解 方案1：首先根据用hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词 100w个数中找出最大的100个数。 方案1：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。 方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为O(100w*100)。 方案3：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。 密匙二、多层划分 多层划分----其实本质上还是分而治之的思想，重在“分”的技巧上！ 适用范围：第k大，中位数，不重复或重复的数字 基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。 有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。也就是说只要有足够的磁盘空间，就可以很方便的解决。 5亿个int找它们的中位数 思路一：这个例子比上面那个更明显。首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。 实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。 思路二：同样需要做两遍统计，如果数据存在硬盘上，就需要读取2次。 方法同基数排序有些像，开一个大小为65536的Int数组，第一遍读取，统计Int32的高16位的情况，也就是0-65535，都算作0,65536 - 131071都算作1。就相当于用该数除以65536。Int32 除以 65536的结果不会超过65536种情况，因此开一个长度为65536的数组计数就可以。每读取一个数，数组中对应的计数+1，考虑有负数的情况，需要将结果加32768后，记录在相应的数组内。 第一遍统计之后，遍历数组，逐个累加统计，看中位数处于哪个区间，比如处于区间k，那么0- k-1的区间里数字的数量sum应该 密匙三：Bloom filter/Bitmap 给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？ 根据这个问题我们来计算下内存的占用，4G=2^32大概是40亿*8大概是340亿，n=50亿，如果按出错率0.01算需要的大概是650亿个bit。现在可用的是340亿，相差并不多，这样可能会使出错率上升些。另外如果这些urlip是一一对应的，就可以转换成ip，则大大简单了。 同时，上文的第5题：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。 方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。 方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。” 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 方案1：frome oo，用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。 密匙四、Trie树/数据库/倒排索引 Trie树 适用范围：数据量大，重复多，但是数据种类小可以放入内存 基本原理及要点：实现方式，节点孩子的表示方式 扩展：压缩实现。 问题实例： 上面的第2题：寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。 上面的第5题：有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。请问怎么设计和实现？ 上面的第8题：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。其解决方法是：用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。 数据库索引 关于数据库索引及其优化，更多可参见此文：http://www.cnblogs.com/pkuoliver/archive/2011/08/17/mass-data-topic-7-index-and-optimize.html； 关于MySQL索引背后的数据结构及算法原理，这里还有一篇很好的文章：http://blog.codinglabs.org/articles/theory-of-mysql-index.html； 关于B 树、B+ 树、B* 树及R 树，本blog内有篇绝佳文章：http://blog.csdn.net/v_JULY_v/article/details/6530142。 密匙五、外排序 适用范围：大数据的排序，去重 基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树 问题实例： 1).有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。 这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1M做hash明显不够，所以可以用来排序。内存可以当输入缓冲区使用。 关于多路归并算法及外排序的具体应用场景，请参见blog内此文： 第十章、如何给10^7个数据量的磁盘文件排序 "},"Chapter04/designPatterns.html":{"url":"Chapter04/designPatterns.html","title":"Part IV 设计模式篇","keywords":"","body":"第四章 设计模式 简介 设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结。 使用设计模式是为了重用代码、让代码更容易被他人理解、保证代码可靠性。 毫无疑问，设计模式于己于他人于系统都是多赢的，设计模式使代码编制真正工程化，设计模式是软件工程的基石，如同大厦的一块块砖石一样。 项目中合理地运用设计模式可以完美地解决很多问题，每种模式在现实中都有相应的原理来与之对应，每种模式都描述了一个在我们周围不断重复发生的问题，以及该问题的核心解决方案，这也是设计模式能被广泛应用的原因。 什么是 GOF（四人帮，全拼 Gang of Four）？ 在 1994 年，由 Erich Gamma、Richard Helm、Ralph Johnson 和 John Vlissides 四人合著出版了一本名为 Design Patterns - Elements of Reusable Object-Oriented Software（中文译名：设计模式 - 可复用的面向对象软件元素） 的书， 该书首次提到了软件开发中设计模式的概念。 四位作者合称 GOF（四人帮，全拼 Gang of Four）。 他们所提出的设计模式主要是基于以下的面向对象设计原则。 对接口编程而不是对实现编程。 优先使用对象组合而不是继承。 设计模式的类型 总共有 23 种设计模式。这些模式可以分为三大类：创建型模式（Creational Patterns）、结构型模式（Structural Patterns）、行为型模式（Behavioral Patterns）。 创建型模式：对象实例化的模式，创建型模式用于解耦对象的实例化过程。 结构型模式：把类或对象结合在一起形成一个更大的结构。 行为型模式：类和对象如何交互，及划分责任和算法。 详解 面向对象 设计原则 单例模式 工厂模式 抽象工厂模式 构建者模式 建造者与工厂的区别 原型模式 创建性模式总结 一个不是GOF的设计模式 适配器模式 装饰器模式 代理模式 代理模式和装饰器模式对比 外观模式 桥接模式 组合模式 享元模式 结构性模式总结 模板模式 策略模式 观察者模式 迭代器模式 责任链模式 命令模式 状态模式 责任链和状态模式对比 命令、状态、责任链模式区别 备忘录模式 访问者模式 java的动态绑定与双分派 中介模式 解释器模式 行为模式总结 java中的设计模式 spring中的设计模式 "},"Chapter04/objectOriented.html":{"url":"Chapter04/objectOriented.html","title":"面向对象","keywords":"","body":"面向对象 向下：深入理解三大面向对象机制 封装， 继承 多态 向上：深刻把握面向对象机制所 "},"Chapter04/designPrinciples.html":{"url":"Chapter04/designPrinciples.html","title":"设计原则","keywords":"","body":"设计原则 重新认识面向对象 隔离变化 从宏观层面上来看，面向对象的构建方式更能适应软件的变化， 能将变化所带来的影响减为最小 各司其职 从微观层面来看，面向对象的方式更强调各个类的责任。 由于需求变化导致的新增类型不应该影响原来类型的实现 对象是什么 从语言实现层面来看，对象封装了代码和数据 从规格层面讲，对象是一系列可被使用的公共接口 从概念层面讲，对象是某种拥有责任的抽象 八大面向对象设计原则 依赖倒置原则（DIP） 高层模块(稳定)不应该依赖低层模块（变化），二者都应该依赖于抽象（稳定） 抽象（稳定）不应该依赖与实现细节（变化），实现细节应该依赖与抽象（稳定） 开闭原则（OCP） 对扩展开放，对更改封闭 类模块应该可扩展，但是不可修改 单一职责原则（SRP） 一个类应该仅有一个引起它变化原因 变化的方向隐含着类的责任 里式替换原则（LSP） 子类必须能够替换他们的基类（IS—A） 继承表达类型抽象 如果我是你的子类那么所有需要父类的地方我都可以传过去。 接口隔离原则（ISP） 不应该强迫客户程序依赖他们不用的方法 接口应该小而完备 所谓小就是不必要的方法不要public出去，如果子类就使用protect修饰； 如果本类使用就private修饰。因为public出去你就需要维护。 优先使用对象组合，而不是类继承 类继承通常为“白箱复用”，对象组合通常为“黑箱复用” 继承在某种程度上破坏了封装性，子类父类耦合度高 而对象组合则只要求被组合的对象具有良好的定义的接口。耦合度低 封装变化点 使用封装来创建对象之间的分界层，让设计者可以在分界的一侧进行修改，而不会对另一侧有影响 针对接口编程，而不是针对实现编程 不将变量类型声明为某个特定的具体类，而是声明为某个接口 客户程序无需获知对象的具体类型，只需知道对象的接口 减少系统中各部分的依赖从而实现“高内聚低耦合”的类型设计方案 "},"Chapter04/singleton.html":{"url":"Chapter04/singleton.html","title":"单例模式","keywords":"","body":"单例模式 在面试中此模式是被问的最多的,因为可发散的思维点太多了 所谓单例，就是整个程序有且仅有一个实例。该类负责创建自己的对象，同时确保只有一个对象被创建。 在Java中，一般常用在工具类的实现或创建对象需要消耗大量资源时。 特点 类构造器私有 持有自己类型的属性 对外提供获取实例的静态方法 懒汉模式 线程不安全，延迟初始化，严格意义上不是不是单例模式；在高并发时可能产生多个对象。 public class Singleton { private static Singleton instance; private Singleton (){} public static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } } 饿汉模式 线程安全，比较常用，但容易产生垃圾，因为一开始就初始化，但全局不使用。 public class Singleton { private static Singleton instance = new Singleton(); private Singleton (){} public static Singleton getInstance() { return instance; } } 双重锁模式 线程安全，延迟初始化。这种方式采用双锁机制，安全且在多线程情况下能保持高性能。 public class Singleton { private volatile static Singleton singleton; private Singleton (){} public static Singleton getSingleton() { if (singleton == null) { synchronized (Singleton.class) { if (singleton == null) { singleton = new Singleton(); } } } return singleton; } } 双重检查模式，进行了两次的判断，第一次是为了避免不必要的实例;第二次是为了进行同步，避免多线程问题。 由于singleton = new Singleton()对象的创建在JVM中可能会进行重排序， 在多线程访问下存在风险，使用volatile修饰signleton实例变量，解决该问题。 推荐一个知乎上的问题: 双重检查锁失效是因为对象的初始化并非原子操作? 是因为指令重排造成的。 直接原因也就是初始化一个对象并使一个引用指向他这个过程不是原子的。 导致了可能会出现引用指向了对象并未初始化好的那块堆内存, 使用volatile修饰对象引用，防止重排序即可解决。 拿出来解释一下吧： help = new Help(); 主要原因就是这个操作不是原子性的，从而留给了JVM重排序的机会。 JVM的重排序也是有原则的，在单线程中，不管怎么排，保证最终结果一致。 注意这里是单线程。 多线程的情况下指令重排序就会给程序带来问题。 如下help = new Help()这个操作可以拆成如下四步： 栈内存开辟空间给help引用 堆内存开辟空间准备初始化对象 初始化对象 栈中引用指向这个堆内存空间地址指令 重排之后可能会是1、2、4、3；这样重排之后对单个线程来说效果是一样的， 所以JVM认为是合法的重排序。但是这样在多线程环境下就会出问题， 这里到4的时候help已经指向了一块堆内存，只是这块堆内存还没初始化就直接返回了， 假如使用的时候就会抛NullPointException。 当然这里的几个步骤并不算真正的指令，指令的粒度只会比这个还小，但是可以说明问题。 加入volatile之后查看汇编代码可以发现多了一句 lock addl $0x0,(%esp)相当于一个内存屏障。 volatile的作用：保证内存可见性，防止指令重排序，并不保证操作原子性。 这里用到的就是防止指令重排序的性质。 如何实现这些性质的 保证可见性：使用该变量必须重新去主内存读取，修改了该变量必须立刻刷新主内存。 防止重排序：通过插入内存屏障。 静态内部类单例模式 public class Singleton { private Singleton(){ } public static Singleton getInstance(){ return Inner.instance; } private static class Inner { private static final Singleton instance = new Singleton(); } } 只有第一次调用getInstance方法时，虚拟机才加载 Inner 并初始化instance。 只有一个线程可以获得对象的初始化锁，其他线程无法进行初始化，保证对象的唯一性。 这个同步过程由JVM实现了，更合适，更可靠。 目前此方式是所有单例模式中最推荐的模式，但具体还是根据项目选择。 里面提到一个对象的初始化锁是什么意思，这里就不得不提类的加载顺序: 对于静态变量、静态初始化块、变量、初始化块、构造器， 它们的初始化顺序依次是（静态变量、静态初始化块）>（变量、初始化块）> 构造器。 而且一个类的静态变量、静态初始化块 全局加载且仅加载一次。及第一次实例化此类时。 这里又引出一个新的概念：java 的init方法与clinit方法 /** * 类的初始化顺序 * 静态变量 -> 静态初始化块 -> 变量 -> 初始化块 -> 构造器 */ public class InitialOrderTest { // 静态变量 public static String staticField = \"静态变量\"; // 静态初始化块 static { System.out.println(staticField); System.out.println(\"静态初始化块\"); } // 变量 public String field = \"变量\"; // 初始化块 { System.out.println(field); System.out.println(\"初始化块\"); } // 构造器 public InitialOrderTest() { System.out.println(\"构造器\"); } public static void main(String[] args) { new InitialOrderTest(); System.out.println(\"====\"); new InitialOrderTest(); } } 执行结果如下： 静态变量 静态初始化块 变量 初始化块 构造器 ==== 变量 初始化块 构造器 如下是不是就有人提出用静态代码块实现单例 静态代码块实现单例模式，事实上就是饿汉单例模式的变种。 public class T{ public static void main(String[] args) { Singleton.print(); /*************/ Singleton2.print(); } } class Singleton{ private static Singleton instance; private Singleton(){} static{ System.out.println(\"Singleton--我在被调用的时候加载，而且只加载一次\"); instance = new Singleton(); } public static Singleton getInstance(){ return instance; } public static void print(){ System.out.println(\"我只想调用这个方法，不想初始化实例对象\"); } } class Singleton2{ private Singleton2(){} private static class Handler{ static{ System.out.println(\"Singleton2--我在调用的时候被加载，而且只加载一次\"); } private static Singleton2 instance = new Singleton2(); } public static Singleton2 getInstance(){ return Handler.instance; } public static void print(){ System.out.println(\"我只想调用这个方法，不想初始化实例对象\"); } } 输出结果： Singleton–我在被调用的时候加载，而且只加载一次 我只想调用这个方法，不想初始化实例对象 ======= 我只想调用这个方法，不想初始化实例对象 很明显，这里在不想初始化的时候，它初始化了，所以是一种变种的饿汉单例模式。 枚举单例模式 public enum Singleton { INSTANCE; } 默认枚举实例的创建是线程安全的，并且在任何情况下都是单例。 实际上 枚举类隐藏了私有的构造器。 枚举类的域是相应类型的一个实例对象 那么枚举类型日常用例是这样子的： public enum Singleton { INSTANCE //doSomething 该实例支持的行为 //可以省略此方法，通过Singleton.INSTANCE进行操作 public static Singleton get Instance() { return Singleton.INSTANCE; } } 枚举单例模式在《Effective Java》中推荐的单例模式之一。 但枚举实例在日常开发是很少使用的，就是很简单以导致可读性较差。 在以上所有的单例模式中，推荐静态内部类单例模式。主要是非常直观，即保证线程安全又保证唯一性。 spring中的单例 大家应该都知道spring的对象默认是单例的，那么spring中是运用了怎样的单例模式呢？ 如下是spring5.x DefaultSingletonBeanRegistry的源码 @Nullable protected Object getSingleton(String beanName, boolean allowEarlyReference) { Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) { synchronized (this.singletonObjects) { singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null && allowEarlyReference) { ObjectFactory singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return singletonObject; } 什么情况下会破坏单例模式 反序列化 众所周知，单例模式是创建型模式，都会新建一个实例。 那么一个重要的问题就是反序列化。 当实例被写入到文件到反序列化成实例时， 我们需要重写readResolve方法，以让实例唯一。 private Object readResolve() throws ObjectStreamException{ return singleton; } 测试代码： package cp2; import java.io.Serializable; import java.util.Objects; public class SerSingleton implements Serializable { String name; private SerSingleton(){ System.out.println(\"Singleton is creating\"); } private static SerSingleton instance = new SerSingleton(); public static SerSingleton getInstance(){ return instance; } public static void createString(){ System.out.println(\"create string in singleton\"); } } import cp2.SerSingleton; import junit.framework.Assert; import org.junit.Test; import java.io.FileInputStream; import java.io.FileOutputStream; import java.io.ObjectInputStream; import java.io.ObjectOutputStream; public class SerSingletonTest { @Test public void test() throws Exception{ SerSingleton s1 = null; SerSingleton s = SerSingleton.getInstance(); FileOutputStream fos = new FileOutputStream(\"a.txt\"); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(s); oos.flush(); oos.close(); FileInputStream fis = new FileInputStream(\"a.txt\"); ObjectInputStream ois = new ObjectInputStream(fis); s1 = (SerSingleton) ois.readObject(); Assert.assertEquals(s,s1); } } 结果： junit.framework.AssertionFailedError: Expected :cp2.SerSingleton@7ab2bfe1 Actual :cp2.SerSingleton@497470ed 问题来了，怎么解决呢？jdk其实预料到这种情况了。 解决方法：加入readResolve() 在jdk中ObjectInputStream的类中有readUnshared（）方法，上面详细解释了原因。我简单描述一下，那就是如果被反序列化的对象的类存在readResolve这个方法，他会调用这个方法来返回一个“array”（我也不明白），然后浅拷贝一份，作为返回值，并且无视掉反序列化的值，即使那个字节码已经被解析。 所以，完整的单例模式是： package cp2; import java.io.Serializable; import java.util.Objects; public class SerSingleton implements Serializable { String name; private SerSingleton(){ System.out.println(\"Singleton is creating\"); } private static SerSingleton instance = new SerSingleton(); public static SerSingleton getInstance(){ return instance; } public static void createString(){ System.out.println(\"create string in singleton\"); } private Object readResolve(){ System.out.println(\"read resolve\"); return instance; } } 反射 一个单例类： public class Singleton { private static Singleton instance = new Singleton(); private Singleton() {} public static Singleton getInstance() { return instance; } } 通过反射破坏单例模式： public class Test { public static void main(String[] args) throws Exception{ Singleton s1 = Singleton.getInstance(); Constructor constructor = Singleton.class.getDeclaredConstructor(); constructor.setAccessible(true); Singleton s2 = constructor.newInstance(); System.out.println(s1.hashCode()); System.out.println(s2.hashCode()); } } 输出结果： 671631440 935563443 结果表明s1和s2是两个不同的实例了。 通过反射获得单例类的构造函数，由于该构造函数是private的， 通过setAccessible(true)指示反射的对象在使用时应该取消 Java 语言访问检查, 使得私有的构造函数能够被访问，这样使得单例模式失效。 如果要抵御这种攻击，要防止构造函数被成功调用两次。 需要在构造函数中对实例化次数进行统计，大于一次就抛出异常。 public class Singleton { private static int count = 0; private static Singleton instance = null; private Singleton(){ synchronized (Singleton.class) { if(count > 0){ throw new RuntimeException(\"创建了两个实例\"); } count++; } } public static Singleton getInstance() { if(instance == null) { instance = new Singleton(); } return instance; } public static void main(String[] args) throws Exception { Constructor constructor = Singleton.class.getDeclaredConstructor(); constructor.setAccessible(true); Singleton s1 = constructor.newInstance(); Singleton s2 = constructor.newInstance(); } } 执行结果： Exception in thread \"main\" java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Unknown Source) at com.yzz.reflect.Singleton.main(Singleton.java:33) Caused by: java.lang.RuntimeException: 创建了两个实例 at com.yzz.reflect.Singleton.(Singleton.java:14) ... 5 more 优点 提供了对唯一实例的受控访问。因为单例类封装了它的唯一实例，所以它可以严格控制客户怎样以及何时访问它，并为设计及开发团队提供了共享的概念。 由于在系统内存中只存在一个对象，因此可以节约系统资源，对于一些需要频繁创建和销毁的对象，单例模式无疑可以提高系统的性能。 允许可变数目的实例。我们可以基于单例模式进行扩展，使用与单例控制相似的方法来获得指定个数的对象实例。 缺点 由于单例模式中没有抽象层，因此单例类的扩展有很大的困难。 单例类的职责过重，在一定程度上违背了“单一职责原则”。因为单例类既充当了工厂角色，提供了工厂方法，同时又充当了产品角色，包含一些业务方法，将产品的创建和产品的本身的功能融合到一起。 滥用单例将带来一些负面问题，如为了节省资源将数据库连接池对象设计为单例类，可能会导致共享连接池对象的程序过多而出现连接池溢出；现在很多面向对象语言(如Java、C#)的运行环境都提供了自动垃圾回收的技术，因此，如果实例化的对象长时间不被利用，系统会认为它是垃圾，会自动销毁并回收资源，下次利用时又将重新实例化，这将导致对象状态的丢失。 适用环境 系统只需要一个实例对象，如系统要求提供一个唯一的序列号生成器，或者需要考虑资源消耗太大而只允许创建一个对象。 客户调用类的单个实例只允许使用一个公共访问点，除了该公共访问点，不能通过其他途径访问该实例。 在一个系统中要求一个类只有一个实例时才应当使用单例模式。反过来，如果一个类可以有几个实例共存，就需要对单例模式进行改进，使之成为多例模式 模式应用 一个具有自动编号主键的表可以有多个用户同时使用，但数据库中只能有一个地方分配下一个主键编号， 否则会出现主键重复，因此该主键编号生成器必须具备唯一性，可以通过单例模式来实现。 "},"Chapter04/Factory.html":{"url":"Chapter04/Factory.html","title":"工厂模式","keywords":"","body":"工厂模式 工厂模式(Factory Pattern)是Java中最常用的设计模式之一。 这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。 介绍 意图：定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。 主要解决：主要解决接口选择的问题。 何时使用：我们明确地计划不同条件下创建不同实例时。 如何解决：让其子类实现工厂接口，返回的也是一个抽象的产品。 关键代码：创建过程在其子类执行。 应用实例： 您需要一辆汽车，可以直接从工厂里面提货，而不用去管这辆汽车是怎么做出来的，以及这个汽车里面的具体实现。 Hibernate 换数据库只需换方言和驱动就可以。 优点 一个调用者想创建一个对象，只要知道其名称就可以了。 扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 屏蔽产品的具体实现，调用者只关心产品的接口。 缺点 每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加， 在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。 使用场景： 日志记录器：记录可能记录到本地硬盘、系统事件、远程服务器等，用户可以选择记录日志到什么地方。 数据库访问，当用户不知道最后系统采用哪一类数据库，以及数据库可能有变化时。 设计一个连接服务器的框架，需要三个协议，\"POP3\"、\"IMAP\"、\"HTTP\"，可以把这三个作为产品类，共同实现一个接口。 注意事项 作为一种创建类模式，在任何需要生成复杂对象的地方，都可以使用工厂方法模式。 有一点需要注意的地方就是复杂对象适合使用工厂模式， 而简单对象，特别是只需要通过 new 就可以完成创建的对象，无需使用工厂模式。 如果使用工厂模式，就需要引入一个工厂类，会增加系统的复杂度。 代码展示 定义发动机 package designpatterns.factory; public class Engine { public void getStyle(){ System.out.println(\"这是汽车的发动机\"); } } 定义地盘 package designpatterns.factory; public class UnderPan { public void getStyle(){ System.out.println(\"这是汽车的底盘\"); } } 定义轮胎 package designpatterns.factory; public class Wheel { public void getStyle(){ System.out.println(\"这是汽车的轮胎\"); } } 定义汽车接口 package designpatterns.factory; public interface ICar { void show(); } 汽车实现 package designpatterns.factory; import lombok.AllArgsConstructor; import lombok.Getter; import lombok.Setter; @Getter @Setter @AllArgsConstructor public class Car implements ICar { private Engine engine; private UnderPan underpan; private Wheel wheel; public void show() { engine.getStyle(); underpan.getStyle(); wheel.getStyle(); System.out.println(\"造了一个汽车\"); } } 定义工厂 package designpatterns.factory; public interface IFactory { ICar createCar(); } 工厂实现 package designpatterns.factory; public class Factory implements IFactory { public ICar createCar() { Engine engine = new Engine(); UnderPan underpan = new UnderPan(); Wheel wheel = new Wheel(); ICar car = new Car(engine, underpan, wheel); return car; } } 运行 package designpatterns.factory; public class FactoryMain { public static void main(String[] args) { IFactory factory = new Factory(); ICar car = factory.createCar(); car.show(); } } 运行结果 D:\\Java\\jdk1.8.0_161\\bin\\java.exe \"-javaagent:D:\\JetBrains\\IntelliJ IDEA 2019.3.3\\lib\\idea_rt.jar=49320:D:\\JetBrains\\IntelliJ IDEA 2019.3.3\\bin\" -Dfile.encoding=UTF-8 -classpath D:\\Java\\jdk1.8.0_161\\jre\\lib\\charsets.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\deploy.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\access-bridge-64.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\cldrdata.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\dnsns.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\jaccess.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\jfxrt.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\localedata.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\nashorn.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunec.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunjce_provider.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunmscapi.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunpkcs11.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\zipfs.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\javaws.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jce.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jfr.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jfxswt.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jsse.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\management-agent.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\plugin.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\resources.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\rt.jar;D:\\github\\program\\target\\classes;D:\\firerepository\\org\\projectlombok\\lombok\\1.16.22\\lombok-1.16.22.jar designpatterns.factory.FactoryMain 这是汽车的发动机 这是汽车的底盘 这是汽车的轮胎 造了一个汽车 Process finished with exit code 0 "},"Chapter04/AbstractFactory.html":{"url":"Chapter04/AbstractFactory.html","title":"抽象工厂模式","keywords":"","body":"抽象工厂模式 抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂;该超级工厂又称为其他工厂的工厂。 抽象工厂模式提供一个创建一系列相关或相互依赖对象的接口，而无须指定它们具体的类。抽象工厂模式又称为Kit模式，它是一种对象创建型模式，提供了一种创建对象的最佳方式。 在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。 在抽象工厂模式中，每一个具体工厂都提供了多个工厂方法用于产生多种不同类型的产品。 抽象工厂模式是工厂方法模式的升级版本，他用来创建一组相关或者相互依赖的对象。 与工厂方法模式的区别 工厂方法模式针对的是一个产品等级结构；而抽象工厂模式则是针对的多个产品等级结构 介绍 意图：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。 主要解决：主要解决接口选择的问题。 何时使用：系统的产品有多于一个的产品族，而系统只消费其中某一族的产品。 如何解决：在一个产品族里面，定义多个产品。 关键代码：在一个工厂里聚合多个同类产品。 应用实例 工作了，为了参加一些聚会，肯定有两套或多套衣服吧，比如说有商务装（成套，一系列具体产品）、时尚装（成套，一系列具体产品），甚至对于一个家庭来说，可能有商务女装、商务男装、时尚女装、时尚男装，这些也都是成套的，即一系列具体产品。假设一种情况（现实中是不存在的，要不然，没法进入共产主义了，但有利于说明抽象工厂模式）， 在您的家中，某一个衣柜（具体工厂）只能存放某一种这样的衣服（成套，一系列具体产品），每次拿这种成套的衣服时也自然要从这个衣柜中取出了。用 OO 的思想去理解，所有的衣柜（具体工厂）都是衣柜类的（抽象工厂）某一个，而每一件成套的衣服又包括具体的上衣（某一具体产品），裤子（某一具体产品），这些具体的上衣其实也都是上衣（抽象产品），具体的裤子也都是裤子（另一个抽象产品）。 优点 当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。 缺点 产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 Creator 里加代码，又要在具体的里面加代码。 使用场景 QQ 换皮肤，一整套一起换。 生成不同操作系统的程序。 注意事项 产品族难扩展，产品等级易扩展。 角色 AbstractFactory（抽象工厂）：它声明了一组用于创建一族产品的方法，每一个方法对应一种产品。 ConcreteFactory（具体工厂）：它实现了在抽象工厂中声明的创建产品的方法，生成一组具体产品，这些产品构成了一个产品族，每一个产品都位于某个产品等级结构中。 AbstractProduct（抽象产品）：它为每种产品声明接口，在抽象产品中声明了产品所具有的业务方法 ConcreteProduct（具体产品）：它定义具体工厂生产的具体产品对象，实现抽象产品接口中声明的业务方法。 在抽象工厂中声明了多个工厂方法，用于创建不同类型的产品，抽象工厂可以是接口，也可以是抽象类或者具体类 具体工厂实现了抽象工厂，每一个具体的工厂方法可以返回一个特定的产品对象，而同一个具体工厂所创建的产品对象构成了一个产品族 代码实现 定义发动机 package designpatterns.factory; public class Engine { public void getStyle(){ System.out.println(\"这是汽车的发动机\"); } } 定义地盘 package designpatterns.factory; public class UnderPan { public void getStyle(){ System.out.println(\"这是汽车的底盘\"); } } 定义轮胎 package designpatterns.factory; public class Wheel { public void getStyle(){ System.out.println(\"这是汽车的轮胎\"); } } 定义汽车接口 package designpatterns.abstractfactory; public interface ICar { void show(); } 奥迪汽车实现 package designpatterns.abstractfactory; import lombok.AllArgsConstructor; import lombok.Getter; import lombok.Setter; @Getter @Setter @AllArgsConstructor public class AudiCar implements ICar { private Engine engine; private Underpan underpan; private Wheel wheel; public void show() { engine.getStyle(); underpan.getStyle(); wheel.getStyle(); System.out.println(\"造了一台奥迪汽车\"); } } 奔驰汽车实现 package designpatterns.abstractfactory; import lombok.AllArgsConstructor; import lombok.Getter; import lombok.Setter; @Getter @Setter @AllArgsConstructor public class BenzCar implements ICar { private Engine engine; private Underpan underpan; private Wheel wheel; public void show() { engine.getStyle(); underpan.getStyle(); wheel.getStyle(); System.out.println(\"造了一台奔驰汽车\"); } } 定义工厂 package designpatterns.abstractfactory; public interface IFactory { ICar createBenzCar(); ICar createAudiCar(); } 工厂实现 package designpatterns.abstractfactory; public class Factory implements IFactory { public ICar createBenzCar() { Engine engine = new Engine(); Underpan underpan = new Underpan(); Wheel wheel = new Wheel(); ICar car = new BenzCar(engine, underpan, wheel); return car; } public ICar createAudiCar() { Engine engine = new Engine(); Underpan underpan = new Underpan(); Wheel wheel = new Wheel(); ICar car = new AudiCar(engine, underpan, wheel); return car; } } 运行 package designpatterns.abstractfactory; public class AbstractFactoryMain { public static void main(String[] args) { IFactory factory = new Factory(); ICar benzCar = factory.createBenzCar(); benzCar.show(); ICar audi = factory.createAudiCar(); audi.show(); } } 运行结果 这是汽车的发动机 这是汽车的底盘 这是汽车的轮胎 造了一台奔驰汽车 这是汽车的发动机 这是汽车的底盘 这是汽车的轮胎 造了一台奥迪汽车 "},"Chapter04/Builder.html":{"url":"Chapter04/Builder.html","title":"构建者模式","keywords":"","body":"建造者模（构建者模式） 建造者模式（Builder Pattern）将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 一个 Builder 类会一步一步构造复杂的最终对象，它允许用户只通过指定复杂对象的类型和内容就可以构建它们， 用户不需要知道内部的具体构建细节。该 Builder 类是独立于其他对象的。 这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 在建造者模式的定义中提到了复杂对象，那么什么是复杂对象？ 简单来说，复杂对象是指那些包含多个成员属性的对象； 这些成员属性也称为部件或零件，如汽车包括方向盘、发动机、轮胎等部件，电子邮件包括发件人、收件人、主题、内容、附件等部件 介绍 意图：将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。 主要解决：主要解决在软件系统中，有时候面临着\"一个复杂对象\"的创建工作，其通常由各个部分的子对象用一定的算法构成；由于需求的变化，这个复杂对象的各个部分经常面临着剧烈的变化，但是将它们组合在一起的算法却相对稳定。 何时使用：一些基本部件不会变，而其组合经常变化的时候。 如何解决：将变与不变分离开。 关键代码：建造者：创建和提供实例，导演：管理建造出来的实例的依赖关系。 应用实例 去肯德基，汉堡、可乐、薯条、炸鸡翅等是不变的，而其组合是经常变化的，生成出所谓的\"套餐\"。 JAVA 中的 StringBuilder。 优点 建造者独立，易扩展。 便于控制细节风险。 在建造者模式中，客户端不必知道产品内部组成的细节，将产品本身与产品的创建过程解耦， 使得相同的创建过程可以创建不同的产品对象。 每一个具体建造者都相对独立，而与其他的具体建造者无关，因此可以很方便地替换具体建造者或增加新的具体建造者，用户使用不同的具体建造者即可得到不同的产品对象。 由于指挥者类针对抽象建造者编程，增加新的具体建造者无须修改原有类库的代码，系统扩展方便，符合 “开闭原则”。 可以更加精细地控制产品的创建过程。 将复杂产品的创建步骤分解在不同的方法中，使得创建过程更加清晰，也更方便使用程序来控制创建过程。 缺点 产品必须有共同点，范围有限制。 如内部变化复杂，会有很多的建造类。 建造者模式所创建的产品一般具有较多的共同点，其组成部分相似，如果产品之间的差异性很大，例如很多组成部分都不相同，不适合使用建造者模式，因此其使用范围受到一定的限制。 如果产品的内部变化复杂，可能会导致需要定义很多具体建造者类来实现这种变化，导致系统变得很庞大，增加系统的理解难度和运行成本。 使用场景： 需要生成的对象具有复杂的内部结构。 需要生成的对象内部属性本身相互依赖。 需要生成的产品对象有复杂的内部结构，这些产品对象通常包含多个成员属性。 需要生成的产品对象的属性相互依赖，需要指定其生成顺序。 对象的创建过程独立于创建该对象的类。在建造者模式中通过引入了指挥者类，将创建过程封装在指挥者类中，而不在建造者类和客户类中。 隔离复杂对象的创建和使用，并使得相同的创建过程可以创建不同的产品。 注意事项： 与工厂模式的区别 建造者模式更加关注与零件装配的顺序。 角色 Builder（抽象建造者）：它为创建一个产品Product对象的各个部件指定抽象接口，在该接口中一般声明两类方法，一类方法是buildPartX()，它们用于创建复杂对象的各个部件；另一类方法是getResult()，它们用于返回复杂对象。Builder既可以是抽象类，也可以是接口。 ConcreteBuilder（具体建造者）：它实现了Builder接口，实现各个部件的具体构造和装配方法，定义并明确它所创建的复杂对象，也可以提供一个方法返回创建好的复杂产品对象。 Product（产品角色）：它是被构建的复杂对象，包含多个组成部件，具体建造者创建该产品的内部表示并定义它的装配过程。 Director（指挥者）：指挥者又称为导演类，它负责安排复杂对象的建造次序，指挥者与抽象建造者之间存在关联关系，可以在其construct()建造方法中调用建造者对象的部件构造与装配方法，完成复杂对象的建造。客户端一般只需要与指挥者进行交互，在客户端确定具体建造者的类型，并实例化具体建造者对象（也可以通过配置文件和反射机制），然后通过指挥者类的构造函数或者Setter方法将该对象传入指挥者类中。 代码展示 抽象构建者 定义产品 package designpatterns.builder; public class Product { private String name; private String type; public void showProduct(){ System.out.println(\"名称：\"+name); System.out.println(\"型号：\"+type); } public void setName(String name) { this.name = name; } public void setType(String type) { this.type = type; } } package designpatterns.builder; public abstract class Builder { public abstract void setPart(String arg1, String arg2); public abstract Product getProduct(); } 构建者实现 package designpatterns.builder; public class ConcreteBuilder extends Builder { private Product product = new Product(); public Product getProduct() { return product; } public void setPart(String arg1, String arg2) { product.setName(arg1); product.setType(arg2); } } 构建者目录 package designpatterns.builder; public class Director { private Builder builder = new ConcreteBuilder(); public Product getAProduct(){ builder.setPart(\"宝马汽车\",\"X7\"); return builder.getProduct(); } public Product getBProduct(){ builder.setPart(\"奥迪汽车\",\"Q5\"); return builder.getProduct(); } } 运行 package designpatterns.builder; public class BuilderMain { public static void main(String[] args) { Director director = new Director(); Product product1 = director.getAProduct(); product1.showProduct(); Product product2 = director.getBProduct(); product2.showProduct(); } } 运行结果 名称：宝马汽车 型号：X7 名称：奥迪汽车 型号：Q5 建造者模式的典型应用和源码分析 java.lang.StringBuilder 中的建造者模式 StringBuilder 中的 append 方法使用了建造者模式，不过装配方法只有一个，并不算复杂，append 方法返回的是 StringBuilder 自身 StringBuilder 的父类 AbstractStringBuilder 实现了 Appendable 接口 abstract class AbstractStringBuilder implements Appendable, CharSequence { char[] value; int count; public AbstractStringBuilder append(String str) { if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this; } private void ensureCapacityInternal(int minimumCapacity) { // overflow-conscious code if (minimumCapacity - value.length > 0) { value = Arrays.copyOf(value, newCapacity(minimumCapacity)); } } // ...省略... } 我们可以看出，Appendable 为抽象建造者，定义了建造方法，StringBuilder 既充当指挥者角色，又充当产品角色，又充当具体建造者，建造方法的实现由 AbstractStringBuilder 完成，而 StringBuilder 继承了 AbstractStringBuilder java.lang.StringBuffer 中的建造者方法 public final class StringBuffer extends AbstractStringBuilder implements java.io.Serializable, CharSequence { @Override public synchronized StringBuffer append(String str) { toStringCache = null; super.append(str); return this; } //...省略... } 看 StringBuffer 的源码如上，它们的区别就是： StringBuffer 中的 append 加了 synchronized 关键字，所以StringBuffer 是线程安全的，而 StringBuilder 是非线程安全的 StringBuffer 中的建造者模式与 StringBuilder 是一致的 mybatis 中的建造者模式 我们来看 org.apache.ibatis.session 包下的 SqlSessionFactoryBuilder 类 里边很多重载的 build 方法，返回值都是 SqlSessionFactory，除了最后两个所有的 build 最后都调用下面这个 build 方法 public SqlSessionFactory build(Reader reader, String environment, Properties properties) { SqlSessionFactory var5; try { XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties); var5 = this.build(parser.parse()); } catch (Exception var14) { throw ExceptionFactory.wrapException(\"Error building SqlSession.\", var14); } finally { ErrorContext.instance().reset(); try { reader.close(); } catch (IOException var13) { ; } } return var5; } 其中最重要的是 XMLConfigBuilder 的 parse 方法，代码如下 public SqlSessionFactory build(Reader reader, String environment, Properties properties) { SqlSessionFactory var5; try { XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties); var5 = this.build(parser.parse()); } catch (Exception var14) { throw ExceptionFactory.wrapException(\"Error building SqlSession.\", var14); } finally { ErrorContext.instance().reset(); try { reader.close(); } catch (IOException var13) { ; } } return var5; } 其中最重要的是 XMLConfigBuilder 的 parse 方法，代码如下 public class XMLConfigBuilder extends BaseBuilder { public Configuration parse() { if (this.parsed) { throw new BuilderException(\"Each XMLConfigBuilder can only be used once.\"); } else { this.parsed = true; this.parseConfiguration(this.parser.evalNode(\"/configuration\")); return this.configuration; } } private void parseConfiguration(XNode root) { try { Properties settings = this.settingsAsPropertiess(root.evalNode(\"settings\")); this.propertiesElement(root.evalNode(\"properties\")); this.loadCustomVfs(settings); this.typeAliasesElement(root.evalNode(\"typeAliases\")); this.pluginElement(root.evalNode(\"plugins\")); this.objectFactoryElement(root.evalNode(\"objectFactory\")); this.objectWrapperFactoryElement(root.evalNode(\"objectWrapperFactory\")); this.reflectorFactoryElement(root.evalNode(\"reflectorFactory\")); this.settingsElement(settings); this.environmentsElement(root.evalNode(\"environments\")); this.databaseIdProviderElement(root.evalNode(\"databaseIdProvider\")); this.typeHandlerElement(root.evalNode(\"typeHandlers\")); this.mapperElement(root.evalNode(\"mappers\")); } catch (Exception var3) { throw new BuilderException(\"Error parsing SQL Mapper Configuration. Cause: \" + var3, var3); } } // ...省略... } parse 方法最终要返回一个 Configuration 对象，构建 Configuration 对象的建造过程都在 parseConfiguration 方法中，这也就是 Mybatis 解析 XML配置文件 来构建 Configuration 对象的主要过程 所以 XMLConfigBuilder 是建造者 SqlSessionFactoryBuilder 中的建造者，复杂产品对象分别是 SqlSessionFactory 和 Configuration "},"Chapter04/BuilderFactory.html":{"url":"Chapter04/BuilderFactory.html","title":"建造者模式与工厂模式的区别","keywords":"","body":"建造者模式与工厂模式的区别 我们可以看到，建造者模式与工厂模式是极为相似的，总体上，建造者模式仅仅只比工厂模式多了一个\"导演类\"的角色。 在建造者模式的类图中，假如把这个导演类看做是最终调用的客户端，那么图中剩余的部分就可以看作是一个简单的工厂模式了。 与工厂模式相比，建造者模式一般用来创建更为复杂的对象，因为对象的创建过程更为复杂，因此将对象的创建过程独立出来组成一个新的类——导演类。也就是说，工厂模式是将对象的全部创建过程封装在工厂类中，由工厂类向客户端提供最终的产品； 而建造者模式中，建造者类一般只提供产品类中各个组件的建造，而将具体建造过程交付给导演类。由导演类负责将各个组件按照特定的规则组建为产品，然后将组建好的产品交付给客户端。 工厂模式一般都是创建一个产品，注重的是把这个产品创建出来就行，只要创建出来，不关心这个产品的组成部分。 从代码上看，工厂模式就是一个方法，用这个方法就能生产出产品。 建造者模式也是创建一个产品，但是不仅要把这个产品创建出来， 还要关系这个产品的组成细节、组成过程。 从代码上看，建造者模式在建造产品时，这个产品有很多方法， 建造者模式会根据这些相同方法但是不同执行顺序建造出不同组成细节的产品。 总结 建造者模式与工厂模式类似，他们都是建造者模式，适用的场景也很相似。一般来说，如果产品的建造很复杂，那么请用工厂模式；如果产品的建造更复杂，那么请用建造者模式。 "},"Chapter04/Prototype.html":{"url":"Chapter04/Prototype.html","title":"原型模式","keywords":"","body":"原型模式 原型模式(Prototype Pattern)：使用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。 原型模式是一种对象创建型模式。 原型模式的工作原理很简单：将一个原型对象传给那个要发动创建的对象，这个要发动创建的对象通过请求原型对象拷贝自己来实现创建过程。 原型模式是一种“另类”的创建型模式，创建克隆对象的工厂就是原型类自身，工厂方法由克隆方法来实现。 需要注意的是通过克隆方法所创建的对象是全新的对象，它们在内存中拥有新的地址，通常对克隆所产生的对象进行修改对原型对象不会造成任何影响，每一个克隆对象都是相互独立的。 通过不同的方式修改可以得到一系列相似但不完全相同的对象。 角色 Prototype（抽象原型类）：它是声明克隆方法的接口，是所有具体原型类的公共父类，可以是抽象类也可以是接口，甚至还可以是具体实现类。 ConcretePrototype（具体原型类）：它实现在抽象原型类中声明的克隆方法，在克隆方法中返回自己的一个克隆对象。 Client（客户类）：让一个原型对象克隆自身从而创建一个新的对象，在客户类中只需要直接实例化或通过工厂方法等方式创建一个原型对象，再通过调用该对象的克隆方法即可得到多个相同的对象。 由于客户类针对抽象原型类Prototype编程，因此用户可以根据需要选择具体原型类，系统具有较好的可扩展性，增加或更换具体原型类都很方便。 核心 原型模式的核心在于如何实现克隆方法。 使用原始模式的时候一定要注意为深克隆还是浅克隆。 优点 当创建新的对象实例较为复杂时，使用原型模式可以简化对象的创建过程，通过复制一个已有实例可以提高新实例的创建效率。 扩展性较好，由于在原型模式中提供了抽象原型类，在客户端可以针对抽象原型类进行编程，而将具体原型类写在配置文件中，增加或减少产品类对原有系统都没有任何影响。 原型模式提供了简化的创建结构，工厂方法模式常常需要有一个与产品类等级结构相同的工厂等级结构，而原型模式就不需要这样，原型模式中产品的复制是通过封装在原型类中的克隆方法实现的，无须专门的工厂类来创建产品。 可以使用深克隆的方式保存对象的状态，使用原型模式将对象复制一份并将其状态保存起来，以便在需要的时候使用（如恢复到某一历史状态），可辅助实现撤销操作。 缺点 需要为每一个类配备一个克隆方法，而且该克隆方法位于一个类的内部，当对已有的类进行改造时，需要修改源代码，违背了“开闭原则”。 在实现深克隆时需要编写较为复杂的代码，而且当对象之间存在多重的嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来可能会比较麻烦。 适用场景 创建新对象成本较大（如初始化需要占用较长的时间，占用太多的CPU资源或网络资源），新的对象可以通过原型模式对已有对象进行复制来获得，如果是相似对象，则可以对其成员变量稍作修改。 如果系统要保存对象的状态，而对象的状态变化很小，或者对象本身占用内存较少时，可以使用原型模式配合备忘录模式来实现。 需要避免使用分层次的工厂类来创建分层次的对象，并且类的实例对象只有一个或很少的几个组合状态，通过复制原型对象得到新实例可能比使用构造函数创建一个新实例更加方便。 注意事项 使用原型模式复制对象不会调用类的构造方法。因为对象的复制是通过调用Object类的clone方法来完成的，它直接在内存中复制数据，因此不会调用到类的构造方法。不但构造方法中的代码不会执行，甚至连访问权限都对原型模式无效。还记得单例模式吗？单例模式中，只要将构造方法的访问权限设置为private型，就可以实现单例。但是clone方法直接无视构造方法的权限，所以，单例模式与原型模式是冲突的，在使用时要特别注意。 深拷贝与浅拷贝。Object类的clone方法只会拷贝对象中的基本的数据类型，对于数组、容器对象、引用对象等都不会拷贝，这就是浅拷贝。如果要实现深拷贝，必须将原型模式中的数组、容器对象、引用对象等另行拷贝。例如： PS：深拷贝与浅拷贝问题中，会发生深拷贝的有java中的8中基本类型以及他们的封装类型，另外还有String类型。其余的都是浅拷贝。 代码展示 定义产品 package designpatterns.prototype; public interface Product extends Cloneable { void use(String word); Product createClone(); void setCh(char ch); } 第一种实现 package designpatterns.prototype; public class Underline implements Product { char ch; public Underline(char ch) { this.ch = ch; } public void use(String word) { System.out.print(ch); System.out.print(word); System.out.println(ch); for (int i = 0; i 第二种实现 package designpatterns.prototype; public class MessageBox implements Product { char ch; public MessageBox(char ch) { this.ch = ch; } public void use(String word) { for (int i = 0; i 管理类 package designpatterns.prototype; import java.util.HashMap; public class Manager { HashMap hashmap = new HashMap(); public void register(String key, Product p) { hashmap.put(key, p); } public Product create(String key) { Product p = (Product) hashmap.get(key); return p.createClone(); } } 运行类 package designpatterns.prototype; public class PrototypeMain { public static void main(String[] args) { Manager m = new Manager(); Product p1 = new Underline('@'); m.register(\"line\", p1); Product p2 = new MessageBox('$'); m.register(\"msg\", p2); Product p3 = m.create(\"line\"); p3.setCh('%'); Product p4 = m.create(\"msg\"); p4.setCh('^'); p1.use(\"fire\"); p2.use(\"huo\"); p3.use(\"love\"); p4.use(\"1314\"); } } 运行结果 @fire@ @@@@@ $$$$ $huo$ $$$$ %love% %%%%% ^^^^^ ^1314^ ^^^^^ Java语言提供的clone()方法 学过Java语言的人都知道，所有的Java类都继承自 java.lang.Object。事实上，Object 类提供一个 clone() 方法，可以将一个Java对象复制一份。因此在Java中可以直接使用 Object 提供的 clone() 方法来实现对象的克隆，Java语言中的原型模式实现很简单。 需要注意的是能够实现克隆的Java类必须实现一个 标识接口 Cloneable，表示这个Java类支持被复制。如果一个类没有实现这个接口但是调用了clone()方法，Java编译器将抛出一个 CloneNotSupportedException 异常。 原型模式的典型应用 Object 类中的 clone 接口 Cloneable 接口的实现类，可以看到至少一千多个，找几个例子譬如： ArrayList 对 clone 的重写如下： public class ArrayList extends AbstractList implements List, RandomAccess, Cloneable, java.io.Serializable { public Object clone() { try { ArrayList v = (ArrayList) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // this shouldn't happen, since we are Cloneable throw new InternalError(e); } } //...省略 } 调用 super.clone(); 之后把 elementData 数据 copy 了一份 HashMap 对 clone 方法的重写 public class HashMap extends AbstractMap implements Map, Cloneable, Serializable { @Override public Object clone() { HashMap result; try { result = (HashMap)super.clone(); } catch (CloneNotSupportedException e) { // this shouldn't happen, since we are Cloneable throw new InternalError(e); } result.reinitialize(); result.putMapEntries(this, false); return result; } // ...省略... } mybatis 中的 org.apache.ibatis.cache.CacheKey 对 clone 方法的重写： public class CacheKey implements Cloneable, Serializable { private List updateList; public CacheKey clone() throws CloneNotSupportedException { CacheKey clonedCacheKey = (CacheKey)super.clone(); clonedCacheKey.updateList = new ArrayList(this.updateList); return clonedCacheKey; } // ... 省略... } 这里又要注意，updateList 是 List 类型，所以可能是值类型的List，也可能是引用类型的List，克隆的结果需要注意是否为深克隆或者浅克隆 "},"Chapter04/create.html":{"url":"Chapter04/create.html","title":"创建性模式总结","keywords":"","body":"创建类模式总结篇 创建类模式主要关注对象的创建过程，将对象的创建过程进行封装， 使客户端可以直接得到对象，而不用去关心如何创建对象。 为什么需要创建性模式 首先，在编程中，对象的创建通常是一件比较复杂的事， 因为，为了达到降低耦合的目的，我们通常采用面向抽象编程的方式， 对象间的关系不会硬编码到类中，而是等到调用的时候再进行组装， 这样虽然降低了对象间的耦合，提高了对象复用的可能， 但在一定程度上将组装类的任务都交给了最终调用的客户端程序， 大大增加了客户端程序的复杂度。 采用创建类模式的优点之一就是将组装对象的过程封装到一个单独的类中， 这样，既不会增加对象间的耦合，又可以最大限度的减小客户端的负担。 其次，使用普通的方式创建对象，一般都是返回一个具体的对象， 即所谓的面向实现编程，这与设计模式原则是相违背的。 采用创建类模式则可以实现面向抽象编程。 客户端要求的只是一个抽象的类型，具体返回什么样的对象，由创建者来决定。 再次，可以对创建对象的过程进行优化， 客户端关注的只是得到对象，对对象的创建过程则不关心， 因此，创建者可以对创建的过程进行优化， 例如在特定条件下，如果使用单例模式或者是使用原型模式， 都可以优化系统的性能。 总结 所有的创建类模式本质上都是对对象的创建过程进行封装。 "},"Chapter04/staticFactory.html":{"url":"Chapter04/staticFactory.html","title":"一个不是GOF设计模式的设计模式","keywords":"","body":"静态（简单）工厂模式 简单工厂模式是最初自然而然就有的设计思想，它只是把创建过程比较自然的封装了一下， 又称为静态工厂模式，是直接根据条件决定创建的产品。 Animal public abstract class Animal { public abstract void call(); } // Dog public static class Dog extends Animal { @Override public void call() { System.out.println(\"Dog\"); } } //Cat public static class Cat extends Animal { @Override public void call() { System.out.println(\"Cat\"); } } //Factory public class Factory { public int Type_Dog=0; public int Type_Cat=1; public static Animal readAnimal(int Type){ switch (Type){ default: return new Dog(); case 0: return new Dog(); case 1: return new Cat(); } } public static void main(String[] args){ Animal animal=Factory.readAnimal(Type_Cat); animal.call(); } } 控制台输出：Cat Effective+Java作者joshua bloch（Java 集合框架创办人、谷歌首席java架构师）建议， 考虑用静态工厂方法来代替多个构造函数。 第一个优点,有名称，有时候一个类的构造器不止一个， 名称往往相同，参数不同，很难理解他们有什么不同的含义， 如果使用静态工厂方法， 就一目了然知道设计者想表达的意图。 第二个优点，不用重复创建一个对象。 第三个优点，可以返回类型的任何子类型.举个例子, List list = Collections.synchronizedList(new ArrayList())　 这个例子就说明了可以返回原返回类型的任何子类型的对象。 缺点： 公有的静态方法所返回的非公有类不能被实例化， 也就是说Collections.synchronizedList返回的SynchronizedList不能被实例化。 查找API比较麻烦， 它们不像普通的类有构造器在API中标识出来， 在文档中要详细说明实例化一个类，非常困难。 由于工厂类集中了所有实例的创建逻辑， 这就直接导致一旦这个工厂出了问题， 所有的客户端都会受到牵连； 而且由于简单工厂模式的产品室基于一个共同的抽象类或者接口， 这样一来，若产品的种类增加时， 即有不同的产品接口或者抽象类的时候， 工厂类就需要判断何时创建何种种类的产品， 更改其中逻辑， 这就和创建何种种类产品的产品相互混淆在了一起 ，违背了单一职责， 导致系统丧失灵活性和可维护性。 而且更重要的是， 简单工厂模式违背了“开放封闭原则”， 就是违背了“系统对扩展开放，对修改关闭”的原则， 因为当我新增加一个产品的时候必须修改工厂类， 相应的工厂类就需要重新编译一遍。 总结一下 简单工厂模式分离产品的创建者和消费者， 有利于软件系统结构的优化； 但是由于一切逻辑都集中在一个工厂类中， 导致了没有很高的内聚性， 同时也违背了“开放封闭原则”。 另外，简单工厂模式的方法一般都是静态的， 而静态工厂方法是无法让子类继承的， 因此，简单工厂模式无法形成基于基类的继承树结构。 "},"Chapter04/Adaper.html":{"url":"Chapter04/Adaper.html","title":"适配器模式","keywords":"","body":"适配器模式 适配器模式(Adapter Pattern)：将一个接口转换成客户希望的另一个接口， 使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。 适配器模式既可以作为类结构型模式，也可以作为对象结构型模式。 在适配器模式中，我们通过增加一个新的适配器类来解决接口不兼容的问题 ，使得原本没有任何关系的类可以协同工作。 根据适配器类与适配者类的关系不同，适配器模式可分为对象适配器和类适配器两种， 在对象适配器模式中，适配器与适配者之间是关联关系； 在类适配器模式中，适配器与适配者之间是继承（或实现）关系。 优缺点 将目标类和适配者类解耦，通过引入一个适配器类来重用现有的适配者类，无须修改原有结构。 增加了类的透明性和复用性，将具体的业务实现过程封装在适配者类中，对于客户端类而言是透明的， 而且提高了适配者的复用性，同一个适配者类可以在多个不同的系统中复用。 灵活性和扩展性都非常好，通过使用配置文件，可以很方便地更换适配器， 也可以在不修改原有代码的基础上增加新的适配器类，完全符合“开闭原则”。 类适配器模式还有如下优点 由于适配器类是适配者类的子类，因此可以在适配器类中置换一些适配者的方法，使得适配器的灵活性更强。 对象适配器模式还有如下优点： 一个对象适配器可以把多个不同的适配者适配到同一个目标； 可以适配一个适配者的子类，由于适配器和适配者之间是关联关系， 根据“里氏代换原则”，适配者的子类也可通过该适配器进行适配。 类适配器模式的缺点如下： 对于Java、C#等不支持多重类继承的语言，一次最多只能适配一个适配者类，不能同时适配多个适配者； 适配者类不能为最终类，如在Java中不能为final类，C#中不能为sealed类； 在Java、C#等语言中，类适配器模式中的目标抽象类只能为接口，不能为类，其使用有一定的局限性。 对象适配器模式的缺点 与类适配器模式相比，要在适配器中置换适配者类的某些方法比较麻烦。 如果一定要置换掉适配者类的一个或多个方法，可以先做一个适配者类的子类， 将适配者类的方法置换掉，然后再把适配者类的子类当做真正的适配者进行适配，实现过程较为复杂。 适用场景 系统需要使用一些现有的类，而这些类的接口（如方法名）不符合系统的需要，甚至没有这些类的源代码。 想创建一个可以重复使用的类，用于与一些彼此之间没有太大关联的一些类，包括一些可能在将来引进的类一起工作。 心得 尽量使用对象适配器的实现方式，多用组合、少用继承。 代码展示 对象适配器1 package designpatterns.adapter.object.zyr; public class Banner { private String name; public Banner(String name){ this.name=name; } public void showWithParen(){ System.out.println(\"(\"+name+\")\"); } public void showWithAster(){ System.out.println(\"*\"+name+\"*\"); } } package designpatterns.adapter.object.zyr; public abstract class Print { public abstract void printWeak(); public abstract void printStrong(); } package designpatterns.adapter.object.zyr; /** * 可以看到Main函数、Banner类都没有改动，将Print接口变成抽象类，那么PrintBanner不能同时继承两个类， * 因此将Banner对象组合到适配器之中，因此叫做对象适配器，这样也可以实现预期的结果。 * 两者的区别也是非常明显的，最好推荐使用前者，或者根据实际情况需要进行甄别。 * */ public class PrintBanner extends Print { Banner banner; public PrintBanner(String name) { banner=new Banner(name); } public void printWeak() { System.out.println(\"...开始弱适配...\"); banner.showWithParen(); System.out.println(\"...弱适配成功...\"); System.out.println(); } public void printStrong() { System.out.println(\"...开始强适配...\"); banner.showWithAster(); System.out.println(\"...强适配成功...\"); System.out.println(); } } package designpatterns.adapter.object.zyr; public class PrintBannerMain { public static void main(String[] args) { Print p=new PrintBanner(\"Fire\"); p.printStrong(); p.printWeak(); } } 对象适配器2 package designpatterns.adapter.object.ac; public interface AC { int outputAC(); } package designpatterns.adapter.object.ac; public class AC110 implements AC { public final int output = 110; public int outputAC() { return 110; } } package designpatterns.adapter.object.ac; public class AC220 implements AC { public final int output = 220; public int outputAC() { return output; } } package designpatterns.adapter.object.ac; /** * 适配器接口 * */ public interface DC5Adapter { //用于检查输入的电压是否与适配器匹配， boolean support(AC ac); //用于将输入的电压变换为 5V 后输出 int outputDC5V(AC ac); } package designpatterns.adapter.object.ac; public class ChinaPowerAdapter implements DC5Adapter { public static final int voltage = 220; public boolean support(AC ac) { return (voltage == ac.outputAC()); } public int outputDC5V(AC ac) { int adapterInput = ac.outputAC(); //变压器... int adapterOutput = adapterInput / 44; System.out.println(\"使用ChinaPowerAdapter变压适配器，输入AC:\" + adapterInput + \"V\" + \"，输出DC:\" + adapterOutput + \"V\"); return adapterOutput; } } package designpatterns.adapter.object.ac; public class JapanPowerAdapter implements DC5Adapter { public static final int voltage = 110; public boolean support(AC ac) { return (voltage == ac.outputAC()); } public int outputDC5V(AC ac) { int adapterInput = ac.outputAC(); //变压器... int adapterOutput = adapterInput / 22; System.out.println(\"使用JapanPowerAdapter变压适配器，输入AC:\" + adapterInput + \"V\" + \"，输出DC:\" + adapterOutput + \"V\"); return adapterOutput; } } package designpatterns.adapter.object.ac; import java.util.LinkedList; import java.util.List; public class ACMain { private List adapters = new LinkedList(); ACMain() { this.adapters.add(new ChinaPowerAdapter()); this.adapters.add(new JapanPowerAdapter()); } // 根据电压找合适的变压器 public DC5Adapter getPowerAdapter(AC ac) { DC5Adapter adapter = null; for (DC5Adapter ad : this.adapters) { if (ad.support(ac)) { adapter = ad; break; } } if (adapter == null) { throw new IllegalArgumentException(\"没有找到合适的变压适配器\"); } return adapter; } public static void main(String[] args) { ACMain test = new ACMain(); AC chinaAC = new AC220(); DC5Adapter adapter = test.getPowerAdapter(chinaAC); adapter.outputDC5V(chinaAC); // 去日本旅游，电压是 110V AC japanAC = new AC110(); adapter = test.getPowerAdapter(japanAC); adapter.outputDC5V(japanAC); } } 类适配器1 package designpatterns.adapter.clazz.example; /** * 定义一个目标接口 */ public interface Target { void request(); } package designpatterns.adapter.clazz.example; /** * 一个将被适配的类 */ public class Adaptee { public void adapteeRequest() { System.out.println(\"被适配者的方法\"); } } package designpatterns.adapter.clazz.example; /** * 一种错误的实现方式 * * 怎么才可以在目标接口中的 request() 调用 Adaptee 的 adapteeRequest() 方法呢？ * * 如果直接实现 Target 是不行的 * */ public class ConcreteTarget implements Target { public void request() { System.out.println(\"concreteTarget目标方法\"); } } package designpatterns.adapter.clazz.example; /** * 一个正确的方式 * * 如果通过一个适配器类，实现 Target 接口，同时继承了 Adaptee 类，然后在实现的 request() 方法中调用父类的 adapteeRequest() 即可实现 * */ public class Adapter extends Adaptee implements Target { public void request() { //...一些操作... super.adapteeRequest(); //...一些操作... } } package designpatterns.adapter.clazz.example; public class AdapterMain { public static void main(String[] args) { Target target = new ConcreteTarget(); target.request(); Target adapterTarget = new Adapter(); adapterTarget.request(); } } 类适配器2 package designpatterns.adapter.clazz.zyr; public interface Print { void printWeak(); void printStrong(); } package designpatterns.adapter.clazz.zyr; public class Banner { private String name; public Banner(String name){ this.name=name; } public void showWithParen(){ System.out.println(\"(\"+name+\")\"); } public void showWithAster(){ System.out.println(\"*\"+name+\"*\"); } } package designpatterns.adapter.clazz.zyr; public class PrintBanner extends Banner implements Print { public PrintBanner(String name) { super(name); } public void printWeak() { System.out.println(\"...开始弱适配...\"); showWithParen(); System.out.println(\"...弱适配成功...\"); System.out.println(); } public void printStrong() { System.out.println(\"...开始强适配...\"); showWithAster(); System.out.println(\"...强适配成功...\"); System.out.println(); } } package designpatterns.adapter.clazz.zyr; public class PrintBannerMain { public static void main(String[] args) { Print p = new PrintBanner(\"Fire\"); p.printStrong(); p.printWeak(); } } 源码分析适配器模式的典型应用 spring AOP中的适配器模式 在Spring的Aop中，使用的 Advice（通知） 来增强被代理类的功能。 Advice的类型有：MethodBeforeAdvice、AfterReturningAdvice、ThrowsAdvice 在每个类型 Advice 都有对应的拦截器，MethodBeforeAdviceInterceptor、AfterReturningAdviceInterceptor、ThrowsAdviceInterceptor Spring需要将每个 Advice 都封装成对应的拦截器类型，返回给容器，所以需要使用适配器模式对 Advice 进行转换 三个适配者类 Adaptee 如下： public interface MethodBeforeAdvice extends BeforeAdvice { void before(Method var1, Object[] var2, @Nullable Object var3) throws Throwable; } public interface AfterReturningAdvice extends AfterAdvice { void afterReturning(@Nullable Object var1, Method var2, Object[] var3, @Nullable Object var4) throws Throwable; } public interface ThrowsAdvice extends AfterAdvice { } 目标接口 Target，有两个方法，一个判断 Advice 类型是否匹配，一个是工厂方法，创建对应类型的 Advice 对应的拦截器 public interface AdvisorAdapter { boolean supportsAdvice(Advice var1); MethodInterceptor getInterceptor(Advisor var1); } 三个适配器类 Adapter 分别如下，注意其中的 Advice、Adapter、Interceptor之间的对应关系 class MethodBeforeAdviceAdapter implements AdvisorAdapter, Serializable { @Override public boolean supportsAdvice(Advice advice) { return (advice instanceof MethodBeforeAdvice); } @Override public MethodInterceptor getInterceptor(Advisor advisor) { MethodBeforeAdvice advice = (MethodBeforeAdvice) advisor.getAdvice(); return new MethodBeforeAdviceInterceptor(advice); } } @SuppressWarnings(\"serial\") class AfterReturningAdviceAdapter implements AdvisorAdapter, Serializable { @Override public boolean supportsAdvice(Advice advice) { return (advice instanceof AfterReturningAdvice); } @Override public MethodInterceptor getInterceptor(Advisor advisor) { AfterReturningAdvice advice = (AfterReturningAdvice) advisor.getAdvice(); return new AfterReturningAdviceInterceptor(advice); } } class ThrowsAdviceAdapter implements AdvisorAdapter, Serializable { @Override public boolean supportsAdvice(Advice advice) { return (advice instanceof ThrowsAdvice); } @Override public MethodInterceptor getInterceptor(Advisor advisor) { return new ThrowsAdviceInterceptor(advisor.getAdvice()); } } 客户端 DefaultAdvisorAdapterRegistry public class DefaultAdvisorAdapterRegistry implements AdvisorAdapterRegistry, Serializable { private final List adapters = new ArrayList(3); public DefaultAdvisorAdapterRegistry() { // 这里注册了适配器 this.registerAdvisorAdapter(new MethodBeforeAdviceAdapter()); this.registerAdvisorAdapter(new AfterReturningAdviceAdapter()); this.registerAdvisorAdapter(new ThrowsAdviceAdapter()); } public MethodInterceptor[] getInterceptors(Advisor advisor) throws UnknownAdviceTypeException { List interceptors = new ArrayList(3); Advice advice = advisor.getAdvice(); if (advice instanceof MethodInterceptor) { interceptors.add((MethodInterceptor)advice); } Iterator var4 = this.adapters.iterator(); while(var4.hasNext()) { AdvisorAdapter adapter = (AdvisorAdapter)var4.next(); if (adapter.supportsAdvice(advice)) { // 这里调用适配器方法 interceptors.add(adapter.getInterceptor(advisor)); // 这里调用适配器方法 } } if (interceptors.isEmpty()) { throw new UnknownAdviceTypeException(advisor.getAdvice()); } else { return (MethodInterceptor[])interceptors.toArray(new MethodInterceptor[0]); } } // ...省略... } 这里看 while 循环里，逐个取出注册的适配器，调用 supportsAdvice() 方法来判断 Advice 对应的类型，然后调用 getInterceptor() 创建对应类型的拦截器 这里应该属于对象适配器模式，关键字 instanceof 可看成是 Advice 的方法，不过这里的 Advice 对象是从外部传进来，而不是成员属性 spring JPA中的适配器模式 在Spring的ORM包中，对于JPA的支持也是采用了适配器模式，首先定义了一个接口的 JpaVendorAdapter，然后不同的持久层框架都实现此接口。 jpaVendorAdapter：用于设置实现厂商JPA实现的特定属性，如设置Hibernate的是否自动生成DDL的属性generateDdl；这些属性是厂商特定的，因此最好在这里设置；目前Spring提供 HibernateJpaVendorAdapter、OpenJpaVendorAdapter、EclipseLinkJpaVendorAdapter、TopLinkJpaVendorAdapter 四个实现。其中最重要的属性是 database，用来指定使用的数据库类型，从而能根据数据库类型来决定比如如何将数据库特定异常转换为Spring的一致性异常，目前支持如下数据库（DB2、DERBY、H2、HSQL、INFORMIX、MYSQL、ORACLE、POSTGRESQL、SQL_SERVER、SYBASE） public interface JpaVendorAdapter { // 返回一个具体的持久层提供者 public abstract PersistenceProvider getPersistenceProvider(); // 返回持久层提供者的包名 public abstract String getPersistenceProviderRootPackage(); // 返回持久层提供者的属性 public abstract Map getJpaPropertyMap(); // 返回JpaDialect public abstract JpaDialect getJpaDialect(); // 返回持久层管理器工厂 public abstract Class getEntityManagerFactoryInterface(); // 返回持久层管理器 public abstract Class getEntityManagerInterface(); // 自定义回调方法 public abstract void postProcessEntityManagerFactory(EntityManagerFactory paramEntityManagerFactory); } 我们来看其中一个适配器实现类 HibernateJpaVendorAdapter public class HibernateJpaVendorAdapter extends AbstractJpaVendorAdapter { //设定持久层提供者 private final PersistenceProvider persistenceProvider; //设定持久层方言 private final JpaDialect jpaDialect; public HibernateJpaVendorAdapter() { this.persistenceProvider = new HibernatePersistence(); this.jpaDialect = new HibernateJpaDialect(); } //返回持久层方言 public PersistenceProvider getPersistenceProvider() { return this.persistenceProvider; } //返回持久层提供者 public String getPersistenceProviderRootPackage() { return \"org.hibernate\"; } //返回JPA的属性 public Map getJpaPropertyMap() { Map jpaProperties = new HashMap(); if (getDatabasePlatform() != null) { jpaProperties.put(\"hibernate.dialect\", getDatabasePlatform()); } else if (getDatabase() != null) { Class databaseDialectClass = determineDatabaseDialectClass(getDatabase()); if (databaseDialectClass != null) { jpaProperties.put(\"hibernate.dialect\", databaseDialectClass.getName()); } } if (isGenerateDdl()) { jpaProperties.put(\"hibernate.hbm2ddl.auto\", \"update\"); } if (isShowSql()) { jpaProperties.put(\"hibernate.show_sql\", \"true\"); } return jpaProperties; } //设定数据库 protected Class determineDatabaseDialectClass(Database database) { switch (1.$SwitchMap$org$springframework$orm$jpa$vendor$Database[database.ordinal()]) { case 1: return DB2Dialect.class; case 2: return DerbyDialect.class; case 3: return H2Dialect.class; case 4: return HSQLDialect.class; case 5: return InformixDialect.class; case 6: return MySQLDialect.class; case 7: return Oracle9iDialect.class; case 8: return PostgreSQLDialect.class; case 9: return SQLServerDialect.class; case 10: return SybaseDialect.class; } return null; } //返回JPA方言 public JpaDialect getJpaDialect() { return this.jpaDialect; } //返回JPA实体管理器工厂 public Class getEntityManagerFactoryInterface() { return HibernateEntityManagerFactory.class; } //返回JPA实体管理器 public Class getEntityManagerInterface() { return HibernateEntityManager.class; } } spring MVC中的适配器模式 Spring MVC中的适配器模式主要用于执行目标 Controller 中的请求处理方法。 在Spring MVC中，DispatcherServlet 作为用户，HandlerAdapter 作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller 作为需要适配的类。 为什么要在 Spring MVC 中使用适配器模式？Spring MVC 中的 Controller 种类众多，不同类型的 Controller 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet 直接获取对应类型的 Controller，需要的自行来判断，像下面这段代码一样： if(mappedHandler.getHandler() instanceof MultiActionController){ ((MultiActionController)mappedHandler.getHandler()).xxx }else if(mappedHandler.getHandler() instanceof XXX){ ... }else if(...){ ... } 这样假设如果我们增加一个 HardController,就要在代码中加入一行 if(mappedHandler.getHandler() instanceof HardController)，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。 我们来看看源码，首先是适配器接口 HandlerAdapter public interface HandlerAdapter { boolean supports(Object var1); ModelAndView handle(HttpServletRequest var1, HttpServletResponse var2, Object var3) throws Exception; long getLastModified(HttpServletRequest var1, Object var2); } 现该接口的适配器每一个 Controller 都有一个适配器与之对应，这样的话，每自定义一个 Controller 需要定义一个实现 HandlerAdapter 的适配器。 springmvc 中提供的 Controller 实现类有如下 "},"Chapter04/Decorato.html":{"url":"Chapter04/Decorato.html","title":"装饰器模式","keywords":"","body":"装饰器模式 装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能， 同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。 这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。 介绍 意图：动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式相比生成子类更为灵活。 主要解决：一般的，我们为了扩展一个类经常使用继承方式实现，由于继承为类引入静态特征，并且随着扩展功能的增多，子类会很膨胀。 何时使用：在不想增加很多子类的情况下扩展类。 如何解决：将具体功能职责划分，同时继承装饰者模式。 关键代码 Component 类充当抽象角色，不应该具体实现。 修饰类引用和继承 Component 类，具体扩展类重写父类方法。 应用实例 孙悟空有 72 变，当他变成\"庙宇\"后，他的根本还是一只猴子，但是他又有了庙宇的功能。 不论一幅画有没有画框都可以挂在墙上，但是通常都是有画框的，并且实际上是画框被挂在墙上。在挂在墙上之前，画可以被蒙上玻璃，装到框子里；这时画、玻璃和画框形成了一个物体。 优点 装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式， 装饰模式可以动态扩展一个实现类的功能。 缺点 多层装饰比较复杂。 使用场景 扩展一个类的功能。 动态增加功能，动态撤销。 注意事项 可代替继承。 代码实现 package designpatterns.decorato; /** * 目标接口：房子 */ public interface House { void output(); } package designpatterns.decorato; /** * 房子实现类 */ public class DongHaoHouse implements House { public void output() { System.out.println(\"这是董浩的房子\"); } } package designpatterns.decorato; /** * 房子实现类 */ public class DongLiangHouse implements House { public void output() { System.out.println(\"这是董量的房子\"); } } package designpatterns.decorato; //装饰器 public class Decorator implements House { private House house; public Decorator(House house) { this.house = house; } public void output() { System.out.println(\"这是针对房子的前段装饰增强\"); house.output(); System.out.println(\"这是针对房子的后段装饰增强\"); } } package designpatterns.decorato; public class DecoratoMain { public static void main(String[] args) { House dongHaoHouse = new DongHaoHouse(); House decorator = new Decorator(dongHaoHouse); decorator.output(); } } 装饰模式的应用场景 前面讲解了关于装饰模式的结构与特点，下面介绍其适用的应用场景，装饰模式通常在以下几种情况使用。 当需要给一个现有类添加附加职责，而又不能采用生成子类的方法进行扩充时。 例如，该类被隐藏或者该类是终极类或者采用继承方式会产生大量的子类。 当需要通过对现有的一组基本功能进行排列组合而产生非常多的功能时， 采用继承关系很难实现，而采用装饰模式却很好实现。 当对象的功能要求可以动态地添加，也可以再动态地撤销时。 装饰模式在 Java 语言中的最著名的应用莫过于 Java I/O 标准库的设计了。 例如，InputStream 的子类 FilterInputStream，OutputStream 的子类 FilterOutputStream， Reader 的子类 BufferedReader 以及 FilterReader， 还有 Writer 的子类 BufferedWriter、FilterWriter 以及 PrintWriter 等，它们都是抽象装饰类。 下面代码是为 FileReader 增加缓冲区而采用的装饰类 BufferedReader 的例子： BufferedReader in=new BufferedReader(new FileReader(\"filename.txtn)); String s=in.readLine() "},"Chapter04/Proxy.html":{"url":"Chapter04/Proxy.html","title":"代理模式","keywords":"","body":"代理模式 在代理模式（Proxy Pattern）中，一个类代表另一个类的功能。这种类型的设计模式属于结构型模式。 在代理模式中，我们创建具有现有对象的对象，以便向外界提供功能接口。 代理就是中介，中间人。法律上也有代理，比如代理律师之类，委托人将自己的一部分权限委托给代理者， 代理者就拥有被代理者（委托人）的部分权限，并且可以以被代理人的名义来实行这些权限， 此时代理者与委托人等同，当然代理人也可以在实行权限时配合自己的能力来进行，当然不能超出这个权限。 Java中的代理模式类似于上面的代理，我们也是为一个类（委托类）创建一个代理类，来代表它来对外提供功能。　　 介绍 意图：为其他对象提供一种代理以控制对这个对象的访问。 主要解决：在直接访问对象时带来的问题，比如说：要访问的对象在远程的机器上。在面向对象系统中，有些对象由于某些原因（比如对象创建开销很大，或者某些操作需要安全控制，或者需要进程外的访问），直接访问会给使用者或者系统结构带来很多麻烦，我们可以在访问此对象时加上一个对此对象的访问层。 何时使用：想在访问一个类时做一些控制。 如何解决：增加中间层。 实现与被代理类组合。 应用实例： Windows 里面的快捷方式。 猪八戒去找高翠兰结果是孙悟空变的，可以这样理解：把高翠兰的外貌抽象出来，高翠兰本人和孙悟空都实现了这个接口，猪八戒访问高翠兰的时候看不出来这个是孙悟空，所以说孙悟空是高翠兰代理类。 买火车票不一定在火车站买，也可以去代售点。 一张支票或银行存单是账户中资金的代理。支票在市场交易中用来代替现金，并提供对签发人账号上资金的控制。 spring aop。 优点 职责清晰。 高扩展性。 智能化。 缺点 由于在客户端和真实主题之间增加了代理对象，因此有些类型的代理模式可能会造成请求的处理速度变慢。 实现代理模式需要额外的工作，有些代理模式的实现非常复杂。 增加了代理类与委托类之间的强耦合 使用场景 按职责来划分，通常有以下使用场景： 远程代理。 虚拟代理。 Copy-on-Write 代理。 保护（Protect or Access）代理。 Cache代理。 防火墙（Firewall）代理。 同步化（Synchronization）代理。 智能引用（Smart Reference）代理。 代理模式场景描述： 当我们想要隐藏某个类时，可以为其提供代理类 当一个类需要对不同的调用者提供不同的调用权限时，可以使用代理类来实现（代理类不一定只有一个，我们可以建立多个代理类来实现，也可以在一个代理类中金进行权限判断来进行不同权限的功能调用） 当我们要扩展某个类的某个功能时，可以使用代理模式，在代理类中进行简单扩展（只针对简单扩展，可在引用委托类的语句之前与之后进行） 注意事项 和适配器模式的区别：适配器模式主要改变所考虑对象的接口，而代理模式不能改变所代理类的接口。 和装饰器模式的区别：装饰器模式为了增强功能，而代理模式是为了加以控制。 代码实现 代理模式很简单，只要记住以下关键点，简单易实现 代理类与委托类实现同一接口 在委托类中实现功能，在代理类的方法中中引用委托类的同名方法 外部类调用委托类某个方法时，直接以接口指向代理类的实例，这正是代理的意义所在：屏蔽。 package designpatterns.proxy; public interface ZiRanRen { void quanLi(); } package designpatterns.proxy; //委托人： public class MaYun implements ZiRanRen { private void eat() { System.out.println(\"今天吃满汉全席\"); } private void drink() { System.out.println(\"今天喝大西洋\"); } public void quanLi() { System.out.println(\"我赋予我的代理律师来行使这些权利,此时代理律师全权代理我处理某些事务\"); eat(); } } package designpatterns.proxy; //代理律师 public class LvShi implements ZiRanRen { public void quanLi() { new MaYun().quanLi(); } } package designpatterns.proxy; public class ProxyMain { public static void main(String[] args) { ZiRanRen ls = new LvShi(); ls.quanLi(); } } 上面是一个很简单的例子，可以看出，我们想对外开放某些功能， 就可以将这些功能在代理类中被引用，如此一来，屏蔽了我们不想外露的功能， 只将我们想开放的功能开放出来。亦即委托类中其实是可以有很多方法的， 很多功能的，我们可以酌情对外开放，代理类犹如一道大门， 将委托类与外部调用者隔绝开来，只将部分功能赋予这个大门， 来代替委托类行使这个功能，哪怕最终还是要牵扯到自身（因为最终还是要调用委托类的对应方法实现）。 "},"Chapter04/ProxyDecorato.html":{"url":"Chapter04/ProxyDecorato.html","title":"代理模式和装饰器模式对比","keywords":"","body":"代理模式和装饰器模式对比 装饰器模式作用是针对目标方法进行增强，提供新的功能或者额外的功能。 不同于适配器模式和桥接模式，装饰器模式涉及的是单方，和代理模式相同，而且目标必须是抽象的。 而实际上，装饰器模式和代理模式的实现方式基本一致，只在目标的存在上有些差别。 我们需要从概念上了解代理和装饰的区别 代理是全权代理，目标根本不对外，全部由代理类来完成。 装饰是增强，是辅助，目标仍然可以自行对外提供服务，装饰器只起增强作用。 上面两点提现到代码实现中是这样的 代理模式 public class Proxy implements House { private House house; public Proxy(){ this.house = new DonghaoHouse(); } @Override public void output() { System.out.println(\"这是针对目标的前段增强\"); house.output(); System.out.println(\"这是针对目标的后段增强\"); } } 装饰模式 public class Decorator implements House { private House house; public Decorator(House house){ this.house = house; } @Override public void output() { System.out.println(\"这是针对房子的前段装饰增强\"); house.output(); System.out.println(\"这是针对房子的后段装饰增强\"); } } 看出来了吗，装饰器中持有的目标实例是从构造器传入的， 而代理中持有的目标实例是自己创建的。 那么这里又出现一个区别，代理模式和装饰器模式虽然都依赖于目标接口， 但是代理针对的目标实现类是固定的， 而装饰器模式可以随意指定，也就是说目标是可以自有扩展的。 我们要明白代理模式和装饰器模式的区别，区分二者的使用场景，如下图： "},"Chapter04/Facade.html":{"url":"Chapter04/Facade.html","title":"外观模式","keywords":"","body":"外观模式 外观模式（Facade Pattern）隐藏系统的复杂性，并向客户端提供了一个客户端可以访问系统的接口。 这种类型的设计模式属于结构型模式，它向现有的系统添加一个接口，来隐藏系统的复杂性。 其实直接调用也会得到相同的结果，但是采用外观模式能规范代码， 外观类就是子系统对外的一个总接口，我们要访问子系统是， 直接去子系统对应的外观类进行访问即可！ 这种模式涉及到一个单一的类，该类提供了客户端请求的简化方法和对现有系统类方法的委托调用。 介绍 意图：为子系统中的一组接口提供一个一致的界面，外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。 主要解决：降低访问复杂系统的内部子系统时的复杂度，简化客户端与之的接口。 如何解决：客户端不与系统耦合，外观类与系统耦合。 关键代码：在客户端和复杂系统之间再加一层，这一层将调用顺序、依赖关系等处理好。 何时使用： 客户端不需要知道系统内部的复杂联系，整个系统只需提供一个\"接待员\"即可。 定义系统的入口。 应用实例： 去医院看病，可能要去挂号、门诊、划价、取药，让患者或患者家属觉得很复杂，如果有提供接待人员，只让接待人员来处理，就很方便。 JAVA 的三层开发模式。 优点： 减少系统相互依赖。 提高灵活性。 提高了安全性。 缺点： 不符合开闭原则，如果要改东西很麻烦，继承重写都不合适。 使用场景： 为复杂的模块或子系统提供外界访问的模块。 子系统相对独立。 预防低水平人员带来的风险。 注意事项： 在层次化结构中，可以使用外观模式定义系统中每一层的入口。 外观模式应用场景 当我们访问的子系统拥有复杂额结构，内部调用繁杂，初接触者根本无从下手时， 不凡由资深者为这个子系统设计一个外观类来供访问者使用，统一访问路径（集中到外观类中）， 将繁杂的调用结合起来形成一个总调用写到外观类中，之后访问者不用再繁杂的方法中寻找需要的方法进行调用， 直接在外观类中找对应的方法进行调用即可。 还有就是在系统与系统之间发生调用时，也可以为被调用子系统设计外观类， 这样方便调用也，屏蔽了系统的复杂性。 代码实现 package designpatterns.facade; public class SubMethod1 { public void method1() { System.out.println(\"子系统中类1的方法1\"); } public void method3() { System.out.println(\"子系统类1方法3\"); } } package designpatterns.facade; public class SubMethod2 { public void method2() { System.out.println(\"子系统中类2方法2\"); } } package designpatterns.facade; public class SubMethod3 { public void method3() { System.out.println(\"子系统类3方法3\"); } } package designpatterns.facade; public class Facader { private SubMethod1 sm1 = new SubMethod1(); private SubMethod2 sm2 = new SubMethod2(); private SubMethod3 sm3 = new SubMethod3(); public void facMethod1() { sm1.method1(); sm2.method2(); sm1.method3(); } public void facMethod2() { sm2.method2(); sm3.method3(); sm1.method1(); } } package designpatterns.facade; public class FacaderMain { public static void main(String[] args) { Facader face = new Facader(); face.facMethod1(); // face.facMethod2(); } } "},"Chapter04/Bridge.html":{"url":"Chapter04/Bridge.html","title":"桥接模式","keywords":"","body":"桥接模式 桥接（Bridge）是用于把抽象化与实现化解耦，使得二者可以独立变化。 这种类型的设计模式属于结构型模式，它通过提供抽象化和实现化之间的桥接结构，来实现二者的解耦。 这种模式涉及到一个作为桥接的接口，使得实体类的功能独立于接口实现类。 这两种类型的类可被结构化改变而互不影响。 介绍 意图：将抽象部分与实现部分分离，使它们都可以独立的变化。 主要解决：在有多种可能会变化的情况下，用继承会造成类爆炸问题，扩展起来不灵活。 何时使用：实现系统可能有多个角度分类，每一种角度都可能变化。 关键代码：抽象类依赖实现类。 应用实例： 猪八戒从天蓬元帅转世投胎到猪，转世投胎的机制将尘世划分为两个等级，即：灵魂和肉体，前者相当于抽象化，后者相当于实现化。生灵通过功能的委派，调用肉体对象的功能，使得生灵可以动态地选择。 墙上的开关，可以看到的开关是抽象的，不用管里面具体怎么实现的。 优点： 抽象和实现的分离。 优秀的扩展能力。 实现细节对客户透明。 缺点： 桥接模式的引入会增加系统的理解与设计难度，由于聚合关联关系建立在抽象层，要求开发者针对抽象进行设计与编程。 使用场景： 如果一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的继承联系，通过桥接模式可以使它们在抽象层建立一个关联关系。 对于那些不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统，桥接模式尤为适用。 一个类存在两个独立变化的维度，且这两个维度都需要进行扩展。 注意事项 对于两个独立变化的维度，使用桥接模式再适合不过了。 个人理解： 桥接是一个接口，它与一方应该是绑定的，也就是解耦的双方中的一方必然是继承这个接口的， 这一方就是实现方，而另一方正是要与这一方解耦的抽象方，如果不采用桥接模式， 一般我们的处理方式是直接使用继承来实现，这样双方之间处于强链接，类之间关联性极强， 如要进行扩展，必然导致类结构急剧膨胀。采用桥接模式，正是为了避免这一情况的发生， 将一方与桥绑定，即实现桥接口，另一方在抽象类中调用桥接口（指向的实现类）， 这样桥方可以通过实现桥接口进行单方面扩展，而另一方可以继承抽象类而单方面扩展， 而之间的调用就从桥接口来作为突破口，不会受到双方扩展的任何影响。 下面的实例能真正体现着一点 实例准备：我们假设有一座桥，桥左边为A，桥右边为B，A有A1，A2，A3等，表示桥左边的三个不同地方， B有B1，B2，B3等，表示桥右边的三个不同地方，假设我们要从桥左侧A出发到桥的右侧B， 我们可以有多重方案，A1到B1，A1到B2，A1到B3，A2到B1...等等 桥接模式模式的扩展 在软件开发中，有时桥接（Bridge）模式可与适配器模式联合使用。当桥接（Bridge）模式的实现化角色的接口与现有类的接口不一致时， 可以在二者中间定义一个适配器将二者连接起来 注意点 定义一个桥接口，使其与一方绑定，这一方的扩展全部使用实现桥接口的方式。 定义一个抽象类，来表示另一方，在这个抽象类内部要引入桥接口，而这一方的扩展全部使用继承该抽象类的方式。 其实我们可以发现桥接模式应对的场景有方向性的，桥绑定的一方都是被调用者，属于被动方，抽象方属于主动方。 其实我的JDK提供的JDBC数据库访问接口API正是经典的桥接模式的实现者， 接口内部可以通过实现接口来扩展针对不同数据库的具体实现来进行扩展， 而对外的仅仅只是一个统一的接口调用，调用方过于抽象， 可以将其看做每一个JDBC调用程序（这是真实实物，当然不存在抽象） 下面来理解一下开头的概念： 桥接（Bridge）是用于把抽象化与实现化解耦，使得二者可以独立变化。这种类型的设计模式属于结构型模式， 它通过提供抽象化和实现化之间的桥接结构，来实现二者的解耦。 这种模式涉及到一个作为桥接的接口，使得实体类的功能独立于接口实现类。 这两种类型的类可被结构化改变而互不影响。 理解 此处抽象化与实现化分别指代实例中的双方，而且实现化对应目的地方（通过实现桥接口进行扩展）， 抽象方对应来源地方（通过继承抽象类来进行扩展），如果我们不使用桥接模式，我们会怎么想实现这个实例呢？ 很简单，我们分别定义来源地A1、A2、A3类和目的地B1、B2、B3，然后具体的实现就是，A1到B1一个类，A1到B2一个类，等，如果我们要扩展了A和B ,要直接增加An类和Bn类，如此编写不说类内部重复性代码多，而且还会导致类结构的急剧膨胀，最重要的是，在通过继承实现路径的时候，会造成双方耦合性增大，而这又进一步加剧了扩展的复杂性。使用桥结构模式可以很好地规避这些问题：重在解耦。 代码实现 示例1 package designpatterns.bridge.bky; public interface Qiao { //目的地B void targetAreaB(); } package designpatterns.bridge.bky; public abstract class AreaA { //引用桥接口 Qiao qiao; //来源地 abstract void fromAreaA(); } package designpatterns.bridge.bky; /** * 来源地A1 */ public class AreaA1 extends AreaA { void fromAreaA() { System.out.println(\"我来自A1\"); qiao.targetAreaB(); } } package designpatterns.bridge.bky; public class AreaA2 extends AreaA { void fromAreaA() { System.out.println(\"我来自A2\"); qiao.targetAreaB(); } } package designpatterns.bridge.bky; public class AreaA3 extends AreaA { void fromAreaA() { System.out.println(\"我来自A3\"); qiao.targetAreaB(); } } package designpatterns.bridge.bky; /** * 目的地B1 */ public class AreaB1 implements Qiao { public void targetAreaB() { System.out.println(\"我要去B1\"); } } package designpatterns.bridge.bky; public class AreaB2 implements Qiao { public void targetAreaB() { System.out.println(\"我要去B2\"); } } package designpatterns.bridge.bky; public class AreaB3 implements Qiao { public void targetAreaB() { System.out.println(\"我要去B3\"); } } package designpatterns.bridge.bky; public class QiaoMain { public static void main(String[] args) { AreaA a = new AreaA2(); a.qiao = new AreaB3(); a.fromAreaA(); // a.qiao.targetAreaB(); } } 示例2 package designpatterns.bridge.csdn; public interface Implementor { void OperationImpl(); } package designpatterns.bridge.csdn; //抽象化角色 public abstract class Abstraction { protected Implementor imple; protected Abstraction(Implementor imple) { this.imple = imple; } public abstract void Operation(); } package designpatterns.bridge.csdn; //具体实现化角色 public class ConcreteImplementorA implements Implementor { public void OperationImpl() { System.out.println(\"具体实现化(Concrete Implementor)角色被访问\"); } } package designpatterns.bridge.csdn; //扩展抽象化角色 public class RefinedAbstraction extends Abstraction { protected RefinedAbstraction(Implementor imple) { super(imple); } public void Operation() { System.out.println(\"扩展抽象化(Refined Abstraction)角色被访问\"); imple.OperationImpl(); } } package designpatterns.bridge.csdn; public class BridgeTest { public static void main(String[] args) { Implementor imple = new ConcreteImplementorA(); Abstraction abs = new RefinedAbstraction(imple); abs.Operation(); } } "},"Chapter04/Composite.html":{"url":"Chapter04/Composite.html","title":"组合模式","keywords":"","body":"组合（Composite）模式 组合模式（Composite Pattern），又叫部分整体模式，是用于把一组相似的对象当作一个单一的对象。 组合模式依据树形结构来组合对象，用来表示部分以及整体层次。 这种类型的设计模式属于结构型模式，它创建了对象组的树形结构。 这种模式创建了一个包含自己对象组的类。该类提供了修改相同对象组的方式。 它是一种将对象组合成树状的层次结构的模式，用来表示“部分-整体”的关系，使用户对单个对象和组合对象具有一致的访问性。 这个模式在我们的生活中也经常使用，比如说如果读者有使用Java的GUI编写过程序的， 肯定少不了定义一些组件，初始化之后，然后使用容器的add方法，将这些组件有顺序的组织成一个界面出来； 或者读者如果编写过前端的页面，肯定使用过等标签定义一些格式，然后格式之间互相组合， 通过一种递归的方式组织成相应的结构，这种方式其实就是组合，将部分的组件镶嵌到整体之中； 又或者文件和文件夹的组织关系，通过目录表项作为共同的特质（父类），一个文件夹可以包含多个文件夹和多个文件， 一个文件容纳在一个文件夹之中。那么凭什么可以这样做呢，需要满足以下两点， 首先整体的结构应该是一棵树， 第二，所有的组件应该有一个共同的父类（有共同的本质），这个父类使得组件中的共同的本质可以提取出来 （有了共同语言（父类）），进行互融，其实就是父类使用add方法，这样子类就可以通过抽象的方式通过父类来表达了， 可能有点绕口 介绍 意图：将对象组合成树形结构以表示\"部分-整体\"的层次结构。组合模式使得用户对单个对象和组合对象的使用具有一致性。 主要解决：它在我们树型结构的问题中，模糊了简单元素和复杂元素的概念，客户程序可以向处理简单元素一样来处理复杂元素，从而使得客户程序与复杂元素的内部结构解耦。 何时使用： 您想表示对象的部分-整体层次结构（树形结构）。 您希望用户忽略组合对象与单个对象的不同，用户将统一地使用组合结构中的所有对象。 如何解决：树枝和叶子实现统一接口，树枝内部组合该接口。 关键代码：树枝内部组合该接口，并且含有内部属性 List，里面放 Component。 应用实例： 算术表达式包括操作数、操作符和另一个操作数，其中，另一个操作符也可以是操作数、操作符和另一个操作数。 在 JAVA AWT 和 SWING 中，对于 Button 和 Checkbox 是树叶，Container 是树枝。 优点 高层模块调用简单。 节点自由增加。 组合模式使得客户端代码可以一致地处理单个对象和组合对象，无须关心自己处理的是单个对象，还是组合对象，这简化了客户端代码； 更容易在组合体内加入新的对象，客户端不会因为加入了新的对象而更改源代码，满足“开闭原则”； 缺点 在使用组合模式时，其叶子和树枝的声明都是实现类，而不是接口，违反了依赖倒置原则。 设计较复杂，客户端需要花更多时间理清类之间的层次关系；不容易限制容器中的构件；不容易用继承的方法来增加构件的新功能； 使用场景 部分、整体场景，如树形菜单，文件、文件夹的管理。 在需要表示一个对象整体与部分的层次结构的场合。 要求对用户隐藏组合对象与单个对象的不同，用户可以用统一的接口使用组合结构中的所有对象的场合。 注意事项 定义时为具体类。 代码展示 示例一 package designpatterns.composite.shopping; //抽象构件：物品 public interface Articles { public float calculation(); //计算 public void show(); } package designpatterns.composite.shopping; import java.util.ArrayList; //树枝构件：袋子 public class Bags implements Articles { private String name; //名字 private ArrayList bags = new ArrayList(); public Bags(String name) { this.name = name; } public void add(Articles c) { bags.add(c); } public void remove(Articles c) { bags.remove(c); } public Articles getChild(int i) { return bags.get(i); } public float calculation() { float s = 0; for (Object obj : bags) { s += ((Articles) obj).calculation(); } return s; } public void show() { for (Object obj : bags) { ((Articles) obj).show(); } } } package designpatterns.composite.shopping; //树叶构件：商品 public class Goods implements Articles { private String name; //名字 private int quantity; //数量 private float unitPrice; //单价 public Goods(String name, int quantity, float unitPrice) { this.name = name; this.quantity = quantity; this.unitPrice = unitPrice; } public float calculation() { return quantity * unitPrice; } public void show() { System.out.println(name + \"(数量：\" + quantity + \"，单价：\" + unitPrice + \"元)\"); } } package designpatterns.composite.shopping; public class ShoppingTest { public static void main(String[] args) { float s = 0; Bags BigBag, mediumBag, smallRedBag, smallWhiteBag; Goods sp; BigBag = new Bags(\"大袋子\"); mediumBag = new Bags(\"中袋子\"); smallRedBag = new Bags(\"红色小袋子\"); smallWhiteBag = new Bags(\"白色小袋子\"); sp = new Goods(\"婺源特产\", 2, 7.9f); smallRedBag.add(sp); sp = new Goods(\"婺源地图\", 1, 9.9f); smallRedBag.add(sp); sp = new Goods(\"韶关香菇\", 2, 68); smallWhiteBag.add(sp); sp = new Goods(\"韶关红茶\", 3, 180); smallWhiteBag.add(sp); sp = new Goods(\"景德镇瓷器\", 1, 380); mediumBag.add(sp); mediumBag.add(smallRedBag); sp = new Goods(\"李宁牌运动鞋\", 1, 198); BigBag.add(sp); BigBag.add(smallWhiteBag); BigBag.add(mediumBag); System.out.println(\"您选购的商品有：\"); BigBag.show(); s = BigBag.calculation(); System.out.println(\"要支付的总价是：\" + s + \"元\"); } } 示例二 package designpatterns.composite.zyr; /** * Entry 抽象类：共同特质 */ public abstract class Entry { public abstract String getName(); public abstract int getSize(); public abstract void printList(String prefix); public void printList() { printList(\"\"); } public Entry add(Entry entry) throws RuntimeException { throw new RuntimeException(); } public String toString() { return getName() + \"\"; } } package designpatterns.composite.zyr; /** * File 类：实现类，叶子结点 */ public class File extends Entry { private String name; private int size; public File(String name, int size) { this.name = name; this.size = size; } public String getName() { return name; } public int getSize() { return size; } public void printList(String prefix) { System.out.println(prefix + \"/\" + this); } } package designpatterns.composite.zyr; import java.util.ArrayList; import java.util.Iterator; public class Directory extends Entry { String name; ArrayList entrys = new ArrayList(); public Directory(String name) { this.name = name; } public String getName() { return name; } public int getSize() { int size = 0; Iterator it = entrys.iterator(); while (it.hasNext()) { size += ((Entry) it.next()).getSize(); } return size; } public Entry add(Entry entry) { entrys.add(entry); return this; } public void printList(String prefix) { System.out.println(prefix + \"/\" + this); Iterator it = entrys.iterator(); Entry entry; while (it.hasNext()) { entry = (Entry) it.next(); entry.printList(prefix + \"/\" + name); } } } package designpatterns.composite.zyr; public class CompositeZYRMain { public static void main(String[] args) { Directory life = new Directory(\"我的生活\"); File eat = new File(\"吃火锅\", 100); File sleep = new File(\"睡觉\", 100); File study = new File(\"学习\", 100); life.add(eat); life.add(sleep); life.add(study); Directory work = new Directory(\"我的工作\"); File write = new File(\"写博客\", 200); File paper = new File(\"写论文\", 200); File homework = new File(\"写家庭作业\", 200); work.add(write); work.add(paper); work.add(homework); Directory relax = new Directory(\"我的休闲\"); File music = new File(\"听听音乐\", 200); File walk = new File(\"出去转转\", 200); relax.add(music); relax.add(walk); Directory read = new Directory(\"我的阅读\"); File book = new File(\"学习书籍\", 200); File novel = new File(\"娱乐小说\", 200); read.add(book); read.add(novel); Directory root = new Directory(\"根目录\"); root.add(life); root.add(work); root.add(relax); root.add(read); root.printList(\"D:\"); System.out.println(\"=================\"); work.printList(\"work\"); System.out.println(\"=================\"); novel.printList(\"novel\"); } } "},"Chapter04/Flyweight.html":{"url":"Chapter04/Flyweight.html","title":"享元模式","keywords":"","body":"享元模式 享元模式（Flyweight Pattern）主要用于减少创建对象的数量，以减少内存占用和提高性能。这种类型的设计模式属于结构型模式，它提供了减少对象数量从而改善应用所需的对象结构的方式。 享元模式尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象。我们将通过创建 5 个对象来画出 20 个分布于不同位置的圆来演示这种模式。由于只有 5 种可用的颜色，所以 color 属性被用来检查现有的 Circle 对象。 享元模式：“享”就是分享之意，指一物被众人共享，而这也正是该模式的终旨所在。 享元模式有点类似于单例模式，都是只生成一个对象来被共享使用。这里有个问题，那就是对共享对象的修改，为了避免出现这种情况，我们将这些对象的公共部分，或者说是不变化的部分抽取出来形成一个对象。这个对象就可以避免到修改的问题。 享元的目的是为了减少不会要额内存消耗，将多个对同一对象的访问集中起来，不必为每个访问者创建一个单独的对象，以此来降低内存的消耗。 介绍 意图：运用共享技术有效地支持大量细粒度的对象。 主要解决：在有大量对象时，有可能会造成内存溢出，我们把其中共同的部分抽象出来，如果有相同的业务请求，直接返回在内存中已有的对象，避免重新创建。 何时使用： 系统中有大量对象。 这些对象消耗大量内存。 这些对象的状态大部分可以外部化。 这些对象可以按照内蕴状态分为很多组，当把外蕴对象从对象中剔除出来时，每一组对象都可以用一个对象来代替。 系统不依赖于这些对象身份，这些对象是不可分辨的。 如何解决：用唯一标识码判断，如果在内存中有，则返回这个唯一标识码所标识的对象。 关键代码：用 HashMap 存储这些对象。 应用实例 JAVA 中的 String，如果有则返回，如果没有则创建一个字符串保存在字符串缓存池里面。 数据库的数据池。 优点 大大减少对象的创建，降低系统的内存，使效率提高。 缺点 提高了系统的复杂度，需要分离出外部状态和内部状态，而且外部状态具有固有化的性质，不应该随着内部状态的变化而变化，否则会造成系统的混乱。 使用场景 系统有大量相似对象。 需要缓冲池的场景。 当我们项目中创建很多对象，而且这些对象存在许多相同模块， 这时，我们可以将这些相同的模块提取出来采用享元模式生成单一对象， 再使用这个对象与之前的诸多对象进行配合使用， 这样无疑会节省很多空间。 注意事项： 注意划分外部状态和内部状态，否则可能会引起线程安全问题。 这些类必须有一个工厂对象加以控制。 其实在Java中就存在这种类型的实例：String。 Java中将String类定义为final（不可改变的）， JVM中字符串一般保存在字符串常量池中，这个字符串常量池在jdk 6.0以前是位于常量池中， 位于永久代，而在JDK 7.0中，JVM将其从永久代拿出来放置于堆中。 我们使用如下代码定义的两个字符串指向的其实是同一个字符串常量池中的字符串值。 1 String s1 = \"abc\"; 2 String s2 = \"abc\"; 如果我们以s1==s2进行比较的话所得结果为：true，因为s1和s2保存的是字符串常量池中的同一个字符串地址。 这就类似于我们今天所讲述的享元模式， 字符串一旦定义之后就可以被共享使用，因为他们是不可改变的， 同时被多处调用也不会存在任何隐患。 代码展示 package designpatterns.flyweight; public interface Jianzhu { void use(); } package designpatterns.flyweight; public class TiYuGuan implements Jianzhu { private String name; private String shape; private String yundong; public TiYuGuan(String yundong) { this.setYundong(yundong); } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getShape() { return shape; } public void setShape(String shape) { this.shape = shape; } public String getYundong() { return yundong; } public void setYundong(String yundong) { this.yundong = yundong; } public void use() { System.out.println(\"该体育馆被使用来召开奥运会\" + \" 运动为：\" + yundong + \" 形状为：\" + shape + \" 名称为：\" + name); } } package designpatterns.flyweight; import java.util.HashMap; import java.util.Map; public class JianZhuFactory { private static final Map tygs = new HashMap(); public static TiYuGuan getTyg(String yundong) { TiYuGuan tyg = tygs.get(yundong); if (tyg == null) { tyg = new TiYuGuan(yundong); tygs.put(yundong, tyg); } return tyg; } public static int getSize() { return tygs.size(); } } package designpatterns.flyweight; /** * 使用工厂模式进行配合，创建对象池， * 测试类中的循环，你可以想象成为要举行5场比赛，每场比赛的场地就是体育馆 * * 通过执行结果可以看出，在这个对象池（HashMap）中， * 一直都只有一个对象存在，第一次使用的时候创建对象， * 之后的每次调用都用的是那个对象，不会再重新创建。 */ public class FlyweightMain { public static void main(String[] args) { String yundong =\"足球\"; for(int i = 1;i "},"Chapter04/SummaryOfStructuralPatterns.html":{"url":"Chapter04/SummaryOfStructuralPatterns.html","title":"结构性模式总结","keywords":"","body":"结构性模式总结 结构型模式，顾名思义讨论的是类和对象的结构，它采用继承机制来组合接口或实现（类结构型模式），或者通过组合一些对象，从而实现新的功能（对象结构型模式）。 这些结构型模式，它们在某些方面具有很大的相似性，仔细推敲，侧重点却各有不同。 Adapter模式：通过类的继承或者对象的组合侧重于转换已有的接口； Bridge模式：通过将抽象和实现相分离，让它们可以分别独立的变化，它强调的是系统沿着多个方向的变化； Decorator模式：采用对象组合而非继承的手法，实现了在运行时动态的扩展对象功能的能力，它强调的是扩展接口； Composite模式：模糊了简单元素和复杂元素的概念，它强调的是一种类层次式的结构； Façade 模式：将复杂系统的内部子系统与客户程序之间的依赖解耦，它侧重于简化接口，更多的是一种架构模式； Flyweight模式：解决的是由于大量的细粒度对象所造成的内存开销的问题，它与Façade模式恰好相反，关注的重点是细小的对象； Proxy模式：为其他对象提供一种代理以控制对这个对象的访问，它注重于增加间接层来简化复杂的问题。 适配器模式：将一个类的接口转换成客户希望的另外一种接口，这样就能实现已有接口的复用。适配器主要有类适配器和对象适配器两种实现方式，通常情况下，推荐优先使用对象适配器方式。 桥接模式：将抽象部分与实现部分分离，使它们都可以独立地变化。它主要用于应对多维度变化点问题，通过对象组合的方式，可以极大地减少子类的数目，同时还能让不同维度独立扩展变化。 组合模式：将对象组合成树形结构以表示“整合-部分”的层次结构，从而使得用户对单个对象和组合对象的使用具有一致性，也就是客户端能够透明地无区别地操作两者。 装饰模式：动态地给一个对象添加一些额外的职责，就增加功能来说，装饰模式相比生成子类更为灵活。假若使用多继承的方式来完成职责的添加，将会不可避免地造成子类数目的“爆炸性”增长，此外，因为是静态增加的，那也就不可能在运行状态时动态地添加或者删除额外职责呢。 外观模式：为子系统中的一组接口提供一个一致的接口，外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。这样原来需要客户直接与复杂的子系统打交道、交互，现在这一过程将完全将交由外观对象来完成，极大地方便了客户端的调用。 享元模式：运用共享技术有效地支持大量细粒度的对象。享元模式关键是将对象的内部状态和外部状态分离，尽可能地对“稳定”的内部状态进行共享，而将会随运用场景而改变的状态通过外部状态传入。 代理模式：为其他对象提供一种代理以控制对这个对象的访问。主要是在客户端和目标对象间增加一层间接层，通过这个间接层来完成对目标对象的种种控制操作，所以也就形成了不同功能类型的代理呢，比如远程代理、保护代理和虚代理等等。 结构型模式主要用于描述如何组合类和对象以获得更大的结构。 其中，结构型类模式采用继承机制来组合接口和实现，而结构型对象模式则采用组合/聚合方式来组合对象以实现新功能， 因为它可以在运行时刻改变对象组合关系，所以对象组合方式具有更大的灵活性，这种机制是无法通过静态类组合来实现的。 当然两者都有彼此擅长之处，具体的取舍需要根据实际的应用场景而定。我们介绍过的结构型模式总共有七种， 简写为：ABCDFFP(Adapter,Bridge,Composite,Decorator,Façade,Flyweight,Proxy)，也就是对应的的适配器模式、桥接模式、组合模式、装饰模式、外观模式、享元模式和代理模式。 从之前对以上七种结构型模式的介绍里，我们了解到优先使用对象组合，而不是类继承原则在结构型模式中得到淋漓尽致的体现，当然并不是说不能使用继承方式，毕竟类继承可是面向对象的三大特性之一（多态、继承和封装）。 应该说继承是随便可见的，但是每一种继承只适合于封装一种变化，面对多维变化场景时，如果仍强行使用多继承方式来实现，必然会造成子类数目成“爆炸性”增长问题，这方面叙述在桥接模式和装饰器模式中都有很好的体现。 比如在桥接模式中，通过简单的继承方式并不能很好地处理抽象化与实现化都独立变化的情况，但是通过对象组合的方式却可以很好地应对这方面需求，使多维变化点能够独立地扩展和变化。 桥接模式与装饰模式 这两个模式在一定程度上都是为了减少子类的数目，避免出现复杂的继承关系。 但是它们解决的方法却各有不同，装饰模式把子类中比基类中多出来的部分放到单独的类里面， 以适应新功能增加的需要，当我们把描述新功能的类封装到基类的对象里面时，就得到了所需要的子类对象， 这些描述新功能的类通过组合可以实现很多的功能组合，装饰模式的简略图如下： 装饰模式图 桥接模式则把原来的基类的实现化细节抽象出来，在构造到一个实现化的结构中，然后再把原来的基类改造成一个抽象化的等级结构，这样就可以实现系统在多个维度上的独立变化，桥接模式的简略图如下： 桥接模式图 外观模式和代理模式 外观模式和代理模式解决问题的侧重点不同，但是它们解决问题的手法却是一样的，即都是引入了间接层的手法，这也是我们软件系统中经常用的一种手法。外观模式虽然侧重于简化接口， 但是在某些情况下，外观模式也可以兼任代理模式的责任，例如外观对象有可能是另一个位于另一个地址空间对象的远程代理，这时候我们可以叫做外观代理模式，或者代理外观模式。它们的类简略图如下： 外观模式图 代理模式图 适配器模式 适配器模式重在转换接口，它能够使原本不能在一起工作的两个类一起工作，所以经常用在类库复用，代码迁移等方面，有一种亡羊补牢的味道。 类适配器和对象适配器可以根据具体实际情况来选用，但一般情况建议使用对象适配器模式，如下图所示，左边是类适配器模式，右边是对象适配器模式： 对变化的封装 如何应对变化，是软件开发的一个永恒的主题，也许我们不能够杜绝变化的发生，但至少我们可以通过一些手段让变化降到最低。“找到系统可变的因素，将之封装起来”， 通常就叫做对变化的封装。关于这个问题的解释在《Java与模式》中讲的很清晰，抽象化与实现化的简单实现，也就是“开-闭”原则在类层次上的最简单实现，如下图所示： 在这个继承结构中，第一层是抽象化，它封装了抽象的业务逻辑，这是系统中不变的部分；第二层是实现化，它是具体的业务逻辑的实现， 封装了系统中变化的部分，这个实现允许实现化角色多态性的变化： 也就是说，客户端依赖的是业务逻辑的抽象化类型的对象，而与抽象化的具体实现无关，不在乎它到底是“实现化”， “实现化2”还是“实现化3”，如下图所示： 每一种继承关系都封装了一个变化因素，而一个继承关系不应当处理两个变化因素，换言之，这种简单继承关系不能处理抽象化与实现化都变化的情况， 如下图所示： 上图中的两个变化因素应当是独立的，可以在不影响另一者的情况下独立的变化，如下面这两个等级结构分别封装了自己的变化因素，由于每一个变化因素都是可以通过静态关系表达的，因此分别使用继承关系实现，如下图： 在抽象化和实现化之间的联系怎么办呢？好的设计只有一个，不好的设计却有很多中，下面这种设计就是继续使用继承进行静态关系设计的类图： 这样的设计其实存在着很多的问题，首先出现的是多重的继承关系，随着具体实现化的增多，子类的继承关系会变得异常复杂；其次如果出现新的抽象化修正或者新的具体实现角色，就只好重新修改现有系统中的静态关系， 以适应新的角色，这就违背了开放-封闭原则。 正确是设计应该是使用两个独立的等级结构封装两个独立的变化因素， 并在它们之间使用聚合关系，以达到功能复用的目的， 这就回到了我们的桥接模式上，如下图所示： 从另一个角度讲，一个好的设计通常没有多于两层的继承等级结构，或者说，如果出现两个以上的变化因素，就需要找出哪一个因素是静态的，可以使用静态关系，哪一个是动态的，必须使用聚合关系。 "},"Chapter04/template.html":{"url":"Chapter04/template.html","title":"模板模式","keywords":"","body":"模板方法模式 模板方法就是将实现具体的实现交给子类，而父类只是做一些全局的任务安排， 子类和父类需要紧密的配合才能实现一个任务的功能，因为工作的紧密结合， 我们在写代码的时候一定要做好注释，这样才能使得程序具有强健的可读性。 模板方法模式是一种基于继承的代码复用技术， 它使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤， 它是一种类行为型模式。 角色 AbstractClass（抽象类）：在抽象类中定义了一系列基本操作(PrimitiveOperations)，这些基本操作可以是具体的，也可以是抽象的，每一个基本操作对应算法的一个步骤，在其子类中可以重定义或实现这些步骤。 同时，在抽象类中实现了一个模板方法(Template Method)，用于定义一个算法的框架，模板方法不仅可以调用在抽象类中实现的基本方法，也可以调用在抽象类的子类中实现的基本方法，还可以调用其他对象中的方法。 ConcreteClass（具体子类）：它是抽象类的子类，用于实现在父类中声明的抽象基本操作以完成子类特定算法的步骤，也可以覆盖在父类中已经实现的具体基本操作。 一个模板方法是定义在抽象类中的、把基本操作方法组合在一起形成一个总算法或一个总行为的方法。 这个模板方法定义在抽象类中，并由子类不加以修改地完全继承下来。模板方法是一个具体方法，它给出了一个顶层逻辑框架，而逻辑的组成步骤在抽象类中可以是具体方法，也可以是抽象方法。 基本方法是实现算法各个步骤的方法，是模板方法的组成部分。 基本方法又可以分为三种： 抽象方法(Abstract Method)：一个抽象方法由抽象类声明、由其具体子类实现。 具体方法(Concrete Method)：一个具体方法由一个抽象类或具体类声明并实现，其子类可以进行覆盖也可以直接继承。 钩子方法(Hook Method)：可以与一些具体步骤 “挂钩” ，以实现在不同条件下执行模板方法中的不同步骤 优点 在父类中形式化地定义一个算法，而由它的子类来实现细节的处理，在子类实现详细的处理算法时并不会改变算法中步骤的执行次序。 模板方法模式是一种代码复用技术，它在类库设计中尤为重要，它提取了类库中的公共行为，将公共行为放在父类中，而通过其子类来实现不同的行为，它鼓励我们恰当使用继承来实现代码复用。 可实现一种反向控制结构，通过子类覆盖父类的钩子方法来决定某一特定步骤是否需要执行。 在模板方法模式中可以通过子类来覆盖父类的基本方法，不同的子类可以提供基本方法的不同实现，更换和增加新的子类很方便，符合单一职责原则和开闭原则。 缺点 需要为每一个基本方法的不同实现提供一个子类，如果父类中可变的基本方法太多，将会导致类的个数增加，系统更加庞大，设计也更加抽象，此时，可结合桥接模式来进行设计。 适用场景 对一些复杂的算法进行分割，将其算法中固定不变的部分设计为模板方法和父类具体方法，而一些可以改变的细节由其子类来实现。即：一次性实现一个算法的不变部分，并将可变的行为留给子类来实现。 各子类中公共的行为应被提取出来并集中到一个公共父类中以避免代码重复。 需要通过子类来决定父类算法中某个步骤是否执行，实现子类对父类的反向控制。 代码展示 示例一 package designpatterns.template; public abstract class AbstractDisplay { public abstract void open(); public abstract void print(); public abstract void close(); public final void display() { open(); print(); close(); } } package designpatterns.template; public class CharDisplay extends AbstractDisplay { String word; CharDisplay(String word) { this.word = word; } public void open() { System.out.print(\">\"); } } package designpatterns.template; public class StringDisplay extends AbstractDisplay { String word; int width; StringDisplay(String word) { this.word = word; width = word.getBytes().length; } public void open() { printString(); } public void print() { for (int i = 0; i package designpatterns.template; /** * 因此说模板非常容易理解，使用起来也很简单，但是在工程中我们往往将模板与其他模式结合起来，因此我们要认清模板的本质，将具有相同操作的多种事物抽象出这些相同的操作，然后将这些操作有机的整合起来变成模板类， * 另外也要注意在模板方法的定义final表示此方法不能被继承和重写，这无疑是重要的，规定和法则不能被其他人所改变。 */ public class TemplateMain { public static void main(String[] args) { AbstractDisplay p = new CharDisplay(\"zyr\"); p.display(); System.out.println(\"----------------\"); p = new StringDisplay(\"zyr\"); p.display(); } } 总结 因此说模板非常容易理解，使用起来也很简单，但是在工程中我们往往将模板与其他模式结合起来，因此我们要认清模板的本质，将具有相同操作的多种事物抽象出这些相同的操作，然后将这些操作有机的整合起来变成模板类， 另外也要注意在模板方法的定义final表示此方法不能被继承和重写，这无疑是重要的，规定和法则不能被其他人所改变。在后面的工厂方法里，我们可以看到模板的应用，以及迭代器的影子。 源码分析模板方法模式的典型应用 Servlet 中的模板方法模式 Servlet（Server Applet）是Java Servlet的简称，用Java编写的服务器端程序，主要功能在于交互式地浏览和修改数据，生成动态Web内容。 在每一个 Servlet 都必须要实现 Servlet 接口，GenericServlet 是个通用的、不特定于任何协议的Servlet，它实现了 Servlet 接口，而 HttpServlet 继承于 GenericServlet，实现了 Servlet 接口，为 Servlet 接口提供了处理HTTP协议的通用实现，所以我们定义的 Servlet 只需要继承 HttpServlet 即可。 Mybatis BaseExecutor接口中的模板方法模式 Executor 是 Mybatis 的核心接口之一，其中定义了数据库操作的基本方法，该接口的代码如下： "},"Chapter04/Strategy.html":{"url":"Chapter04/Strategy.html","title":"策略模式","keywords":"","body":"策略模式 策略模式是对算法的封装， 把一系列的算法分别封装到对应的类中， 并且这些类实现相同的接口，相互之间可以替换。 与模版方法模式的区别： 对照类图可以看到，策略模式与模版方法模式的区别仅仅是多了一个单独的封装类Context。 在模版方法模式中，调用算法的主体在抽象的父类中， 而在策略模式中，调用算法的主体则是封装到了封装类Context中， 抽象策略Strategy一般是一个接口，目的只是为了定义规范， 里面一般不包含逻辑。其实，这只是通用实现，而在实际编程中， 因为各个具体策略实现类之间难免存在一些相同的逻辑，为了避免重复的代码， 我们常常使用抽象类来担任Strategy的角色，在里面封装公共的代码 ，因此，在很多应用的场景中，在策略模式中一般会看到模版方法模式的影子。 策略模式的结构 封装类：也叫上下文，对策略进行二次封装，目的是避免高层模块对策略的直接调用。 抽象策略：通常情况下为一个接口，当各个实现类中存在着重复的逻辑时，则使用抽象类来封装这部分公共的代码，此时，策略模式看上去更像是模版方法模式。 具体策略：具体策略角色通常由一组封装了算法的类来担任，这些类之间可以根据需要自由替换。 缺点 策略类之间可以自由切换，由于策略类实现自同一个抽象，所以他们之间可以自由切换。 易于扩展，增加一个新的策略对策略模式来说非常容易，基本上可以在不改变原有代码的基础上进行扩展。 避免使用多重条件，如果不使用策略模式，对于所有的算法，必须使用条件语句进行连接，通过条件判断来决定使用哪一种算法。 缺点 维护各个策略类会给开发带来额外开销，可能大家在这方面都有经验：一般来说，策略类的数量超过5个，就比较令人头疼了。 必须对客户端（调用者）暴露所有的策略类，因为使用哪种策略是由客户端来决定的，因此，客户端应该知道有什么策略，并且了解各种策略之间的区别，否则，后果很严重。 例如，有一个排序算法的策略模式，提供了快速排序、冒泡排序、选择排序这三种算法，客户端在使用这些算法之前，是不是先要明白这三种算法的适用情况？再比如，客户端要使用一个容器，有链表实现的，也有数组实现的，客户端是不是也要明白链表和数组有什么区别？就这一点来说是有悖于迪米特法则的。 适用场景 做面向对象设计的，对策略模式一定很熟悉，因为它实质上就是面向对象中的继承和多态，在看完策略模式的通用代码后，我想，即使之前从来没有听说过策略模式，在开发过程中也一定使用过它吧？ 至少在在以下两种情况下，大家可以考虑使用策略模式， 几个类的主要逻辑相同，只在部分逻辑的算法和行为上稍有区别的情况。 有几种相似的行为，或者说算法，客户端需要动态地决定使用哪一种，那么可以使用策略模式，将这些算法封装起来供客户端调用。 策略模式是一种简单常用的模式，我们在进行开发的时候，会经常有意无意地使用它， 一般来说，策略模式不会单独使用，跟模版方法模式、工厂模式等混合使用的情况比较多。 代码展示 package designpatterns.strategy; public interface IStrategy { void doSomething(); } package designpatterns.strategy; public class Context { private IStrategy strategy; public Context(IStrategy strategy) { this.strategy = strategy; } public void execute() { strategy.doSomething(); } } package designpatterns.strategy; public class ConcreteStrategy1 implements IStrategy { @Override public void doSomething() { System.out.println(\"具体策略1\"); } } package designpatterns.strategy; public class ConcreteStrategy2 implements IStrategy { @Override public void doSomething() { System.out.println(\"具体策略2\"); } } package designpatterns.strategy; public class StrategyMain { public static void main(String[] args) { Context context; System.out.println(\"-----执行策略1-----\"); context = new Context(new ConcreteStrategy1()); context.execute(); System.out.println(\"-----执行策略2-----\"); context = new Context(new ConcreteStrategy2()); context.execute(); } } "},"Chapter04/Observer.html":{"url":"Chapter04/Observer.html","title":"观察者模式","keywords":"","body":"观察者模式 在软件系统中经常会有这样的需求：如果一个对象的状态发生改变， 某些与它相关的对象也要随之做出相应的变化。 比如，我们要设计一个右键菜单的功能，只要在软件的有效区域内点击鼠标右键， 就会弹出一个菜单；再比如，我们要设计一个自动部署的功能，就像eclipse开发时， 只要修改了文件，eclipse就会自动将修改的文件部署到服务器中。 这两个功能有一个相似的地方，那就是一个对象要时刻监听着另一个对象， 只要它的状态一发生改变，自己随之要做出相应的行动。 其实，能够实现这一点的方案很多，但是，无疑使用观察者模式是一个主流的选择。 观察者模式的结构 在最基础的观察者模式中，包括以下四个角色： 被观察者：从类图中可以看到，类中有一个用来存放观察者对象的Vector容器（之所以使用Vector而不使用List，是因为多线程操作时，Vector在是安全的，而List则是不安全的），这个Vector容器是被观察者类的核心，另外还有三个方法：attach方法是向这个容器中添加观察者对象；detach方法是从容器中移除观察者对象；notify方法是依次调用观察者对象的对应方法。这个角色可以是接口，也可以是抽象类或者具体的类，因为很多情况下会与其他的模式混用，所以使用抽象类的情况比较多。 观察者：观察者角色一般是一个接口，它只有一个update方法，在被观察者状态发生变化时，这个方法就会被触发调用。 具体的被观察者：使用这个角色是为了便于扩展，可以在此角色中定义具体的业务逻辑。 具体的观察者：观察者接口的具体实现，在这个角色中，将定义被观察者对象状态发生变化时所要处理的逻辑。 优点 观察者与被观察者之间是属于轻度的关联关系，并且是抽象耦合的，这样，对于两者来说都比较容易进行扩展。 观察者模式是一种常用的触发机制，它形成一条触发链，依次对各个观察者的方法进行处理。但同时，这也算是观察者模式一个缺点，由于是链式触发，当观察者比较多的时候，性能问题是比较令人担忧的。并且，在链式结构中，比较容易出现循环引用的错误，造成系统假死。 代码展示 示例一 package designpatterns.observer.jike; public interface Observer { void update(); } package designpatterns.observer.jike; public class ConcreteObserver1 implements Observer { public void update() { System.out.println(\"观察者1收到信息，并进行处理。\"); } } package designpatterns.observer.jike; public class ConcreteObserver2 implements Observer { public void update() { System.out.println(\"观察者2收到信息，并进行处理。\"); } } package designpatterns.observer.jike; import java.util.Vector; public abstract class Subject { private Vector obs = new Vector(); public void addObserver(Observer obs){ this.obs.add(obs); } public void delObserver(Observer obs){ this.obs.remove(obs); } protected void notifyObserver(){ for(Observer o : obs){ o.update(); } } public abstract void doSomething(); } package designpatterns.observer.jike; public class ConcreteSubject extends Subject { public void doSomething(){ System.out.println(\"被观察者事件反生\"); this.notifyObserver(); } } package designpatterns.observer.jike; public class Client { public static void main(String[] args) { Subject sub = new ConcreteSubject(); sub.addObserver(new ConcreteObserver1()); //添加观察者1 sub.addObserver(new ConcreteObserver2()); //添加观察者2 sub.doSomething(); } } 示例二 package designpatterns.observer.bky; public interface Observer { void update(String message, String name); } package designpatterns.observer.bky; public interface HuaiRen { //添加便衣观察者 void addObserver(Observer observer); //移除便衣观察者 void removeObserver(Observer observer); //通知观察者 void notice(String message); } package designpatterns.observer.bky; import java.util.ArrayList; import java.util.List; public class XianFan implements HuaiRen { //别称 private String name = \"大熊\"; //定义观察者集合 private List observerList = new ArrayList(); //增加观察者 @Override public void addObserver(Observer observer) { if (!observerList.contains(observer)) { observerList.add(observer); } } //移除观察者 @Override public void removeObserver(Observer observer) { if (observerList.contains(observer)) { observerList.remove(observer); } } //通知观察者 @Override public void notice(String message) { for (Observer observer : observerList) { observer.update(message, name); } } } package designpatterns.observer.bky; public class BianYi implements Observer { //定义姓名 private String bName = \"张昊天\"; @Override public void update(String message, String name) { System.out.println(bName + \":\" + name + \"那里有新情况：\" + message); } } package designpatterns.observer.bky; public class Clienter { public static void main(String[] args) { //定义两个嫌犯 HuaiRen xf1 = new XianFan(); // Huairen xf2 = new XianFan2(); //定义三个观察便衣警察 Observer o1 = new BianYi(); // Observer o2 = new Bianyi2(); // Observer o3 = new Bianyi3(); //为嫌犯增加观察便衣 xf1.addObserver(o1); // xf1.addObserver(o2); // xf2.addObserver(o1); // xf2.addObserver(o3); //定义嫌犯1的情况 String message1 = \"又卖了一批货\"; String message2 = \"老大要下来视察了\"; xf1.notice(message1); // xf2.notice(message2); } } 总结 java语言中，有一个接口Observer，以及它的实现类Observable，对观察者角色常进行了实现。我们可以在jdk的api文档具体查看这两个类的使用方法。 做过VC++、javascript DOM或者AWT开发的朋友都对它们的事件处理感到神奇， 了解了观察者模式，就对事件处理机制的原理有了一定的了解了。 如果要设计一个事件触发处理机制的功能 ，使用观察者模式是一个不错的选择，AWT中的事件处理DEM （委派事件模型Delegation Event Model）就是使用观察者模式实现的。 观察者模式，又可以称之为发布-订阅模式，观察者，顾名思义，就是一个监听者，类似监听器的存在，一旦被观察/监听的目标发生的情况，就会被监听者发现，这么想来目标发生情况到观察者知道情况，其实是由目标将情况发送到观察者的。 观察者模式多用于实现订阅功能的场景，例如微博的订阅，当我们订阅了某个人的微博账号，当这个人发布了新的消息，就会通知我们。 关键点： 针对观察者与被观察者分别定义接口，有利于分别进行扩展。 重点就在被观察者的实现中： 定义观察者集合，并定义针对集合的添加、删除操作，用于增加、删除订阅者（观察者） 定义通知方法，用于将新情况通知给观察者用户（订阅者用户） 观察者中需要有个接收被观察者通知的方法。 观察者模式定义的是一对多的依赖关系，一个被观察者可以拥有多个观察者，并且通过接口对观察者与被观察者进行逻辑解耦，降低二者的直接耦合。 如此这般，想了一番之后，突然发现这种模式与桥接模式有点类似的感觉。 桥接模式也是拥有双方，同样是使用接口（抽象类）的方式进行解耦，使双方能够无限扩展而互不影响，其实二者还是有者明显的区别： 主要就是使用场景不同，桥接模式主要用于实现抽象与实现的解耦，主要目的也正是如此，为了双方的自由扩展而进行解耦，这是一种多对多的场景。观察者模式侧重于另一方面的解耦，侧重于监听方面，侧重于一对多的情况，侧重于一方发生情况，多方能获得这个情况的场景。 另一方面就是编码方面的不同，在观察者模式中存在许多独有的内容，如观察者集合的操作，通知的发送与接收，而在桥接模式中只是简单的接口引用。 "},"Chapter04/iterator.html":{"url":"Chapter04/iterator.html","title":"迭代器模式","keywords":"","body":"迭代器模式 迭代器模式(Iterator Pattern)：提供一种方法来访问聚合对象，而不用暴露这个对象的内部表示，其别名为游标(Cursor)。迭代器模式是一种对象行为型模式。 角色 Iterator（抽象迭代器）：它定义了访问和遍历元素的接口，声明了用于遍历数据元素的方法，例如：用于获取第一个元素的first()方法，用于访问下一个元素的next()方法，用于判断是否还有下一个元素的hasNext()方法，用于获取当前元素的currentItem()方法等，在具体迭代器中将实现这些方法。 ConcreteIterator（具体迭代器）：它实现了抽象迭代器接口，完成对聚合对象的遍历，同时在具体迭代器中通过游标来记录在聚合对象中所处的当前位置，在具体实现时，游标通常是一个表示位置的非负整数。 Aggregate（抽象聚合类）：它用于存储和管理元素对象，声明一个createIterator()方法用于创建一个迭代器对象，充当抽象迭代器工厂角色。 ConcreteAggregate（具体聚合类）：它实现了在抽象聚合类中声明的createIterator()方法，该方法返回一个与该具体聚合类对应的具体迭代器ConcreteIterator实例。 在迭代器模式中，提供了一个外部的迭代器来对聚合对象进行访问和遍历，迭代器定义了一个访问该聚合元素的接口，并且可以跟踪当前遍历的元素，了解哪些元素已经遍历过而哪些没有。迭代器的引入，将使得对一个复杂聚合对象的操作变得简单。 在迭代器模式中应用了工厂方法模式，抽象迭代器对应于抽象产品角色，具体迭代器对应于具体产品角色，抽象聚合类对应于抽象工厂角色，具体聚合类对应于具体工厂角色。 优点 它支持以不同的方式遍历一个聚合对象，在同一个聚合对象上可以定义多种遍历方式。在迭代器模式中只需要用一个不同的迭代器来替换原有迭代器即可改变遍历算法，我们也可以自己定义迭代器的子类以支持新的遍历方式。 迭代器简化了聚合类。由于引入了迭代器，在原有的聚合对象中不需要再自行提供数据遍历等方法，这样可以简化聚合类的设计。 在迭代器模式中，由于引入了抽象层，增加新的聚合类和迭代器类都很方便，无须修改原有代码，满足 “开闭原则” 的要求。 缺点 由于迭代器模式将存储数据和遍历数据的职责分离，增加新的聚合类需要对应增加新的迭代器类，类的个数成对增加，这在一定程度上增加了系统的复杂性。 抽象迭代器的设计难度较大，需要充分考虑到系统将来的扩展，例如JDK内置迭代器Iterator就无法实现逆向遍历，如果需要实现逆向遍历，只能通过其子类ListIterator等来实现，而ListIterator迭代器无法用于操作Set类型的聚合对象。在自定义迭代器时，创建一个考虑全面的抽象迭代器并不是件很容易的事情。 适用场景: 访问一个聚合对象的内容而无须暴露它的内部表示。将聚合对象的访问与内部数据的存储分离，使得访问聚合对象时无须了解其内部实现细节。 需要为一个聚合对象提供多种遍历方式。 为遍历不同的聚合结构提供一个统一的接口，在该接口的实现类中为不同的聚合结构提供不同的遍历方式，而客户端可以一致性地操作该接口。 代码展示 package designpatterns.iterator.book; public interface FireIterator { boolean hasNext(); Object next(); } package designpatterns.iterator.book; public interface Aggregate { FireIterator iterator(); } package designpatterns.iterator.book; import lombok.AllArgsConstructor; import lombok.Getter; @Getter @AllArgsConstructor public class Book { private String name; } package designpatterns.iterator.book; public class BookShelf implements Aggregate { private Book[] books; int pointer = 0; public BookShelf(int max_size) { books = new Book[max_size]; } public void appendBook(Book book) { books[pointer] = book; pointer++; } public Book findBookAt(int index) { return books[index]; } public int getLength() { return pointer; } /** * @return */ public FireIterator iterator() { return new BookShelfIterator(this); } private class BookShelfIterator implements FireIterator { BookShelf bookShelf; int index; public BookShelfIterator(BookShelf bookShelf) { this.bookShelf = bookShelf; index = 0; } public boolean hasNext() { if (index package designpatterns.iterator.book; public class BookMain { public static void main(String[] args) { Book book1 = new Book(\"朝花夕拾\"); Book book2 = new Book(\"围城\"); Book book3 = new Book(\"遮天\"); Book book4 = new Book(\"寻秦记\"); Book book5 = new Book(\"骆驼祥子\"); BookShelf bookShelf = new BookShelf(5); bookShelf.appendBook(book1); bookShelf.appendBook(book2); bookShelf.appendBook(book3); bookShelf.appendBook(book4); bookShelf.appendBook(book5); /** * MyIterator，而不是BookShelfIterator，这样做的好处是完全屏蔽了内部的细节， * 在用户使用的时候，完全不知道BookShelfIterator的存在。 * * 引入迭代器之后，可以将元素的遍历和实现分离开来， * 如下面的代码中的while循环，没有依赖与BookShelf的实现， * 没有使用BookShelf的其他方法，只是用了迭代器中hasNext和next方法。 * 可复用指的是将一个类作为一个组件，当一个组件改变时，不需要对其他组件进行修改或者只进行少量的修改就可以实现修改后的功能。 * MyIterator it= bookShelf.iterator();面向接口编程，便于程序的修改和维护。 */ FireIterator it = bookShelf.iterator(); while (it.hasNext()) { Book book = (Book) it.next(); System.out.println(\"书的名字为《\" + book.getName() + \"》\"); } } } 源码分析迭代器模式的典型应用 Java集合中的迭代器模式 看 java.util.ArrayList 类 public class ArrayList extends AbstractList implements List, RandomAccess, Cloneable, java.io.Serializable { transient Object[] elementData; // non-private to simplify nested class access private int size; public E get(int index) { rangeCheck(index); return elementData(index); } public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } public ListIterator listIterator() { return new ListItr(0); } public ListIterator listIterator(int index) { if (index size) throw new IndexOutOfBoundsException(\"Index: \"+index); return new ListItr(index); } public Iterator iterator() { return new Itr(); } private class Itr implements Iterator { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; public boolean hasNext() { return cursor != size; } public E next() { //... } public E next() { //... } public void remove() { //... } //... } private class ListItr extends Itr implements ListIterator { public boolean hasPrevious() { return cursor != 0; } public int nextIndex() { return cursor; } public int previousIndex() { return cursor - 1; } public E previous() { //... } public void set(E e) { //... } public void add(E e) { //... } //... } 从 ArrayList 源码中看到了有两个迭代器 Itr 和 ListItr，分别实现 Iterator 和 ListIterator 接口； 第一个当然很容易看明白，它跟我们示例的迭代器的区别是这里是一个内部类，可以直接使用 ArrayList 的数据列表；第二个迭代器是第一次见到， ListIterator 跟 Iterator 有什么区别呢？ 先看 ListIterator 源码 public interface ListIterator extends Iterator { boolean hasNext(); E next(); boolean hasPrevious(); // 返回该迭代器关联的集合是否还有上一个元素 E previous(); // 返回该迭代器的上一个元素 int nextIndex(); // 返回列表中ListIterator所需位置后面元素的索引 int previousIndex(); // 返回列表中ListIterator所需位置前面元素的索引 void remove(); void set(E var1); // 从列表中将next()或previous()返回的最后一个元素更改为指定元素e void add(E var1); } 接着是 Iterator 的源码 public interface Iterator { boolean hasNext(); E next(); default void remove() { throw new UnsupportedOperationException(\"remove\"); } // 备注：JAVA8允许接口方法定义默认实现 default void forEachRemaining(Consumer action) { Objects.requireNonNull(action); while (hasNext()) action.accept(next()); } } 可以看出 ListIterator 的 add、set、remove 方法会直接改变原来的 List 对象，而且可以通过 previous 反向遍历 Mybatis中的迭代器模式 当查询数据库返回大量的数据项时可以使用游标 Cursor，利用其中的迭代器可以懒加载数据，避免因为一次性加载所有数据导致内存奔溃，Mybatis 为 Cursor 接口提供了一个默认实现类 DefaultCursor，代码如下 public interface Cursor extends Closeable, Iterable { boolean isOpen(); boolean isConsumed(); int getCurrentIndex(); } public class DefaultCursor implements Cursor { private final DefaultResultSetHandler resultSetHandler; private final ResultMap resultMap; private final ResultSetWrapper rsw; private final RowBounds rowBounds; private final ObjectWrapperResultHandler objectWrapperResultHandler = new ObjectWrapperResultHandler(); // 游标迭代器 private final CursorIterator cursorIterator = new CursorIterator(); protected T fetchNextUsingRowBound() { T result = fetchNextObjectFromDatabase(); while (result != null && indexWithRowBound iterator() { if (iteratorRetrieved) { throw new IllegalStateException(\"Cannot open more than one iterator on a Cursor\"); } iteratorRetrieved = true; return cursorIterator; } private class CursorIterator implements Iterator { T object; int iteratorIndex = -1; @Override public boolean hasNext() { if (object == null) { object = fetchNextUsingRowBound(); } return object != null; } @Override public T next() { T next = object; if (next == null) { next = fetchNextUsingRowBound(); } if (next != null) { object = null; iteratorIndex++; return next; } throw new NoSuchElementException(); } @Override public void remove() { throw new UnsupportedOperationException(\"Cannot remove element from Cursor\"); } } // ... } 游标迭代器 CursorIterator 实现了 java.util.Iterator 迭代器接口，这里的迭代器模式跟 ArrayList 中的迭代器几乎一样 迭代器模式的优缺点 迭代器模式的优点有： 简化了遍历方式，对于对象集合的遍历，还是比较麻烦的，对于数组或者有序列表，我们尚可以通过游标来取得，但用户需要在对集合了解很清楚的前提下，自行遍历对象，但是对于hash表来说，用户遍历起来就比较麻烦了。而引入了迭代器方法后，用户用起来就简单的多了。 可以提供多种遍历方式，比如说对有序列表，我们可以根据需要提供正序遍历，倒序遍历两种迭代器，用户用起来只需要得到我们实现好的迭代器，就可以方便的对集合进行遍历了。 封装性良好，用户只需要得到迭代器就可以遍历，而对于遍历算法则不用去关心。 迭代器模式的缺点： 对于比较简单的遍历（像数组或者有序列表），使用迭代器方式遍历较为繁琐，大家可能都有感觉，像ArrayList，我们宁可愿意使用for循环和get方法来遍历集合。 迭代器模式的适用场景 迭代器模式是与集合共生共死的，一般来说，我们只要实现一个集合，就需要同时提供这个集合的迭代器，就像java中的Collection，List、Set、Map等，这些集合都有自己的迭代器。假如我们要实现一个这样的新的容器，当然也需要引入迭代器模式，给我们的容器实现一个迭代器。 但是，由于容器与迭代器的关系太密切了，所以大多数语言在实现容器的时候都给提供了迭代器，并且这些语言提供的容器和迭代器在绝大多数情况下就可以满足我们的需要，所以现在需要我们自己去实践迭代器模式的场景还是比较少见的，我们只需要使用语言中已有的容器和迭代器就可以了。 "},"Chapter04/chainOfResponsibility.html":{"url":"Chapter04/chainOfResponsibility.html","title":"责任链模式","keywords":"","body":"责任链 责任链（chain of responsibility）模式很像异常的捕获和处理， 当一个问题发生的时候，当前对象看一下自己是否能够处理， 不能的话将问题抛给自己的上级去处理， 但是要注意这里的上级不一定指的是继承关系的父类， 这点和异常的处理是不一样的。 所以可以这样说，当问题不能解决的时候，将问题交给另一个对象去处理，就这样一直传递下去直至当前对象找不到下线了，处理结束。 在实际中的应用 try-catch语句 每一个catch语句是根据Exception异常类型进行匹配的，一般会有多个catch语句，就形成了一个责任链； 此时如果有一个catch语句与当前所要处理的异常Exception符合时，该Exception就会交给相应的catch语句进行处理，之后的catch语句就不会再执行了。 异常处理机制 方法的调用构成了一个栈，当栈顶的方法产生异常时，需要将异常抛出，被抛出的异常沿着调用栈向下发展，寻找一个处理的块，被栈顶抛出的方法作为一个请求，而调用栈上的每一个方法就相当于一个handler，该Handler即可以选择自行处理这个被抛出的异常，也可以选择将异常沿着调用栈传递下去。 异常：请求 调用栈中的每一级：Handler 调用栈中的handler：责任链 栈底元素：上一级元素的直接后继 过滤器链（一般链条中只有一个对象处理请求，但是过滤器链可以有多个对象同时处理请求） Spring Security框架 通过多个filter类构成一个链条来处理Http请求，从而为应用提供一个认证与授权的框架。 责任链模式内部处理 在责任链模式中，作为请求接受者的多个对象通过对其后继的引用而连接起来形成一条链。 请求在这条链上传递，直到链上某一个接收者处理这个请求。 每个接收者都可以选择自行处理请求或是向后继传递请求 Handler设了一个自身类型的对象作为其后继，Handler是抽象的，从而整条链也是抽象的， 这种抽象的特性使得在运行时可以动态的绑定链条中的对象，从而提供了足够的空间。 在代码中直接后继successor的类型是PriceHandler,而不是任何其他具体的类（Sales、Manager等）， 使得在后面变更需求时，加入了lead层次，可以简单的实现。 所以责任链模式遵循了OO中的依赖倒置原则，即依赖于抽象而非依赖于具体。降低了程序的耦合度。 发出请求的客户端并不知道链上的哪一个接收者会处理这个请求， 从而实现了客户端和接收者之间的解耦。 责任链模式的优缺点 开闭原则：对扩展开放，对变更关闭； 有业务变更时，希望通过新增一个类，而非修改原有的代码来满足业务需求。 执行性能：在结构上，责任链模式由处理器首尾相接构成的一条链，当由请求到来之时，需要从链的头部开始遍历整条责任链，直到有一个处理器处理了请求，或者是整个链条遍历完成。 在这个过程中性能的损耗体现在两个方面： 时间：相对于单个handler处理请求的时间而言，整个链条遍历的过程可能会消耗更多的时间。 内存：创建了大量的对象来表示处理器对象，但是实际仅仅使用了其中的少部分，剩余的大部分处理器都未被使用到。 代码展示 package designpatterns.chainofresponsibility.cor; import lombok.AllArgsConstructor; import lombok.Getter; import lombok.Setter; @Getter @Setter @AllArgsConstructor public class Trouble { private int number; public String toString() { return \"问题编号：[\" + number + \"]\"; } } package designpatterns.chainofresponsibility.cor; /** * （抽象类，使用了模板方法） */ public abstract class Support { protected abstract boolean resolve(Trouble trouble); String name; Support next; public Support(String name) { this.name = name; } public String toString() { return \"对象：\"; } public Support setAndReturnNext(Support next) { this.next = next; return next; } public final void support(Trouble trouble) { if (resolve(trouble)) { done(trouble); } else if (next != null) { next.support(trouble); } else { fail(trouble); } } protected void fail(Trouble trouble) { System.out.println(this + \"解决问题失败，\" + trouble); } protected void done(Trouble trouble) { System.out.println(this + \"已经解决问题，\" + trouble); } } package designpatterns.chainofresponsibility.cor; public class LimitSupport extends Support { private int limit; public LimitSupport(String name, int limit) { super(name); this.limit = limit; } protected boolean resolve(Trouble trouble) { return trouble.getNumber() package designpatterns.chainofresponsibility.cor; public class NoSupport extends Support { public NoSupport(String name) { super(name); } protected boolean resolve(Trouble trouble) { return false; } } package designpatterns.chainofresponsibility.cor; public class OddSupport extends Support { public OddSupport(String name) { super(name); } protected boolean resolve(Trouble trouble) { return (trouble.getNumber() % 2) == 1 ? true : false; } } package designpatterns.chainofresponsibility.cor; public class SpecialSupport extends Support { public int specialNumber; public SpecialSupport(String name, int specialNumber) { super(name); this.specialNumber = specialNumber; } protected boolean resolve(Trouble trouble) { return trouble.getNumber() == specialNumber ? true : false; } } package designpatterns.chainofresponsibility.cor; public class Main { public static void main(String[] args) { Support limitSupportLess = new LimitSupport(\"有限支持小\", 5); Support limitSupportMore = new LimitSupport(\"有限支持大\", 15); Support oddSupport = new OddSupport(\"奇数支持\"); Support specialSupport = new SpecialSupport(\"特定支持\", 36); Support noSupport = new NoSupport(\"没有支持\"); limitSupportLess.setAndReturnNext(limitSupportMore).setAndReturnNext(oddSupport).setAndReturnNext(specialSupport).setAndReturnNext(noSupport); System.out.println(\"===尝试解决问题===\"); for (int i = 0; i 尝试解决问题===\"); for (int i = 0; i "},"Chapter04/Command.html":{"url":"Chapter04/Command.html","title":"命令模式","keywords":"","body":"命令模式 将一个请求封装成一个对象，从而让你使用不同的请求把客户端参数化，对请求排队或者记录请求日志， 可以提供命令的撤销和恢复功能。 结构 Command类：是一个抽象类，类中对需要执行的命令进行声明， 一般来说要对外公布一个execute方法用来执行命令。 ConcreteCommand类：Command类的实现类，对抽象类中声明的方法进行实现。 Client类：最终的客户端调用类。 Invoker类：调用者，负责调用命令。 Receiver类：接收者，负责接收命令并且执行命令。 所谓对命令的封装，说白了，无非就是把一系列的操作写到一个方法中， 然后供客户端调用就行了，反映到类图上， 只需要一个ConcreteCommand类和Client类就可以完成对命令的封装， 即使再进一步，为了增加灵活性，可以再增加一个Command类进行适当地抽象， 这个调用者和接收者到底是什么作用呢？ 其实大家可以换一个角度去想： 假如仅仅是简单地把一些操作封装起来作为一条命令供别人调用， 怎么能称为一种模式呢？命令模式作为一种行为类模式， 首先要做到低耦合，耦合度低了才能提高灵活性， 而加入调用者和接收者两个角色的目的也正是为此。 通过代码我们可以看到，当我们调用时， 执行的时序首先是调用者类，然后是命令类， 最后是接收者类。也就是说一条命令的执行被分成了三步， 它的耦合度要比把所有的操作都封装到一个类中要低的多， 而这也正是命令模式的精髓所在：把命令的调用者与执行者分开， 使双方不必关心对方是如何操作的。 优点 首先，命令模式的封装性很好：每个命令都被封装起来， 对于客户端来说，需要什么功能就去调用相应的命令， 而无需知道命令具体是怎么执行的。比如有一组文件操作的命令： 新建文件、复制文件、删除文件。如果把这三个操作都封装成一个命令类， 客户端只需要知道有这三个命令类即可，至于命令类中封装好的逻辑， 客户端则无需知道。 其次，命令模式的扩展性很好，在命令模式中， 在接收者类中一般会对操作进行最基本的封装， 命令类则通过对这些基本的操作进行二次封装， 当增加新命令的时候，对命令类的编写一般不是从零开始的， 有大量的接收者类可供调用，也有大量的命令类可供调用， 代码的复用性很好。比如，文件的操作中， 我们需要增加一个剪切文件的命令， 则只需要把复制文件和删除文件这两个命令组合一下就行了，非常方便。 缺点 命令如果很多，开发起来就要头疼了。特别是很多简单的命令，实现起来就几行代码的事， 而使用命令模式的话，不用管命令多简单，都需要写一个命令类来封装。 适用场景 对于大多数请求-响应模式的功能，比较适合使用命令模式， 正如命令模式定义说的那样， 命令模式对实现记录日志、撤销操作等功能比较方便。 代码展示 package designpatterns.command; public abstract class Command { public abstract void execute(); } package designpatterns.command; public class Receiver { public void doSomething(){ System.out.println(\"接受者-业务逻辑处理\"); } } package designpatterns.command; public class Invoker { private Command command; public void setCommand(Command command) { this.command = command; } public void action(){ this.command.execute(); } } package designpatterns.command; public class ConcreteCommand extends Command { private Receiver receiver; public ConcreteCommand(Receiver receiver){ this.receiver = receiver; } public void execute() { this.receiver.doSomething(); } } package designpatterns.command; public class Client { public static void main(String[] args) { Receiver receiver = new Receiver(); Command command = new ConcreteCommand(receiver); //客户端直接执行具体命令方式（此方式与类图相符） command.execute(); //客户端通过调用者来执行命令 Invoker invoker = new Invoker(); invoker.setCommand(command); invoker.action(); } } 总结 对于一个场合到底用不用模式， 这对所有的开发人员来说都是一个很纠结的问题。 有时候，因为预见到需求上会发生的某些变化， 为了系统的灵活性和可扩展性而使用了某种设计模式， 但这个预见的需求偏偏没有，相反，没预见到的需求倒是来了不少， 导致在修改代码的时候，使用的设计模式反而起了相反的作用， 以至于整个项目组怨声载道。这样的例子，我相信每个程序设计者都遇到过。 所以，基于敏捷开发的原则，我们在设计程序的时候，如果按照目前的需求， 不使用某种模式也能很好地解决，那么我们就不要引入它， 因为要引入一种设计模式并不困难， 我们大可以在真正需要用到的时候再对系统进行一下，引入这个设计模式。 拿命令模式来说吧，我们开发中， 请求-响应模式的功能非常常见， 一般来说，我们会把对请求的响应操作封装到一个方法中， 这个封装的方法可以称之为命令，但不是命令模式。 到底要不要把这种设计上升到模式的高度就要另行考虑了， 因为，如果使用命令模式，就要引入调用者、接收者两个角色， 原本放在一处的逻辑分散到了三个类中， 设计时，必须考虑这样的代价是否值得。 "},"Chapter04/State.html":{"url":"Chapter04/State.html","title":"状态模式","keywords":"","body":"状态模式 当一个对象的内在状态改变时允许改变其行为，这个对象看起来像是改变了其类。 状态模式主要解决的是当控制一个对象状态的条件表达式过于复杂时的情况。 把状态的判断逻辑转移到表示不同状态的一系列类中，可以把复杂的判断逻辑简化。 态模式 (State Pattern)是设计模式的一种，属于行为模式。 模式中的角色 上下文环境（Context）：它定义了客户程序需要的接口并维护一个具体状态角色的实例，将与状态相关的操作委托给当前的Concrete State对象来处理。 抽象状态（State）：定义一个接口以封装使用上下文环境的的一个特定状态相关的行为。 具体状态（Concrete State）：实现抽象状态定义的接口。 这里来看看状态模式的标准代码； 首先我们先定义一个State抽象状态类，里面定义了一个接口以封装 与Context的一个特定状态相关的行为； /** * 抽象状态类 * */ public abstract class State { public abstract void Handle(Context context); } package designpatterns.state; //Context类，维护一个ConcreteState子类的实例，这个实例定义当前的状态 public class Context { State state; public Context(State state) { //定义Context的初始状态 super(); this.state = state; } public State getState() { return state; } public void setState(State state) { this.state = state; System.out.println(\"当前状态为\"+state); } public void request(){ state.Handle(this); //对请求做处理并且指向下一个状态 } } package designpatterns.state; public class ConcreteStateA extends State { @Override public void Handle(Context context) { context.setState(new ConcreteStateB()); //设置A的下一个状态是B } } package designpatterns.state; public class ConcreteStateB extends State { @Override public void Handle(Context context) { context.setState(new ConcreteStateA()); //设置B的下一个状态是A } } package designpatterns.state; /** * 节点接口 * */ public abstract class Node { private static String name; //当前节点名称 //节点跳转 public abstract void nodeHandle(FlowContext context); public String getName() { return name; } public void setName(String name) { this.name = name; } } package designpatterns.state; public class HrNode extends Node { @Override public void nodeHandle(FlowContext context) { //先判断流程是否结束 if(!context.isFlag()){ // 根据当前流程的状态，来控制流程的走向 if (context != null && 0 == context.getStatus()) { //只有上一级审核通过后才能轮到HR审核 // 设置当前节点的名称 setName(\"HR李\"); //读取上一级的审核内容并加上自己的意见 System.out.println(context.getMessage()+getName()+\"审核通过\"); // 审核通过 context.setStatus(0); //HR审核通过并指向下一个节点 ,如果没有下一个节点就把状态设置为终结 context.setFlag(true); } }else{ System.out.println(\"流程已经结束\"); } } } package designpatterns.state; /** * 领导节点 * */ public class LeadNode extends Node { @Override public void nodeHandle(FlowContext context) { //根据当前流程的状态，来控制流程的走向 //先判断流程是否结束 if(!context.isFlag()){ System.out.println(context.getMessage()); //先读取申请的内容 if(context!=null&&3==context.getStatus()){ //只有出于已经申请的状态才又部门领导审核 //设置当前节点的名称 setName(\"张经理\"); //加上审核意见 context.setMessage(context.getMessage()+getName()+\"审核通过;\"); //审核通过 context.setStatus(0); //审核通过并指向下一个节点 context.setNode(new HrNode()); context.getNode().nodeHandle(context); } }else{ System.err.println(\"流程已经结束\"); } } } package designpatterns.state; /** * 流程控制 * */ public class FlowContext { private boolean flag; // 代表流程是否结束 /** * 流程状态 0：通过 1:驳回 2.退回整改 3.已申请 * */ private int status; private String message; // 消息 private Node node; // 节点信息 public boolean isFlag() { return flag; } public void setFlag(boolean flag) { this.flag = flag; } public int getStatus() { return status; } public void setStatus(int status) { this.status = status; } public String getMessage() { return message; } public void setMessage(String message) { this.message = message; } public Node getNode() { return node; } public void setNode(Node node) { this.node = node; } public static boolean start(FlowContext context) { Node node = new LeadNode(); context.setNode(node); // 设置初始节点 context.setStatus(3); // 设置状态为申请中 context.getNode().nodeHandle(context); // 发起请求 // 最后要知道是否申请成功 //判断当前是最后一个节点并且审核通过，而且流程结束 if(\"HR李\".equals(node.getName())&&0==context.getStatus()&&context.isFlag()){ System.out.println(\"审核通过,流程结束\"); return true; }else{ System.out.println(\"审核未通过，流程已经结束\"); return false; } } public FlowContext() { super(); } } package designpatterns.state; public class StateMain { public static void main(String[] args) { FlowContext context=new FlowContext(); context.setMessage(\"本人王小二，因为十一家里有事情，所以要多请三天假，希望公司能够审核通过\"); context.start(context); } } 打印结果如下 本人王小二，因为十一家里有事情，所以要多请三天假，希望公司能够审核通过 本人王小二，因为十一家里有事情，所以要多请三天假，希望公司能够审核通过张经理审核通过;HR李审核通过 审核通过,流程结束； 上面这个例子只是很简单的模仿了一下工作流控制状态的跳转。 状态模式最主要的好处就是把状态的判断与控制放到了其服务端的内部， 使得客户端不需要去写很多代码判断，来控制自己的节点跳转，而且这样实现的话， 我们可以把每个节点都分开来处理，当流程流转到某个节点的时候，可以去写自己的节点流转方法。 当然状态模式的缺点也很多，比如类的耦合度比较高，基本上三个类要同时去写，而且会创建很多的节点类。 "},"Chapter04/StateExt.html":{"url":"Chapter04/StateExt.html","title":"责任链模式和状态模式对比","keywords":"","body":"责任链模式和状态模式对比 常用23中设计模式中，有三种模式容易混淆，这三种模式即责任链模式、状态模式以及策略模式。 状态模式与策略模式比较 定义 状态模式：通过改变对象的内部状态而改变对象自身的行为，这个对象表现得就好像修改了它的类一样。 策略模式：定义一组算法，并将每一个算法封装起来，使得它们可以互换。算法的替换不影响调用算法的客户端； 异同 态模式关键是各个状态子类必须知道下一个状态是啥，且要把逻辑判断转移到各个状态子类中， 客户端不需要了解状态迁移的顺序，且状态模式虽然类图还尼玛和策略模式几乎一样， 但是策略目的是针对单一算法的运行时替换，客户端需要事先了解策略，主动去选择合适的策略， 不存在状态的自动迁移！ 责任链模式与状态模式 定义： 责任链Chain of Responsibility（CoR）——责任链模式，也叫职责链模式或者职责连锁模式，同状态模式一样，也是对象的行为模式之一，该模式构造一系列分别担当不同的职责的类的对象来共同完成一个任务，对象由每一个对象对其下家的引用而连接起来形成一条链，这些类的对象之间像链条一样紧密相连，而客户端发出的请求在这个链上传递，直到链上的某一个对象决定处理此请求，发出这个请求的客户端并不知道链上的哪一个对象最终处理这个请求，这使得系统可以在不影响客户端的情况下动态地重新组织和分配责任，所以该模式被称作职责链模式。 责任链模式特点是各个职责类（类比状态模式的状态类们）职责单一不彼此依赖，且职责自动转移，但是和状态模式不同的是，责任链模式的责任类不知道自己的下一个需要转移到的职责是哪个，等价于——发出完成某任务请求的客户端并不知道链上的哪一个对象最终处理这个请求，这个组装过程需要交给环境类去完成，所以非常灵活！ 比如客户Client要完成一个任务，这个任务包括a,b,c,d四个部分，首先客户Client把任务交给A，A完成a部分之后，把任务交给B，B完成b部分……直到D完成d部分。再看，政府部分的某项工作，县政府先完成自己能处理的部分，不能处理的部分交给省政府，省政府再完成自己职责范围内的部分，不能处理的部分交给中央政府，中央政府最后完成该项工作。还有，软件窗口的消息传播……但是以上的责任的转移，或者说在责任链上的移动，各个责任类不知道具体顺序和下一个责任，链条的组装过程是环境类（或客户端完成的）。 责任链模式和状态模式的区别： 不可否认，状态模式也好，责任链模式也罢，都能解耦和优化大量的逻辑判断， 责任链模式使多个对象都有机会处理请求，从而避免请求的发送者和接受者之间的耦合关系。 将这个对象练成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。 各个责任类不知道也没必要知道下一个责任对象是谁！由环境类统一设置顺序和谁连接到链条， 谁不连接到链条……从代码中我们可以看出，职责链在client（环境类）连接，也就是说， 如果我们的生产线一旦改变，比如说我们不需要美容了，我们需要增加新的组装项目了， 或者是先组装车头后，直接请求去保存到仓库……这都是很容易实现的，职责链模式要比状态模式灵活很多。 但是，这时候有人要问，既然他们都可以解决逻辑判断的分支过多的问题， 那么，是不是责任链模式比状态模式好呢？ 职责链模式过于灵活，在客户端使用时，需要环境去确定下一个对象是谁， 一些列的set操作……在多次设置的时候很容易出问题， 而且状态模式是一个对象的内在状态发生改变（一个对象，相对比较稳定，处理完一个对象下一个对象的处理一般都已确定）， 而职责链模式是多个对象之间的改变（多个对象之间的话，就会出现某个对象不存在的情景，就像之前讲状态模式时的公司请假系统，可能存在不同级别，不同类型员工请假流程不一样，此时用状态模式不太好）， 这也说明他们两个模式处理的情况不同。 这两个设计模式最大的区别就是： 状态模式是让各个状态对象自己知道其下一个处理的对象是谁，即在编译时便设定。 相当于If ，else-if，else-if……， 设计思路是把逻辑判断转移到各个State类的内部实现(相当于If，else If)， 执行时客户端通过调用环境—Context类的方法来间接执行状态类的行为，客户端不直接和状态交互。 职责链模式中的各个对象并不指定其下一个处理的对象到底是谁，只有在客户端才设定某个类型的链条， 请求发出后穿越链条，直到被某个职责类处理或者链条结束。 本质相当于swich-case，设计思路是把各个业务逻辑判断封装到不同职责类，且携带下一个职责的对应引用， 但不像状态模式那样需要明确知道这个引用指向谁，而是在环境类设置链接方式或者过程。 使用时，向链的第一个子类的执行方法传递参数就可以。客户端去通过环境类调用责任链，全自动运转起来。 针对具体业务，有人用状态模式，从头到尾提前定义好下一个处理的对象，有人采用责任链， 随时都有可能调整链的顺序……甚至不复杂的业务判断，或者只需要使用一次的情景下， 那就没必要搞这些鸡毛模式，本着够用原则和具体业务的适合原则！ "},"Chapter04/ChainExt.html":{"url":"Chapter04/ChainExt.html","title":"命令模式、状态模式、责任链模式区别","keywords":"","body":"命令模式、状态模式、责任链模式区别 命令模式：一次设定，统一执行。 状态模式： 相当于If else if else； 设计路线：各个State类的内部实现(相当于If，else If内的条件)执行时通过State调用Context方法来执行。 职责链模式： 相当于Swich case 设计路线：客户设定，每个子类(case)的参数是下一个子类(case)。使用时，向链的第一个子类的执行方法传递参数就可以。 命令模式：将多个命令只提交给一个执行该命令的对象 而职责链模式相反：只将一个请求提交给多个能执行该命令的对象 状态模式与职责链模式的区别： 状态模式是让各个状态对象自己知道其下一个处理的对象是谁，即在编译时便设定好了的； 而职责链模式中的各个对象并不指定其下一个处理的对象到底是谁，只有在客户端才设定。 职责链模式 链的组织是从最特殊的到最一般的，并且不能保证请求在任何情况下都回有相应。 职责链将程序中每个对象能做什么的内容隔离，即职责链减少了对象之间的耦合，每个对象都能独立操作。职责链也可用于构成主程序的对象和包含其它对象实例的对象。 适用场景： 具有相同方法的几个对象都适合于执行程序请求操作，但由对象决定由谁去完成操作，比把决策建立在调用代码中更合适 其中某个对象可能最适合处理请求，但你不想通过一些列if-else语句或switch语句去选择一个特定的对象 程序执行时，需要向处理选项链中添加新的对象 在多个对象都能执行一个请求的情况下，你不想把这些相互作用的内容放在调用程序里 链中每个对象都是“自治”的，最后一个对象决定是默认处理请求，还是抛弃 命令模式 职责链沿类链转发请求，而命令模式只将请求转发给一个特定对象。命令模式把一个申请特定操作的请求封装到一个对象中，并给该对象一个众所周知的公共接口，使客户端不用了解实际执行的操作就能产生请求，也可以使你改变操作而丝毫不影响客户端程序。 命令模式的效果： 命令模式的主要缺点是，增加了使程序散乱的小类，不过，即使有单独的单击事件，也通常都调用小的私有方法完成具体功能。最后的结果是，私有方法和我们这些小类的代码长度几乎一样，因此，构建 Command类和编写较多的方法在复杂性上通常没有区别，主要区别是命令模式生成的小类更容易理解。 使用命令设计模式的另一个主要原因是，他们提供了一个便捷的存储方法并能完成Undo功能。每个命令对象都记住刚刚做过的事，并在有Undo请求时，只要计算量和内存需求不太过分，就能恢复到刚才的状态 "},"Chapter04/Memo.html":{"url":"Chapter04/Memo.html","title":"备忘录模式","keywords":"","body":"备忘录模式 我们在编程的时候，经常需要保存对象的中间状态，当需要的时候，可以恢复到这个状态。 比如，我们使用Eclipse进行编程时，假如编写失误（例如不小心误删除了几行代码）， 我们希望返回删除前的状态，便可以使用Ctrl+Z来进行返回。这时我们便可以使用备忘录模式来实现。 定义 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。 这样就可以将该对象恢复到原先保存的状态 结构 发起人：记录当前时刻的内部状态，负责定义哪些属于备份范围的状态，负责创建和恢复备忘录数据。 备忘录：负责存储发起人对象的内部状态，在需要的时候提供发起人需要的内部状态。 管理角色：对备忘录进行管理，保存和提供备忘录。 优点 当发起人角色中的状态改变时，有可能这是个错误的改变，我们使用备忘录模式就可以把这个错误的改变还原。 备份的状态是保存在发起人角色之外的，这样，发起人角色就不需要对各个备份的状态进行管理。 缺点： 在实际应用中，备忘录模式都是多状态和多备份的，发起人角色的状态需要存储到备忘录对象中，对资源的消耗是比较严重的。 如果有需要提供回滚操作的需求，使用备忘录模式非常适合，比如jdbc的事务操作，文本编辑器的Ctrl+Z恢复等。 通用代码实现 class Originator { private String state = \"\"; public String getState() { return state; } public void setState(String state) { this.state = state; } public Memento createMemento(){ return new Memento(this.state); } public void restoreMemento(Memento memento){ this.setState(memento.getState()); } } class Memento { private String state = \"\"; public Memento(String state){ this.state = state; } public String getState() { return state; } public void setState(String state) { this.state = state; } } class Caretaker { private Memento memento; public Memento getMemento(){ return memento; } public void setMemento(Memento memento){ this.memento = memento; } } public class Client { public static void main(String[] args){ Originator originator = new Originator(); originator.setState(\"状态1\"); System.out.println(\"初始状态:\"+originator.getState()); Caretaker caretaker = new Caretaker(); caretaker.setMemento(originator.createMemento()); originator.setState(\"状态2\"); System.out.println(\"改变后状态:\"+originator.getState()); originator.restoreMemento(caretaker.getMemento()); System.out.println(\"恢复后状态:\"+originator.getState()); } } 代码演示了一个单状态单备份的例子，逻辑非常简单：Originator类中的state变量需要备份，以便在需要的时候恢复； Memento类中，也有一个state变量，用来存储Originator类中state变量的临时状态； 而Caretaker类就是用来管理备忘录类的，用来向备忘录对象中写入状态或者取回状态。 多状态多备份备忘录 通用代码演示的例子中，Originator类只有一个state变量需要备份， 而通常情况下，发起人角色通常是一个javaBean，对象中需要备份的变量不止一个，需要备份的状态也不止一个， 这就是多状态多备份备忘录。 实现备忘录的方法很多，备忘录模式有很多变形和处理方式，像通用代码那样的方式一般不会用到， 多数情况下的备忘录模式，是多状态多备份的。其实实现多状态多备份也很简单，最常用的方法是， 我们在Memento中增加一个Map容器来存储所有的状态，在Caretaker类中同样使用一个Map容器才存储所有的备份。 下面我们给出一个多状态多备份的例子： class Originator { private String state1 = \"\"; private String state2 = \"\"; private String state3 = \"\"; public String getState1() { return state1; } public void setState1(String state1) { this.state1 = state1; } public String getState2() { return state2; } public void setState2(String state2) { this.state2 = state2; } public String getState3() { return state3; } public void setState3(String state3) { this.state3 = state3; } public Memento createMemento(){ return new Memento(BeanUtils.backupProp(this)); } public void restoreMemento(Memento memento){ BeanUtils.restoreProp(this, memento.getStateMap()); } public String toString(){ return \"state1=\"+state1+\"state2=\"+state2+\"state3=\"+state3; } } class Memento { private Map stateMap; public Memento(Map map){ this.stateMap = map; } public Map getStateMap() { return stateMap; } public void setStateMap(Map stateMap) { this.stateMap = stateMap; } } class BeanUtils { public static Map backupProp(Object bean){ Map result = new HashMap(); try{ BeanInfo beanInfo = Introspector.getBeanInfo(bean.getClass()); PropertyDescriptor[] descriptors = beanInfo.getPropertyDescriptors(); for(PropertyDescriptor des: descriptors){ String fieldName = des.getName(); Method getter = des.getReadMethod(); Object fieldValue = getter.invoke(bean, new Object[]{}); if(!fieldName.equalsIgnoreCase(\"class\")){ result.put(fieldName, fieldValue); } } }catch(Exception e){ e.printStackTrace(); } return result; } public static void restoreProp(Object bean, Map propMap){ try { BeanInfo beanInfo = Introspector.getBeanInfo(bean.getClass()); PropertyDescriptor[] descriptors = beanInfo.getPropertyDescriptors(); for(PropertyDescriptor des: descriptors){ String fieldName = des.getName(); if(propMap.containsKey(fieldName)){ Method setter = des.getWriteMethod(); setter.invoke(bean, new Object[]{propMap.get(fieldName)}); } } } catch (Exception e) { e.printStackTrace(); } } } class Caretaker { private Map memMap = new HashMap(); public Memento getMemento(String index){ return memMap.get(index); } public void setMemento(String index, Memento memento){ this.memMap.put(index, memento); } } class Client { public static void main(String[] args){ Originator ori = new Originator(); Caretaker caretaker = new Caretaker(); ori.setState1(\"中国\"); ori.setState2(\"强盛\"); ori.setState3(\"繁荣\"); System.out.println(\"===初始化状态===n\"+ori); caretaker.setMemento(\"001\",ori.createMemento()); ori.setState1(\"软件\"); ori.setState2(\"架构\"); ori.setState3(\"优秀\"); System.out.println(\"===修改后状态===n\"+ori); ori.restoreMemento(caretaker.getMemento(\"001\")); System.out.println(\"===恢复后状态===n\"+ori); } } "},"Chapter04/Visitor.html":{"url":"Chapter04/Visitor.html","title":"访问者模式","keywords":"","body":"访问者（Visitor）模式 什么叫做访问，如果大家学过数据结构，对于这点就很清晰了，遍历就是访问的一般形式， 单独读取一个元素进行相应的处理也叫作访问，读取到想要查看的内容+对其进行处理就叫做访问， 那么我们平常是怎么访问的，基本上就是直接拿着需要访问的地址（引用）来读写内存就可以了。 为什么还要有一个访问者模式呢，这就要放到OOP之中了，在面向对象编程的思想中，我们使用类来组织属性， 以及对属性的操作，那么我们理所当然的将访问操作放到了类的内部，这样看起来没问题， 但是当我们想要使用另一种遍历方式要怎么办呢，我们必须将这个类进行修改，这在设计模式中是大忌， 在设计模式中就要保证，对扩展开放，对修改关闭的开闭原则。 因此，我们思考，可不可以将访问操作独立出来变成一个新的类，当我们需要增加访问操作的时候， 直接增加新的类，原来的代码不需要任何的改变，如果可以这样做，那么我们的程序就是好的程序，因为可以扩展， 符合开闭原则。而访问者模式就是实现这个的，使得使用不同的访问方式都可以对某些元素进行访问。 在访问者模式中，主要包括下面几个角色： 抽象访问者：抽象类或者接口，声明访问者可以访问哪些元素，具体到程序中就是visit方法中的参数定义哪些对象是可以被访问的。 访问者：实现抽象访问者所声明的方法，它影响到访问者访问到一个类后该干什么，要做什么事情。 抽象元素类：接口或者抽象类，声明接受哪一类访问者访问，程序上是通过accept方法中的参数来定义的。抽象元素一般有两类方法，一部分是本身的业务逻辑，另外就是允许接收哪类访问者来访问。 元素类：实现抽象元素类所声明的accept方法，通常都是visitor.visit(this)，基本上已经形成一种定式了。 结构对象：一个元素的容器，一般包含一个容纳多个不同类、不同接口的容器，如List、Set、Map等，在项目中一般很少抽象出这个角色。 代码展示 package designpatterns.visitor; public interface IVisitor { void visit(ConcreteElement1 el1); void visit(ConcreteElement2 el2); } package designpatterns.visitor; public abstract class Element { public abstract void accept(IVisitor visitor); public abstract void doSomething(); } package designpatterns.visitor; public class ConcreteElement1 extends Element { public void doSomething() { System.out.println(\"这是元素1\"); } public void accept(IVisitor visitor) { visitor.visit(this); } } package designpatterns.visitor; public class ConcreteElement2 extends Element { public void doSomething() { System.out.println(\"这是元素2\"); } public void accept(IVisitor visitor) { visitor.visit(this); } } package designpatterns.visitor; public class Visitor implements IVisitor { public void visit(ConcreteElement1 el1) { el1.doSomething(); } public void visit(ConcreteElement2 el2) { el2.doSomething(); } } package designpatterns.visitor; import java.util.ArrayList; import java.util.List; import java.util.Random; public class ObjectStruture { public static List getList() { List list = new ArrayList(); Random ran = new Random(); for (int i = 0; i 50) { list.add(new ConcreteElement1()); } else { list.add(new ConcreteElement2()); } } return list; } } package designpatterns.visitor; import java.util.List; public class VisitorMain { public static void main(String[] args) { List list = ObjectStruture.getList(); for (Element e : list) { e.accept(new Visitor()); } } } 优点 符合单一职责原则：凡是适用访问者模式的场景中，元素类中需要封装在访问者中的操作必定是与元素类本身关系不大且是易变的操作，使用访问者模式一方面符合单一职责原则， 另一方面，因为被封装的操作通常来说都是易变的，所以当发生变化时，就可以在不改变元素类本身的前提下，实现对变化部分的扩展。 扩展性良好：元素类可以通过接受不同的访问者来实现对不同操作的扩展。 访问者模式的适用场景 假如一个对象中存在着一些与本对象不相干（或者关系较弱）的操作，为了避免这些操作污染这个对象，则可以使用访问者模式来把这些操作封装到访问者中去。 假如一组对象中，存在着相似的操作，为了避免出现大量重复的代码，也可以将这些重复的操作封装到访问者中去。 但是，访问者模式并不是那么完美，它也有着致命的缺陷：增加新的元素类比较困难。通过访问者模式的代码可以看到，在访问者类中，每一个元素类都有它对应的处理方法， 也就是说，每增加一个元素类都需要修改访问者类（也包括访问者类的子类或者实现类），修改起来相当麻烦。也就是说，在元素类数目不确定的情况下，应该慎用访问者模式。 所以，访问者模式比较适用于对已有功能的重构，比如说，一个项目的基本功能已经确定下来，元素类的数据已经基本确定下来不会变了，会变的只是这些元素内的相关操作， 这时候，我们可以使用访问者模式对原有的代码进行重构一遍，这样一来，就可以在不修改各个元素类的情况下，对原有功能进行修改。 总结 正如《设计模式》的作者GoF对访问者模式的描述：大多数情况下，你并需要使用访问者模式，但是当你一旦需要使用它时，那你就是真的需要它了。当然这只是针对真正的大牛而言。 在现实情况下（至少是我所处的环境当中），很多人往往沉迷于设计模式，他们使用一种设计模式时，从来不去认真考虑所使用的模式是否适合这种场景，而往往只是想展示一下自己对面向对象设计的驾驭能力。 编程时有这种心理，往往会发生滥用设计模式的情况。所以，在学习设计模式时，一定要理解模式的适用性。必须做到使用一种模式是因为了解它的优点，不使用一种模式是因为了解它的弊端； 而不是使用一种模式是因为不了解它的弊端，不使用一种模式是因为不了解它的优点。 "},"Chapter04/VisitorExt.html":{"url":"Chapter04/VisitorExt.html","title":"java的动态绑定与双分派","keywords":"","body":"访问者模式讨论篇：java的动态绑定与双分派 java的动态绑定 所谓的动态绑定就是指程执行期间（而不是在编译期间） 判断所引用对象的实际类型，根据其实际的类型调用其相应的方法。 java继承体系中的覆盖就是动态绑定的，看一下如下的代码： class Father { public void method(){ System.out.println(\"This is Father's method\"); } } class Son1 extends Father{ public void method(){ System.out.println(\"This is Son1's method\"); } } class Son2 extends Father{ public void method(){ System.out.println(\"This is Son2's method\"); } } public class Test { public static void main(String[] args){ Father s1 = new Son1(); s1.method(); Father s2 = new Son2(); s2.method(); } } 运行结果如下： This is Son1's method This is Son2's method 通过运行结果可以看到，尽管我们引用的类型是Father类型的， 但是运行时却是调用的它实际类型（也就是Son1和Son2）的方法， 这就是动态绑定。在java语言中，继承中的覆盖就是是动态绑定的， 当我们用父类引用实例化子类时，会根据引用的实际类型调用相应的方法。 java的静态绑定 相对于动态绑定，静态绑定就是指在编译期就已经确定执行哪一个方法。 在java中，方法的重载（方法名相同而参数不同）就是静态绑定的， 重载时，执行哪一个方法在编译期就已经确定下来了。看一下代码： class Father {} class Son1 extends Father{} class Son2 extends Father{} class Execute { public void method(Father father){ System.out.println(\"This is Father's method\"); } public void method(Son1 son){ System.out.println(\"This is Son1's method\"); } public void method(Son2 son){ System.out.println(\"This is Son2's method\"); } } public class Test { public static void main(String[] args){ Father father = new Father(); Father s1 = new Son1(); Father s2 = new Son2(); Execute exe = new Execute(); exe.method(father); exe.method(s1); exe.method(s2); } } 运行结果如下： This is Father's method This is Father's method This is Father's method 在这里，程序在编译的时候就已经确定使用method(Father father)方法了， 不管我们在运行的时候传入的实际类型是什么， 它永远都只会执行method(Father father)这个方法。 也就是说，java的重载是静态绑定的。 instanceof操作符与转型 有时候，我们希望在使用重载的时候， 程序能够根据传入参数的实际类型动态地调用相应的方法， 也就是说，我们希望java的重载是动态的，而不是静态的。 但是由于java的重载不是动态绑定，我们只能通过程序来人为的判断， 我们一般会使用instanceof操作符来进行类型的判断。 我们要对method(Father father)进行修改 ，在方法体中判断运行期间的实际类型， 修改后的method(Father father)方法如下： public void method(Father father){ if(father instanceof Son1){ method((Son1)father); }else if(father instanceof Son2){ method((Son2)father); }else if(father instanceof Father){ System.out.println(\"This is Father's method\"); } } 请注意，我们必须把判断是否是父类的条件 （也就是判断是否为Father类的条件）放到最后， 否则将一律会被判断为Father类，达不到我们动态判断的目的。 修改代码后，程序就可以动态地根据参数的实际类型来调用相应的方法了。 运行结果如下： This is Father's method This is Son1's method This is Son2's method 但是这种实现方式有一个明显的缺点，它是伪动态的， 仍然需要我们来通过程序来判断类型。 假如Father有100个子类的话，还是这样来实现显然是不合适的。 必须通过其他更好的方式实现才行，我们可以使用双分派方式来实现动态绑定。 用双分派实现动态绑定 首先，什么是双分派？ 还记得23种设计模式（9）： 访问者模式中一开始举的例子吗？ 类A中的方法method1和method2的区别就是， method2是双分派。我们可以看一下java双分派的特点： 首先要有一个访问类B，类B提供一个showA(A a) 方法， 在方法中，调用类A的method1方法， 然后类A的method2方法中调用类B的showA方法并将自己作为参数传给showA。 双分派的核心就是这个this对象。说到这里，我们已经明白双分派是怎么回事了，但是它有什么效果呢？就是可以实现方法的动态绑定，我们可以对上面的程序进行修改，代码如下： class Father { public void accept(Execute exe){ exe.method(this); } } class Son1 extends Father{ public void accept(Execute exe){ exe.method(this); } } class Son2 extends Father{ public void accept(Execute exe){ exe.method(this); } } class Execute { public void method(Father father){ System.out.println(\"This is Father's method\"); } public void method(Son1 son){ System.out.println(\"This is Son1's method\"); } public void method(Son2 son){ System.out.println(\"This is Son2's method\"); } } public class Test { public static void main(String[] args){ Father father = new Father(); Father s1 = new Son1(); Father s2 = new Son2(); Execute exe = new Execute(); father.accept(exe); s1.accept(exe); s2.accept(exe); } } 可以看到我们修改的地方， 在Father，Son1，Son2中分别加入一个双分派的方法。 调用的时候，原本是调用Execute的method方法 ，现在改为调用Father的accept方法。运行结果如下： This is Father's method This is Son1's method This is Son2's method 运行结果符合我们的预期，实现了动态绑定。 双分派实现动态绑定的本质， 就是在重载方法委派的前面加上了继承体系中覆盖的环节， 由于覆盖是动态的，所以重载就是动态的了， 与使用instanceof操作符的效果是一样的 （用instanceof操作符可以实现重载方法动态绑定的原因也是因为instanceof操作符是动态的）。 但是与使用instanceof操作符实现动态绑定相比， 双分派方式的可扩展性要好的多。 "},"Chapter04/Intermediary.html":{"url":"Chapter04/Intermediary.html","title":"中介模式","keywords":"","body":"中介者模式(调停者模式) 结构 抽象中介者：定义好同事类对象到中介者对象的接口，用于各个同事类之间的通信。一般包括一个或几个抽象的事件方法，并由子类去实现。 中介者实现类：从抽象中介者继承而来，实现抽象中介者中定义的事件方法。从一个同事类接收消息，然后通过消息影响其他同时类。 同事类：如果一个对象会影响其他的对象，同时也会被其他对象影响，那么这两个对象称为同事类。在类图中，同事类只有一个，这其实是现实的省略，在实际应用中，同事类一般由多个组成，他们之间相互影响，相互依赖。同事类越多，关系越复杂。并且，同事类也可以表现为继承了同一个抽象类的一组实现组成。在中介者模式中，同事类之间必须通过中介者才能进行消息传递。 为什么要使用中介者模式? 一般来说，同事类之间的关系是比较复杂的，多个同事类之间互相关联时，他们之间的关系会呈现为复杂的网状结构， 这是一种过度耦合的架构，即不利于类的复用，也不稳定。 如果引入中介者模式，那么同事类之间的关系将变为星型结构， 从图中可以看到，任何一个类的变动，只会影响的类本身，以及中介者，这样就减小了系统的耦合。 一个好的设计，必定不会把所有的对象关系处理逻辑封装在本类中， 而是使用一个专门的类来管理那些不属于自己的行为。 我们使用一个例子来说明一下什么是同事类：有两个类A和B，类中各有一个数字， 并且要保证类B中的数字永远是类A中数字的100倍。也就是说，当修改类A的数时， 将这个数字乘以100赋给类B，而修改类B时，要将数除以100赋给类A。类A类B互相影响，就称为同事类。 代码如下： abstract class AbstractColleague { protected int number; public int getNumber() { return number; } public void setNumber(int number){ this.number = number; } //抽象方法，修改数字时同时修改关联对象 public abstract void setNumber(int number, AbstractColleague coll); } class ColleagueA extends AbstractColleague{ public void setNumber(int number, AbstractColleague coll) { this.number = number; coll.setNumber(number*100); } } class ColleagueB extends AbstractColleague{ public void setNumber(int number, AbstractColleague coll) { this.number = number; coll.setNumber(number/100); } } public class Client { public static void main(String[] args){ AbstractColleague collA = new ColleagueA(); AbstractColleague collB = new ColleagueB(); System.out.println(\"==========设置A影响B==========\"); collA.setNumber(1288, collB); System.out.println(\"collA的number值：\"+collA.getNumber()); System.out.println(\"collB的number值：\"+collB.getNumber()); System.out.println(\"==========设置B影响A==========\"); collB.setNumber(87635, collA); System.out.println(\"collB的number值：\"+collB.getNumber()); System.out.println(\"collA的number值：\"+collA.getNumber()); } } 上面的代码中，类A类B通过直接的关联发生关系，假如我们要使用中介者模式，类A类B之间则不可以直接关联， 他们之间必须要通过一个中介者来达到关联的目的。 abstract class AbstractColleague { protected int number; public int getNumber() { return number; } public void setNumber(int number){ this.number = number; } //注意这里的参数不再是同事类，而是一个中介者 public abstract void setNumber(int number, AbstractMediator am); } class ColleagueA extends AbstractColleague{ public void setNumber(int number, AbstractMediator am) { this.number = number; am.AaffectB(); } } class ColleagueB extends AbstractColleague{ @Override public void setNumber(int number, AbstractMediator am) { this.number = number; am.BaffectA(); } } abstract class AbstractMediator { protected AbstractColleague A; protected AbstractColleague B; public AbstractMediator(AbstractColleague a, AbstractColleague b) { A = a; B = b; } public abstract void AaffectB(); public abstract void BaffectA(); } class Mediator extends AbstractMediator { public Mediator(AbstractColleague a, AbstractColleague b) { super(a, b); } //处理A对B的影响 public void AaffectB() { int number = A.getNumber(); B.setNumber(number*100); } //处理B对A的影响 public void BaffectA() { int number = B.getNumber(); A.setNumber(number/100); } } public class Client { public static void main(String[] args){ AbstractColleague collA = new ColleagueA(); AbstractColleague collB = new ColleagueB(); AbstractMediator am = new Mediator(collA, collB); System.out.println(\"==========通过设置A影响B==========\"); collA.setNumber(1000, am); System.out.println(\"collA的number值为：\"+collA.getNumber()); System.out.println(\"collB的number值为A的10倍：\"+collB.getNumber()); System.out.println(\"==========通过设置B影响A==========\"); collB.setNumber(1000, am); System.out.println(\"collB的number值为：\"+collB.getNumber()); System.out.println(\"collA的number值为B的0.1倍：\"+collA.getNumber()); } } 虽然代码比较长，但是还是比较容易理解的，其实就是把原来处理对象关系的代码重新封装到一个中介类中， 通过这个中介类来处理对象间的关系。 优点 适当地使用中介者模式可以避免同事类之间的过度耦合，使得各同事类之间可以相对独立地使用。 使用中介者模式可以将对象间一对多的关联转变为一对一的关联，使对象间的关系易于理解和维护。 使用中介者模式可以将对象的行为和协作进行抽象，能够比较灵活的处理对象间的相互作用。 适用场景 在面向对象编程中，一个类必然会与其他的类发生依赖关系，完全独立的类是没有意义的。 一个类同时依赖多个类的情况也相当普遍，既然存在这样的情况， 说明，一对多的依赖关系有它的合理性，适当的使用中介者模式可以使原本凌乱的对象关系清晰， 但是如果滥用，则可能会带来反的效果。一般来说，只有对于那种同事类之间是网状结构的关系， 才会考虑使用中介者模式。可以将网状结构变为星状结构，使同事类之间的关系变的清晰一些。 中介者模式是一种比较常用的模式，也是一种比较容易被滥用的模式。对于大多数的情况， 同事类之间的关系不会复杂到混乱不堪的网状结构，因此，大多数情况下， 将对象间的依赖关系封装的同事类内部就可以的，没有必要非引入中介者模式。 滥用中介者模式，只会让事情变的更复杂。 "},"Chapter04/Interpreter.html":{"url":"Chapter04/Interpreter.html","title":"解释器模式","keywords":"","body":"解释器模式 解释器模式是一个比较少用的模式，给定一种语言，定义他的文法的一种表示， 并定义一个解释器，该解释器使用该表示来解释语言中句子。 结构 抽象解释器：声明一个所有具体表达式都要实现的抽象接口（或者抽象类），接口中主要是一个interpret()方法，称为解释操作。具体解释任务由它的各个实现类来完成，具体的解释器分别由终结符解释器TerminalExpression和非终结符解释器NonterminalExpression完成。 终结符表达式：实现与文法中的元素相关联的解释操作，通常一个解释器模式中只有一个终结符表达式，但有多个实例，对应不同的终结符。终结符一半是文法中的运算单元，比如有一个简单的公式R=R1+R2，在里面R1和R2就是终结符，对应的解析R1和R2的解释器就是终结符表达式。 非终结符表达式：文法中的每条规则对应于一个非终结符表达式，非终结符表达式一般是文法中的运算符或者其他关键字，比如公式R=R1+R2中，+就是非终结符，解析+的解释器就是一个非终结符表达式。非终结符表达式根据逻辑的复杂程度而增加，原则上每个文法规则都对应一个非终结符表达式。 环境角色：这个角色的任务一般是用来存放文法中各个终结符所对应的具体值，比如R=R1+R2，我们给R1赋值100，给R2赋值200。这些信息需要存放到环境角色中，很多情况下我们使用Map来充当环境角色就足够了。 文法递归的代码部分需要根据具体的情况来实现，因此在代码中没有体现。抽象表达式是生成语法集合的关键， 每个非终结符表达式解释一个最小的语法单元，然后通过递归的方式将这些语法单元组合成完整的文法，这就是解释器模式。 优点 解释器是一个简单的语法分析工具，它最显著的优点就是扩展性， 修改语法规则只需要修改相应的非终结符就可以了，若扩展语法， 只需要增加非终结符类就可以了。 缺点 解释器模式会引起类的膨胀，每个语法都需要产生一个非终结符表达式，语法规则比较复杂时， 就可能产生大量的类文件，为维护带来非常多的麻烦。同时，由于采用递归调用方法， 每个非终结符表达式只关心与自己相关的表达式，每个表达式需要知道最终的结果，必须通过递归方式， 无论是面向对象的语言还是面向过程的语言，递归都是一个不推荐的方式。由于使用了大量的循环和递归， 效率是一个不容忽视的问题。特别是用于解释一个解析复杂、冗长的语法时，效率是难以忍受的。 适用场景 有一个简单的语法规则，比如一个sql语句，如果我们需要根据sql语句进行rm转换， 就可以使用解释器模式来对语句进行解释。一些重复发生的问题，比如加减乘除四则运算， 但是公式每次都不同，有时是a+b-cd，有时是ab+c-d，等等等等个，公式千变万化，但是都是由加减乘除四个非终结符来连接的， 这时我们就可以使用解释器模式。 注意事项 解释器模式真的是一个比较少用的模式，因为对它的维护实在是太麻烦了，想象一下，一坨一坨的非终结符解释器， 假如不是事先对文法的规则了如指掌，或者是文法特别简单，则很难读懂它的逻辑。解释器模式在实际的系统开发中使用的很少， 因为他会引起效率、性能以及维护等问题。 代码展示 package designpatterns.interpreter; public class Context { } package designpatterns.interpreter; public class TerminalExpression { public Object interpreter(Context ctx){ return null; } } package designpatterns.interpreter; public abstract class Expression { public abstract Object interpreter(Context ctx); } package designpatterns.interpreter; public class NonterminalExpression { public NonterminalExpression(Expression...expressions){ } public Object interpreter(Context ctx){ return null; } } package designpatterns.interpreter; import java.util.Stack; public class Client { public static void main(String[] args) { String expression = \"\"; char[] charArray = expression.toCharArray(); Context ctx = new Context(); Stack stack = new Stack(); for (int i = 0; i "},"Chapter04/SummaryOfBehaviorPatterns.html":{"url":"Chapter04/SummaryOfBehaviorPatterns.html","title":"行为模式总结","keywords":"","body":"行为模式总结 "},"Chapter04/designInJava.html":{"url":"Chapter04/designInJava.html","title":"java中的设计模式","keywords":"","body":"java 中的设计模式 Iterable 本身是迭代器模式里面的 default void forEach(Consumer action) { Objects.requireNonNull(action); for (T t : this) { action.accept(t); } } 其中action.accept(t)不是访问者模式 "},"Chapter04/designInSpring.html":{"url":"Chapter04/designInSpring.html","title":"spring中的设计模式","keywords":"","body":"spring中的设计模式 简单工厂模式 又叫做静态工厂方法（StaticFactory Method）模式，但不属于23种GOF设计模式之一。 简单工厂模式的实质是由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。 spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。 工厂方法模式 通常由应用程序直接使用new创建新的对象，为了将对象的创建和使用相分离，采用工厂模式,即应用程序将对象的创建及初始化职责交给工厂对象。 一般情况下,应用程序有自己的工厂对象来创建bean.如果将应用程序自己的工厂对象交给Spring管理,那么Spring管理的就不是普通的bean,而是工厂Bean。 单例模式 保证一个类仅有一个实例，并提供一个访问它的全局访问点。spring中的单例模式完成了后半句话，即提供了全局的访问点BeanFactory。但没有从构造器级别去控制单例，这是因为spring管理的是是任意的java对象。 核心提示点：Spring下默认的bean均为singleton，可以通过singleton=“true|false” 或者 scope=\"?\"来指定。 适配器模式 在Spring的Aop中，使用的Advice（通知）来增强被代理类的功能。Spring实现这一AOP功能的原理就使用代理模式（1、JDK动态代理。2、CGLib字节码生成技术代理。）对类进行方法级别的切面增强，即，生成被代理类的代理类， 并在代理类的方法前，设置拦截器，通过执行拦截器重的内容增强了代理方法的功能，实现的面向切面编程。 Adapter类接口：Target 包装器模式 在我们的项目中遇到这样一个问题：我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。我们以往在spring和hibernate框架中总是配置一个数据源，因而sessionFactory的dataSource属性总是指向这个数据源并且恒定不变，所有DAO在使用sessionFactory的时候都是通过这个数据源访问数据库。 但是现在，由于项目的需要，我们的DAO在访问sessionFactory的时候都不得不在多个数据源中不断切换，问题就出现了：如何让sessionFactory在执行数据持久化的时候，根据客户的需求能够动态切换不同的数据源？我们能不能在spring的框架下通过少量修改得到解决？是否有什么设计模式可以利用呢？ 首先想到在spring的applicationContext中配置所有的dataSource。这些dataSource可能是各种不同类型的，比如不同的数据库：Oracle、SQL Server、MySQL等，也可能是不同的数据源：比如apache 提供的org.apache.commons.dbcp.BasicDataSource、spring提供的org.springframework.jndi.JndiObjectFactoryBean等。然后sessionFactory根据客户的每次请求，将dataSource属性设置成不同的数据源，以到达切换数据源的目的。 spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。基本上都是动态地给一个对象添加一些额外的职责。 代理模式 为其他对象提供一种代理以控制对这个对象的访问。 从结构上来看和Decorator模式类似，但Proxy是控制，更像是一种对功能的限制，而Decorator是增加职责。spring的Proxy模式在aop中有体现，比如JdkDynamicAopProxy和Cglib2AopProxy。 观察者模式 定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。spring中Observer模式常用的地方是listener的实现。如ApplicationListener。 策略模式 定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。spring中在实例化对象的时候用到Strategy模式在SimpleInstantiationStrategy中有如下代码说明了策略模式的使用情况： 模板方法模式 定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。Template Method使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 Template Method模式一般是需要继承的。这里想要探讨另一种对Template Method的理解。spring中的JdbcTemplate，在用这个类时并不想去继承这个类，因为这个类的方法太多，但是我们还是想用到JdbcTemplate已有的稳定的、公用的数据库连接，那么我们怎么办呢？我们可以把变化的东西抽出来作为一个参数传入JdbcTemplate的方法中。但是变化的东西是一段代码，而且这段代码会用到JdbcTemplate中的变量。怎么办？那我们就用回调对象吧。 在这个回调对象中定义一个操纵JdbcTemplate中变量的方法，我们去实现这个方法，就把变化的东西集中到这里了。然后我们再传入这个回调对象到JdbcTemplate，从而完成了调用。这可能是Template Method不需要继承的另一种实现方式。 以下是一个具体的例子：JdbcTemplate中的execute方法 ``` ```java ``` ```java ``` ```java ``` ```java ``` ```java ``` ```java ``` ```java ``` ```java "},"Chapter05/java.html":{"url":"Chapter05/java.html","title":"Part V JAVA篇","keywords":"","body":"第五章 java篇 init and clinit Hashcode 深入分析java中的关键字void 深入分析java中的关键字static 深入分析java中的关键字final 深入分析java中的关键字this 深入分析java中的关键字super 深入分析java中的String 深入理解wait和notify 深入理解LockSupport Transient modCount 13.Thread Executors SourceCode KeyTool JDK动态代理 反射 深入理解StringBuilder "},"Chapter05/init.html":{"url":"Chapter05/init.html","title":"init and clinit","keywords":"","body":"java 的init方法与clinit方法 一、 clinit静态方法：类型初始化方法主要是对static变量进行初始化操作，对static域和static代码块初始化的逻辑全部封装在方法中。 java.lang.Class.forName(String name, boolean initialize,ClassLoader loader)，其中第二个参数就是是否需要初始化。 Java类型初始化过程中对static变量的初始化操作依赖于static域和static代码块的前后关系，static域与static代码块声明的位置关系会导致java编译器生成方法字节码。 类型的初始化方法只在该类型被加载时才执行，且只执行一次。 二、 对象实例化方法：Java对象在被创建时，会进行实例化操作。 该部分操作封装在方法中，并且子类的方法中会首先对父类方法的调用。 Java对象实例化过程中对实例域的初始化赋值操作全部在方法中进行， 方法显式的调用父类的方法， 实例域的声明以及实例初始化语句块同样的位置关系会影响编译器生成的方法的字节码顺序， 方法以构造方法作为结束。 三、init和clinit区别： ①init和clinit方法执行时机不同 init是对象构造器方法，也就是说在程序执行 new 一个对象调用该对象类的 constructor 方法时才会执行init方法，而clinit是类构造器方法，也就是在jvm进行类加载—–验证—-解析—–初始化，中的初始化阶段jvm会调用clinit方法。 ②init和clinit方法执行目的不同 init is the (or one of the) constructor(s) for the instance, and non-static field initialization. clinit are the static initialization blocks for the class, and static field initialization. 上面这两句是Stack Overflow上的解析，很清楚init是instance实例构造器，对非静态变量解析初始化，而clinit是class类构造器对静态变量，静态代码块进行初始化。看看下面的这段程序就很清楚了。 class X { static Log log = LogFactory.getLog(); // private int x = 1; // X(){ // } static { // } } clinit一定优先于init 今天先来分析一下经常遇到的一个问题，在笔试面试中可能会经常遇见，类中字段代码块的加载顺序等，从jvm角度分析一下这个问题。我们先来看下知识点，接下来进行代码实践验证。 ，类构造器方法，在jvm第一次加载class文件时调用，因为是类级别的，所以只加载一次，是编译器自动收集类中所有类变量（static修饰的变量）和静态语句块（static{}），中的语句合并产生的，编译器收集的顺序，是由程序员在写在源文件中的代码的顺序决定的。 ，实例构造器方法，在实例创建出来的时候调用，包括调用new操作符；调用Class或java.lang.reflect.Constructor对象的newInstance()方法；调用任何现有对象的clone()方法；通过java.io.ObjectInputStream类的getObject()方法反序列化。 1.方法和类的构造函数不同，它不需要显示调用父类的构造方法，虚拟机会保证子类的方法执行之前，父类的此方法已经执行完毕，因此虚拟机中第一个被执行的方法的类肯定是java.lang.Object 2、接口中不能使用static块，但是接口仍然有变量初始化的操作，因此接口也会生成方法。但接口和类不同的是，不会先去执行继承接口的方法，而是在调用父类变量的时候，才会去调用方法。接口的实现类也是一样的。 "},"Chapter05/Hashcode.html":{"url":"Chapter05/Hashcode.html","title":"Hashcode","keywords":"","body":"Hashcode Hash的定义   散列（哈希）函数:把任意长度的输入（又叫做预映射pre-image）通过散列算法变换成固定长度的输出， 该输出就是散列值，是一种压缩映射。或者说一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。 Hash函数特性   h(k1)≠h(k2)则k1≠k2，即散列值不相同，则输入值即预映射不同   如果k1≠k2，h(k1)=h(k2) 则发生碰撞   如果h(k1)=h(k2)，k1不一定等于k2 Hash的使用场景   比如说我们下载一个文件，文件的下载过程中会经过很多网络服务器、路由器的中转，如何保证这个文件就是我们所需要的呢？ 我们不可能去一一检测这个文件的每个字节，也不能简单地利用文件名、文件大小这些极容易伪装的信息， 这时候，就需要一种指纹一样的标志来检查文件的可靠性，这种指纹就是我们现在所用的Hash算法(也叫散列算法)。   散列算法就是一种以较短的信息来保证文件唯一性的标志，这种标志与文件的每一个字节都相关，而且难以找到逆向规律。 因此，当原有文件发生改变时，其标志值也会发生改变，从而告诉文件使用者当前的文件已经不是你所需求的文件。 这种标志有何意义呢？之前文件下载过程就是一个很好的例子，事实上，现在大部分的网络部署和版本控制工具都在使用散列算法来保证文件可靠性。 HashCode是什么   HashCode是Object的一个方法，hashCode方法返回一个hash code值，且这个方法是为了更好的支持hash表，比如String、 Set、 HashTable、HashMap等 hash code（hash值|hash码）是什么   哈希码是按照某种规则生成的int类型的数值。   哈希码并不是完全唯一的。   让同一个类的对象按照自己不同的特征尽量的有不同的哈希码，但不是说不同的对象哈希码就一定不同，也有相同的情况。 HashCode规范 规范1:   若重写了某个类的equals方法，请一定重写hashCode方法，要能保证通过equals方法判断为true的两个对象，其hashCode方法的返回值相等， 换句话说，就是要保证”两个对象相等，其hashCode一定相同”始终成立; 规范2:   若equals方法返回false，即两个对象不相等，并不要求这两个对象的hashCode得到不同的数； HashCode的四条推论   两个对象相等，其HashCode一定相同;   两个对象不相等，其HashCode有可能相同;   HashCode相同的两个对象，不一定相等;   HashCode不相同的两个对象，一定不相等; 改写equals时总是要改写hashCode java.lnag.Object中对hashCode的约定：   1. 在一个应用程序执行期间，如果一个对象的equals方法做比较所用到的信息没有被修改的话，则对该对象调用hashCode方法多次，它必须始终如一地返回同一个整数。   2. 如果两个对象根据equals(Object o)方法是相等的，则调用这两个对象中任一对象的hashCode方法必须产生相同的整数结果。   3. 如果两个对象根据equals(Object o)方法是不相等的，则调用这两个对象中任一个对象的hashCode方法，不要求产生不同的整数结果。但如果能不同，则可能提高散列表的性能。   有一个概念要牢记，两个相等对象的equals方法一定为true, 但两个hashcode相等的对象不一定是相等的对象。   所以hashcode相等只能保证两个对象在一个HASH表里的同一条HASH链上，继而通过equals方法才能确定是不是同一对象， 如果结果为true, 则认为是同一对象在插入，否则认为是不同对象继续插入。 Object的代码: /** * Returns a hash code value for the object. This method is * supported for the benefit of hash tables such as those provided by * {@link java.util.HashMap}. * * The general contract of {@code hashCode} is: * * Whenever it is invoked on the same object more than once during * an execution of a Java application, the {@code hashCode} method * must consistently return the same integer, provided no information * used in {@code equals} comparisons on the object is modified. * This integer need not remain consistent from one execution of an * application to another execution of the same application. * If two objects are equal according to the {@code equals(Object)} * method, then calling the {@code hashCode} method on each of * the two objects must produce the same integer result. * It is not required that if two objects are unequal * according to the {@link java.lang.Object#equals(java.lang.Object)} * method, then calling the {@code hashCode} method on each of the * two objects must produce distinct integer results. However, the * programmer should be aware that producing distinct integer results * for unequal objects may improve the performance of hash tables. * * * As much as is reasonably practical, the hashCode method defined by * class {@code Object} does return distinct integers for distinct * objects. (This is typically implemented by converting the internal * address of the object into an integer, but this implementation * technique is not required by the * Java&trade; programming language.) * * @return a hash code value for this object. * @see java.lang.Object#equals(java.lang.Object) * @see java.lang.System#identityHashCode */ public native int hashCode(); 翻译内容如下： 返回对象的哈希码值。支持此方法的好处是哈希表，例如java.util.HashMap提供的哈希表。 hashCode的一般契约是： 每当在执行Java应用程序期间多次在同一对象上调用它时，hashCode方法必须始终返回相同的整数，前提是不修改对象的equals比较中使用的信息。从应用程序的一次执行到同一应用程序的另一次执行，该整数不需要保持一致。 如果两个对象根据equals（Object）方法相等，则对两个对象中的每一个调用hashCode方法必须生成相同的整数结果。 如果两个对象根据equals（Object）方法不相等则不是必需的，则对两个对象中的每一个调用hashCode方法必须产生不同的整数结果。但是，程序员应该知道为不等对象生成不同的整数结果可能会提高哈希表的性能。 尽可能合理，Object类定义的hashCode方法确实为不同的对象返回不同的整数。 （ 这通常通过将对象的内部地址转换为整数来实现，但Java™编程语言不需要此实现技术。） Object.hashCode不可以代表内存地址   首先上面注意括号里的这句 这通常通过将对象的内部地址转换为整数来实现，但Java™编程语言不需要此实现技术 为了解决这个谜团，还是得看看#Object.java#hashCode的具体实现方法了。 native方法本身非java实现，如果想要看源码，只有下载完整的jdk呗（openJdk1.8）。 找到Object.c文件，查看上面的方法映射表发现，hashCode被映射到了一个叫JVM_IHashCode上去了 static JNINativeMethod methods[] = { {\"hashCode\", \"()I\", (void *)&JVM_IHashCode}, {\"wait\", \"(J)V\", (void *)&JVM_MonitorWait}, {\"notify\", \"()V\", (void *)&JVM_MonitorNotify}, {\"notifyAll\", \"()V\", (void *)&JVM_MonitorNotifyAll}, {\"clone\", \"()Ljava/lang/Object;\", (void *)&JVM_Clone}, }; 顺藤摸瓜去看看JVM_IHashCode到底干了什么？熟悉的味道，我猜在jvm.h里面有方法声明，那实现一定在jvm.cpp里面。 果然处处有惊喜，和猜想的没错，不过jvm.cpp对于JVM_IHashCode的实现调用的是ObjectSynchronizer::FastHashCode的方法。看来革命尚未成功啊！ JVM_ENTRY(jint, JVM_IHashCode(JNIEnv* env, jobject handle)) JVMWrapper(\"JVM_IHashCode\"); // as implemented in the classic virtual machine; return 0 if object is NULL return handle == NULL ? 0 : ObjectSynchronizer::FastHashCode (THREAD, JNIHandles::resolve_non_null(handle)) ; JVM_END 找了一会儿，没找到，这就尴尬了。后面百度了一下，发现声明在synchronizer.hpp 实现在这里synchronizer.cpp。感谢前辈们走出的路啊！ // hashCode() generation : // // Possibilities: // * MD5Digest of {obj,stwRandom} // * CRC32 of {obj,stwRandom} or any linear-feedback shift register function. // * A DES- or AES-style SBox[] mechanism // * One of the Phi-based schemes, such as: // 2654435761 = 2^32 * Phi (golden ratio) // HashCodeValue = ((uintptr_t(obj) >> 3) * 2654435761) ^ GVars.stwRandom ; // * A variation of Marsaglia's shift-xor RNG scheme. // * (obj ^ stwRandom) is appealing, but can result // in undesirable regularity in the hashCode values of adjacent objects // (objects allocated back-to-back, in particular). This could potentially // result in hashtable collisions and reduced hashtable efficiency. // There are simple ways to \"diffuse\" the middle address bits over the // generated hashCode values: static inline intptr_t get_next_hash(Thread * Self, oop obj) { intptr_t value = 0; if (hashCode == 0) { // This form uses global Park-Miller RNG. // On MP system we'll have lots of RW access to a global, so the // mechanism induces lots of coherency traffic. value = os::random(); } else if (hashCode == 1) { // This variation has the property of being stable (idempotent) // between STW operations. This can be useful in some of the 1-0 // synchronization schemes. intptr_t addrBits = cast_from_oop(obj) >> 3; value = addrBits ^ (addrBits >> 5) ^ GVars.stwRandom; } else if (hashCode == 2) { value = 1; // for sensitivity testing } else if (hashCode == 3) { value = ++GVars.hcSequence; } else if (hashCode == 4) { value = cast_from_oop(obj); } else { // Marsaglia's xor-shift scheme with thread-specific state // This is probably the best overall implementation -- we'll // likely make this the default in future releases. unsigned t = Self->_hashStateX; t ^= (t _hashStateX = Self->_hashStateY; Self->_hashStateY = Self->_hashStateZ; Self->_hashStateZ = Self->_hashStateW; unsigned v = Self->_hashStateW; v = (v ^ (v >> 19)) ^ (t ^ (t >> 8)); Self->_hashStateW = v; value = v; } value &= markOopDesc::hash_mask; if (value == 0) value = 0xBAD; assert(value != markOopDesc::no_hash, \"invariant\"); TEVENT(hashCode: GENERATE); return value; } intptr_t ObjectSynchronizer::FastHashCode(Thread * Self, oop obj) { if (UseBiasedLocking) { // NOTE: many places throughout the JVM do not expect a safepoint // to be taken here, in particular most operations on perm gen // objects. However, we only ever bias Java instances and all of // the call sites of identity_hash that might revoke biases have // been checked to make sure they can handle a safepoint. The // added check of the bias pattern is to avoid useless calls to // thread-local storage. if (obj->mark()->has_bias_pattern()) { // Handle for oop obj in case of STW safepoint Handle hobj(Self, obj); // Relaxing assertion for bug 6320749. assert(Universe::verify_in_progress() || !SafepointSynchronize::is_at_safepoint(), \"biases should not be seen by VM thread here\"); BiasedLocking::revoke_and_rebias(hobj, false, JavaThread::current()); obj = hobj(); assert(!obj->mark()->has_bias_pattern(), \"biases should be revoked by now\"); } } // hashCode() is a heap mutator ... // Relaxing assertion for bug 6320749. assert(Universe::verify_in_progress() || DumpSharedSpaces || !SafepointSynchronize::is_at_safepoint(), \"invariant\"); assert(Universe::verify_in_progress() || DumpSharedSpaces || Self->is_Java_thread() , \"invariant\"); assert(Universe::verify_in_progress() || DumpSharedSpaces || ((JavaThread *)Self)->thread_state() != _thread_blocked, \"invariant\"); ObjectMonitor* monitor = NULL; markOop temp, test; intptr_t hash; markOop mark = ReadStableMark(obj); // object should remain ineligible for biased locking assert(!mark->has_bias_pattern(), \"invariant\"); if (mark->is_neutral()) { hash = mark->hash(); // this is a normal header if (hash) { // if it has hash, just return it return hash; } hash = get_next_hash(Self, obj); // allocate a new hash code temp = mark->copy_set_hash(hash); // merge the hash code into header // use (machine word version) atomic operation to install the hash test = obj->cas_set_mark(temp, mark); if (test == mark) { return hash; } // If atomic operation failed, we must inflate the header // into heavy weight monitor. We could add more code here // for fast path, but it does not worth the complexity. } else if (mark->has_monitor()) { monitor = mark->monitor(); temp = monitor->header(); assert(temp->is_neutral(), \"invariant\"); hash = temp->hash(); if (hash) { return hash; } // Skip to the following code to reduce code size } else if (Self->is_lock_owned((address)mark->locker())) { temp = mark->displaced_mark_helper(); // this is a lightweight monitor owned assert(temp->is_neutral(), \"invariant\"); hash = temp->hash(); // by current thread, check if the displaced if (hash) { // header contains hash code return hash; } // WARNING: // The displaced header is strictly immutable. // It can NOT be changed in ANY cases. So we have // to inflate the header into heavyweight monitor // even the current thread owns the lock. The reason // is the BasicLock (stack slot) will be asynchronously // read by other threads during the inflate() function. // Any change to stack may not propagate to other threads // correctly. } // Inflate the monitor to set hash code monitor = ObjectSynchronizer::inflate(Self, obj, inflate_cause_hash_code); // Load displaced header and check it has hash code mark = monitor->header(); assert(mark->is_neutral(), \"invariant\"); hash = mark->hash(); if (hash == 0) { hash = get_next_hash(Self, obj); temp = mark->copy_set_hash(hash); // merge hash code into header assert(temp->is_neutral(), \"invariant\"); test = Atomic::cmpxchg(temp, monitor->header_addr(), mark); if (test != mark) { // The only update to the header in the monitor (outside GC) // is install the hash code. If someone add new usage of // displaced header, please update this code hash = test->hash(); assert(test->is_neutral(), \"invariant\"); assert(hash != 0, \"Trivial unexpected object/monitor header usage.\"); } } // We finally get the hash return hash; } 没想到代码这么长，确实比 int var; return &var; 可以看到在get_next_hash函数中，有五种不同的hashCode生成策略。 第一种：是使用全局的os::random()随机数生成策略。os::random()的实现方式在os.cpp中，代码如下 void os::init_random(unsigned int initval) { _rand_seed = initval; } static int random_helper(unsigned int rand_seed) { /* standard, well-known linear congruential random generator with * next_rand = (16807*seed) mod (2**31-1) * see * (1) \"Random Number Generators: Good Ones Are Hard to Find\", * S.K. Park and K.W. Miller, Communications of the ACM 31:10 (Oct 1988), * (2) \"Two Fast Implementations of the 'Minimal Standard' Random * Number Generator\", David G. Carta, Comm. ACM 33, 1 (Jan 1990), pp. 87-88. */ const unsigned int a = 16807; const unsigned int m = 2147483647; const int q = m / a; assert(q == 127773, \"weird math\"); const int r = m % a; assert(r == 2836, \"weird math\"); // compute az=2^31p+q unsigned int lo = a * (rand_seed & 0xFFFF); unsigned int hi = a * (rand_seed >> 16); lo += (hi & 0x7FFF) m) { lo &= m; ++lo; } lo += hi >> 15; // if (p+q) overflowed, ignore the overflow and increment (p+q) if (lo > m) { lo &= m; ++lo; } return lo; } int os::random() { // Make updating the random seed thread safe. while (true) { unsigned int seed = _rand_seed; unsigned int rand = random_helper(seed); if (Atomic::cmpxchg(rand, &_rand_seed, seed) == seed) { return static_cast(rand); } } } 根据代码注解的提示，随机数的生成策略是一种线性取余方式生成的。 第二种：addrBits ^ (addrBits >> 5) ^ GVars.stwRandom。 这里是第一次 看到和地址相关的变量，addrBits通过调用cast_from_oop方法得到。 cast_from_oop实现在oopsHierarchy.cpp。具体代码如下 template inline oop cast_to_oop(T value) { return (oop)(CHECK_UNHANDLED_OOPS_ONLY((void *))(value)); } //以下部分内容来源于 oopsHierachy.hpp template inline T cast_from_oop(oop o) { return (T)(CHECK_UNHANDLED_OOPS_ONLY((void*))o); } 很遗憾的是我还是没有看到 cast_to_oop具体是怎么实现的，后面会更新的 第三种：敏感测试 value = 1; 第四种：自增序列 value = ++GVars.hcSequence; 官方将会默认。利用位移生成随机数 // Marsaglia's xor-shift scheme with thread-specific state // This is probably the best overall implementation -- we'll // likely make this the default in future releases. unsigned t = Self->_hashStateX; t ^= (t _hashStateX = Self->_hashStateY; Self->_hashStateY = Self->_hashStateZ; Self->_hashStateZ = Self->_hashStateW; unsigned v = Self->_hashStateW; v = (v ^ (v >> 19)) ^ (t ^ (t >> 8)); Self->_hashStateW = v; value = v; 最后来回答 一开始的问题。 1.hashCode 是怎么来的？——原来有很多，自增序列，随机数，内存地址。这里又有个新问题产生了，为什么不用时间戳了？ 2.可以预测值？——这很难说啊！ hash table（hash表）是什么 Hash表数据结构常识：   哈希表基于数组。   缺点：基于数组的，数组创建后难以扩展。某些哈希表被基本填满时，性能下降得非常严重。   没有一种简便得方法可以以任何一种顺序遍历表中数据项。   如果不需要有序遍历数据，并且可以提前预测数据量的大小，那么哈希表在速度和易用性方面是无与伦比的。 Hash表定义   根据关键码值（KEY-VALUE）而直接进行访问的数据结构；它通过把关键码值（KEY-VALUE）映射到表中一个位置来访问记录， 以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。 如何理解Hashcode的作用   以java.lang.Object来理解,JVM每new一个Object,它都会将这个Object丢到一个Hash哈希表中去, 这样的话,下次做Object的比较或者取这个对象的时候,它会根据对象的hashcode再从Hash表中取这个对象。这样做的目的是提高取对象的效率。具体过程是这样:   1.new Object(),JVM根据这个对象的Hashcode值,放入到对应的Hash表对应的Key上, 如果不同的对象确产生了相同的hash值,也就是发生了Hash key相同导致冲突的情况,那么就在这个Hash key的地方产生一个链表,将所有产生相同hashcode的对象放到这个单链表上去,串在一起。   2.比较两个对象的时候,首先根据他们的hashcode去hash表中找他的对象, 当两个对象的hashcode相同,那么就是说他们这两个对象放在Hash表中的同一个key上,那么他们一定在这个key上的链表上。 那么此时就只能根据Object的equal方法来比较这个对象是否equal。当两个对象的hashcode不同的话，肯定他们不能equal. 为什么HashCode对于对象是如此的重要   一个对象的HashCode就是一个简单的Hash算法的实现，虽然它和那些真正的复杂的Hash算法相比还不能叫真正的算法，如何实现它，不仅仅是程序员的编程水平问题， 而是关系到你的对象存取性能。有可能，不同的HashCode算法可能会使你的对象存取产生成百上千倍的性能差别. 先来看一下，在JAVA中两个重要的数据结构:HashMap和Hashtable，虽然它们有很大的区别，如继承关系不同，对value的约束条件(是否允许null)不同，以及线程安全性等有着特定的区别， 但从实现原理上来说，它们是一致的.所以，我们只以Hashtable来说明：   在java中，存取数据的性能，一般来说当然是首推数组，但是在数据量稍大的容器选择中，Hashtable将有比数组更高的查询速度.具体原因看下面的内容.   Hashtable在存储数据时，一般先将该对象的HashCode和0x7FFFFFFF做与操作，因为一个对象的HashCode可以为负数，这样操作后可以保证它为一个正整数.然后以Hashtable的长度取模，得到该对象在Hashtable中的索引. Hashtable中源码HashMap没有 java int hash = key.hashCode(); int index = (hash & 0x7FFFFFFF) % tab.length;   这个对象就会直接放在Hashtable的每index位置，但如果是查询，经过同样的算法，Hashtable可以直接从第index取得这个对象，而数组却要做循环比较.所以对于数据量稍大时，Hashtable的查询比数据具有更高的性能. 既然一个对象可以根据HashCode直接定位它在Hashtable中的位置，那么为什么Hashtable还要用key来做映射呢?这就是关系Hashtable性能问题的最重要的问题\"Hash冲突\". Hash冲突(Hash碰撞) "},"Chapter05/void.html":{"url":"Chapter05/void.html","title":"深入分析java中的关键字void","keywords":"","body":"深入分析java中的关键字void 在平时写代码的时候我们会经常用到void，我们都知道他代表着方法不返回任何东西， 但这只是表面意思，面试的时候也会经常会问到，这篇文章有必要对其进行一个深入的分析。 void关键字到底是什么类型？ java不像是php这些弱类型的语言，java语言是强类型的，意 思就是说我们的方法必须要有一个确定类型的返回值，举个例子 public String test(){}; 上面这个test方法有一个String类型的返回值，我们也可以返回int等基础类型的。 不管返回什么都要返回一个确定的类型。 现在！！！出现了一个问题，我们的方法也可以返回void，那么void肯定也是一种数据类型吧。 但是java好像只提供了两种数据类型：基本数据类型和引用数据类型 。那这个void到底是什么呢？ 其实你可以把他理解成一个特殊的数据类型也可以理解成一个方法的修饰符。 从Void看void 我们的基础类型好像都有一个封装类，比如int基本类型的封装类是Integer，char基本类型的封装类是Character， void也不例外，他也有一个封装类叫做Void，没错就是把“v”换成了大写的V。你可以这样去理解Void： 其实Void类是一个不可实例化的占位符类，用来保存一个引用代表Java关键字void的Class对象。 Void类型不可以继承和实例化。而且修饰方法时候必须返回null。 下面我们再来研究研究这个Void。 1、确定类型：Void是一个类，void就是一个基本类型 public class Test { public static void main(String[] args) { System.out.println(Void.class); System.out.println(void.class); } } //output //class java.lang.Void //void 2、 基本使用：必须且只能返回null public class Test { //返回void，return可有可无 public void a1() { return; } //必须且只能返回null public Void a2() { return null; } } 3、使用场景：在反射中确定某个函数的返回类型 public class Test { // 在这里定义两个方法： //（1）a方法返回void //（2）b方法返回int public void a() {} public int b() { return 1; } public static void main(String args[]) { for (Method method : Test.class.getMethods()) { if (method.getReturnType().equals(Void.TYPE)) { System.out.println(\"返回void的方法是：\"+method.getName()); } else if(method.getReturnType().equals(Integer.TYPE)) { System.out.println(\"返回int的方法是：\"+method.getName()); } } } } //output //返回void的方法是：main //返回int的方法是：b //返回void的方法是：a 4、使用场景：泛型中使用 Future用来保存结果。Future的get方法返回结果(类型为T)。但如果操作并没有返回值呢？这种情况下就可以用Future表示。当调用get后结果计算完毕则返回后将会返回null。 Void也用于无值的Map中，例如Map这样map将具Set有一样的功能。 "},"Chapter05/Static.html":{"url":"Chapter05/Static.html","title":"深入分析java中的关键字static","keywords":"","body":"深入分析java中的关键字static 在平时开发当中，我们经常会遇见static关键字。 这篇文章就把java中static关键字的使用方法的原理进行一个深入的分析。 先给出这篇文章的大致脉络： 首先，描述了static关键字去修饰java类、方法、变量、代码块的方法 然后，从底层分析static关键字 接下来，给出static的一些使用场景和案例 最后，对static进行一个总结，包括和普通变量的区分。 static关键字的基本用法 static关键字基本概念 我们可以一句话来概括：方便在没有创建对象的情况下来进行调用。 也就是说：被static关键字修饰的不需要创建对象去调用，直接根据类名就可以去访问。 对于这个概念，下面根据static关键字的四个基本使用来描述。 然后在下一部分再来去分析static的原理，希望你能认真读完。 static关键字修饰类 java里面static一般用来修饰成员变量或函数。 但有一种特殊用法是用static修饰内部类，普通类是不允许声明为静态的，只有内部类才可以。 下面看看如何使用。 public class StaticTest { //static关键字修饰内部类 public static class InnerClass{ InnerClass(){ System.out.println(\"============= 静态内部类=============\"); } public void InnerMethod() { System.out.println(\"============= 静态内部方法=============\"); } } public static void main(String[] args) { //直接通过StaticTest类名访问静态内部类InnerClass InnerClass inner=new StaticTest.InnerClass(); //静态内部类可以和普通类一样使用 inner.InnerMethod(); } } /* 输出是 * ============= 静态内部类============= * ============= 静态内部方法============= */ 如果没有用static修饰InterClass，则只能new 一个外部类实例。再通过外部实例创建内部类。 static关键字修饰方法 修饰方法的时候，其实跟类一样，可以直接通过类名来进行调用： public class StaticMethod { public static void test() { System.out.println(\"============= 静态方法=============\"); }; public static void main(String[] args) { //方式一：直接通过类名 StaticMethod.test(); //方式二： StaticMethod fdd=new StaticMethod(); fdd.test(); } } static关键字修饰变量 被static修饰的成员变量叫做静态变量，也叫做类变量，说明这个变量是属于这个类的，而不是属于是对象，没有被static修饰的成员变量叫做实例变量，说明这个变量是属于某个具体的对象的。 我们同样可以使用上面的方式进行调用变量： public class StaticVar { private static String name=\"java的架构师技术栈\"； public static void main(String[] args) { //直接通过类名 StaticVar.name; } } static关键字修饰代码块 静态代码块在类第一次被载入时执行，在这里主要是想验证一下，类初始化的顺序。 父类静态变量 父类静态代码块 子类静态变量 子类静态代码块 父类普通变量 父类普通代码块 父类构造函数 子类普通变量 子类普通代码块 子类构造函数 代码验证一下： 首先我们定义一个父类 public class Father{ //父类静态代码块 static{ System.out.println(\"Father static\"); } //父类构造函数 public Father(){ System.out.println(\"Father constructor\"); } } 然后定义一个子类 public class Son extends Father{ //静态代码块 static{ System.out.println(\"Son static\"); } //构造方法 public Son(){ System.out.println(\"Son constructor\"); } public static void main(String[] args) { new Son(); } } 看个结果 深入分析static关键字 上面我们只是描述了一下static关键字的基本使用场景，下面主要解析一下static关键字的深层原理。 要理解static为什么会有上面的特性，首先我们还需要从jvm内存说起。 我们先给出一张java的内存结构图，然后通过案例描述一下static修饰的变量存放在哪？ 从上图我们可以发现，静态变量存放在方法区中，并且是被所有线程所共享的。 这里要说一下java堆，java堆存放的就是我们创建的一个个实例变量。 堆区: 存储的全部是对象，每个对象都包含一个与之对应的class的信息。(class的目的是得到操作指令) jvm只有一个堆区(heap)被所有线程共享，堆中不存放基本类型和对象引用，只存放对象本身 栈区: 每个线程包含一个栈区，栈中只保存基础数据类型的对象和自定义对象的引用(不是对象)，对象都存放在堆区中 每个栈中的数据(原始类型和对象引用)都是私有的，其他栈不能访问。 栈分为3个部分：基本类型变量区、执行环境上下文、操作指令区(存放操作指令)。 方法区: 又叫静态区，跟堆一样，被所有的线程共享。方法区包含所有的class和static变量。 方法区中包含的都是在整个程序中永远唯一的元素，如class，static变量。 下面通过一个案例说明一下，从内存的角度来看，static关键字为什么会有这样的特性。 首先我们定义一个类 public class Person { //静态变量 static String firstName; String lastName; public void showName(){ System.out.println(firstName+lastName); } //静态方法 public static void viewName(){ System.out.println(firstName); } public static void main(String[] args) { Person p =new Person(); Person.firstName = \"张\"; p.lastName=\"三\"; p.showName(); Person p2 =new Person(); Person.firstName=\"李\"; p2.lastName=\"四\"; p2.showName(); } } //输出。张三、李四 接下来我们从内存的角度出发，看看 从上面可以看到，我们的方法在调用的时候，是从方法区调用的，但是堆内存不一样，堆内存中的成员变量lastname是随着对象的产生而产生。随着对象的消失而消失。静态变量是所有线程共享的，所以不会消失。这也就能解释上面static关键字的真正原因。 下面对static关键字进行一个小结： 特点： static是一个修饰符，用于修饰成员。（成员变量，成员函数）static修饰的成员变量 称之为静态变量或类变量。 static修饰的成员被所有的对象共享。 static优先于对象存在，因为static的成员随着类的加载就已经存在。 static修饰的成员多了一种调用方式，可以直接被类名所调用，（类名.静态成员）。 static修饰的数据是共享数据，对象中的存储的是特有的数据。 成员变量和静态变量的区别： 生命周期的不同： 成员变量随着对象的创建而存在随着对象的回收而释放。 静态变量随着类的加载而存在随着类的消失而消失。 调用方式不同： 成员变量只能被对象调用。 静态变量可以被对象调用，也可以用类名调用。（推荐用类名调用） 别名不同： 成员变量也称为实例变量。 静态变量称为类变量。 数据存储位置不同： 成员变量数据存储在堆内存的对象中，所以也叫对象的特有数据。 静态变量数据存储在方法区（共享数据区）的静态区，所以也叫对象的共享数据。 静态使用时需要注意的事项： 静态方法只能访问静态成员。（非静态既可以访问静态，又可以访问非静态） 静态方法中不可以使用this或者super关键字。 主函数是静态的 为什么java中静态方法不能调用非静态方法和变量？ 我们先看效果： 我们在静态方法main中调用非静态变量或者是方法都会报错。 我们反过来看看： 反过来没有一点问题，接下来我们解释一下原因： 原因解释 我们需要首先知道的是静态方法和静态变量是属于某一个类， 而不属于类的对象。我们不直接讲原因，先从jvm说起： 这是一张类加载的生命周期图。 加载 ”加载“是”类加机制”的第一个过程，在加载阶段，虚拟机主要完成三件事： 通过一个类的全限定名来获取其定义的二进制字节流 将这个字节流所代表的的静态存储结构转化为方法区的运行时数据结构 在堆中生成一个代表这个类的Class对象，作为方法区中这些数据的访问入口。 注意此时会扫描到我们的代码中是否有静态变量或者是静态方法等等这些静态数据结构，还未分配内存。 验证 验证的主要作用就是确保被加载的类的正确性。 准备 准备阶段主要为类变量分配内存并设置初始值。这些内存都在方法区分配。注意此时就会为我们的类变量也就是静态变量分配内存，但是普通成员变量还没。 解析 解析阶段主要是虚拟机将常量池中的符号引用转化为直接引用的过程。 初始化 这是类加载机制的最后一步，在这个阶段，java程序代码才开始真正执行。 我们知道，在准备阶段已经为类变量赋过一次值。在初始化阶端，程序员可以根据自己的需求来赋值了。初始化时候才会为我们的普通成员变量赋值。 写到这答案已经出来了，静态方法是属于类的，动态方法属于实例对象， 在类加载的时候就会分配内存，可以 通过类名直接去访问， 非静态成员（变量和方法）属于类的对象，所以只有该对象初始化之后才存在，然后通过类的对象去访问。 也就是说如果我们在静态方法中调用非静态成员变量会超前， 可能会调用了一个还未初始化的变量。因此编译器会报错。 静态类 静态类（只有内部类才能被声明为静态类，即静态内部类） 只能在内部类中定义静态类 静态内部类与外层类绑定，即使没有创建外层类的对象，它一样存在。 静态类的方法可以是静态的方法也可以是非静态的方法，静态的方法可以在外层通过静态类调用，而非静态的方法必须要创建类的对象之后才能调用。 只能引用外部类的static成员变量（也就是类变量）。 如果一个内部类不是被定义成静态内部类，那么在定义成员变量或者成员方法的时候，是不能够被定义成静态的。 总结 是否能拥有静态成员：静态内部类可以有静态成员(方法，属性)，而非静态内部类则不能有静态成员(方法，属性)。 访问外部类的成员：静态内部类只能够访问外部类的静态成员,而非静态内部类则可以访问外部类的所有成员(方法，属性)。 静态内部类和非静态内部类在创建时有区别 假设类A有静态内部类B和非静态内部类C，创建B和C的区别为： A a=new A(); A.B b=new A.B(); A.C c=a.new C(); Java 局部变量 局部变量声明在方法、构造方法或者语句块中； 局部变量在方法、构造方法、或者语句块被执行的时候创建，当它们执行完成后，变量将会被销毁； 访问修饰符不能用于局部变量； 局部变量只在声明它的方法、构造方法或者语句块中可见； 局部变量是在栈上分配的。 局部变量没有默认值，所以局部变量被声明后，必须经过初始化，才可以使用。 实例变量 实例变量声明在一个类中，但在方法、构造方法和语句块之外；当一个对象被实例化之后，每个实例变量的值就跟着确定； 实例变量在对象创建的时候创建，在对象被销毁的时候销毁； 实例变量的值应该至少被一个方法、构造方法或者语句块引用，使得外部能够通过这些方式获取实例变量信息； 实例变量可以声明在使用前或者使用后； 访问修饰符可以修饰实例变量； 实例变量对于类中的方法、构造方法或者语句块是可见的。一般情况下应该把实例变量设为私有。通过使用访问修饰符可以使实例变量对子类可见； 实例变量具有默认值。数值型变量的默认值是0，布尔型变量的默认值是false，引用类型变量的默认值是null。变量的值可以在声明时指定，也可以在构造方法中指定； 实例变量可以直接通过变量名访问。但在静态方法以及其他类中，就应该使用完全限定名：ObejectReference.VariableName。 类变量（静态变量） 类变量也称为静态变量，在类中以static关键字声明，但必须在方法构造方法和语句块之外。 无论一个类创建了多少个对象，类只拥有类变量的一份拷贝。 静态变量除了被声明为常量外很少使用。常量是指声明为public/private，final和static类型的变量。常量初始化后不可改变。 静态变量储存在静态存储区。经常被声明为常量，很少单独使用static声明变量。 静态变量在程序开始时创建，在程序结束时销毁。 与实例变量具有相似的可见性。但为了对类的使用者可见，大多数静态变量声明为public类型。 默认值和实例变量相似。数值型变量默认值是0，布尔型默认值是false，引用类型默认值是null。变量的值可以在声明的时候指定，也可以在构造方法中指定。此外，静态变量还可以在静态语句块中初始化。 静态变量可以通过：ClassName.VariableName的方式访问。 类变量被声明为public static final类型时，类变量名称一般建议使用大写字母。如果静态变量不是public和final类型，其命名方式与实例变量以及局部变量的命名方式一致。 "},"Chapter05/final.html":{"url":"Chapter05/final.html","title":"深入分析java中的关键字final","keywords":"","body":"深入分析java中的关键字final 对于final大家从字面意思就能看出来，主要是“最终的不可改变的意思”。 可以修饰类、方法和变量。先给出这篇文章的大致脉络 首先，先给出final关键字的三种使用场景，也就是修饰类，方法和变量 然后，深入分析final关键字主要注意的几个问题 最后，总结一下final关键字 final关键字的基本使用 认识final关键字 final可以修饰类、方法、变量。那么分别是什么作用呢？ 修饰类：表示类不可被继承 修饰方法：表示方法不可被覆盖 修饰变量：表示变量一旦被赋值就不可以更改它的值。java中规定final修饰成员变量必须由程序员显示指定变量的值。 final关键字修饰类 final关键字修饰类表示这个类是不可被继承的，如何去验证呢？ final关键字修饰方法 final修饰的方法不能被重写。但是可以重载。 下面给出了一个代码例子。主要注意的是：父类中private的方法， 在子类中不能访问该方法，但是子类与父类private方法相同的方法名、 形参列表和返回值的方法，不属于方法重写，只是定义了一个新的方法。 public class FinalClass{ public final void test(){} public final void test(int i){} } final关键字修饰变量 final关键字修饰变量，是比较麻烦的。但是我们只需要对其进行一个分类介绍就能理解清楚了。 修饰成员变量 如果final修饰的是类变量，只能在静态初始化块中指定初始值或者声明该类变量时指定初始值。 如果final修饰的是成员变量，可以在非静态初始化块、声明该变量或者构造器中执行初始值。 修饰局部变量 系统不会为局部变量进行初始化，局部变量必须由程序员显示初始化。 因此使用final修饰局部变量时，即可以在定义时指定默认值（后面的代码不能对变量再赋值）， 也可以不指定默认值，而在后面的代码中对final变量赋初值（仅一次）。 下面使用代码去验证一下这两种情况 public class FinalVar { final static int a = 0;//再声明的时候就需要赋值 public static void main(String[] args) { final int localA; //局部变量只声明没有初始化，不会报错,与final无关。 localA = 0;//在使用之前一定要赋值 //localA = 1; 但是不允许第二次赋值 } } 修饰基本类型数据和引用类型数据 如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改； 如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。但是引用的值是可变的。 修饰基本类型的数据，在上面的代码中基本上能够看出，下面主要是描述引用类型的变量 public class FinalReferenceTest{ public static void main(){ final int[] iArr={1,2,3,4}; iArr[2]=-3;//合法 iArr=null;//非法，对iArr不能重新赋值 final Person p = new Person(25); p.setAge(24);//合法 p=null;//非法 } } final关键字需要注意的几个问题 final和static的区别 其实如果你看过我上一篇文章，基本上都能够很容易得区分开来。 static作用于成员变量用来表示只保存一份副本，而final的作用是用来保证变量不可变。 下面代码验证一下 public class FinalTest { public static void main(String[] args) { AA aa1 = new AA(); AA aa2 = new AA(); System.out.println(aa1.i); System.out.println(aa1.j); System.out.println(aa2.i); System.out.println(aa2.j); } } //j值两个都一样，因为是static修饰的,全局只保留一份 //i值不一样，两个对象可能产生两个不同的值， class AA { public final int i = (int) (Math.random()*100); public static int j = (int) (Math.random()*100); } //结果是 65、23、67、23 为什么局部内部类和匿名内部类只能访问局部final变量？ 为了解决这个问题，我们先要去使用代码去验证一下。 public class Test { public static void main(String[] args) { } //局部final变量a,b public void test(final int b) { final int a = 10; //匿名内部类 new Thread(){ public void run() { System.out.println(a); System.out.println(b); }; }.start(); } } 上段代码中，如果把变量a和b前面的任一个final去掉，这段代码都编译不过。 这段代码会被编译成两个class文件：Test.class和Test1.class。默认情况下，编译器会为匿名内部类和局部内部类起名为Outter1.class。 原因是为什么呢？这是因为test()方法里面的参数a和b，在运行时，main线程快要结束，但是thread还没有开始。因此需要有一种机制，在使得运行thread线程时候能够调用a和b的值，怎办呢？java采用了一种复制的机制， 也就说如果局部变量的值在编译期间就可以确定，则直接在匿名内部里面创建一个拷贝。如果局部变量的值无法在编译期间确定，则通过构造器传参的方式来对拷贝进行初始化赋值。 总结 final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 细节七、写 final 域的重排序规则，你知道吗？ 这个规则是指禁止对 final 域的写重排序到构造函数之外，这个规则的实现主要包含了两个方面： JMM 禁止编译器把 final 域的写重排序 到 构造函数 之外 编译器会在 final 域写之后，构造函数 return 之前，插入一个 StoreStore 屏障。这个屏障可以禁止处理器把 final 域的写重排序到构造函数之外 给举个例子，要不太抽象了，先看一段代码 public class FinalTest{ private int a; //普通域 private final int b; //final域 private static FinalTest finalTest; public FinalTest() { a = 1; // 1. 写普通域 b = 2; // 2. 写final域 } public static void writer() { finalTest = new FinalTest(); } public static void reader() { FinalTest demo = finalTest; // 3.读对象引用 int a = demo.a; //4.读普通域 int b = demo.b; //5.读final域 } } 假设线程 A 在执行 writer()方法，线程 B 执行 reader()方法。 由于变量 a 和变量 b 之间没有依赖性，所以就有可能会出现下图所示的重排序 由于普通变量 a 可能会被重排序到构造函数之外，所以线程 B 就有可能读到的是普通变量 a 初始化之前的值（零值），这样就可能出现错误。 而 final 域变量 b，根据重排序规则，会禁止 final 修饰的变量 b 重排序到构造函数之外，从而 b 能够正确赋值，线程 B 就能够读到 final 域变量 b初始化后的值。 结论：写 final 域的重排序规则可以确保在对象引用为任意线程可见之前，对象的 final 域已经被正确初始化过了，而普通域就不具有这个保障。 细节八：读 final 域的重排序规则，你知道吗？ 这个规则是指在一个线程中，初次读对象引用和初次读该对象包含的 final 域，JMM 会禁止这两个操作的重排序。 还是上面那段代码 public class FinalTest{ private int a; //普通域 private final int b; //final域 private static FinalTest finalTest; public FinalTest() { a = 1; // 1. 写普通域 b = 2; // 2. 写final域 } public static void writer() { finalTest = new FinalTest(); } public static void reader() { FinalTest demo = finalTest; // 3.读对象引用 int a = demo.a; //4.读普通域 int b = demo.b; //5.读final域 } } 假设线程 A 在执行 writer()方法，线程 B 执行 reader()方法。 线程 B 可能就会出现下图所示的重排序 可以看到，由于读对象的普通域被重排序到了读对象引用的前面，就会出现线程 B 还未读到对象引用就在读取该对象的普通域变量，这显然是错误的操作。而 final 域的读操作就“限定”了在读 final 域变量前已经读到了该对象的引用，从而就可以避免这种情况。 结论：读 final 域的重排序规则可以确保在读一个对象的 final 域之前，一定会先读包含这个 final 域的对象的引用。 "},"Chapter05/enum.html":{"url":"Chapter05/enum.html","title":"深入分析java中的enum","keywords":"","body":"深入分析java中的enum 枚举类介绍 如果一个类的实例是有限且确定的，那么可以使用枚举类。比如：季节类，只有春夏秋冬四个实例。 枚举类使用enum进行创建，其实例必须从”第一行“开始显示写出。 enum Season{ 　　 SPRING,SUMMER,FALL,WINTER; } 枚举类的构造器都是private,所以无法在外部创建其实例，这也决定了枚举类实例的个数的确定性（写了几个就是几个）。 enum类默认extends java.lang.Enum,所以无法再继承其他类 enum为什么不能被继承 有一种说法是 枚举类的对象默认都是public static final。 这个经过实验是不对的，因为枚举类不能被final修饰。这个再编译阶段就是错误。 但枚举类使用enum定义后在编译后默认继承了java.lang.Enum类，而不是普通的继承Object类。 enum声明类继承了Serializable和Comparable两个接口。且采用enum声明后， 该类会被编译器加上\"final\"声明（同String），故该类是无法继承的。 枚举类的内部定义的枚举值就是该类的实例（且必须在第一行定义，当类初始化时，这些枚举值会被实例化）。 由于这些枚举值的实例化是在类初始化阶段，所以应该将枚举类的构造器（如果存在），采用private声明（这种情况下默认也是private）。 "},"Chapter05/this.html":{"url":"Chapter05/this.html","title":"深入分析java中的关键字this","keywords":"","body":"深入分析java中的关键字this 为什么要引入this关键字？ 现在出现一个问题，就是你希望在方法的内部去获得当前对象的引用。 现在java提供了一个关键字this。他就表示当前对象的引用。 使用this关键字 一个方法调用同一个类的另外一个方法， 这种情况是不需要使用this的。直接使用即可。 class MyClass{ void f1(){}; void f2(){ f1(); } } 当成员变量和局部变量重名时，在方法中使用this时，表示的是该方法所在类中的成员变量。（this是当前对象自己） public class Hello { String s = \"Hello\";//这里的S与Hello()方法里面的成员变量重名 public Hello(String s) { System.out.println(\"s = \" + s); System.out.println(\"1 -> this.s = \" + this.s); this.s = s;//把参数值赋给成员变量，成员变量的值改变 System.out.println(\"2 -> this.s = \" + this.s); } public static void main(String[] args) { Hello x = new Hello(\"HelloWorld!\"); System.out.println(\"s=\" + x.s);//验证成员变量值的改变 } } 在这个例子中，构造函数Hello中，参数s与类Hello的成员变量s同名， 这时如果直接对s进行操作则是对参数s进行操作。 若要对类Hello的成员变量s进行操作就应该用this进行引用。 运行结果的第一行就是直接对构造函数中传递过来的参数s进行打印结果； 第二行是对成员变量s的打印； 第三行是先对成员变量s赋传过来的参数s值后再打印， 所以结果是HelloWorld! 而第四行是主函数中直接打印类中的成员变量的值，也可以验证成员变量值的改变。 把自己当作参数传递时，也可以用this.(this作当前参数进行传递) class A { public A() { new B(this).print();// 调用B的方法 } public void print() { System.out.println(\"HelloAA from A!\"); } } class B { A a; public B(A a) { this.a = a; } public void print() { a.print();//调用A的方法 System.out.println(\"HelloAB from B!\"); } } public class HelloA { public static void main(String[] args) { A aaa = new A(); aaa.print(); B bbb = new B(aaa); bbb.print(); } } 在这个例子中，对象A的构造函数中，用new B(this)把对象A自己作为参数传递给了对象B的构造函数。 当在匿名类中用this时，这个this则指的是匿名类或内部类本身。 这时如果我们要使用外部类的方法和变量的话，则应该加上外部类的类名。如： public class HelloB { int i = 1; public HelloB() { Thread thread = new Thread() { public void run() { for (int j=0;j 在上面这个例子中, thread 是一个匿名类对象，在它的定义中， 它的 run 函数里用到了外部类的 run 函数。这时由于函数同名，直接调用就不行了。 这时有两种办法，一种就是把外部的 run 函数换一个名字， 但这种办法对于一个开发到中途的应用来说是不可取的。 那么就可以用这个例子中的办法用外部类的类名加上 this 引用来说明要调用的是外部类的方法 run。 在构造函数中，通过this可以调用同一类中别的构造函数。如： public class ThisTest { ThisTest(String str) { System.out.println(str); } ThisTest() { this(\"this测试成功！\"); } public static void main(String[] args) { ThisTest thistest = new ThisTest(); } } 为了更确切的说明this用法，另外一个例子为： public class ThisTest { private int age; private String str; ThisTest(String str) { this.str=str; System.out.println(str); } ThisTest(String str,int age) { this(str); this.age=age; System.out.println(age); } public static void main(String[] args) { ThisTest thistest = new ThisTest(\"this测试成功\",25); } } 值得注意的是： 　　1：在构造调用另一个构造函数，调用动作必须置于最起始的位置。 　　2：不能在构造函数以外的任何函数内调用构造函数。 　　3：在一个构造函数内只能调用一个构造函数。这一点在第二个构造方法内可以看到，第一个this(str)，第二个为this.age=age； this同时传递多个参数 public class TestClass { int x; int y; static void showtest(TestClass tc) {//实例化对象 System.out.println(tc.x + \" \" + tc.y); } void seeit() { showtest(this); } public static void main(String[] args) { TestClass p = new TestClass(); p.x = 9; p.y = 10; p.seeit(); } } "},"Chapter05/super.html":{"url":"Chapter05/super.html","title":"深入分析java中的关键字super","keywords":"","body":"深入分析java中的关键字super 概念 它是一个指代变量，用于在子类中指代父类对象。 应用范围 super的三种使用情况： 访问父类的方法。 调用父类构造方法。 访问父类中的隐藏成员变量。 使用 访问父类中的方法 第一步：定义father类 public class Father { private String father_a; public Father() { father_a=\"父亲：曹操\"; System.out.println(\"我是，\"+father_a); } void dosomething(){ System.out.println(\"曹操：挟天子以令诸侯\"); } } 第二步：定义son类 public class Son extends Father { private String son_a; public Son() { super(); son_a=\"儿子：曹冲\"; System.out.println(\"我是，\"+son_a); } @Override void dosomething() { //访问父类中的方法 super.dosomething(); } } 第三步：测试一下 public class Test { public static void main(String[] args) { Son son=new Son(); son.dosomething(); } } //output： //我是，父亲：曹操 //我是，儿子：曹冲 //曹操：挟天子以令诸侯 我们会发现调用了super.dosomething();就会执行父类的dosomething方法。 重写父类变量 我们还拿上面的例子来说明一下： 第一步：定义父类 public class Father { protected String father_a=\"我是父亲曹操的变量\"; void dosomething(){ System.out.println(\"曹操：挟天子以令诸侯\"); } } 第二步：定义子类 public class Son extends Father { private String son_a=\"我是儿子曹冲的变量\"; @Override void dosomething() { //super.father_a可以调用父类的变量（public和protected） System.out.println(super.father_a); System.out.println(son_a); } } 第三步：测试一下 public class Test { public static void main(String[] args) { Son son=new Son(); son.dosomething(); } } //output //我是父亲曹操的变量 //我是儿子曹冲的变量 子类构造方法调用 第一步：定义父类 public class Father { protected String father_a=\"我是父亲曹操的变量\"; //无参构造器 public Father() { System.out.println(\"无参构造器:\"+father_a); } //有参构造器 public Father(String father_a) { this.father_a = father_a; System.out.println(\"有参构造器:\"+father_a); } } 第二步：定义子类 public class Son extends Father { public Son() { //在子类中调用父类构造器 //第一种：super（执行父类中的无参构造器，默认可以不写） //super(); //第二种：执行父类中的有参构造器，参数写到super中就可以了 super(\"子类传给父类的\"); System.out.println(\"子类的构造方法\"); } } 第三步：测试 public class Test { public static void main(String[] args) { Son son=new Son(); } } //output //有参构造器:子类传给父类的 //子类的构造方法 从上面我们可以看到，其实是有两种情况 第一种：直接调用super()会执行父类的无参构造方法，可以默认不写。 第二种：使用super(“父类参数”),调用父类有参构造方法，把参数传递进来就好。 这两种情况还是比较简单的，不过还有几种比较特殊的情况需要我们去注意， 网上的大神也都提到了，你可以试着去了解一下。 注意问题一：父类只有带参构造器（无参构造器没有），子类必须有相同参数的构造方法 我们还是使用代码去测试一下 首先我们定义一个父类：只有一个带参构造器 public class Father { protected String father_a=\"我是父亲曹操的变量\"; //有参构造器 public Father(String father_a) { this.father_a = father_a; System.out.println(\"有参构造器:\"+father_a); } } 下面我们看看在子类中，如果什么都没有会出现什么 意思已经很明确了，我们的子类必须要有一个显示的构造方法去匹配父类。 注意问题二：子类必须有相同参数的构造方法，并且还需要调用super(参数) 在注意问题一种我们知道，在子类中需要定义一个构造方法去匹配父类构造方法， 现在我们在子类中去定义一下不就解决了嘛，但随之而来由出来了个问题，我们看看。 为什么会出现犯错误呢？其实我们只是定义了Son类的无参构造器， 其默认调用super(),他只是调用了父类的无参构造器，并没有调用有参构造器， 因此需要我们再去调用一下有参的。 我们把son类中的构造方法中加上一句话就好了。 "},"Chapter05/String.html":{"url":"Chapter05/String.html","title":"深入分析java中的String","keywords":"","body":"深入分析java中的String 想要完全了解String，在这里我们需要解决以下几个问题 什么是不可变对象？ String如何被设计成不可变对象的？ 有什么办法能够改变String？ JAVA语言为什么把String类型设计成不可变？ 什么是不可变对象 从字面意思也能够理解，也就是我们的创建的对象不可改变。 那什么是不可变呢？为了实现创建的对象不可变，java语言要求我们需要遵守以下5条规则： 类内部所有的字段都是final修饰的。 类内部所有的字段都是私有的，也就是被private修饰。 类不能够被集成和拓展。 类不能够对外提供哪些能够修改内部状态的方法，setter方法也不行。 类内部的字段如果是引用，也就是说可以指向可变对象，那我们程序员不能获取这个引用。 正是由于我们的String类型遵循了上面5条规则，所以才说String对象是不可变的。 想要去了解他还是看看String类型内部长什么样子再来看上面5条规则吧。 String如何被设计成不可变对象的 疑惑一 在看之前，我们先给出一个疑惑问题，我们看下面的代码， public class Test2 { public static void main(String[] args) { String a=\"张三\"; System.out.println(a); a=\"李四\"; System.out.println(a); } } //output: //张三 //李四 在文章一开始我们就说了，String对象是不可变的，这里a=张三，然后a=李四， 这符合String的不可变性嘛？答案是当然符合。 源码解释疑惑 既然a指向的引用地址改变了，那么其String内部肯定有一个变量，能够指向不同的实际对象， 想要进一步弄清楚我们就进入其String的内部来看看。 我们在这里主要通过String类的源码来分析，看一下Java语言是如何设计， 能把String类型设计成不可变的。这里给出的是jdk1.8的一部分源码。 public final class String implements java.io.Serializable, Comparable, CharSequence { /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 ...... } 上面最主要的是两个字段：value和hash。我们在这里主要是看value数组，hash和主题无关所以这里不再讲解了， 我有专门的文章介绍hash。 我们的String对象其实在内部就是一个个字符然后存储在这个value数组里面的。 但是value对外没有setValue的方法，所以整个String对象在外部看起来就是不可变的。我们画一张图解释一下上面的疑惑 现在明白了吧，也就是说真正改变引用的是value，因为value也是一个数组引用。 这也可以很方便的解释下一个疑惑问题了。 疑惑二 既然我们的String是不可变的，好像内部还有很多substring， replace， replaceAll这些操作的方法。好像都是对String对象改变了，解释起来也很简单，我们每次的replace这些操作，其实就是在堆内存中创建了一个新的对象。然后我们的value指向不同的对象罢了。 面试的时候我们只是解释上面的原因其实不是那么尽善尽美，想要更好的去加薪去装逼，我们还需更进一步回答。 有什么办法能够改变String 既然有这个标题。那肯定就是有办法的，别忘了我们的反射机制， 在通常情况下，他可以做出一些违反语言设计原则的事情。 这也是一个技巧，每当面试官问一些违反语言设计原则的问题， 你就可以拿反射来反驳他。下面我们来看一下： public class Test2 { public static void main(String[] args) { String str = \"张三\"; System.out.println(str); try { //我们通过反射获取内部的value字符数组 Field field = String.class.getDeclaredField(\"value\"); field.setAccessible(true); char[] value; value = (char[]) field.get(str); //把字符串第一个字符变成王 value[0] = '王'; System.out.println(str); } catch (Exception e) { e.printStackTrace(); } } } //output: //张三 //王三 我们可以通过反射来改变String。 JAVA语言为什么把String类型设计成不可变 这里有几个特点。 第一：在Java程序中String类型是使用最多的，这就牵扯到大量的增删改查，每次增删改差之前其实jvm需要检查一下这个String对象的安全性， 就是通过hashcode，当设计成不可变对象时候，就保证了每次增删改查的hashcode的唯一性，也就可以放心的操作。 第二：网络连接地址URL,文件路径path通常情况下都是以String类型保存, 假若String不是固定不变的,将会引起各种安全隐患。 就好比我们的密码不能以String的类型保存，如果你将密码以明文的形式保存成字符串，那么它将一直留在内存中，直到垃圾收集器把它清除。 而由于字符串被放在字符串缓冲池中以方便重复使用，所以它就可能在内存中被保留很长时间，而这将导致安全隐患 第三：字符串值是被保留在常量池中的，也就是说假若字符串对象允许改变,那么将会导致各种逻辑错误 java 字符串String的最大长度 java字符串String的最大长度，要分两个阶段，编译阶段及运行时阶段 编译阶段： 在我们使用字符串字面量直接定义String的时候，会把字符串在常量池中存储一份。 常量池中的每一项常量都是一个表，都有自己对应的类型。String类型，有一张固定长度的CONSTANT_String_info表用来存储文字字符串值， 注意：该表只存储文字字符串值，不存储符号引用。 JVM的常量池最多可放65535个项。第0项不用。最后一项最多只能是65534(下标值)。而每一项中，若是放一个UTF-8的常量串， 其长度最长是：65535个字节(不是字符)。 运行时阶段： String内部是以char数组的形式存储，数组的长度是int类型，那么String允许的最大长度就是Integer.MAX_VALUE了,2147483647； 又由于java中的字符是以16位存储的，因此大概需要4GB的内存才能存储最大长度的字符串。 format() 转换符 详细说明 示例 %s 字符串类型 “喜欢请收藏” %d 整数类型（十进制） 88 %b 布尔类型 true %% 百分比类型 ％(%特殊字符%%才能显示%) %n 换行符 不举例(基本用不到) "},"Chapter05/wait-notify.html":{"url":"Chapter05/wait-notify.html","title":"深入理解wait和notify","keywords":"","body":"深入理解wait和notify 生产者和消费者模式：有个盛装数据的容器(list)即缓冲区, 一个往容器里放数据即生产者,一个从容器中取数据即消费者。 但当容器满的时候，生产者就不能往里放东西了，此时需要等待缓冲区不满， 即有消费者从容器中取出数据了，这就需要一个等待和通知的功能了。 这个功能我当时是直接使用Java 中 Object 定义的wait、notify和notifyAll来实现的。 如果线程调用了Object对象的wait()方法，那么线程会处于该对象的等待池中，等待池中的线程不会去竞争该对象的锁。 当有线程调用了Object对象的notify()方法（只随机唤醒一个wait线程）或是notifyAll()方法（唤醒所有wait线程） 被唤醒的的线程会进入该对象的锁池中，锁池中的线程会去竞争该对象锁。 优先级高的线程竞争到对象锁的概率大，假若某线程没有竞争到该对象锁，它还会留在锁池中， 只有线程再次调用wait()方法，它才会重新回到等待池中。而竞争到对象锁的线程则继续往下执行，直到执行完了synchronized代码块， 它会释放掉该对象锁，这时锁池中的线程会继续竞争该对象锁 有问题却一眼看不出问题的消费者和生产者模式 下文将使用Java 实现一个简单的消费者和生产者模式，代码如下： public class Producer { //缓冲区 private CircleQueue cache; public Producer(CircleQueue cache) { this.cache = cache; } public synchronized void produce(String e) throws InterruptedException { if (cache.isFull()) { cache.wait(); } cache.put(e); cache.notifyAll(); } } public class Consumer { private CircleQueue cache; public Consumer(CircleQueue container) { this.cache = container; } public synchronized void consume() throws InterruptedException { if (cache .isEmpty()) { cache.wait(); } System.out.println( String.format(\"thread:%s,consume a element:%s\", Thread.currentThread().getName(), cache .take())); cache.notifyAll(); } } 大家看了一遍，有没有看出来上面代码运行会报错？运行一下测试代码(见最下面)大家就明白了： 生产者执行produce 函数的时候，执行到cache.notifyAll() 会抛出这个异常，这个异常代表什么意思呢？ 我们来看看IllegalMonitorStateException 的注释： Thrown to indicate that a thread has attempted to wait on an object's monitor or to notify other threads waiting on an object's monitor without owning the specified monitor. 上面这个注释, 它想表达两个意思 在不拥有当前对象监视器的情况下调用当前对象的wait方法 在不拥有当前对象监视器的情况下调用当前对象的notify和notifyAll 上面报错的原因就是因为满足了第2条,其实同样 cache.wait() 这里也有问题。下面我们来改下代码： public void produce(String e) throws InterruptedException { synchronized (cache) { if (cache.isFull()) { cache .wait(); } cache.put(e); cache.notifyAll(); } } public void consume() throws InterruptedException { synchronized (cache) { if (cache.isEmpty()) { cache.wait(); } System.out.println( String.format(\"thread:%s,consume a element:%s\", Thread.currentThread().getName(), cache.take())); cache.notifyAll(); } } 上面只粘贴了主要的代码，synchronized 关键字修饰的是cache,因此当线程进来的时候获取的是cache对象的监视器， 因此到下面无论执行cache.wait()还是cache.notifyAll() 都不会抛出异常。 当然上面的代码还是有问题的，不知道大家有没有看出来？我们来执行下测试代码，发现报错了， 但是也有可能你执行的时候并没有报错，因为这是多线程，存在很多偶然性，多运行几遍你就会发现你中奖了，报错如下： 上面报错的是Consumer 中的cache.take()处抛的异常， cache对象的类型CircleQueue 是我自己实现的一个环形队列,抛出该异常是因为队列已空， 如果这样大家就奇怪了，能执行到cache.take() 不是因为队列不为空才唤醒当前线程的吗？ 我先介绍下当某个线程调用wait的时候发生了什么？ 它会释放调用对象上的监视器即锁，然后进入一个条件等待队列中等待被唤醒， 此时有别的线程改变了状态(eg: 队列为空，队列已满),然后调用notifyAll(), 条件等待队列中的线程再次获取调用对象上的监视器，然后继续向下执行。 画一个草图给大家理解下： 在一个线程被唤醒到获取锁的这个时间里， 可能有另外一个线程改变了状态 ( 往队列插入元素或者从队列中取出元素 ), 然后该线程往下执行的时候发现缓冲区为空或者已满这种现象。 大家估计也想到了解决办法，就是被唤醒再次获取锁之后再判断一次状态， 加一个循环就搞定了，代码如下： public void consume() throws InterruptedException { synchronized (cache) { // 这里把if改为while 就可以了 while (cache.isEmpty()) { cache.wait(); } System.out.println( String.format(\"thread:%s,consume a element:%s\", Thread.currentThread().getName(), cache.take())); cache.notifyAll(); } } 为啥需要操作同一个共享资源cache对象呢? 先说明： object类的wait/notify 方法都是native方法,需要额外下载jvm的源码才可以看到传送门：openJDK 下载， 打开源码：openjdk\\hotspot\\src\\share\\vm\\runtime 目录下objectMonitor.cpp objectMonitor.hpp 2个文件， 这些属于c语言源码。 其中在objectMonitor.hpp文件中定义一个非常重要的对象ObjectMonitor对象 ObjectMonitor对象: 主要用来监视创立的Object , 说白了就是我们经常说的所对象在jvm中的具体化(c++的结构体， 单纯看做java对象就可以) 上面对象结构体中有4个非常关键的属性 _owner：指向于当前持有锁的线程 _object：指向例子中的shop对象 _WaitSet：调用wait方法之后的线程会被挂入到这个队列 _EntryList：等待获取锁(那个shop对象锁)的线程，被挂入到这个队列。 注意：_WaitSet跟_EntryList 实现类是ObjectWaiter,是一个双向链表。 调用wait方法;在objectMonitor.cpp 文件中，有个方法： 去掉一些看不懂， 留下几行比较有用的操作 ObjectWaiter node(Self);java线程调用shop.wait方法,jvm马上当前线程进行封装成一个ObjectWaiter对象 AddWaiter(&node);接着将该对象添加到_WaitSet队列中 _waiters++; 队列的长度加1 exit (true, Self) 这里注意， 直接退出，这里表示马上释放锁。也就是当线程调用了wait方法之后， 马上释放对象锁。 Self->_ParkEvent->park () ; 表示停止，等待， 也就是挂起来， 等待被唤醒。 调用notify方法,在objectMonitor.cpp 文件中，有个方法： 总结：调用wait方法之后， 会将线程挂起， 暂存到_waitSet队列中，然后释放锁。 调用了DequeueWaiter方法 再调用DequeueSpecificWaiter方法 总结：调用notify方法之后， 会从_waitSet队列中随机唤醒一个线程， 然后暂存到_entryList队列中， 参与cpu争夺。注意此时还没有释放锁。 需要等notify方法调用方法执行完之后再释放。 "},"Chapter05/Transient.html":{"url":"Chapter05/Transient.html","title":"Transient","keywords":"","body":"transient 初识transient关键字 其实这个关键字的作用很好理解，就是简单的一句话： 将不需要序列化的属性前添加关键字transient，序列化对象的时候， 这个属性就不会被序列化。 概念也很好理解，下面使用代码去验证一下： package keyword; import lombok.Getter; import lombok.Setter; import lombok.ToString; import java.io.*; public class FireTransient { public static void main(String[] args) throws IOException, ClassNotFoundException { serializeUser(); deSerializeUser(); } private static void serializeUser() throws IOException { File file = new File(\"/opt/test/\"); if (!file.exists()){ file.mkdirs(); } User u = new User(); u.setAge(10); u.setName(\"fire\"); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"/opt/test/template\")); oos.writeObject(u); oos.close(); } private static void deSerializeUser() throws IOException, ClassNotFoundException { File file = new File(\"/opt/test/template\"); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file)); User u = (User) ois.readObject(); System.out.println(u.toString()); } } @Getter @Setter @ToString class User implements Serializable{ private transient int age; private String name; } 从上面可以看出，在序列化SerializeUser方法中， 首先创建一个序列化user类， 然后将其写入到/opt/test/template\"路径中。 在反序列化DeSerializeUser方法中， 首先创建一个File，然后读取/opt/test/template\"路径中的数据。 这就是序列化和反序列化的基本实现 ，而且我们看一下结果， 也就是被transient关键字修饰的age属性是否被序列化。 D:\\Java\\jdk1.8.0_161\\bin\\java.exe \"-javaagent:D:\\JetBrains\\IntelliJ IDEA 2019.3.3\\lib\\idea_rt.jar=62658:D:\\JetBrains\\IntelliJ IDEA 2019.3.3\\bin\" -Dfile.encoding=UTF-8 -classpath D:\\Java\\jdk1.8.0_161\\jre\\lib\\charsets.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\deploy.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\access-bridge-64.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\cldrdata.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\dnsns.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\jaccess.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\jfxrt.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\localedata.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\nashorn.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunec.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunjce_provider.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunmscapi.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunpkcs11.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\zipfs.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\javaws.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jce.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jfr.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jfxswt.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jsse.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\management-agent.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\plugin.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\resources.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\rt.jar;D:\\github\\program\\target\\classes;D:\\firerepository\\org\\projectlombok\\lombok\\1.16.22\\lombok-1.16.22.jar keyword.FireTransient User(age=0, name=fire) Process finished with exit code 0 从上面的这张图可以看出，age属性变为了0，说明被transient关键字修饰之后没有被序列化。 深入分析transient关键字 为了更加深入的去分析transient关键字，我们需要带着几个问题去解读： transient底层实现的原理是什么？ 被transient关键字修饰过得变量真的不能被序列化嘛？ 静态变量能被序列化吗？被transient关键字修饰之后呢？ transient底层实现原理是什么？ java的serialization提供了一个非常棒的存储对象状态的机制， 说白了serialization就是把对象的状态存储到硬盘上 去， 等需要的时候就可以再把它读出来使用。 有些时候像银行卡号这些字段是不希望在网络上传输的， transient的作用就是把这个字段的生命周期仅存于调用者的内存中而不会写到磁盘里持久化，意思是transient修饰的age字段， 他的生命周期仅仅在内存中，不会被写到磁盘中。 被transient关键字修饰过得变量真的不能被序列化嘛？ 想要解决这个问题，首先还要再重提一下对象的序列化方式： Java序列化提供两种方式。 一种是实现Serializable接口 另一种是实现Exteranlizable接口。 需要重写writeExternal和readExternal方法， 它的效率比Serializable高一些 ，并且可以决定哪些属性需要序列化（即使是transient修饰的）， 但是对大量对象，或者重复对象，则效率低。 从上面的这两种序列化方式，我想你已经看到了， 使用Exteranlizable接口实现序列化时， 我们自己指定那些属性是需要序列化的，即使是transient修饰的。 下面就验证一下 首先我们定义User1类：这个类是被Externalizable接口修饰的 package keyword; import lombok.Getter; import lombok.Setter; import lombok.ToString; import java.io.*; public class FireTransient { public static void main(String[] args) throws IOException, ClassNotFoundException { serializeUser1(); deSerializeUser1(); } private static void serializeUser1() throws IOException { File file = new File(\"/opt/test/\"); if (!file.exists()){ file.mkdirs(); } User1 u = new User1(); u.setAge(18); u.setName(\"fire\"); ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"/opt/test/template1\")); oos.writeObject(u); oos.close(); } private static void deSerializeUser1() throws IOException, ClassNotFoundException { File file = new File(\"/opt/test/template1\"); ObjectInputStream ois = new ObjectInputStream(new FileInputStream(file)); User1 u = (User1) ois.readObject(); System.out.println(u.toString()); } } @Getter @Setter @ToString class User1 implements Externalizable{ private transient int age; private String name; //由于实现了Externalizable接口的类，会调用构造函数， // 而User1的构造函数是私有的。无法訪问，从而导致抛出异常。 public User1(){ } @Override public void writeExternal(ObjectOutput out) throws IOException { out.writeObject(age); out.writeObject(name); } @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException { age = (int)in.readObject(); name = (String) in.readObject(); } } 上面，代码分了两个方法，一个是序列化，一个是反序列化。 然后看一下结果： D:\\Java\\jdk1.8.0_161\\bin\\java.exe \"-javaagent:D:\\JetBrains\\IntelliJ IDEA 2019.3.3\\lib\\idea_rt.jar=63203:D:\\JetBrains\\IntelliJ IDEA 2019.3.3\\bin\" -Dfile.encoding=UTF-8 -classpath D:\\Java\\jdk1.8.0_161\\jre\\lib\\charsets.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\deploy.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\access-bridge-64.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\cldrdata.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\dnsns.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\jaccess.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\jfxrt.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\localedata.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\nashorn.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunec.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunjce_provider.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunmscapi.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\sunpkcs11.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\ext\\zipfs.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\javaws.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jce.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jfr.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jfxswt.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\jsse.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\management-agent.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\plugin.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\resources.jar;D:\\Java\\jdk1.8.0_161\\jre\\lib\\rt.jar;D:\\github\\program\\target\\classes;D:\\firerepository\\org\\projectlombok\\lombok\\1.16.22\\lombok-1.16.22.jar keyword.FireTransient User1(age=18, name=fire) Process finished with exit code 0 结果基本上验证了我们的猜想，也就是说， 实现了Externalizable接口，哪一个属性被序列化使我们手动去指定的， 即使是transient关键字修饰也不起作用。 静态变量能被序列化吗？没被transient关键字修饰之后呢？ 这个我可以提前先告诉结果，静态变量是不会被序列化的， 即使没有transient关键字修饰。下面去验证一下，然后再解释原因。 首先，在User类中对age属性添加transient关键字和static关键字修饰。 结果已经很明显了。现在解释一下，为什么会是这样，其实在前面已经提到过了。因为静态变量在全局区,本来流里面就没有写入静态变量,我打印静态变量当然会去全局区查找,而我们的序列化是写到磁盘上的，所以JVM查找这个静态变量的值，是从全局区查找的，而不是磁盘上。user.setAge(18);年龄改成18之后，被写到了全局区，其实就是方法区，只不过被所有的线程共享的一块空间。因此可以总结一句话： 静态变量不管是不是transient关键字修饰，都不会被序列化 transient关键字总结 java 的transient关键字为我们提供了便利， 你只需要实现Serilizable接口， 将不需要序列化的属性前添加关键字transient， 序列化对象的时候，这个属性就不会序列化到指定的目的地中。 像银行卡、密码等等这些数据。这个需要根据业务情况了。 "},"Chapter05/modCount.html":{"url":"Chapter05/modCount.html","title":"modCount","keywords":"","body":"Java集合ArrayList中modCount详解及subList函数要点 modCount属性是从AbstractList抽象类继承而来的。 查看javadoc文档中的解释: The number of times this list has been structurally modified. Structural modifications are those that change the size of the list, or otherwise perturb it in such a fashion that iterations in progress may yield incorrect results. This field is used by the iterator and list iterator implementation returned by the iterator and listIterator methods. If the value of this field changes unexpectedly, the iterator (or list iterator) will throw a ConcurrentModificationException in response to the next, remove, previous, set or add operations. This provides fail-fast behavior, rather than non-deterministic behavior in the face of concurrent modification during iteration. 我们知道该参数用来记录集合被修改的次数，之所以要记录修改的次数， 是因为ArrayList不是线程安全的， 为了防止在使用迭代器和子序列的过程当中对原集合的修改导致迭代器及子序列的失效， 故保存了修改次数的记录，在迭代器的操作及子序列的操作过程当中， 会首先去检查modCount是否相等（函数checkForComodification()）， 如果不想等的话，则说明集合被修改了，那么为了防止后续不明确的错误发生， 于是便抛出了该异常。为了防止该异常的出现，在使用迭代器进行集合的迭代是， 若要对集合进行修改，需要通过迭代器提供的对集合进行操作的函数来进行。 "},"Chapter05/Thread.html":{"url":"Chapter05/Thread.html","title":"Thread","keywords":"","body":"线程 一个异常 Thread starvation or clock leap detected 2018-05-27 13:56:50.820 WARN 111644 --- [ Thread-49] c.g.htmlunit.IncorrectnessListenerImpl : Obsolete content type encountered: 'text/javascript'. 2018-05-27 13:58:26.957 WARN 111644 --- [l-1 housekeeper] com.zaxxer.hikari.pool.HikariPool : HikariPool-1 - Thread starvation or clock leap detected (housekeeper delta=51s792ms365µs576ns). 2018-05-27 14:02:55.861 WARN 111644 --- [l-1 housekeeper] com.zaxxer.hikari.pool.HikariPool : HikariPool-1 - Thread starvation or clock leap detected (housekeeper delta=1m33s765ms258µs244ns). 2018-05-27 14:08:47.880 WARN 111644 --- [l-1 housekeeper] com.zaxxer.hikari.pool.HikariPool : HikariPool-1 - Thread starvation or clock leap detected (housekeeper delta=6m53s824ms351µs439ns). 2018-05-27 14:17:02.136 WARN 111644 --- [l-1 housekeeper] com.zaxxer.hikari.pool.HikariPool : HikariPool-1 - Thread starvation or clock leap detected (housekeeper delta=8m17s388ms195µs302ns). This runs on the housekeeper thread, which executes every 30 seconds. If you are on Mac OS X, the clockSource is System.currentTimeMillis(), any other platform the clockSource is System.nanoTime(). Both in theory are monotonically increasing, but various things can affect that such as NTP servers. Most OSes are designed to handle backward NTP time adjustments to preserve the illusion of the forward flow of time. This code is saying, if time moves backwards (now A couple of things might be going on: You could be running in a virtual container (VMWare, AWS, etc.) that for some reason is doing a particularly poor job of maintaining the illusion of the forward flow of time. Because other things occur in the housekeeper thread -- specifically, closing idle connections -- it is possible that for some reason closing connections is blocking the housekeeper thread for more than two housekeeping periods (60 seconds). The server is so busy, with all CPUs pegged, that thread starvation is occurring, which is preventing the housekeeper thread from running for more than two housekeeping periods. Considering these, maybe you can provide additional context. EDIT: Note that this is based on HikariCP 2.4.1 code. Make sure you are running the most up-to-date version available. 翻译如下： 这在管家线程上运行，该线程每30秒执行一次。如果在Mac OS X上，clockSource是System.currentTimeMillis（），则任何其他平台上的clockSource是System.nanoTime（）。从理论上讲，两者都在单调增加，但是诸如NTP服务器之类的各种因素都可能影响到这一点。大多数操作系统旨在处理向后NTP时间调整，以保留对时间的前向错觉的幻想。 这段代码说的是，如果时间倒退（现在 可能发生了几件事情： 您可能正在某个虚拟容器（VMWare，AWS等）中运行，由于某种原因，该容器在维持时间上的错觉方面做得特别差。 由于管家线程中发生了其他事情-特别是关闭空闲连接-出于某种原因，关闭连接可能会阻塞管家线程两个以上的维护周期（60秒）。 服务器太忙了，所有CPU都挂了，导致线程出现饥饿，这导致管家线程无法运行两个以上的管家周期。 考虑到这些，也许您可​​以提供其他上下文。 编辑：请注意，这是基于HikariCP 2.4.1代码的。确保您正在运行最新的可用版本。 Java同一个线程对象能否多次调用start方法 下面看下start方法源码： /**线程成员变量，默认为0，volatile修饰可以保证线程间可见性*/ private volatile int threadStatus = 0; /* 当前线程所属的线程组 */ private ThreadGroup group; /** * 同步方法，同一时间，只能有一个线程可以调用此方法 */ public synchronized void start() { //threadStatus if (threadStatus != 0) throw new IllegalThreadStateException(); //线程组 group.add(this); boolean started = false; try { //本地方法，该方法会实际调用run方法 start0(); started = true; } finally { try { if (!started) { //创建失败，则从线程组中删除该线程 group.threadStartFailed(this); } } catch (Throwable ignore) { /* start0抛出的异常不用处理，将会在堆栈中传递 */ } } } 通过断点跟踪，可以看到当线程对象第一次调用start方法时会进入同步方法，会判断threadStatus是否为0，如果为0，则进行往下走，否则抛出非法状态异常； 将当前线程对象加入线程组； 调用本地方法start0执行真正的创建线程工作，并调用run方法，可以看到在start0执行完后，threadStatus的值发生了改变，不再为0； finally块用于捕捉start0方法调用发生的异常。 继续回到原话题，当start调用后，并且run方法内容执行完后，线程是如何终止的呢？实际上是由虚拟机调用Thread中的exit方法来进行资源清理并终止线程的，看下exit方法源码： /** * 系统调用该方法用于在线程实际退出之前释放资源 */ private void exit() { //释放线程组资源 if (group != null) { group.threadTerminated(this); group = null; } //清理run方法实例对象 target = null; /*加速资源释放。快速垃圾回收 */ threadLocals = null; inheritableThreadLocals = null; inheritedAccessControlContext = null; blocker = null; uncaughtExceptionHandler = null; } 到这里，t1 线程经历了从新建（NEW），就绪（RUNNABLE），运行（RUNNING），定时等待（TIMED_WAITING），终止（TERMINATED）这样一个过程； 2, 由于在第一次 start 方法后，threadStatus 值被改变，因此第二次调用start时会抛出非法状态异常； 在调用start0方法后，如果run方法体内容被快速执行完，那么系统会自动调用exit方法释放资源，销毁对象，所以第二次调用start方法时，有可能内部资源已经被释放。 初步结论：同一个线程对象不可以多次调用 start 方法。 通过反射修改threadStatus来多次执行start方法 public static void main(String[] args) throws Exception { //创建一个线程t1 Thread t1 = new Thread(() -> { try { //睡眠10秒，防止run方法执行过快， //触发exit方法导致线程组被销毁 TimeUnit.SECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } }); //第一次启动 t1.start(); //修改threadStatus，重新设置为0，即 NEW 状态 Field threadStatus = t1.getClass().getDeclaredField(\"threadStatus\"); threadStatus.setAccessible(true); //重新将线程状态设置为0，新建（NEW）状态 threadStatus.set(t1, 0); //第二次启动 t1.start(); } 截取start后半截源码： boolean started = false; try { //第二次执行start0会抛异常，这时started仍然为false start0(); started = true; } finally { try { if (!started) { //创建失败，则从线程组中删除该线程 group.threadStartFailed(this); } } catch (Throwable ignore) { /* start0抛出的异常不用处理，将会在堆栈中传递 */ } } 在上面代码中，在第一次调用start方法后，我通过反射修改threadStatus值，这样在第二次调用时可以跳过状态值判断语句，达到多次调用start方法； 当我第二次调用t1.start时，需要设置run方法运行时间长一点，防止系统调用exit方法清理线程资源； 经过以上两步，我成功绕开 threadStatus 判断和线程组增加方法，开始执行start0方法，但是在执行start0的时候抛出异常，并走到了finally块中，由于start为false，所以会执行group.threadStartFailed(this)操作，将该线程从线程组中移除； 所以start0中还是会对当前线程状态进行了一个判断，不允许重复创建线程。 最后结论：无论是直接二次调用还是通过反射二次调用，同一个线程对象都无法多次调用start方法，仅可调用一次。 "},"Chapter05/Executors.html":{"url":"Chapter05/Executors.html","title":"Executors","keywords":"","body":"线程池 Java通过Executors提供四种线程池，分别为：   newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。   newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。   newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。   newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行   newWorkStealingPool 适合使用在很耗时的操作，但是newWorkStealingPool不是ThreadPoolExecutor的扩展，它是新的线程池类ForkJoinPool的扩展， 但是都是在统一的一个Executors类中实现，由于能够合理的使用CPU进行对任务操作（并行操作），所以适合使用在很耗时的任务中： 线程池是如何工作的 首先我们来看下如何创建一个线程池 ThreadPoolExecutor threadPool = new ThreadPoolExecutor(10, 20, 600L, TimeUnit.SECONDS, new LinkedBlockingQueue<>(4096), new NamedThreadFactory(\"common-work-thread\")); // 设置拒绝策略，默认为 AbortPolicy threadPool.setRejectedExecutionHandler(new ThreadPoolExecutor.AbortPolicy()); 看下其构造方法签名如下 rtPolicy()); 看下其构造方法签名如下 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { // 省略代码若干 } 要理解这些参数具体代表的意义，必须清楚线程池提交任务与执行任务流程，如下 步骤如下 1、corePoolSize：如果提交任务后线程还在运行，当线程数小于 corePoolSize 值时，无论线程池中的线程是否忙碌，都会创建线程，并把任务交给此新创建的线程进行处理，如果线程数少于等于 corePoolSize，那么这些线程不会回收，除非将 allowCoreThreadTimeOut 设置为 true，但一般不这么干，因为频繁地创建销毁线程会极大地增加系统调用的开销。 2、workQueue：如果线程数大于核心数（corePoolSize）且小于最大线程数（maximumPoolSize），则会将任务先丢到阻塞队列里，然后线程自己去阻塞队列中拉取任务执行。 3、maximumPoolSize: 线程池中最大可创建的线程数，如果提交任务时队列满了且线程数未到达这个设定值，则会创建线程并执行此次提交的任务，如果提交任务时队列满了但线池数已经到达了这个值，此时说明已经超出了线池程的负载能力，就会执行拒绝策略，这也好理解，总不能让源源不断地任务进来把线程池给压垮了吧，我们首先要保证线程池能正常工作。 4、RejectedExecutionHandler：一共有以下四种拒绝策略 AbortPolicy：丢弃任务并抛出异常，这也是默认策略； CallerRunsPolicy：用调用者所在的线程来执行任务，所以开头的问题「线程把任务丢给线程池后肯定就马上返回了?」我们可以回答了，如果用的是 CallerRunsPolicy 策略，提交任务的线程（比如主线程）提交任务后并不能保证马上就返回，当触发了这个 reject 策略不得不亲自来处理这个任务。 DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务。 DiscardPolicy：直接丢弃任务，不抛出任何异常，这种策略只适用于不重要的任务。 5、keepAliveTime: 线程存活时间，如果在此时间内超出 corePoolSize 大小的线程处于 idle 状态，这些线程会被回收 6、threadFactory：可以用此参数设置线程池的命名，指定 defaultUncaughtExceptionHandler（有啥用，后文阐述）,甚至可以设定线程为守护线程。 首先来看线程大小设置 线程池提交任务的两种方式 线程池创建好了，该怎么给它提交任务，有两种方式，调用 execute 和 submit 方法,区别在于调用 execute 无返回值，而调用 submit 可以返回 Future，那么这个 Future 能到底能干啥呢，看它的接口 可以用 Future 取消任务，判断任务是否已取消/完成，甚至可以阻塞等待结果。 submit 为啥能提交任务（Runnable）的同时也能返回任务（Future）的执行结果呢 原来在最后执行 execute 前用 newTaskFor 将 task 封装成了 RunnableFuture，newTaskFor 返回了 FutureTask 这个类， 可以看到 FutureTask 这个接口既实现了 Runnable 接口，也实现 Future 接口，所以在提交任务的同时也能利用 Future 接口来执行任务的取消，获取任务的状态，等待执行结果这些操作。 execute 与 submit 除了是否能返回执行结果这一区别外，还有一个重要区别，那就是使用 execute 执行如果发生了异常，是捕获不到的，默认会执行 ThreadGroup 的 uncaughtException 方法 所以如果你想监控执行 execute 方法时发生的异常，需要通过 threadFactory 来指定一个 UncaughtExceptionHandler，这样就会执行上图中的 1，进而执行 UncaughtExceptionHandler 中的逻辑,如下所示: 如何实现核心线程池的预热 使用 prestartAllCoreThreads() 方法，这个方法会一次性创建 corePoolSize 个线程，无需等到提交任务时才创建，提交创建好线程的话，一有任务提交过来，这些线程就可以立即处理。 下面简单画了一下核心线程的序列图： 跟踪 execute 方法源码，查看核心线程是如何被加添到池中的： public void execute(Runnable command) { if (command == null) throw new NullPointerException(); //获取线程池控制状态 int c = ctl.get(); //通过workerCountOf计算出实际线程数 if (workerCountOf(c) 根据方法内容和断点跟踪可以得出以下结论： 核心线程数未超过 corePoolSize，每添加新的任务（command），都会创建新的线程（Worker中创建），即使有空闲线程存在； 核心线程数等于corePoolSize后，如果继续添加新的任务（command），会将任务添加到阻塞队列 workQueue 中，等待调度； 如果添加到队列失败，则检查 corePoolSize 是否小于 maximumPoolSize，如果小于则创建新的线程执行任务，直到线程总数 等于 maximumPoolSize； 当线程数等于 maximumPoolSize 并且队列已满了，后续新增任务将会触发线程饱和策略。 上面代码中我们关心 addWorker 方法，它有两个参数，第一个是 Runnable 对象，第二参数是标记是否核心线程，true为核心线程，接下来看下源码： private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); // 省略部分代码 ...... for (;;) { //core主要用于判断是否继续创建新线程 int wc = workerCountOf(c); //workCount 大于总容量或者workCount大于核心线程或最大线程将直接返回 if (wc >= CAPACITY || wc >= (core ? corePoolSize : maximumPoolSize)) return false; //通过CAS将c加1，也就是将workCount加1 if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; retry inner loop } } boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { //创建新线程 w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { final ReentrantLock mainLock = this.mainLock; mainLock.lock(); //省略部分代码 ...... workers.add(w); int s = workers.size(); if (s > largestPoolSize) largestPoolSize = s; workerAdded = true; ...... if (workerAdded) { //启动线程 t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } return workerStarted; } 从 addWorker 方法中，可以看到从 Worker 对象中获取到线程对象 t ，并调用 start 方法启动线程，那这个 t 线程是如何来的呢？ 接下来要看下 Worker 是如何创建线程的： private final class Worker extends AbstractQueuedSynchronizer implements Runnable { final Thread thread; /**初始执行任务，有可能为空*/ Runnable firstTask; /**使用firstTask和来自线程工厂中的线程创建了 Worker 对象*/ Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } /**将run方法委托给runWorker执行*/ public void run() { runWorker(this); } } Worker 类实现 Runnable 接口， Worker 类的构造方法中 this.thread = getThreadFactory().newThread(this)比较关键，这行代码的意思是说使用当前 Worker 对象创建了一个线程，那其实也就是说 thread 对象和 当前 Worker 对象中调用的 run 方法是一样的。到这一步我们可以得出上一步 addWorker 方法中的 t.start 调用的其实就是 Worker 类中的 run方法。 那 runWorker 又是如何运行的呢？ final void runWorker(Worker w) { Thread wt = Thread.currentThread(); //获取要执行的任务 Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; //轮询调用 getTask 用于获取任务 while (task != null || (task = getTask()) != null) { w.lock(); //省略部分代码 ...... //执行run方法 task.run(); //省略部分代码 ...... } } runWorker 中使用 while 循环，不断调用 getTask 去获取新任务。 最后看下 getTask 方法做了哪些事： private Runnable getTask() { boolean timedOut = false; //无限循环 for (;;) { int c = ctl.get(); int rs = runStateOf(c); // 检查队列是否为空 if (rs >= SHUTDOWN && (rs >= STOP || workQueue.isEmpty())) { decrementWorkerCount(); return null; } //获取运行线程数，根据allowCoreThreadTimeOut决定是否允许定时等待 int wc = workerCountOf(c); boolean timed = allowCoreThreadTimeOut || wc > corePoolSize; //线程超时并且队列为空时通过CAS将实际运行线程数减1 if ((wc > maximumPoolSize || (timed && timedOut)) && (wc > 1 || workQueue.isEmpty())) { if (compareAndDecrementWorkerCount(c)) return null; continue; } try { //允许超时则调用队列的poll方法定时等待 //否则调用take获取任务 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); //获取任务，返回结果 if (r != null) return r; //继续循环，并且置超时标识为true timedOut = true; } catch (InterruptedException retry) { timedOut = false; } } } 通过以上源码可以看出： 在for无限循环中，通过不断的检查线程池状态和队列容量，来获取可执行任务； 在 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take();代码中，分为两种情况 timed 为 true，允许淘汰 Worker，即实际运行的线程，则通过workQueue.poll的方式定时等待拉取任务，如果在指定keepAliveTime时间内获取任务则返回，如果没有任务则继续for循环并直到timed等于false； timed 为 false，则会调用 workQueue.take 方法，队列中 take 方法的含义是当队列有任务时，立即返回队首任务，没有任务时则一直阻塞当前线程，直到有新任务才返回。 "},"Chapter05/reflection.html":{"url":"Chapter05/reflection.html","title":"反射","keywords":"","body":"反射 要想理解反射的原理，首先要了解什么是类型信息。Java让我们在运行时识别对象和类的信息， 主要有两种方式： 一种是传统的RTTI（Run-Time Type Identification)，它假定我们在编译时已经知道了所有的类型信息； 另一种是反射机制，它允许我们在运行时发现和使用类的信息。 使用的前提条件：必须先得到代表的字节码的Class，Class类用于表示.class文件（字节码） 反射的概述 JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为Java语言的反射机制。 要想解剖一个类，必须先要获取到该类的字节码文件对象。而解剖使用的就是Class类中的方法。所以先要获取到每一个字节码文件对应的Class类型的对象.。 反射就是把Java类中的各种成分映射成一个个的Java对象。 例如： 一个类有：成员变量、方法、构造方法、包等等信息，利用反射技术可以对一个类进行解剖，把各个组成部分映射成一个个对象。 （其实：一个类中这些成员方法、构造方法，在加入类中都有一个类来描述） 如图是类的正常加载过程：反射的原理在于class对象。 熟悉一下加载的时候：Class对象的由来是将class文件读入内存，并为之创建一个Class对象。 反射的思想及作用 上部分了解了一下什么是正射 Map map = new HashMap<>(); map.put(1, 1); 某一天发现，该段程序不适合用 HashMap 存储键值对，更倾向于用LinkedHashMap存储。重新编写代码后变成下面这个样子。 Map map = new LinkedHashMap<>(); map.put(1, 1); 假如又有一天，发现数据还是适合用 HashMap来存储，难道又要重新修改源码吗？ 发现问题了吗？我们每次改变一种需求，都要去重新修改源码，然后对代码进行编译，打包，再到 JVM 上重启项目。这么些步骤下来，效率非常低。 对于这种需求频繁变更但变更不大的场景，频繁地更改源码肯定是一种不允许的操作，我们可以使用一个开关，判断什么时候使用哪一种数据结构。 public Map getMap(String param) { Map map = null; if (param.equals(\"HashMap\")) { map = new HashMap<>(); } else if (param.equals(\"LinkedHashMap\")) { map = new LinkedHashMap<>(); } else if (param.equals(\"WeakHashMap\")) { map = new WeakHashMap<>(); } return map; } 通过传入参数param决定使用哪一种数据结构，可以在项目运行时，通过动态传入参数决定使用哪一个数据结构。 如果某一天还想用TreeMap，还是避免不了修改源码，重新编译执行的弊端。这个时候，反射就派上用场了。 在代码运行之前，我们不确定将来会使用哪一种数据结构，只有在程序运行时才决定使用哪一个数据类，而反射可以在程序运行过程中动态获取类信息和调用类方法。通过反射构造类实例，代码会演变成下面这样。 public Map getMap(String className) { Class clazz = Class.forName(className); Consructor con = clazz.getConstructor(); return (Map) con.newInstance(); } 无论使用什么 Map，只要实现了Map接口，就可以使用全类名路径传入到方法中，获得对应的 Map 实例。例如java.util.HashMap / java.util.LinkedHashMap····如果要创建其它类例如WeakHashMap，我也不需要修改上面这段源码。 反射的基本使用 Java 反射的主要组成部分有4个： Class：任何运行在内存中的所有类都是该 Class 类的实例对象，每个 Class 类对象内部都包含了本来的所有信息。记着一句话，通过反射干任何事，先找 Class 准没错！ Field：描述一个类的属性，内部包含了该属性的所有信息，例如数据类型，属性名，访问修饰符······ Constructor：描述一个类的构造方法，内部包含了构造方法的所有信息，例如参数类型，参数名字，访问修饰符······ Method：描述一个类的所有方法（包括抽象方法），内部包含了该方法的所有信息，与Constructor类似，不同之处是 Method 拥有返回值类型信息，因为构造方法是没有返回值的。 反射中的用法有非常非常多，常见的功能有以下这几个： 在运行时获取一个类的 Class 对象 在运行时构造一个类的实例化对象 在运行时获取一个类的所有信息：变量、方法、构造器、注解 我们在学习反射的基本使用时，我会用一个SmallPineapple类作为模板进行说明，首先我们先来熟悉这个类的基本组成：属性，构造函数和方法 ``` #### 获取类的 Class 对象 在 Java 中，每一个类都会有专属于自己的 Class 对象，当我们编写完.java文件后，使用javac编译后，就会产生一个字节码文件.class，在字节码文件中包含类的所有信息，如属性，构造方法，方法······当字节码文件被装载进虚拟机执行时，会在内存中生成 Class 对象，它包含了该类内部的所有信息，在程序运行时可以获取这些信息。 获取 Class 对象的方法有3种： 类名.class：这种获取方式只有在编译前已经声明了该类的类型才能获取到 Class 对象 ```java Class clazz = SmallPineapple.class; 实例.getClass()：通过实例化对象获取该实例的 Class 对象 SmallPineapple sp = new SmallPineapple(); Class clazz = sp.getClass(); Class.forName(className)：通过类的全限定名获取该类的 Class 对象 Class clazz = Class.forName(\"com.bean.smallpineapple\"); 拿到 Class对象就可以对它为所欲为了：剥开它的皮（获取类信息）、指挥它做事（调用它的方法），看透它的一切（获取属性），总之它就没有隐私了。 不过在程序中，每个类的 Class 对象只有一个，也就是说你只有这一个奴隶。我们用上面三种方式测试，通过三种方式打印各个 Class 对象都是相同的。 public static void main(String[] args) throws ClassNotFoundException { Class clazz1 = Class.forName(\"reflect.SmallPineapple\"); Class clazz2 = SmallPineapple.class; SmallPineapple instance = new SmallPineapple(); Class clazz3 = instance.getClass(); System.out.println(\"Class.forName() == SmallPineapple.class:\" + (clazz1 == clazz2)); System.out.println(\"Class.forName() == instance.getClass():\" + (clazz1 == clazz3)); System.out.println(\"instance.getClass() == SmallPineapple.class:\" + (clazz2 == clazz3)); } Class.forName() == SmallPineapple.class:true Class.forName() == instance.getClass():true instance.getClass() == SmallPineapple.class:true 内存中只有一个 Class 对象的原因要牵扯到 JVM 类加载机制的双亲委派模型，它保证了程序运行时，加载类时每个类在内存中仅会产生一个Class对象。在这里我不打算详细展开说明，可以简单地理解为 JVM 帮我们保证了一个类在内存中至多存在一个 Class 对象。 构造类的实例化对象 通过反射构造一个类的实例方式有2种： Class 对象调用newInstance()方法 Class clazz = Class.forName(\"reflect.SmallPineapple\"); SmallPineapple smallPineapple = (SmallPineapple) clazz.newInstance(); smallPineapple.getInfo(); [null 的年龄是：0] 即使 SmallPineapple 已经显式定义了构造方法，通过 newInstance() 创建的实例中，所有属性值都是对应类型的初始值， 因为 newInstance() 构造实例会调用默认无参构造器。 Constructor 构造器调用newInstance()方法 Class clazz = Class.forName(\"reflect.SmallPineapple\"); // SmallPineapple smallPineapple = (SmallPineapple) clazz.newInstance(); // smallPineapple.getInfo(); Constructor constructor = clazz.getConstructor(String.class, int.class); constructor.setAccessible(true); SmallPineapple smallPineapple2 = (SmallPineapple) constructor.newInstance(\"小菠萝\", 21); smallPineapple2.getInfo(); [小菠萝 的年龄是：21] 通过 getConstructor(Object... paramTypes) 方法指定获取指定参数类型的 Constructor， Constructor 调用 newInstance(Object... paramValues) 时传入构造方法参数的值，同样可以构造一个实例，且内部属性已经被赋值。 通过Class对象调用 newInstance() 会走默认无参构造方法，如果想通过显式构造方法构造实例，需要提前从Class中调用getConstructor()方法获取对应的构造器，通过构造器去实例化对象。 获取一个类的所有信息 Class 对象中包含了该类的所有信息，在编译期我们能看到的信息就是该类的变量、方法、构造器，在运行时最常被获取的也是这些信息。 获取类中的变量（Field） Field[] getFields()：获取类中所有被public修饰的所有变量 Field getField(String name)：根据变量名获取类中的一个变量，该变量必须被public修饰 Field[] getDeclaredFields()：获取类中所有的变量，但无法获取继承下来的变量 Field getDeclaredField(String name)：根据姓名获取类中的某个变量，无法获取继承下来的变量获取类中的方法（Method） Method[] getMethods()：获取类中被public修饰的所有方法 Method getMethod(String name, Class... paramTypes)：根据名字和参数类型获取对应方法，该方法必须被public修饰 Method[] getDeclaredMethods()：获取所有方法，但无法获取继承下来的方法 Method getDeclaredMethod(String name, Class... paramTypes)：根据名字和参数类型获取对应方法，无法获取继承下来的方法获取类的构造器（Constructor） Constuctor[] getConstructors()：获取类中所有被public修饰的构造器 Constructor getConstructor(Class... paramTypes)：根据参数类型获取类中某个构造器，该构造器必须被public修饰 Constructor[] getDeclaredConstructors()：获取类中所有构造器 Constructor getDeclaredConstructor(class... paramTypes)：根据参数类型获取对应的构造器 每种功能内部以 Declared 细分为2类： 有Declared修饰的方法：可以获取该类内部包含的所有变量、方法和构造器，但是无法获取继承下来的信息 无Declared修饰的方法：可以获取该类中public修饰的变量、方法和构造器，可获取继承下来的信息 如果想获取类中所有的（包括继承）变量、方法和构造器，则需要同时调用getXXXs()和getDeclaredXXXs()两个方法，用Set集合存储它们获得的变量、构造器和方法，以防两个方法获取到相同的东西 例如：要获取SmallPineapple获取类中所有的变量，代码应该是下面这样写。 // 获取 public 属性，包括继承 Field[] fields1 = clazz.getFields(); // 获取所有属性，不包括继承 Field[] fields2 = clazz.getDeclaredFields(); // 将所有属性汇总到 set Set allFields = new HashSet<>(); allFields.addAll(Arrays.asList(fields1)); allFields.addAll(Arrays.asList(fields2)); Class clazz2 = SmallPineapple.class; Object object = clazz2.newInstance(); Field field = clazz2.getDeclaredField(\"name\"); field.setAccessible(true); field.set(object, \"小儿\"); Field ageField = clazz2.getDeclaredField(\"age\"); ageField.setAccessible(true); ageField.set(object, 2); Method method = clazz2.getMethod(\"getInfo\"); method.invoke(object); 不知道你有没有发现一件有趣的事情，如果父类的属性用protected修饰，利用反射是无法获取到的。 protected 修饰符的作用范围：只允许同一个包下或者子类访问，可以继承到子类。 getFields() 只能获取到本类的public属性的变量值； getDeclaredFields() 只能获取到本类的所有属性，不包括继承的；无论如何都获取不到父类的 protected 属性修饰的变量，但是它的的确确存在于子类中。 获取注解 获取注解单独拧了出来，因为它并不是专属于 Class 对象的一种信息，每个变量，方法和构造器都可以被注解修饰，所以在反射中，Field，Constructor 和 Method 类对象都可以调用下面这些方法获取标注在它们之上的注解。 Annotation[] getAnnotations()：获取该对象上的所有注解 Annotation getAnnotation(Class annotaionClass)：传入注解类型，获取该对象上的特定一个注解 Annotation[] getDeclaredAnnotations()：获取该对象上的显式标注的所有注解，无法获取继承下来的注解 Annotation getDeclaredAnnotation(Class annotationClass)：根据注解类型，获取该对象上的特定一个注解，无法获取继承下来的注解 只有注解的@Retension标注为RUNTIME时，才能够通过反射获取到该注解，@Retension 有3种保存策略： SOURCE：只在源文件(.java)中保存，即该注解只会保留在源文件中，编译时编译器会忽略该注解，例如 @Override 注解 CLASS：保存在字节码文件(.class)中，注解会随着编译跟随字节码文件中，但是运行时不会对该注解进行解析 RUNTIME：一直保存到运行时，用得最多的一种保存策略，在运行时可以获取到该注解的所有信息 像下面这个例子，SmallPineapple 类继承了抽象类Pineapple，getInfo()方法上标识有 @Override 注解，且在子类中标注了@Transient注解，在运行时获取子类重写方法上的所有注解，只能获取到@Transient的信息。 public abstract class Pineapple { public abstract void getInfo(); } public class SmallPineapple extends Pineapple { @Transient @Override public void getInfo() { System.out.print(\"小菠萝的身高和年龄是:\" + height + \"cm ; \" + age + \"岁\"); } } 启动类Bootstrap获取 SmallPineapple 类中的 getInfo() 方法上的注解信息： public class Bootstrap { /** * 根据运行时传入的全类名路径判断具体的类对象 * @param path 类的全类名路径 */ public static void execute(String path) throws Exception { Class obj = Class.forName(path); Method method = obj.getMethod(\"getInfo\"); Annotation[] annotations = method.getAnnotations(); for (Annotation annotation : annotations) { System.out.println(annotation.toString()); } } public static void main(String[] args) throws Exception { execute(\"com.pineapple.SmallPineapple\"); } } // @java.beans.Transient(value=true) 通过反射调用方法 通过反射获取到某个 Method 类对象后，可以通过调用invoke方法执行。 invoke(Oject obj, Object... args)：参数`1指定调用该方法的**对象**，参数2是方法的参数列表值。 如果调用的方法是静态方法，参数1只需要传入null，因为静态方法不与某个对象有关，只与某个类有关。 可以像下面这种做法，通过反射实例化一个对象，然后获取Method方法对象，调用invoke()指定SmallPineapple的getInfo()方法。 Class clazz = Class.forName(\"com.bean.SmallPineapple\"); Constructor constructor = clazz.getConstructor(String.class, int.class); constructor.setAccessible(true); SmallPineapple sp = (SmallPineapple) constructor.newInstance(\"小菠萝\", 21); Method method = clazz.getMethod(\"getInfo\"); if (method != null) { method.invoke(sp, null); } 反射调用的实现 首先，我们来看看方法的反射调用，也就是 Method.invoke，是怎么实现的。 public final class Method extends Executable { ... public Object invoke(Object obj, Object... args) throws ... { ... //权限检查 MethodAccessor ma = methodAccessor; if (ma == null) { ma = acquireMethodAccessor(); } return ma.invoke(obj, args); } } 如果你查阅 Method.invoke 的源代码，那么你会发现，它实际上委派给MethodAccessor 来处理。MethodAccessor 是一个接口，它有两个已有的具体实现：一个通过本地方法来实现反射调用，另一个则使用了委派模式。为了方便记忆，我便用“本地实现”和“委派实现”来指代这两者。 每个 Method 实例的第一次反射调用都会生成一个委派实现，它所委派的具体实现便是一个本地实现。本地实现非常容易理解。当进入了 Java 虚拟机内部之后，我们便拥有了Method 实例所指向方法的具体地址。这时候，反射调用无非就是将传入的参数准备好，然后调用进入目标方法。 // v0版本 import java.lang.reflect.Method; public class Test { public static void target(int i) { new Exception(\"#\" + i).printStackTrace(); } public static void main(String[] args) throws Exception { Class klass = Class.forName(\"Test\"); Method method = klass.getMethod(\"target\", int.class); method.invoke(null, 0); } } #不同版本的输出略有不同，这里我使用了Java 10。 $ java Test java.lang.Exception: #0 at Test.target(Test.java:5) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl .invoke0(Native Methoa t java.base/jdk.internal.reflect.NativeMethodAccessorImpl. .invoke(NativeMethodAt java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.i .invoke(Delegatin java.base/java.lang.reflect.Method.invoke(Method.java:564) t Test.main(Test.java:131 为了方便理解，我们可以打印一下反射调用到目标方法时的栈轨迹。在上面的 v0 版本代码中，我们获取了一个指向 Test.target 方法的 Method 对象，并且用它来进行反射调用。在 Test.target 中，我会打印出栈轨迹。 可以看到，反射调用先是调用了 Method.invoke，然后进入委派实现（DelegatingMethodAccessorImpl），再然后进入本地实现（NativeMethodAccessorImpl），最后到达目标方法。 这里你可能会疑问，为什么反射调用还要采取委派实现作为中间层？直接交给本地实现不可以么？ 其实，Java 的反射调用机制还设立了另一种动态生成字节码的实现（下称动态实现），直接使用 invoke 指令来调用目标方法。之所以采用委派实现，便是为了能够在本地实现以及动态实现中切换。 //动态实现的伪代码，这里只列举了关键的调用逻辑，其实它还包括调用者检测、参数检测的字节码。 package jdk.internal.reflect; public class GeneratedMethodAccessor1 extends ... { @Overrides public Object invoke(Object obj, Object[] args) throws ... { Test.target((int) args[0]); return null; } } 动态实现和本地实现相比，其运行效率要快上 20 倍。这是因为动态实现无需经过 Java到 C++ 再到 Java 的切换，但由于生成字节码十分耗时，仅调用一次的话，反而是本地实现要快上 3 到 4 倍。 考虑到许多反射调用仅会执行一次，Java 虚拟机设置了一个阈值 15（可以通过 -Dsun.reflect.inflationThreshold= 来调整），当某个反射调用的调用次数在 15 之下时，采用本地实现；当达到 15 时，便开始动态生成字节码，并将委派实现的委派对象切换至动态实现，这个过程我们称之为 Inflation。 为了观察这个过程，我将刚才的例子更改为下面的 v1 版本。它会将反射调用循环 20 次。 // v1版本 import java.lang.reflect.Method; public class Test { public static void target(int i) { new Exception(\"#\" + i).printStackTrace(); } public static void main(String[] args) throws Exception { Class klass = Class.forName(\"Test\"); Method method = klass.getMethod(\"target\", int.class); for (int i = 0; i 可以看到，在第 15 次（从 0 开始数）反射调用时，我们便触发了动态实现的生成。这时候，Java 虚拟机额外加载了不少类。其中，最重要的当属GeneratedMethodAccessor1（第 30 行）。并且，从第 16 次反射调用开始，我们便切换至这个刚刚生成的动态实现（第 40 行）。 反射调用的 Inflation 机制是可以通过参数（-Dsun.reflect.noInflation=true）来关闭的。这样一来，在反射调用一开始便会直接生成动态实现，而不会使用委派实现或者本地实现。 反射调用的开销 下面，我们便来拆解反射调用的性能开销。 在刚才的例子中，我们先后进行了 Class.forName，Class.getMethod 以及Method.invoke 三个操作。其中，Class.forName 会调用本地方法，Class.getMethod则会遍历该类的公有方法。如果没有匹配到，它还将遍历父类的公有方法。可想而知，这两个操作都非常费时。 值得注意的是，以 getMethod 为代表的查找方法操作，会返回查找得到结果的一份拷贝。因此，我们应当避免在热点代码中使用返回 Method 数组的 getMethods 或者getDeclaredMethods 方法，以减少不必要的堆空间消耗。 在实践中，我们往往会在应用程序中缓存 Class.forName 和 Class.getMethod 的结果。因此，下面我就只关注反射调用本身的性能开销。 为了比较直接调用和反射调用的性能差距，我将前面的例子改为下面的 v2 版本。它会将反射调用循环二十亿次。此外，它还将记录下每跑一亿次的时间。 我将取最后五个记录的平均值，作为预热后的峰值性能。（注：这种性能评估方式并不严谨，我会在专栏的第三部分介绍如何用 JMH 来测性能。） 在我这个老笔记本上，一亿次直接调用耗费的时间大约在 120ms。这和不调用的时间是一致的。其原因在于这段代码属于热循环，同样会触发即时编译。并且，即时编译会将对Test.target 的调用内联进来，从而消除了调用的开销。 // v2版本 mport java.lang.reflect.Method; public class Test { public static void target(int i) { //空方法 } public static void main(String[] args) throws Exception { Class klass = Class.forName(\"Test\"); Method method = klass.getMethod(\"target\", int.class); long current = System.currentTimeMillis(); for (int i = 1; i 下面我将以 120ms 作为基准，来比较反射调用的性能开销。 由于目标方法 Test.target 接收一个 int 类型的参数，因此我传入 128 作为反射调用的参数，测得的结果约为基准的 2.7 倍。我们暂且不管这个数字是高是低，先来看看在反射调用之前字节码都做了什么。 aload_2 //加载Method对象 aconst_null //反射调用的第一个参数null iconst_1 anewarray Object //生成一个长度为1的Object数组 dup iconst_0 sipush 128 invokestatic Integer.valueOf //将128自动装箱成Integer73: aastore //存入Object数组中 invokevirtual Method.invoke //反射调用 这里我截取了循环中反射调用编译而成的字节码。可以看到，这段字节码除了反射调用外，还额外做了两个操作。 第一，由于 Method.invoke 是一个变长参数方法，在字节码层面它的最后一个参数会是Object 数组（感兴趣的同学私下可以用 javap 查看）。Java 编译器会在方法调用处生成一个长度为传入参数数量的 Object 数组，并将传入参数一一存储进该数组中。 第二，由于 Object 数组不能存储基本类型，Java 编译器会对传入的基本类型参数进行自动装箱。 这两个操作除了带来性能开销外，还可能占用堆内存，使得 GC 更加频繁。（如果你感兴趣的话，可以用虚拟机参数 -XX:+PrintGC 试试。）那么，如何消除这部分开销呢？ 关于第二个自动装箱，Java 缓存了 [-128, 127] 中所有整数所对应的 Integer 对象。当需要自动装箱的整数在这个范围之内时，便返回缓存的 Integer，否则需要新建一个 Integer对象。 因此，我们可以将这个缓存的范围扩大至覆盖 128（对应参数-Djava.lang.Integer.IntegerCache.high=128），便可以避免需要新建 Integer 对象的场景。 或者，我们可以在循环外缓存 128 自动装箱得到的 Integer 对象，并且直接传入反射调用中。这两种方法测得的结果差不多，约为基准的 1.8 倍。 现在我们再回来看看第一个因变长参数而自动生成的 Object 数组。既然每个反射调用对应的参数个数是固定的，那么我们可以选择在循环外新建一个 Object 数组，设置好参数，并直接交给反射调用。改好的代码可以参照文稿中的 v3 版本。 // v3版本 import java.lang.reflect.Method; public class Test { public static void target(int i) { //空方法 } public static void main(String[] args) throws Exception { Class klass = Class.forName(\"Test\"); Method method = klass.getMethod(\"target\", int.class); Object[] arg = new Object[1]; //在循环外构造参数数组 arg[0] = 128; long current = System.currentTimeMillis(); for (int i = 1; i 测得的结果反而更糟糕了，为基准的 2.9 倍。这是为什么呢？ 如果你在上一步解决了自动装箱之后查看运行时的 GC 状况，你会发现这段程序并不会触发 GC。其原因在于，原本的反射调用被内联了，从而使得即时编译器中的逃逸分析将原本新建的 Object 数组判定为不逃逸的对象。 如果一个对象不逃逸，那么即时编译器可以选择栈分配甚至是虚拟分配，也就是不占用堆空间。具体我会在本专栏的第二部分详细解释。 如果在循环外新建数组，即时编译器无法确定这个数组会不会中途被更改，因此无法优化掉访问数组的操作，可谓是得不偿失。 到目前为止，我们的最好记录是 1.8 倍。那能不能再进一步提升呢？ 刚才我曾提到，可以关闭反射调用的 Inflation 机制，从而取消委派实现，并且直接使用动态实现。此外，每次反射调用都会检查目标方法的权限，而这个检查同样可以在 Java 代码里关闭，在关闭了这两项机制之后，也就得到了我们的 v4 版本，它测得的结果约为基准的1.3 倍。 // v4版本 import java.lang.reflect.Method; //在运行指令中添加如下两个虚拟机参数： // -Djava.lang.Integer.IntegerCache.high=128 // -Dsun.reflect.noInflation=true public class Test { public static void target(int i) { //空方法 } public static void main(String[] args) throws Exception { Class klass = Class.forName(\"Test\"); Method method = klass.getMethod(\"target\", int.class); method.setAccessible(true); //关闭权限检查 long current = System.currentTimeMillis(); for (int i = 1; i 到这里，我们基本上把反射调用的水分都榨干了。接下来，我来把反射调用的性能开销给提回去。 首先，在这个例子中，之所以反射调用能够变得这么快，主要是因为即时编译器中的方法内联。在关闭了 Inflation 的情况下，内联的瓶颈在于 Method.invoke 方法中对MethodAccessor.invoke 方法的调用。 我会在后面的文章中介绍方法内联的具体实现，这里先说个结论：在生产环境中，我们往往拥有多个不同的反射调用，对应多个 GeneratedMethodAccessor，也就是动态实现。 由于 Java 虚拟机的关于上述调用点的类型 profile（注：对于 invokevirtual 或者invokeinterface，Java 虚拟机会记录下调用者的具体类型，我们称之为类型 profile）无法同时记录这么多个类，因此可能造成所测试的反射调用没有被内联的情况。 // v5版本 import java.lang.reflect.Method; public class Test { public static void target(int i) { //空方法 } public static void main(String[] args) throws Exception { Class klass = Class.forName(\"Test\"); Method method = klass.getMethod(\"target\", int.class); method.setAccessible(true); //关闭权限检查 polluteProfile(); long current = System.currentTimeMillis(); for (int i = 1; i 在上面的 v5 版本中，我在测试循环之前调用了 polluteProfile 的方法。该方法将反射调用另外两个方法，并且循环上 2000 遍。 而测试循环则保持不变。测得的结果约为基准的 6.7 倍。也就是说，只要误扰了Method.invoke 方法的类型 profile，性能开销便会从 1.3 倍上升至 6.7 倍。 之所以这么慢，除了没有内联之外，另外一个原因是逃逸分析不再起效。这时候，我们便可以采用刚才 v3 版本中的解决方案，在循环外构造参数数组，并直接传递给反射调用。这样子测得的结果约为基准的 5.2 倍。 除此之外，我们还可以提高 Java 虚拟机关于每个调用能够记录的类型数目（对应虚拟机参数 -XX:TypeProfileWidth，默认值为 2，这里设置为 3）。最终测得的结果约为基准的2.8 倍，尽管它和原本的 1.3 倍还有一定的差距，但总算是比 6.7 倍好多了。 总结 在默认情况下，方法的反射调用为委派实现，委派给本地实现来进行方法调用。在调用超过15 次之后，委派实现便会将委派对象切换至动态实现。这个动态实现的字节码是自动生成的，它将直接使用 invoke 指令来调用目标方法。 方法的反射调用会带来不少性能开销，原因主要有三个：变长参数方法导致的 Object 数组，基本类型的自动装箱、拆箱，还有最重要的方法内联。 反射的优势及缺陷 反射的优点： 增加程序的灵活性：面对需求变更时，可以灵活地实例化不同对象 但是，有得必有失，一项技术不可能只有优点没有缺点，反射也有两个比较隐晦的缺点： 反射的缺点点： 破坏类的封装性：可以强制访问 private 修饰的信息 性能损耗：反射相比直接实例化对象、调用方法、访问变量，中间需要非常多的检查步骤和解析步骤，JVM无法对它们优化。 破坏类的封装性 很明显的一个特点，反射可以获取类中被private修饰的变量、方法和构造器，这违反了面向对象的封装特性，因为被 private 修饰意味着不想对外暴露，只允许本类访问，而setAccessable(true)可以无视访问修饰符的限制，外界可以强制访问。 还记得单例模式一文吗？里面讲到反射破坏饿汉式和懒汉式单例模式，所以之后用了枚举避免被反射KO。 回到最初的起点，SmallPineapple 里有一个 weight 属性被 private 修饰符修饰，目的在于自己的体重并不想给外界知道。 性能损耗 在直接 new 对象并调用对象方法和访问属性时，编译器会在编译期提前检查可访问性，如果尝试进行不正确的访问，IDE会提前提示错误，例如参数传递类型不匹配，非法访问 private 属性和方法。 而在利用反射操作对象时，编译器无法提前得知对象的类型，访问是否合法，参数传递类型是否匹配。只有在程序运行时调用反射的代码时才会从头开始检查、调用、返回结果，JVM也无法对反射的代码进行优化。 虽然反射具有性能损耗的特点，但是我们不能一概而论，产生了使用反射就会性能下降的思想，反射的慢，需要同时调用上100W次才可能体现出来，在几次、几十次的调用，并不能体现反射的性能低下。所以不要一味地戴有色眼镜看反射，在单次调用反射的过程中，性能损耗可以忽略不计。如果程序的性能要求很高，那么尽量不要使用反射。 Java 反射效率低的原因 了解了反射的原理以后，我们来分析一下反射效率低的原因。 Method#invoke 方法会对参数做封装和解封操作 我们可以看到，invoke 方法的参数是 Object[] 类型，也就是说，如果方法参数是简单类型的话，需要在此转化成 Object 类型，例如 long ,在 javac compile 的时候 用了Long.valueOf() 转型，也就大量了生成了Long 的 Object, 同时 传入的参数是Object[]数值,那还需要额外封装object数组。 而在上面 MethodAccessorGenerator#emitInvoke 方法里我们看到，生成的字节码时，会把参数数组拆解开来，把参数恢复到没有被 Object[] 包装前的样子，同时还要对参数做校验，这里就涉及到了解封操作。 因此，在反射调用的时候，因为封装和解封，产生了额外的不必要的内存浪费，当调用次数达到一定量的时候，还会导致 GC。 需要检查方法可见性 通过上面的源码分析，我们会发现，反射时每次调用都必须检查方法的可见性（在 Method.invoke 里） 需要校验参数 反射时也必须检查每个实际参数与形式参数的类型匹配性（在NativeMethodAccessorImpl.invoke0 里或者生成的 Java 版 MethodAccessor.invoke 里）； 反射方法难以内联 Method#invoke 就像是个独木桥一样，各处的反射调用都要挤过去，在调用点上收集到的类型信息就会很乱，影响内联程序的判断，使得 Method.invoke() 自身难以被内联到调用方。参见 www.iteye.com/blog/rednax… JIT 无法优化 在 JavaDoc 中提到： Because reflection involves types that are dynamically resolved, certain Java virtual machine optimizations can not be performed. Consequently, reflective operations have slower performance than their non-reflective counterparts, and should be avoided in sections of code which are called frequently in performance-sensitive applications. 因为反射涉及到动态加载的类型，所以无法进行优化。 "},"Chapter05/JDKDynamicProxy.html":{"url":"Chapter05/JDKDynamicProxy.html","title":"JDK动态代理","keywords":"","body":"JDK 动态代理 动态代理可以很方便地对委托类的相关方法进行统一增强处理，如添加方法调用次数、添加日志功能等等。 动态代理主要分为JDK动态代理和cglib动态代理两大类，本文主要对JDK动态代理进行探讨 代理模式(静态代理) 开篇来个链接 代理模式 , 装饰器模式, 代理模式和装饰器模式对比 JDK 动态代理 与静态代理类对照的是动态代理类，动态代理类的字节码在程序运行时由Java反射机制动态生成，无需程序员手工编写它的源代码。 动态代理类不仅简化了编程工作，而且提高了软件系统的可扩展性，因为Java 反射机制可以生成任意类型的动态代理类。 java.lang.reflect 包中的Proxy类和 InvocationHandler 接口提供了生成动态代理类的能力。 实现动态代理的关键技术是反射； 代理对象是对目标对象的增强，以便对消息进行预处理和后处理； InvocationHandler中的invoke()方法是代理类完整逻辑的集中体现，包括要切入的增强逻辑和进行反射执行的真实业务逻辑； 使用JDK动态代理机制为某一真实业务对象生成代理，只需要指定目标接口、目标接口的类加载器以及具体的InvocationHandler即可。 JDK动态代理的典型应用包括但不仅限于AOP、RPC、Struts2、Spring等重要经典框架。 JDK动态代理到底是怎么实现的呢 动态代码涉及了一个非常重要的类 Proxy 。正是通过 Proxy 的静态方法 newProxyInstance才会动态创建代理。具体怎么去创建代理类就不分析了,感兴趣的可以去看下源码。我们直接看下生成的代理类。 如何查看生成的代理类？ 在生成代理类之前加上以下代码(我用的jdk1.8)： //新版本 jdk产生代理类 System.getProperties().put(\"jdk.proxy.ProxyGenerator.saveGeneratedFiles\", \"true\"); 如果上述代码加上不生效可以考虑加下下面的代码： // 老版本jdk System.getProperties().put(\"sun.misc.ProxyGenerator.saveGeneratedFiles\", \"true\"); // 该设置用于输出cglib动态代理产生的类 System.setProperty(DebuggingClassWriter.DEBUG_LOCATION_PROPERTY, \"C:\\\\class\"); 代码如下： 写在最后 JDK动态代理实际上是在运行时通过反射的方式来实现的，将代理的方法调用转到到目标对象上，最终将目标对象生成的任何结果返回给调用方。由于这是个链式调用，所以很方便代理在目标对象方法调用前后增加处理逻辑。根据这种思路可以在多种设计模式中使用JDK的动态代理比如代理模式、Facade、Decorator等。 在面向方面编程(AOP)也应用广泛，如事务管理、日志记录、数据校验等，主要是将横切关注点从业务逻辑中分离出来，所以一通百通。 补充一点，由于JDK的不断优化，到JDK8的时候JDK的动态代理不比CGLIB效率低，大家可以做些实验。 "},"Chapter05/StringBuilder.html":{"url":"Chapter05/StringBuilder.html","title":"深入理解StringBuilder","keywords":"","body":"深入理解StringBuilder 为什么StringBuilder是线程不安全的 原因分析 如果你看了StringBuilder或StringBuffer的源代码会说，因为StringBuilder在append操作时并未使用线程同步，而StringBuffer几乎大部分方法都使用了synchronized关键字进行方法级别的同步处理。 上面这种说法肯定是正确的，对照一下StringBuilder和StringBuffer的部分源代码也能够看出来。 StringBuilder的append方法源代码： @Override public StringBuilder append(String str) { super.append(str); return this; } StringBuffer的append方法源代码： @Override public synchronized StringBuffer append(String str) { toStringCache = null; super.append(str); return this; } 对于上面的结论肯定是没什么问题的，但并没有解释是什么原因导致了StringBuilder的线程不安全？ 为什么要使用synchronized来保证线程安全？如果不是用会出现什么异常情况？ 下面我们来逐一讲解。 异常示例 public static void test() throws InterruptedException { StringBuilder sb = new StringBuilder(); for (int i = 0; i { for (int j = 0; j 上述业务逻辑比较简单，就是构建一个StringBuilder，然后创建10个线程，每个线程中拼接字符串“a”1000次，理论上当线程执行完成之后，打印的结果应该是10000才对。 但多次执行上面的代码打印的结果是10000的概率反而非常小，大多数情况都要少于10000。同时，还有一定的概率出现下面的异常信息“ Exception in thread \"Thread-1\" java.lang.ArrayIndexOutOfBoundsException at java.lang.System.arraycopy(Native Method) at java.lang.String.getChars(String.java:826) at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:449) at java.lang.StringBuilder.append(StringBuilder.java:136) at string.StringB.lambda$test$0(StringB.java:20) at java.lang.Thread.run(Thread.java:748) 9513 线程不安全的原因 StringBuilder中针对字符串的处理主要依赖两个成员变量char数组value和count。StringBuilder通过对value的不断扩容和count对应的增加来完成字符串的append操作。 abstract class AbstractStringBuilder implements Appendable, CharSequence { /** * The value is used for character storage. */ // 存储的字符串（通常情况一部分为字符串内容，一部分为默认值） char[] value; /** * The count is the number of characters used. */ // 数组已经使用数量 int count; 上面的这两个属性均位于它的抽象父类AbstractStringBuilder中。 如果查看构造方法我们会发现，在创建StringBuilder时会设置数组value的初始化长度。 public StringBuilder(String str) { super(str.length() + 16); append(str); } AbstractStringBuilder(int capacity) { value = new char[capacity]; } 默认是传入字符串长度加16。这就是count存在的意义，因为数组中的一部分内容为默认值。 当调用append方法时会对count进行增加，增加值便是append的字符串的长度，具体实现也在抽象父类中 public AbstractStringBuilder append(String str) { if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this; } 我们所说的线程不安全的发生点便是在append方法中count的“+=”操作。我们知道该操作是线程不安全的，那么便会发生两个线程同时读取到count值为5，执行加1操作之后，都变成6，而不是预期的7。这种情况一旦发生便不会出现预期的结果。 抛异常的原因 回头看异常的堆栈信息，回发现有这么一行内容： at java.lang.String.getChars(String.java:826) 对应的代码就是上面AbstractStringBuilder中append方法中的代码。对应String类中getChars方法的源代码如下： public void getChars(int srcBegin, int srcEnd, char dst[], int dstBegin) { if (srcBegin value.length) { throw new StringIndexOutOfBoundsException(srcEnd); } if (srcBegin > srcEnd) { throw new StringIndexOutOfBoundsException(srcEnd - srcBegin); } System.arraycopy(value, srcBegin, dst, dstBegin, srcEnd - srcBegin); } 其实异常是最后一行arraycopy时JVM底层发生的。arraycopy的核心操作就是将传入的String对象copy到value当中。 而异常发生的原因是明明value的下标只到6，程序却要访问和操作下标为7的位置，当然就跑异常了。 那么，为什么会超出这么一个位置呢？这与我们上面讲到到的count被少加有关。在执行str.getChars方法之前还需要根据count校验一下当前的value是否使用完毕，如果使用完了，那么就进行扩容。append中对应的方法如下： ensureCapacityInternal(count + len); private void ensureCapacityInternal(int minimumCapacity) { // overflow-conscious code if (minimumCapacity - value.length > 0) { value = Arrays.copyOf(value, newCapacity(minimumCapacity)); } } count本应该为7，value长度为6，本应该触发扩容。但因为并发导致count为6，假设len为1，则传递的minimumCapacity为7，并不会进行扩容操作。这就导致后面执行str.getChars方法进行复制操作时访问了不存在的位置，因此抛出异常。 这里我们顺便看一下扩容方法中的newCapacity方法： private int newCapacity(int minCapacity) { // overflow-conscious code int newCapacity = (value.length 除了校验部分，最核心的就是将新数组的长度扩充为原来的两倍再加2。把计算所得的新长度作为Arrays.copyOf的参数进行扩容。 "},"Chapter05/spi.html":{"url":"Chapter05/spi.html","title":"spi","keywords":"","body":"spi JDK提供的SPI(Service Provider Interface)机制，可能很多人不太熟悉，因为这个机制是针对厂商或者插件的，也可以在一些框架的扩展中看到。 其核心类 java.util.ServiceLoader可以在jdk1.8的文档中看到详细的介绍。 虽然不太常见，但并不代表它不常用，恰恰相反，你无时无刻不在用它。玄乎了，莫急，思考一下你的项目中是否有用到第三方日志包，是否有用到数据库驱动？其实这些都和SPI有关。 再来思考一下，现代的框架是如何加载日志依赖，加载数据库驱动的，你可能会对class.forName(\"com.mysql.jdbc.Driver\")这段代码不陌生，这是每个java初学者必定遇到过的，但如今的数据库驱动仍然是这样加载的吗？你还能找到这段代码吗？ 这一切的疑问，将在本篇文章结束后得到解答。 什么是SPI机制 那么，什么是SPI机制呢？ SPI是Service Provider Interface 的简称，即服务提供者接口的意思。根据字面意思我们可能还有点困惑，SPI说白了就是一种扩展机制，我们在相应配置文件中定义好某个接口的实现类，然后再根据这个接口去这个配置文件中加载这个实例类并实例化，其实SPI就是这么一个东西。说到SPI机制，我们最常见的就是Java的SPI机制，此外，还有Dubbo和SpringBoot自定义的SPI机制。 有了SPI机制，那么就为一些框架的灵活扩展提供了可能，而不必将框架的一些实现类写死在代码里面。 那么，某些框架是如何利用SPI机制来做到灵活扩展的呢？下面举几个栗子来阐述下： JDBC驱动加载案例：利用Java的SPI机制，我们可以根据不同的数据库厂商来引入不同的JDBC驱动包； SpringBoot的SPI机制：我们可以在spring.factories中加上我们自定义的自动配置类，事件监听器或初始化器等； Dubbo的SPI机制：Dubbo更是把SPI机制应用的淋漓尽致，Dubbo基本上自身的每个功能点都提供了扩展点，比如提供了集群扩展，路由扩展和负载均衡扩展等差不多接近30个扩展点。如果Dubbo的某个内置实现不符合我们的需求，那么我们只要利用其SPI机制将我们的实现替换掉Dubbo的实现即可。 上面的三个栗子先让我们直观感受下某些框架利用SPI机制是如何做到灵活扩展的。 SPI示例 新建一个项目spi-test,并且下面划分4个模块。 org.example spi-test pom 1.0-SNAPSHOT 1.6 interface firelog huolog test interface spi-test org.example 1.0-SNAPSHOT 4.0.0 interface 1.6 package io.github.fire.spi.face; public interface Logger { void info(String log); } firelog spi-test org.example 1.0-SNAPSHOT 4.0.0 firelog 1.6 org.example interface 1.0-SNAPSHOT package io.github.fire.spi; import io.github.fire.spi.face.Logger; public class FireLog implements Logger { public void info(String log) { System.out.printf(\"fire out ->\" + log); } } huolog spi-test org.example 1.0-SNAPSHOT 4.0.0 huolog 1.6 org.example interface 1.0-SNAPSHOT package io.github.fire.spi; import io.github.fire.spi.face.Logger; public class HuoLog implements Logger { public void info(String log) { System.out.printf(\"huo out -> \" + log); } } test spi-test org.example 1.0-SNAPSHOT 4.0.0 test org.apache.maven.plugins maven-compiler-plugin 6 6 1.6 org.example interface 1.0-SNAPSHOT org.example huolog 1.0-SNAPSHOT org.example firelog 1.0-SNAPSHOT package io.github.fire.spi; import io.github.fire.spi.face.Logger; import java.util.ServiceLoader; public class Invoker { public static void main(String[] args) { ServiceLoader LOGGER = ServiceLoader.load(Logger.class); for (Logger log: LOGGER){ log.info(\"hello spi\"); } System.out.printf(\"？？？？？\"); } } 如果直接运行的化，只会打印 ？？？？？ 。因为Logger并不能找到实现类 添加SPI支持 在firelog和huolog的resources\\META-INF\\services下添加文件 io.github.fire.spi.face.Logger 并分别添加如下内容 firelog io.github.fire.spi.FireLog huolog io.github.fire.spi.HuoLog 有没有发现点什么？不错文件名为接口 Logger 的全路径内容为各自实现的全路径 这里需要重点说明，每一个SPI接口都需要在自己项目的静态资源目录中声明一个services文件，文件名为实现规范接口的类名全路径 再次运行 huo out -> hello spifire out ->hello spi？？？？？ 这样一个厂商的实现便完成了。 SPI在实际项目中的应用 mysql 在mysql-connector-java-xxx.jar中发现了META-INF\\services\\java.sql.Driver文件，里面只有两行记录： com.mysql.jdbc.Driver com.mysql.fabric.jdbc.FabricMySQLDriver 我们可以分析出， java.sql.Driver是一个规范接口， com.mysql.jdbc.Driver com.mysql.fabric.jdbc.FabricMySQLDriver则是mysql-connector-java-xxx.jar对这个规范的实现接口。 slf4j 在jcl-over-slf4j-xxxx.jar中发现了META-INF\\services\\org.apache.commons.logging.LogFactory文件，里面只有一行记录： org.apache.commons.logging.impl.SLF4JLogFactory Java的SPI机制的源码解读 通过前面扩展Developer接口的简单Demo，我们看到Java的SPI机制实现跟ServiceLoader这个类有关，那么我们先来看下ServiceLoader的类结构代码： // ServiceLoader实现了【Iterable】接口 public final class ServiceLoader implements Iterable{ private static final String PREFIX = \"META-INF/services/\"; // The class or interface representing the service being loaded private final Class service; // The class loader used to locate, load, and instantiate providers private final ClassLoader loader; // The access control context taken when the ServiceLoader is created private final AccessControlContext acc; // Cached providers, in instantiation order private LinkedHashMap providers = new LinkedHashMap<>(); // The current lazy-lookup iterator private LazyIterator lookupIterator; // 构造方法 private ServiceLoader(Class svc, ClassLoader cl) { service = Objects.requireNonNull(svc, \"Service interface cannot be null\"); loader = (cl == null) ? ClassLoader.getSystemClassLoader() : cl; acc = (System.getSecurityManager() != null) ? AccessController.getContext() : null; reload(); } // ...暂时省略相关代码 // ServiceLoader的内部类LazyIterator,实现了【Iterator】接口 // Private inner class implementing fully-lazy provider lookup private class LazyIterator implements Iterator{ Class service; ClassLoader loader; Enumeration configs = null; Iterator pending = null; String nextName = null; private LazyIterator(Class service, ClassLoader loader) { this.service = service; this.loader = loader; } // 覆写Iterator接口的hasNext方法 public boolean hasNext() { // ...暂时省略相关代码 } // 覆写Iterator接口的next方法 public S next() { // ...暂时省略相关代码 } // 覆写Iterator接口的remove方法 public void remove() { // ...暂时省略相关代码 } } // 覆写Iterable接口的iterator方法，返回一个迭代器 public Iterator iterator() { // ...暂时省略相关代码 } // ...暂时省略相关代码 } 可以看到，ServiceLoader实现了Iterable接口，覆写其iterator方法能产生一个迭代器；同时ServiceLoader有一个内部类LazyIterator，而LazyIterator又实现了Iterator接口，说明LazyIterator是一个迭代器。 ServiceLoader.load方法，为加载服务提供者实现类做前期准备 那么我们现在开始探究Java的SPI机制的源码， 先来看JdkSPITest的第一句代码ServiceLoader serviceLoader = ServiceLoader.load(Developer.class);中的ServiceLoader.load(Developer.class);的源码： // ServiceLoader.java public static ServiceLoader load(Class service) { //获取当前线程上下文类加载器 ClassLoader cl = Thread.currentThread().getContextClassLoader(); // 将service接口类和线程上下文类加载器作为参数传入，继续调用load方法 return ServiceLoader.load(service, cl); } 我们再来看下ServiceLoader.load(service, cl);方法： // ServiceLoader.java public static ServiceLoader load(Class service, ClassLoader loader) { // 将service接口类和线程上下文类加载器作为构造参数，新建了一个ServiceLoader对象 return new ServiceLoader<>(service, loader); } 继续看new ServiceLoader<>(service, loader);是如何构建的？ // ServiceLoader.java private ServiceLoader(Class svc, ClassLoader cl) { service = Objects.requireNonNull(svc, \"Service interface cannot be null\"); loader = (cl == null) ? ClassLoader.getSystemClassLoader() : cl; acc = (System.getSecurityManager() != null) ? AccessController.getContext() : null; reload(); } 可以看到在构建ServiceLoader对象时除了给其成员属性赋值外，还调用了reload方法： // ServiceLoader.java public void reload() { providers.clear(); lookupIterator = new LazyIterator(service, loader); } 可以看到在reload方法中又新建了一个LazyIterator对象，然后赋值给lookupIterator。 // ServiceLoader$LazyIterator.java private LazyIterator(Class service, ClassLoader loader) { this.service = service; this.loader = loader; } 可以看到在构建LazyIterator对象时，也只是给其成员变量service和loader属性赋值呀，我们一路源码跟下来，也没有看到去META-INF/services文件夹加载Developer接口的实现类！这就奇怪了，我们都被ServiceLoader的load方法名骗了。 还记得分析前面的代码时新建了一个LazyIterator对象吗？Lazy顾名思义是懒的意思，Iterator就是迭代的意思。我们此时猜测那么LazyIterator对象的作用应该就是在迭代的时候再去加载Developer接口的实现类了。 ServiceLoader.iterator方法，实现服务提供者实现类的懒加载 我们现在再来看JdkSPITest的第二句代码serviceLoader.forEach(Developer::sayHi);，执行这句代码后最终会调用serviceLoader的iterator方法： // serviceLoader.java public Iterator iterator() { return new Iterator() { Iterator> knownProviders = providers.entrySet().iterator(); public boolean hasNext() { if (knownProviders.hasNext()) return true; // 调用lookupIterator即LazyIterator的hasNext方法 // 可以看到是委托给LazyIterator的hasNext方法来实现 return lookupIterator.hasNext(); } public S next() { if (knownProviders.hasNext()) return knownProviders.next().getValue(); // 调用lookupIterator即LazyIterator的next方法 // 可以看到是委托给LazyIterator的next方法来实现 return lookupIterator.next(); } public void remove() { throw new UnsupportedOperationException(); } }; } 可以看到调用serviceLoader的iterator方法会返回一个匿名的迭代器对象，而这个匿名迭代器对象其实相当于一个门面类，其覆写的hasNext和next方法又分别委托LazyIterator的hasNext和next方法来实现了。 我们继续调试，发现接下来会进入LazyIterator的hasNext方法： // serviceLoader$LazyIterator.java public boolean hasNext() { if (acc == null) { // 调用hasNextService方法 return hasNextService(); } else { PrivilegedAction action = new PrivilegedAction() { public Boolean run() { return hasNextService(); } }; return AccessController.doPrivileged(action, acc); } } 继续跟进hasNextService方法： // serviceLoader$LazyIterator.java private boolean hasNextService() { if (nextName != null) { return true; } if (configs == null) { try { // PREFIX = \"META-INF/services/\" // service.getName()即接口的全限定名 // 还记得前面的代码构建LazyIterator对象时已经给其成员属性service赋值吗 String fullName = PREFIX + service.getName(); // 加载META-INF/services/目录下的接口文件中的服务提供者类 if (loader == null) configs = ClassLoader.getSystemResources(fullName); else // 还记得前面的代码构建LazyIterator对象时已经给其成员属性loader赋值吗 configs = loader.getResources(fullName); } catch (IOException x) { fail(service, \"Error locating configuration files\", x); } } while ((pending == null) || !pending.hasNext()) { if (!configs.hasMoreElements()) { return false; } // 返回META-INF/services/目录下的接口文件中的服务提供者类并赋值给pending属性 pending = parse(service, configs.nextElement()); } // 然后取出一个全限定名赋值给LazyIterator的成员变量nextName nextName = pending.next(); return true; } 可以看到在执行LazyIterator的hasNextService方法时最终将去META-INF/services/目录下加载接口文件的内容即加载服务提供者实现类的全限定名，然后取出一个服务提供者实现类的全限定名赋值给LazyIterator的成员变量nextName。到了这里，我们就明白了LazyIterator的作用真的是懒加载，在用到的时候才会去加载。 思考：为何这里要用懒加载呢？懒加载的思想是怎样的呢？懒加载有啥好处呢？你还能举出其他懒加载的案例吗？ 同样，执行完LazyIterator的hasNext方法后，会继续执行LazyIterator的next方法： // serviceLoader$LazyIterator.java public S next() { if (acc == null) { // 调用nextService方法 return nextService(); } else { PrivilegedAction action = new PrivilegedAction() { public S run() { return nextService(); } }; return AccessController.doPrivileged(action, acc); } } 我们继续跟进nextService方法： // serviceLoader$LazyIterator.java private S nextService() { if (!hasNextService()) throw new NoSuchElementException(); // 还记得在hasNextService方法中为nextName赋值过服务提供者实现类的全限定名吗 String cn = nextName; nextName = null; Class c = null; try { // 【1】去classpath中根据传入的类加载器和服务提供者实现类的全限定名去加载服务提供者实现类 c = Class.forName(cn, false, loader); } catch (ClassNotFoundException x) { fail(service, \"Provider \" + cn + \" not found\"); } if (!service.isAssignableFrom(c)) { fail(service, \"Provider \" + cn + \" not a subtype\"); } try { // 【2】实例化刚才加载的服务提供者实现类，并进行转换 S p = service.cast(c.newInstance()); // 【3】最终将实例化后的服务提供者实现类放进providers集合 providers.put(cn, p); return p; } catch (Throwable x) { fail(service, \"Provider \" + cn + \" could not be instantiated\", x); } throw new Error(); // This cannot happen } 可以看到LazyIterator的nextService方法最终将实例化之前加载的服务提供者实现类，并放进providers集合中，随后再调用服务提供者实现类的方法（比如这里指JavaDeveloper的sayHi方法）。注意，这里是加载一个服务提供者实现类后，若main函数中有调用该服务提供者实现类的方法的话，紧接着会调用其方法；然后继续实例化下一个服务提供者类。 设计模式：可以看到，Java的SPI机制实现代码中应用了迭代器模式，迭代器模式屏蔽了各种存储对象的内部结构差异，提供一个统一的视图来遍历各个存储对象（存储对象可以为集合，数组等）。java.util.Iterator也是迭代器模式的实现：同时Java的各个集合类一般实现了Iterable接口，实现了其iterator方法从而获得Iterator接口的实现类对象（一般为集合内部类），然后再利用Iterator对象的实现类的hasNext和next方法来遍历集合元素。 画外题 既然说到了数据库驱动，索性再多说一点，还记得一道经典的面试题：class.forName(\"com.mysql.jdbc.Driver\")到底做了什么事？ 先思考下：自己会怎么回答？ 都知道class.forName与类加载机制有关，会触发执行com.mysql.jdbc.Driver类中的静态方法，从而使主类加载数据库驱动。 如果再追问，为什么它的静态块没有自动触发？可答：因为数据库驱动类的特殊性质，JDBC规范中明确要求Driver类必须向DriverManager注册自己，导致其必须由class.forName手动触发，这可以在java.sql.Driver中得到解释。 完美了吗？还没，来到最新的DriverManager源码中，可以看到这样的注释,翻译如下： DriverManager 类的方法 getConnection 和 getDrivers 已经得到提高以支持 Java Standard Edition Service Provider 机制。 JDBC 4.0 Drivers 必须包括 META-INF/services/java.sql.Driver 文件。此文件包含 java.sql.Driver 的 JDBC 驱动程序实现的名称。 例如，要加载 my.sql.Driver 类， META-INF/services/java.sql.Driver 文件需要包含下面的条目： my.sql.Driver 应用程序不再需要使用 Class.forName() 显式地加载 JDBC 驱动程序。当前使用 Class.forName() 加载 JDBC 驱动程序的现有程序将在不作修改的情况下继续工作。 可以发现，Class.forName已经被弃用了，所以，这道题目的最佳回答，应当是和面试官牵扯到JAVA中的SPI机制，进而聊聊加载驱动的演变历史。 java.sql.DriverManager 在JdbcTest的main函数调用DriverManager的getConnection方法时，此时必然会先执行DriverManager类的静态代码块的代码，然后再执行getConnection方法，那么先来看下DriverManager的静态代码块： // DriverManager.java static { // 加载驱动实现类 loadInitialDrivers(); println(\"JDBC DriverManager initialized\"); } 继续跟进loadInitialDrivers的代码： // DriverManager.java private static void loadInitialDrivers() { String drivers; try { drivers = AccessController.doPrivileged(new PrivilegedAction() { public String run() { return System.getProperty(\"jdbc.drivers\"); } }); } catch (Exception ex) { drivers = null; } AccessController.doPrivileged(new PrivilegedAction() { public Void run() { // 来到这里，是不是感觉似曾相识，对，没错，我们在前面的JdkSPITest代码中执行过下面的两句代码 // 这句代码前面已经分析过，这里不会真正加载服务提供者实现类 // 而是实例化一个ServiceLoader对象且实例化一个LazyIterator对象用于懒加载 ServiceLoader loadedDrivers = ServiceLoader.load(Driver.class); // 调用ServiceLoader的iterator方法，在迭代的同时，也会去加载并实例化META-INF/services/java.sql.Driver文件 // 的com.mysql.jdbc.Driver和com.mysql.fabric.jdbc.FabricMySQLDriver两个驱动类 /****************【主线，重点关注】**********************/ Iterator driversIterator = loadedDrivers.iterator(); try{ while(driversIterator.hasNext()) { driversIterator.next(); } } catch(Throwable t) { // Do nothing } return null; } }); println(\"DriverManager.initialize: jdbc.drivers = \" + drivers); if (drivers == null || drivers.equals(\"\")) { return; } String[] driversList = drivers.split(\":\"); println(\"number of Drivers:\" + driversList.length); for (String aDriver : driversList) { try { println(\"DriverManager.Initialize: loading \" + aDriver); Class.forName(aDriver, true, ClassLoader.getSystemClassLoader()); } catch (Exception ex) { println(\"DriverManager.Initialize: load failed: \" + ex); } } } 在上面的代码中，我们可以看到Mysql的驱动类加载主要是利用Java的SPI机制实现的，即利用ServiceLoader来实现加载并实例化Mysql的驱动类。 注册Mysql的驱动类 那么，上面的代码只是Mysql驱动类的加载和实例化，那么，驱动类又是如何被注册进DriverManager的registeredDrivers集合的呢？ 这时，我们注意到com.mysql.jdbc.Driver类里面也有个静态代码块，即实例化该类时肯定会触发该静态代码块代码的执行，那么我们直接看下这个静态代码块做了什么事情： // com.mysql.jdbc.Driver.java // Register ourselves with the DriverManager static { try { // 将自己注册进DriverManager类的registeredDrivers集合 java.sql.DriverManager.registerDriver(new Driver()); } catch (SQLException E) { throw new RuntimeException(\"Can't register driver!\"); } } 可以看到，原来就是Mysql驱动类com.mysql.jdbc.Driver在实例化的时候，利用执行其静态代码块的时机时将自己注册进DriverManager的registeredDrivers集合中。 好，继续跟进DriverManager的registerDriver方法： // DriverManager.java public static synchronized void registerDriver(java.sql.Driver driver) throws SQLException { // 继续调用registerDriver方法 registerDriver(driver, null); } public static synchronized void registerDriver(java.sql.Driver driver, DriverAction da) throws SQLException { /* Register the driver if it has not already been added to our list */ if(driver != null) { // 将driver驱动类实例注册进registeredDrivers集合 registeredDrivers.addIfAbsent(new DriverInfo(driver, da)); } else { // This is for compatibility with the original DriverManager throw new NullPointerException(); } println(\"registerDriver: \" + driver); } 分析到了这里，我们就明白了Java的SPI机制是如何加载Mysql的驱动类的并如何将Mysql的驱动类注册进DriverManager的registeredDrivers集合中的。 使用之前注册的Mysql驱动类连接数据库 既然Mysql的驱动类已经被注册进来了，那么何时会被用到呢？ 我们要连接Mysql数据库，自然需要用到Mysql的驱动类，对吧。此时我们回到JDBC的测试代码JdbcTest类的connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/jdbc\", \"root\", \"123456\");这句代码中，看一下getConnection的源码： // DriverManager.java @CallerSensitive public static Connection getConnection(String url, String user, String password) throws SQLException { java.util.Properties info = new java.util.Properties(); if (user != null) { info.put(\"user\", user); } if (password != null) { info.put(\"password\", password); } // 继续调用getConnection方法来连接数据库 return (getConnection(url, info, Reflection.getCallerClass())); } 继续跟进getConnection方法： // DriverManager.java private static Connection getConnection( String url, java.util.Properties info, Class caller) throws SQLException { ClassLoader callerCL = caller != null ? caller.getClassLoader() : null; synchronized(DriverManager.class) { // synchronize loading of the correct classloader. if (callerCL == null) { callerCL = Thread.currentThread().getContextClassLoader(); } } if(url == null) { throw new SQLException(\"The url cannot be null\", \"08001\"); } println(\"DriverManager.getConnection(\\\"\" + url + \"\\\")\"); // Walk through the loaded registeredDrivers attempting to make a connection. // Remember the first exception that gets raised so we can reraise it. SQLException reason = null; // 遍历registeredDrivers集合，注意之前加载的Mysql驱动类实例被注册进这个集合 for(DriverInfo aDriver : registeredDrivers) { // If the caller does not have permission to load the driver then // skip it. // 判断有无权限 if(isDriverAllowed(aDriver.driver, callerCL)) { try { println(\" trying \" + aDriver.driver.getClass().getName()); // 利用Mysql驱动类来连接数据库 /*************【主线，重点关注】*****************/ Connection con = aDriver.driver.connect(url, info); // 只要连接上，那么加载的其余驱动类比如FabricMySQLDriver将会忽略，因为下面直接返回了 if (con != null) { // Success! println(\"getConnection returning \" + aDriver.driver.getClass().getName()); return (con); } } catch (SQLException ex) { if (reason == null) { reason = ex; } } } else { println(\" skipping: \" + aDriver.getClass().getName()); } } // if we got here nobody could connect. if (reason != null) { println(\"getConnection failed: \" + reason); throw reason; } println(\"getConnection: no suitable driver found for \"+ url); throw new SQLException(\"No suitable driver found for \"+ url, \"08001\"); } 可以看到，DriverManager的getConnection方法会从registeredDrivers集合中拿出刚才加载的Mysql驱动类来连接数据库。 好了，到了这里，JDBC驱动加载的源码就基本分析完了。 线程上下文类加载器 前面基本分析完了JDBC驱动加载的源码，但是还有一个很重要的知识点还没讲解，那就是破坏类加载机制的双亲委派模型的线程上下文类加载器。 我们都知道，JDBC规范的相关类（比如前面的java.sql.Driver和java.sql.DriverManager）都是在Jdk的rt.jar包下，意味着这些类将由启动类加载器(BootstrapClassLoader)加载；而Mysql的驱动类由外部数据库厂商实现，当驱动类被引进项目时也是位于项目的classpath中,此时启动类加载器肯定是不可能加载这些驱动类的呀，此时该怎么办？ 由于类加载机制的双亲委派模型在这方面的缺陷，因此只能打破双亲委派模型了。因为项目classpath中的类是由应用程序类加载器(AppClassLoader)来加载，所以我们可否\"逆向\"让启动类加载器委托应用程序类加载器去加载这些外部数据库厂商的驱动类呢？如果可以，我们怎样才能做到让启动类加载器委托应用程序类加载器去加载 classpath中的类呢？ 答案肯定是可以的，我们可以将应用程序类加载器设置进线程里面，即线程里面新定义一个类加载器的属性contextClassLoader，然后在某个时机将应用程序类加载器设置进线程的contextClassLoader这个属性里面，如果没有设置的话，那么默认就是应用程序类加载器。然后启动类加载器去加载java.sql.Driver和java.sql.DriverManager等类时，同时也会从当前线程中取出contextClassLoader即应用程序类加载器去classpath中加载外部厂商提供的JDBC驱动类。因此，通过破坏类加载机制的双亲委派模型，利用线程上下文类加载器完美的解决了该问题。 此时我们再回过头来看下在加载Mysql驱动时是什么时候获取的线程上下文类加载器呢？ 答案就是在DriverManager的loadInitialDrivers方法调用了ServiceLoader loadedDrivers = ServiceLoader.load(Driver.class);这句代码，而取出线程上下文类加载器就是在ServiceLoader的load方法中取出： public static ServiceLoader load(Class service) { // 取出线程上下文类加载器取出的是contextClassLoader，而contextClassLoader装的应用程序类加载器 ClassLoader cl = Thread.currentThread().getContextClassLoader(); // 把刚才取出的线程上下文类加载器作为参数传入，用于后去加载classpath中的外部厂商提供的驱动类 return ServiceLoader.load(service, cl); } 因此，到了这里，我们就明白了线程上下文类加载器在加载JDBC驱动包中充当的作用了。此外，我们应该知道，Java的绝大部分涉及SPI的加载都是利用线程上下文类加载器来完成的，比如JNDI,JCE,JBI等。 扩展：打破类加载机制的双亲委派模型的还有代码的热部署等，另外，Tomcat的类加载机制也值得一读。 扩展：Dubbo的SPI机制 前面也讲到Dubbo框架身上处处是SPI机制的应用，可以说处处都是扩展点，真的是把SPI机制应用的淋漓尽致。但是Dubbo没有采用默认的Java的SPI机制，而是自己实现了一套SPI机制。 那么，Dubbo为什么没有采用Java的SPI机制呢？ 原因主要有两个： Java的SPI机制会一次性实例化扩展点所有实现，如果有扩展实现初始化很耗时，但如果没用上也加载，会很浪费资源; Java的SPI机制没有Ioc和AOP的支持，因此Dubbo用了自己的SPI机制：增加了对扩展点IoC和AOP的支持，一个扩展点可以直接setter注入其它扩展点。 由于以上原因，Dubbo自定义了一套SPI机制，用于加载自己的扩展点。关于Dubbo的SPI机制这里不再详述，感兴趣的小伙伴们可以去Dubbo官网看看是如何扩展Dubbo的SPI的？还有其官网也有Duboo的SPI的源码分析文章。 "},"Chapter05/SourceCode.html":{"url":"Chapter05/SourceCode.html","title":"SourceCode","keywords":"","body":"源码解析 一、sun.misc.VM.getSavedProperty和System.getProperty的区别是什么 java运行的设置： -Djava.lang.Integer.IntegerCache.high=250 -Dhigh=250 public static void main(String[] args) { String a = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); String b = sun.misc.VM.getSavedProperty(\"high\"); String c = System.getProperty(\"java.lang.Integer.IntegerCache.high\"); String d = System.getProperty(\"high\"); System.err.println(a); System.err.println(b); System.err.println(c); System.err.println(d); } 结果： 250 250 null 250 为什么对于java.lang.Integer.IntegerCache.high这个设置的参数值用System.getProperty获取不到， 但是用sun.misc.VM.getSavedProperty是可以获取到的？ 原因如下： 为了将JVM系统所需要的参数和用户使用的参数区别开， java.lang.System.initializeSystemClass在启动时，会将启动参数保存在两个地方： 1、sun.misc.VM.savedProps中保存全部JVM接收的系统参数。 JVM会在启动时，调用java.lang.System.initializeSystemClass方法，初始化该属性。 同时也会调用sun.misc.VM.saveAndRemoveProperties方法，从java.lang.System.props中删除以下属性： sun.nio.MaxDirectMemorySize sun.nio.PageAlignDirectMemory sun.lang.ClassLoader.allowArraySyntax java.lang.Integer.IntegerCache.high sun.zip.disableMemoryMapping sun.java.launcher.diag 以上罗列的属性都是JVM启动需要设置的系统参数，所以为了安全考虑和隔离角度考虑，将其从用户可访问的System.props分开。 2、java.lang.System.props中保存除了以下JVM启动需要的参数外的其他参数。 sun.nio.MaxDirectMemorySize sun.nio.PageAlignDirectMemory sun.lang.ClassLoader.allowArraySyntax java.lang.Integer.IntegerCache.high sun.zip.disableMemoryMapping sun.java.launcher.diag "},"Chapter05/KeyTool.html":{"url":"Chapter05/KeyTool.html","title":"KeyTool","keywords":"","body":"java中Keytool的使用总结 Keytool 是一个Java 数据证书的管理工具 ,Keytool 将密钥（key）和证书（certificates）存在一个称为keystore的文件中 在keystore里，包含两种数据： 密钥实体（Key entity）——密钥（secret key）又或者是私钥和配对公钥（采用非对称加密） 可信任的证书实体（trusted certificate entries）——只包含公钥 ailas(别名)每个keystore都关联这一个独一无二的alias，这个alias通常不区分大小写 JDK中keytool 常用命令: -genkey 在用户主目录中创建一个默认文件\".keystore\",还会产生一个mykey的别名，mykey中包含用户的公钥、私钥和证书 (在没有指定生成位置的情况下,keystore会存在用户系统默认目录，如：对于window xp系统，会生成在系统的C:/Documents and Settings/UserName/文件名为“.keystore”) -alias 产生别名 -keystore 指定密钥库的名称(产生的各类信息将不在.keystore文件中) -keyalg 指定密钥的算法 (如 RSA DSA（如果不指定默认采用DSA）) -validity 指定创建的证书有效期多少天 -keysize 指定密钥长度 -storepass 指定密钥库的密码(获取keystore信息所需的密码) -keypass 指定别名条目的密码(私钥的密码) -dname 指定证书拥有者信息 例如： \"CN=名字与姓氏,OU=组织单位名称,O=组织名称,L=城市或区域名称,ST=州或省份名称,C=单位的两字母国家代码\" -list 显示密钥库中的证书信息 keytool -list -v -keystore 指定keystore -storepass 密码 -v 显示密钥库中的证书详细信息 -export 将别名指定的证书导出到文件 keytool -export -alias 需要导出的别名 -keystore 指定keystore -file 指定导出的证书位置及证书名称 -storepass 密码 -file 参数指定导出到文件的文件名 -delete 删除密钥库中某条目 keytool -delete -alias 指定需删除的别 -keystore 指定keystore -storepass 密码 -printcert 查看导出的证书信息 keytool -printcert -file yushan.crt -keypasswd 修改密钥库中指定条目口令 keytool -keypasswd -alias 需修改的别名 -keypass 旧密码 -new 新密码 -storepass keystore密码 -keystore sage -storepasswd 修改keystore口令 keytool -storepasswd -keystore e:/yushan.keystore(需修改口令的keystore) -storepass 123456(原始密码) -new yushan(新密码) -import 将已签名数字证书导入密钥库 keytool -import -alias 指定导入条目的别名 -keystore 指定keystore -file 需导入的证书 下面是各选项的缺省值。 -alias \"mykey\" -keyalg \"DSA\" -keysize 1024 -validity 90 -keystore 用户宿主目录中名为 .keystore 的文件 -file 读时为标准输入，写时为标准输出 keystore的生成 分阶段生成 keytool -genkey -alias yushan(别名) -keypass yushan(别名密码) -keyalg RSA(算法) -keysize 1024(密钥长度) -validity 365(有效期，天单位) -keystore e:/yushan.keystore(指定生成证书的位置和证书名称) -storepass 123456(获取keystore信息的密码) 一次性生成 keytool -genkey -alias yushan -keypass yushan -keyalg RSA -keysize 1024 -validity 365 -keystore e:/yushan.keystore -storepass 123456 -dname \"CN=(名字(名字与姓氏), OU=(组织单位名称), O=(组织名称), L=(城市或区域名称), ST=(州或省份名称), C=(单位的两字母国家代码)\";(中英文即可) keystore信息的查看 keytool -list -v -keystore e:/keytool /yushan.keystore -storepass 123456 证书的导出 keytool -export -alias yushan -keystore e:/yushan.keystore -file e:/yushan.crt(指定导出的证书位置及证书名称) -storepass 123456 查看导出的证书信息 keytool -printcert -file yushan.crt 证书的导入 "},"Chapter05/javaFace.html":{"url":"Chapter05/javaFace.html","title":"javaFace","keywords":"","body":"java 面试 List 和 Set 的区别 HashSet 是如何保证不重复的 HashMap 是线程安全的吗，为什么不是线程安全的（最好画图说明多线程环境下不安全）? HashMap 1.7 与 1.8 的 区别，说明 1.8 做了哪些优化，如何优化的？ final finally finalize 强引用 、软引用、 弱引用、虚引用 Java反射 Arrays.sort 实现原理和 Collection 实现原理 LinkedHashMap的应用 cloneable接口实现原理 异常分类以及处理机制 wait和sleep的区别 Synchronized 字节码指令 每个对象有一个监视器锁（monitor）。当monitor被占用时就会处于锁定状态。 关于方法的同步：可以看到相对于普通方法，其常量池中多了ACC_SYNCHRONIZED标示符，JVM就是根据该标示符来实现方法的同步的，当方法调用时，调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取monitor，获取成功之后才能执行方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个monitor对象。 关于同步块的同步：线程执行monitorenter指令时尝试获取monitor的所有权，执行monitorexit其他被这个monitor阻塞的线程可以尝试去获取这个 monitor 的所有权 Synchronized的语义底层是通过一个monitor的对象来完成，其实wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，否则会抛出java.lang.IllegalMonitorStateException的异常的原因。 现在我们应该知道，Synchronized是通过对象内部的一个叫做监视器锁（monitor）来实现的。但是监视器锁本质又是依赖于底层的操作系统的Mutex Lock来实现的。而操作系统实现线程之间的切换这就需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized效率低的原因。JDK1.6以后，为了减少获得锁和释放锁所带来的性能消耗，提高性能，引入了“偏向锁”和“轻量级锁” volatile 字节码指令 之所以定位到这两行是因为这里结尾写明了line 14，line 14即volatile变量instance赋值的地方。后面的add dword ptr [rsp],0h都是正常的汇编语句，意思是将双字节的栈指针寄存器+0，这里的关键就是add前面的lock指令，后面详细分析一下lock指令的作用和为什么加上lock指令后就能保证volatile关键字的内存可见性。 cas 如上面源代码所示，程序会根据当前处理器的类型来决定是否为cmpxchg指令添加lock前缀。如果程序是在多处理器上运行，就为cmpxchg指令加上lock前缀（lock cmpxchg）。反之，如果程序是在单处理器上运行，就省略lock前缀（单处理器自身会维护单处理器内的顺序一致性，不需要lock前缀提供的内存屏障效果）。 intel的手册对lock前缀的说明如下： 确保对内存的读-改-写操作原子执行。在Pentium及Pentium之前的处理器中，带有lock前缀的指令在执行期间会锁住总线，使得其他处理器暂时无法通过总线访问内存。很显然，这会带来昂贵的开销。从Pentium 4，Intel Xeon及P6处理器开始，intel在原有总线锁的基础上做了一个很有意义的优化：如果要访问的内存区域（area of memory）在lock前缀指令执行期间已经在处理器内部的缓存中被锁定（即包含该内存区域的缓存行当前处于独占或以修改状态），并且该内存区域被完全包含在单个缓存行（cache line）中，那么处理器将直接执行该指令。由于在指令执行期间该缓存行会一直被锁定，其它处理器无法读/写该指令要访问的内存区域，因此能保证指令执行的原子性。这个操作过程叫做缓存锁定（cache locking），缓存锁定将大大降低lock前缀指令的执行开销，但是当多处理器之间的竞争程度很高或者指令访问的内存地址未对齐时，仍然会锁住总线。 禁止该指令与之前和之后的读和写指令重排序。 把写缓冲区中的所有数据刷新到内存中。 "},"Chapter05/HashMap.html":{"url":"Chapter05/HashMap.html","title":"HashMap","keywords":"","body":"HashMap HashMap主要由数组和链表组成，他不是线程安全的。核心的点就是put插入数据的过程，get查询数据以及扩容的方式。JDK1.7和1.8的主要区别在于头插和尾插方式的修改，头插容易导致HashMap链表死循环，并且1.8之后加入红黑树对性能有提升。 put插入数据流程 往map插入元素的时候首先通过对key hash然后与数组长度-1进行与运算((n-1)&hash)，都是2的次幂所以等同于取模，但是位运算的效率更高。找到数组中的位置之后，如果数组中没有元素直接存入，反之则判断key是否相同，key相同就覆盖，否则就会插入到链表的尾部，如果链表的长度超过8，则会转换成红黑树，最后判断数组长度是否超过默认的长度*负载因子也就是12，超过则进行扩容。 简介 Java为数据结构中的映射定义了一个接口java.util.Map，此接口主要有四个常用的实现类 ，分别是HashMap、Hashtable、LinkedHashMap和TreeMap，类继承关系如下图所示： 下面针对各个实现类的特点做一些说明： (1) HashMap：它根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。 HashMap最多只允许一条记录的键为null，允许多条记录的值为null。HashMap非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections的synchronizedMap方法使HashMap具有线程安全的能力，或者使用ConcurrentHashMap。 (2) Hashtable：Hashtable是遗留类，很多映射的常用功能与HashMap类似，不同的是它承自Dictionary类，并且是线程安全的，任一时间只有一个线程能写Hashtable，并发性不如ConcurrentHashMap，因为ConcurrentHashMap引入了分段锁。Hashtable不建议在新代码中使用，不需要线程安全的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 (3) LinkedHashMap：LinkedHashMap是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 (4) TreeMap：TreeMap实现SortedMap接口，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用TreeMap时，key必须实现Comparable接口或者在构造TreeMap传入自定义的Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 对于上述四种Map类型的类，要求映射中的key是不可变对象。不可变对象是该对象在创建后它的哈希值不会被改变。如果对象的哈希值发生变化，Map对象很可能就定位不到映射的位置了。 通过上面的比较，我们知道了HashMap是Java的Map家族中一个普通成员，鉴于它可以满足大多数场景的使用条件，所以是使用频度最高的一个。下文我们主要结合源码，从存储结构、常用方法分析、扩容以及安全性等方面深入讲解HashMap的工作原理。 内部实现 搞清楚HashMap，首先需要知道HashMap是什么，即它的存储结构-字段；其次弄明白它能干什么，即它的功能实现-方法。下面我们针对这两个方面详细展开讲解。 存储结构-字段 从结构实现来讲，HashMap是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ (1) 从源码可知，HashMap类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个Node的数组。我们来看Node[JDK1.8]是何物。 static class Node implements Map.Entry { final int hash; //用来定位数组索引位置 final K key; //链表的下一个node V value; Node next; Node(int hash, K key, V value, Node next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry)o; if (Objects.equals(key, e.getKey()) && Objects.equals(value, e.getValue())) return true; } return false; } } Node是HashMap的一个内部类，实现了Map.Entry接口，本质是就是一个映射(键值对)。上图中的每个黑色圆点就是一个Node对象。 (2) HashMap就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题，Java中HashMap采用了链地址法。 链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被Hash后，得到数组下标，把数据放在对应下标元素的链表上。 系统将调用\"key\"这个key的hashCode()方法得到其hashCode 值（该方法适用于每个Java对象），然后再通过Hash算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个key会定位到相同的位置，表示发生了Hash碰撞。当然Hash算法计算结果越分散均匀，Hash碰撞的概率就越小，map的存取效率就会越高。 如果哈希桶数组很大，即使较差的Hash算法也会比较分散，如果哈希桶数组数组很小，即使好的Hash算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的hash算法减少Hash碰撞。那么通过什么方式来控制map使得Hash碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？答案就是好的Hash算法和扩容机制。 在理解Hash和扩容流程之前，我们得先了解下HashMap的几个字段。从HashMap的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子 int modCount; int size; 首先，Node[] table的初始化长度length(默认值是16)，Load factor为负载因子(默认值是0.75)，threshold是HashMap所能容纳的最大数据量的Node(键值对)个数。threshold = length * Load factor。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的HashMap容量是之前容量的两倍。默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子loadFactor的值，这个值可以大于1。 size这个字段其实很好理解，就是HashMap中实际存在的键值对数量。注意和table的长度length、容纳最大键值对数量threshold的区别。而modCount字段主要用来记录HashMap内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如put新键值对，但是某个key对应的value值被覆盖不属于结构变化。 在HashMap中，哈希桶数组table的长度length大小必须为2的n次方(一定是合数)，这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考http://blog.csdn.net/liuqiyao_01/article/details/14475159，Hashtable初始化桶大小为11，就是桶大小设计为素数的应用（Hashtable扩容后不能保证还是素数）。HashMap采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考http://blog.csdn.net/v_july_v/article/details/6105630。 功能实现-方法 HashMap的内部功能实现很多，本文主要从根据key获取哈希桶数组索引位置、put方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 确定哈希桶数组索引位置 不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。 HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 方法一： static final int hash(Object key) { //jdk1.8 & jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h >>> 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); } 方法二： static int indexFor(int h, int length) { //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h & (length-1); //第三步 取模运算 } 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h & (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h& (length-1)运算等价于对length取模，也就是h%length，但是&比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h >>> 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 分析HashMap的put方法 HashMap的put方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 JDK1.8HashMap的put方法源码如下: public V put(K key, V value) { // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true); } final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node[] tab; Node p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) & hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // key已经存在直接覆盖value if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size > threshold) resize(); afterNodeInsertion(evict); return null; } 扩容机制 扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 void resize(int newCapacity) { //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; } Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值 } 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 void transfer(Entry[] newTable) { Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j e = src[j]; //取得旧Entry数组的每个元素 if (e != null) { src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do { Entry next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 } while (e != null); } } } newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置， 要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: final Node[] resize() { Node[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap > 0) { // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap = DEFAULT_INITIAL_CAPACITY) newThr = oldThr 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap [] newTab = (Node[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中 for (int j = 0; j e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash & (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode)e).split(this, newTab, j, oldCap); else { // 链表优化重hash的代码块 Node loHead = null, loTail = null; Node hiHead = null, hiTail = null; Node next; do { next = e.next; // 原索引 if ((e.hash & oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引+oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 线程安全性 在多线程使用场景中，应该尽量避免使用线程不安全的HashMap，而使用线程安全的ConcurrentHashMap。那么为什么说HashMap是线程不安全的，下面举例子说明在并发的多线程使用场景中使用HashMap可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： public class HashMapInfiniteLoop { private static HashMap map = new HashMap(2，0.75f); public static void main(String[] args) { map.put(5， \"C\"); new Thread(\"Thread1\") { public void run() { map.put(7, \"B\"); System.out.println(map); }; }.start(); new Thread(\"Thread2\") { public void run() { map.put(3, \"A); System.out.println(map); }; }.start(); } } 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。 线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 HashMap的keySet()和values() 我们通常说，keySet()返回所有的键，values()返回所有的值，其实是不太对的，因为无论是keySet()和values()，其实都没有实质的内容，且容我慢慢说来。 他们前者返回了一个Set，后者返回了一个Collection，但是Set和Collection都只是接口，既然是接口，那就大有文章可以做。很重要的一点就是，接口可以不是new someClass()的来的，也就是说，它可以不对应与一个类，而只提供一些方法。实际上，HashMap中所有的数据都是放在一个Node[]的数组中的，而返回的Set接口也好，Collection也罢，都是直接针对这个Node[]数组的，所以，当使用返回的Set接口或者Collection接口进行操作是，实际上操作的还是那个Node[]数组。但是，返回的Collection只能做有限的操作，限定哪些呢？一句话总结就是：只能读，不能写，但能删能清。 不信？我们可以看源码。 首先来看values方法： public Collection values() { Collection vs = values; if (vs == null) { vs = new Values(); values = vs; } return vs; } 可以看到values其实是返回了一个Values类的，这是个内部类，就在它后面： final class Values extends AbstractCollection { public final int size() { return size; } public final void clear() { HashMap.this.clear(); } public final Iterator iterator() { return new ValueIterator(); } public final boolean contains(Object o) { return containsValue(o); } public final Spliterator spliterator() { return new ValueSpliterator<>(HashMap.this, 0, -1, 0, 0); } public final void forEach(Consumer action) { Node[] tab; if (action == null) throw new NullPointerException(); if (size > 0 && (tab = table) != null) { int mc = modCount; for (int i = 0; i e = tab[i]; e != null; e = e.next) action.accept(e.value); } if (modCount != mc) throw new ConcurrentModificationException(); } } } 看到没有，完全没有提供新数据，完全是操作那个table，或者调用hashMap自己的方法。我之前傻呵呵的找源码是怎么为values（HashMap除了有values这个方法，还有一个属性也叫values，坑爹不？）赋值的，现在才知道自己是这么傻，因为根本就没有复制嘛。 再来看keySet，也是一样的思路： public Set keySet() { Set ks = keySet; if (ks == null) { ks = new KeySet(); keySet = ks; } return ks; } final class KeySet extends AbstractSet { public final int size() { return size; } public final void clear() { HashMap.this.clear(); } public final Iterator iterator() { return new KeyIterator(); } public final boolean contains(Object o) { return containsKey(o); } public final boolean remove(Object key) { return removeNode(hash(key), key, null, false, true) != null; } public final Spliterator spliterator() { return new KeySpliterator<>(HashMap.this, 0, -1, 0, 0); } public final void forEach(Consumer action) { Node[] tab; if (action == null) throw new NullPointerException(); if (size > 0 && (tab = table) != null) { int mc = modCount; for (int i = 0; i e = tab[i]; e != null; e = e.next) action.accept(e.key); } if (modCount != mc) throw new ConcurrentModificationException(); } } } "},"Chapter05/Throwable.html":{"url":"Chapter05/Throwable.html","title":"深入理解Java中异常体系","keywords":"","body":"深入理解Java中异常体系 异常体系简介 异常是指由于各种不期而至的情况，导致程序中断运行的一种指令流,如：文件找不到、非法参数、网络超时等。为了保证正序正常运行， 在设计程序时必须考虑到各种异常情况，并正确的对异常进行处理。异常也是一种对象，java当中定义了许多异常类， 并且定义了基类java.lang.Throwable作为所有异常的超类。Java语言设计者将异常划分为两类：Error和Exception，其体系结构大致如下图所示： Throwable：有两个重要的子类：Exception（异常）和Error（错误），两者都包含了大量的异常处理类。 Error（错误） 是程序中无法处理的错误，表示运行应用程序中出现了严重的错误。此类错误一般表示代码运行时JVM出现问题。通常有Virtual MachineError（虚拟机运行错误）、NoClassDefFoundError（类定义错误）等。比如说当jvm耗完可用内存时，将出现OutOfMemoryError。此类错误发生时，JVM将终止线程。 这些错误是不可查的，非代码性错误。因此，当此类错误发生时，应用不应该去处理此类错误。 Exception（异常） 程序本身可以捕获并且可以处理的异常。 Exception这种异常又分为两类：运行时异常和编译异常。 运行时异常(不受检异常)：RuntimeException类极其子类表示JVM在运行期间可能出现的错误。比如说试图使用空值对象的引用（NullPointerException）、数组下标越界（ArrayIndexOutBoundException）。此类异常属于不可查异常，一般是由程序逻辑错误引起的，在程序中可以选择捕获处理，也可以不处理。 编译异常(受检异常)：Exception中除RuntimeException极其子类之外的异常。如果程序中出现此类异常，比如说IOException，必须对该异常进行处理，否则编译不通过。在程序中，通常不会自定义该类异常，而是直接使用系统提供的异常类。 可查异常与不可查异常：java的所有异常可以分为可查异常（checked exception）和不可查异常（unchecked exception）。 可查异常：编译器要求必须处理的异常。正确的程序在运行过程中，经常容易出现的、符合预期的异常情况。 一旦发生此类异常，就必须采用某种方式进行处理。除RuntimeException及其子类外，其他的Exception异常都属于可查异常。 编译器会检查此类异常，也就是说当编译器检查到应用中的某处可能会此类异常时，将会提示你处理本异常——要么使用try-catch捕获， 要么使用throws语句抛出，否则编译不通过。 不可查异常：编译器不会进行检查并且不要求必须处理的异常，也就说当程序中出现此类异常时，即使我们没有try-catch捕获它， 也没有使用throws抛出该异常，编译也会正常通过。该类异常包括运行时异常（RuntimeException极其子类）和错误（Error）。 "},"Chapter05/LinkedHashMap.html":{"url":"Chapter05/LinkedHashMap.html","title":"LinkedHashMap原理","keywords":"","body":"LinkedHashMap原理 先来一张LinkedHashMap的结构图，不要虚，看完文章再来看这个图，就秒懂了，先混个面熟： 应用场景 HashMap是无序的，当我们希望有顺序地去存储key-value时，就需要使用LinkedHashMap了。 Map hashMap = new HashMap(); hashMap.put(\"name1\", \"josan1\"); hashMap.put(\"name2\", \"josan2\"); hashMap.put(\"name3\", \"josan3\"); Set> set = hashMap.entrySet(); Iterator> iterator = set.iterator(); while(iterator.hasNext()) { Entry entry = iterator.next(); String key = (String) entry.getKey(); String value = (String) entry.getValue(); System.out.println(\"key:\" + key + \",value:\" + value); } key:name3,value:josan3 key:name2,value:josan2 key:name1,value:josan1 输出结果并不是按照插入顺序的。 同样的数据，我们再试试LinkedHashMap Map linkedHashMap = new LinkedHashMap<>(); linkedHashMap.put(\"name2\", \"josan2\"); linkedHashMap.put(\"name1\", \"josan1\"); linkedHashMap.put(\"name3\", \"josan3\"); Set> set1 = linkedHashMap.entrySet(); Iterator> iterator1 = set1.iterator(); while(iterator1.hasNext()) { Map.Entry entry = iterator1.next(); String key = (String) entry.getKey(); String value = (String) entry.getValue(); System.out.println(\"key:\" + key + \",value:\" + value); } key:name2,value:josan2 key:name1,value:josan1 key:name3,value:josan3 结果可知，LinkedHashMap是有序的，且默认为插入顺序。 定义 LinkedHashMap继承了HashMap，所以它们有很多相似的地方。 public class LinkedHashMap extends HashMap implements Map { 构造方法 LinkedHashMap提供了多个构造方法，我们先看空参的构造方法。 public LinkedHashMap() { // 调用HashMap的构造方法，其实就是初始化Entry[] table super(); // 这里是指是否基于访问排序，默认为false accessOrder = false; } 首先使用super调用了父类HashMap的构造方法，其实就是根据初始容量、负载因子去初始化Entry[] table。 然后把accessOrder设置为false，这就跟存储的顺序有关了，LinkedHashMap存储数据是有序的，而且分为两种：插入顺序和访问顺序。 这里accessOrder设置为false，表示不是访问顺序而是插入顺序存储的，这也是默认值，表示LinkedHashMap中存储的顺序是按照调用put方法插入的顺序进行排序的。LinkedHashMap也提供了可以设置accessOrder的构造方法，我们来看看这种模式下，它的顺序有什么特点？ Map linkedHashMap = new LinkedHashMap<>(10, 1, true); linkedHashMap.put(\"name2\", \"josan2\"); linkedHashMap.put(\"name1\", \"josan1\"); linkedHashMap.put(\"name3\", \"josan3\"); Set> set1 = linkedHashMap.entrySet(); Iterator> iterator1 = set1.iterator(); while(iterator1.hasNext()) { Map.Entry entry = iterator1.next(); String key = (String) entry.getKey(); String value = (String) entry.getValue(); System.out.println(\"key:\" + key + \",value:\" + value); } System.out.println(\">>>>>>>>>>>>>>\"); linkedHashMap.get(\"name1\"); //System.out.println(\"通过get方法，导致key为name1对应的Entry到表尾\"); Set> set2 = linkedHashMap.entrySet(); Iterator> iterator2 = set2.iterator(); while(iterator2.hasNext()) { Map.Entry entry = iterator2.next(); String key = (String) entry.getKey(); String value = (String) entry.getValue(); System.out.println(\"key:\" + key + \",value:\" + value); } key:name2,value:josan2 key:name1,value:josan1 key:name3,value:josan3 key:name2,value:josan2 key:name3,value:josan3 key:name1,value:josan1 因为调用了get(\"name1\")导致了name1对应的Entry移动到了最后，这里只要知道LinkedHashMap有插入顺序和访问顺序两种就可以，后面会详细讲原理。 我们知道1.8之后，map的初始化是在put里做的 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node[] tab; Node p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) & hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node e; K k; if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size > threshold) resize(); afterNodeInsertion(evict); return null; } 其中HashMap中有三个方法，被LinkedHashMap重写 // Callbacks to allow LinkedHashMap post-actions void afterNodeAccess(Node p) { } void afterNodeInsertion(boolean evict) { } void afterNodeRemoval(Node p) { } void afterNodeAccess(Node e) { // move node to last LinkedHashMap.Entry last; if (accessOrder && (last = tail) != e) { LinkedHashMap.Entry p = (LinkedHashMap.Entry)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; } } "},"Chapter06/jvm.html":{"url":"Chapter06/jvm.html","title":"Part VI JVM篇","keywords":"","body":"第六章 JVM篇 JVM内存管理 垃圾回收 ZGC jvm指令集 jStack 调优 ZGC "},"Chapter06/JvmMemoryManagement.html":{"url":"Chapter06/JvmMemoryManagement.html","title":"JVM内存管理","keywords":"","body":"JVM内存管理 根据JVM规范，JVM把内存划分了如下几个区域：方法区、堆区、本地方法栈、虚拟机栈、程序计数器。 其中，方法区和堆是所有线程共享的。 方法区 是java虚拟机规范中定义的名字 各个虚拟机实现上有所不同 HostSpot虚拟机中 1.在jdk1.7 以及前的版本实现的方法区称为- - -永久代 2.在java 虚拟机的堆内存中分配 3.里面主要存放的内容：已经被虚拟机加载的类信息，常量，静态变量，即时编译后的代码等 4.内存回收：主要是常量池的回收 和类型的卸载- -目前的回收效果不好 方法区存放了要加载的类的信息(如类名，修饰符)、类中的静态变量、final定义的常量、类中的field、方法信息， 当开发人员调用类对象中的getName、isInterface等方法来获取信息时， 这些数据都来源于方法区。方法区是全局共享的，在一定条件下它也会被GC。 当方法区使用的内存超过它允许的大小时，就会抛出OutOfMemory：PermGen Space异常。 在Hotspot虚拟机中，这块区域对应的是Permanent Generation(持久代)， 一般的，方法区上执行的垃圾收集是很少的，因此方法区又被称为持久代的原因之一， 但这也不代表着在方法区上完全没有垃圾收集，其上的垃圾收集主要是针对常量池的内存回收和对已加载类的卸载。 在方法区上进行垃圾收集，条件苛刻而且相当困难。 运行时常量池（Runtime Constant Pool）是方法区的一部分， 用于存储编译期就生成的字面常量、符号引用、 翻译出来的直接引用（符号引用就是编码是用字符串表示某个变量、接口的位置， 直接引用就是根据符号引用翻译出来的地址，将在类链接阶段完成翻译）； 运行时常量池除了存储编译期常量外，也可以存储在运行时间产生的常量， 比如String类的intern()方法，作用是String维护了一个常量池，如果调用的字符“abc”已经在常量池中， 则返回池中的字符串地址，否则，新建一个常量加入池中，并返回地址。 分配在方法区（永久代）中的，但是1.7版本把 字符串常量池 单独拿到了堆空间中 存的内容：用于存放编译期生成的各种字面量以及符号引用，时机可能是静态编译期 也可以是动态编译时候 堆 堆区是理解Java GC机制最重要的区域。 在JVM所管理的内存中，堆区是最大的一块，堆区也是JavaGC机制所管理的主要内存区域， 堆区由所有线程共享，在虚拟机启动时创建。堆区用来存储对象实例及数组值， 可以认为java中所有通过new创建的对象都在此分配。 对于堆区大小，可以通过参数-Xms和-Xmx来控制，-Xms为JVM启动时申请的最新heap内存， 默认为物理内存的1/64但小于1GB;-Xmx为JVM可申请的最大Heap内存， 默认为物理内存的1/4但小于1GB,默认当剩余堆空间小于40%时，JVM会增大Heap到-Xmx大小， 可通过-XX:MinHeapFreeRadio参数来控制这个比例； 当空余堆内存大于70%时，JVM会减小Heap大小到-Xms指定大小， 可通过-XX:MaxHeapFreeRatio来指定这个比例。对于系统而言，为了避免在运行期间频繁的调整Heap大小， 我们通常将-Xms和-Xmx设置成一样。 为了让内存回收更加高效，从Sun JDK 1.2开始对堆采用了分代管理方式，如下图所示： 年轻代（Young Generation） 对象在被创建时，内存首先是在年轻代进行分配（注意，大对象可以直接在老年代分配）。 当年轻代需要回收时会触发Minor GC(也称作Young GC)。 年轻代由Eden Space和两块相同大小的Survivor Space（又称From Space和To Space）构成， Eden区和Servior区的内存比为8:1，可通过-Xmn参数来调整新生代大小， 也可通过-XX:SurvivorRadio来调整Eden Space和Survivor Space大小。 不同的GC方式会按不同的方式来按此值划分Eden Space和Survivor Space， 有些GC方式还会根据运行状况来动态调整Eden、From Space、To Space的大小。 年轻代的Eden区内存是连续的，所以其分配会非常快； 同样Eden区的回收也非常快 （因为大部分情况下Eden区对象存活时间非常短，而Eden区采用的复制回收算法， 此算法在存活对象比例很少的情况下非常高效）。 如果在执行垃圾回收之后，仍没有足够的内存分配，也不能再扩展 ，将会抛出OutOfMemoryError:Java Heap Space异常。 老年代（Old Generation） 老年代用于存放在年轻代中经多次垃圾回收仍然存活的对象， 可以理解为比较老一点的对象，例如缓存对象； 新建的对象也有可能在老年代上直接分配内存， 这主要有两种情况： 一种为大对象，可以通过启动参数设置-XX:PretenureSizeThreshold=1024， 表示超过多大时就不在年轻代分配，而是直接在老年代分配。此参数在年轻代采用Parallel Scavenge GC时无效， 因为其会根据运行情况自己决定什么对象直接在老年代上分配内存；另一种为大的数组对象， 且数组对象中无引用外部对象。 当老年代满了的时候就需要对老年代进行垃圾回收，老年代的垃圾回收称作Full GC。 老年代所占用的内存大小为-Xmx对应的值减去-Xmn对应的值。 本地方法栈（Native Method Stack） 本地方法栈用于支持native方法的执行，存储了每个native方法调用的状态。 本地方法栈和虚拟机方法栈运行机制一致，它们唯一的区别就是，虚拟机栈是执行Java方法的， 而本地方法栈是用来执行native方法的，在很多虚拟机中（如Sun的JDK默认的HotSpot虚拟机）， 会将本地方法栈与虚拟机栈放在一起使用。 程序计数器（Program Counter Register） 程序计数器是一个比较小的内存区域，可能是CPU寄存器或者操作系统内存， 其主要用于指示当前线程所执行的字节码执行到了第几行，可以理解为是当前线程的行号指示器。 字节码解释器在工作时，会通过改变这个计数器的值来取下一条语句指令。 每个程序计数器只用来记录一个线程的行号，所以它是线程私有（一个线程就有一个程序计数器）的。 如果程序执行的是一个Java方法，则计数器记录的是正在执行的虚拟机字节码指令地址； 如果正在执行的是一个本地（native，由C语言编写完成）方法，则计数器的值为Undefined， 由于程序计数器只是记录当前指令地址，所以不存在内存溢出的情况， 因此，程序计数器也是所有JVM内存区域中唯一一个没有定义OutOfMemoryError的区域。 虚拟机栈（JVM Stack） 虚拟机栈占用的是操作系统内存，每个线程都对应着一个虚拟机栈，它是线程私有的， 而且分配非常高效。一个线程的每个方法在执行的同时，都会创建一个栈帧（Statck Frame）， 栈帧中存储的有局部变量表、操作站、动态链接、方法出口等，当方法被调用时， 栈帧在JVM栈中入栈，当方法执行完成时，栈帧出栈。 局部变量表中存储着方法的相关局部变量，包括各种基本数据类型， 对象的引用，返回地址等。在局部变量表中， 只有long和double类型会占用2个局部变量空间（Slot，对于32位机器，一个Slot就是32个bit）， 其它都是1个Slot。需要注意的是，局部变量表是在编译时就已经确定好的， 方法运行所需要分配的空间在栈帧中是完全确定的，在方法的生命周期内都不会改变。 虚拟机栈中定义了两种异常，如果线程调用的栈深度大于虚拟机允许的最大深度， 则抛出StatckOverFlowError（栈溢出）； 不过多数Java虚拟机都允许动态扩展虚拟机栈的大小(有少部分是固定长度的)， 所以线程可以一直申请栈，直到内存不足，此时，会抛出OutOfMemoryError（内存溢出）。 元数据区 1.8版本，移除永久代，改为元数据区，元数据区分配在本地内存就是系统可用内存空间， 字符串常量池 还是在堆内的 优点：元空间的最大可分配空间就是系统可用内存空间。不容易发生内存溢出的问题 永久代转元数据区的原因： 1.字符串存在永久代中，容易发生内存溢出的问题 2.类及犯法的信息比较难确定，因此对于永久代的大小指定比较困难， 太小容易出现永久代溢出，太大则容易导致老年代溢出。 3.永久代会为GC带来不必要的复杂度 元数据区存在的问题：内存碎片 元空间虚拟机采用了组块分配的形式，同时区块的大小由类加载器类型决定。 类信息并不是固定大小，因此有可能分配的空闲区块和类需要的区块大小不同，这种情况下可能导致碎片存在。 元空间虚拟机目前并不支持压缩操作，所以碎片化是目前最大的问题。 补录 在 Jdk6 以及以前的版本中，字符串的常量池是放在堆的Perm区的，Perm区是一个类静态的区域， 主要存储一些加载类的信息，常量池，方法片段等内容，默认大小只有4m， 一旦常量池中大量使用 intern 是会直接产生java.lang.OutOfMemoryError:PermGen space错误的。 在 jdk7 的版本中，字符串常量池已经从Perm区移到正常的Java Heap区域了。 为什么要移动，Perm 区域太小是一个主要原因， Jdk8已经直接取消了Perm区域，而新建立了一个元区域。 应该是jdk开发者认为Perm区域已经不适合现在 JAVA 的发展了。 Java方法区、永久代、元空间、常量池详解 1.JVM内存模型简介 堆——堆是所有线程共享的，主要用来存储对象。其中，堆可分为：年轻代和老年代两块区域。 使用NewRatio参数来设定比例。对于年轻代，一个Eden区和两个Suvivor区，使用参数SuvivorRatio来设定大小； Java虚拟机栈/本地方法栈——线程私有的，主要存放局部变量表，操作数栈，动态链接和方法出口等； 程序计数器——同样是线程私有的，记录当前线程的行号指示器，为线程的切换提供保障； 方法区——线程共享的，主要存储类信息、常量池、静态变量、JIT编译后的代码等数据。 方法区理论上来说是堆的逻辑组成部分； 运行时常量池——是方法区的一部分，用于存放编译期生成的各种字面量和符号引用； 2.永久代和方法区的关系 涉及到内存模型时，往往会提到永久代，那么它和方法区又是什么关系呢？ 《Java虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。 那么，在不同的 JVM 上方法区的实现肯定是不同的了。 同时大多数用的JVM都是Sun公司的HotSpot。在HotSpot上把GC分代收集扩展至方法区， 或者说使用永久代来实现方法区。 因此，我们得到了结论，永久代是HotSpot的概念，方法区是Java虚拟机规范中的定义， 是一种规范，而永久代是一种实现，一个是标准一个是实现。 其他的虚拟机实现并没有永久带这一说法。在1.7之前在(JDK1.2 ~ JDK6)的实现中， HotSpot 使用永久代实现方法区，HotSpot 使用 GC分代来实现方法区内存回收， 可以使用如下参数来调节方法区的大小: -XX:PermSize 方法区初始大小 -XX:MaxPermSize 方法区最大大小 超过这个值将会抛出OutOfMemoryError异常:java.lang.OutOfMemoryError: PermGen 3.元空间 对于Java8， HotSpots取消了永久代，那么是不是也就没有方法区了呢？ 当然不是，方法区是一个规范，规范没变，它就一直在。那么取代永久代的就是元空间。 它可永久代有什么不同的？存储位置不同，永久代物理是是堆的一部分，和新生代，老年代地址是连续的， 而元空间属于本地内存；存储内容不同，元空间存储类的元信息， 静态变量和常量池等并入堆中。相当于永久代的数据被分到了堆和元空间中。 4.Class文件常量池 Class 文件常量池指的是编译生成的 class 字节码文件， 其结构中有一项是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用， 这部分内容将在类加载后进入方法区的运行时常量池中存放。 这里的字面量是指字符串字面量和声明为 final 的（基本数据类型）常量值， 这些字符串字面量除了类中所有双引号括起来的字符串(包括方法体内的)， 还包括所有用到的类名、方法的名字和这些类与方法的字符串描述、字段(成员变量)的名称和描述符； 声明为final的常量值指的是成员变量，不包含本地变量， 本地变量是属于方法的。这些都在常量池的 UTF-8 表中(逻辑上的划分)； 符号引用，就是指指向 UTF-8 表中向这些字面量的引用， 包括类和接口的全限定名(包括包路径的完整名)、字段的名称和描述符、方法的名称和描述符。 只不过是以一组符号来描述所引用的目标，和内存并无关，所以称为符号引用， 直接指向内存中某一地址的引用称为直接引用； 5.运行时常量池 运行时常量池是方法区的一部分，是一块内存区域。 Class 文件常量池将在类加载后进入方法区的运行时常量池中存放。 一个类加载到 JVM 中后对应一个运行时常量池，运行时常量池相对于 Class 文件常量池来说具备动态性， Class 文件常量只是一个静态存储结构，里面的引用都是符号引用。 而运行时常量池可以在运行期间将符号引用解析为直接引用。 可以说运行时常量池就是用来索引和查找字段和方法名称和描述符的。 给定任意一个方法或字段的索引，通过这个索引最终可得到该方法或字段所属的类型信息和名称及描述符信息， 这涉及到方法的调用和字段获取。 6.字符串常量池 字符串常量池是全局的，JVM 中独此一份，因此也称为全局字符串常量池。 运行时常量池中的字符串字面量若是成员的，则在类的加载初始化阶段就使用到了字符串常量池； 若是本地的，则在使用到的时候（执行此代码时）才会使用到字符串常量池。 其实，“使用常量池”对应的字节码是一个 ldc 指令，在给 String 类型的引用赋值的时候会先执行这个指令， 看常量池中是否存在这个字符串对象的引用，若有就直接返回这个引用， 若没有，就在堆里创建这个字符串对象并在字符串常量池中记录下这个引用（jdk1.7)。 String 类的 intern() 方法还可在运行期间把字符串放到字符串常量池中。 JVM 中除了字符串常量池，8种基本数据类型中除了两种浮点类型剩余的6种基本数据类型的包装类， 都使用了缓冲池技术， 但是 Byte、Short、Integer、Long、Character 这5种整型的包装类也只是在对应值在 [-128,127] 时才会使用缓冲池，超出此范围仍然会去创建新的对象。 其中： 在 jdk1.6（含）之前也是方法区的一部分，并且其中存放的是字符串的实例； 在 jdk1.7（含）之后是在堆内存之中，存储的是字符串对象的引用，字符串实例是在堆中； jdk1.8 已移除永久代，字符串常量池是在本地内存当中，存储的也只是引用。 https://docs.oracle.com/javase/specs/jvms/se13/jvms13.pdf https://docs.oracle.com/javase/specs/jvms/se8/html/jvms-2.html#jvms-2.6 Class对象是什么 可以简单这么说：Class对象就是字节码文件存储的内容。所以将字节码加载进入内存中时，即在内存中生成了Class对象（Class对象和普通对象一样，也是存放在堆中；尽管加载进来的类信息是放在方法区当中的，这点要注意！）。 有Class对象，就有Class类。 Class对象的作用是：在运行时期提供或者获得某个对象的类型信息，这对于反射比较重要。 某种意义上来说，java有两种对象：实例对象和Class对象。每个类的运行时的类型信息就是用Class对象表示的。它包含了与类有关的信息。其实我们的实例对象就通过Class对象来创建的。Java使用Class对象执行其RTTI（运行时类型识别，Run-Time Type Identification），多态是基于RTTI实现的 每一个类都有一个Class对象，每当编译一个新类就产生一个Class对象，基本类型 (boolean, byte, char, short, int, long, float, and double)有Class对象，数组有Class对象，就连关键字void也有Class对象（void.class）  Class类没有公共的构造方法，Class对象是在类加载的时候由Java虚拟机以及通过调用类加载器中的 defineClass 方法自动构造的，因此不能显式地声明一个Class对象。 如何获得Class对象 三种方法： Class.forName(\"xxx\"); ——Class的静态方法 obj.getClass(); ——继承自Object类的普通方法 Object.class(); ——类字面量 通过上文所提及知识，我们可以得出以下几点信息： Class类也是类的一种，与class关键字是不一样的。 手动编写的类被编译后会产生一个Class对象，其表示的是创建的类的类型信息，该Class对象保存在同名.class的文件中(即编译后得到的字节码文件)。 每个通过关键字class标识的类，在内存中有且只有一个与之对应的Class对象来描述其类型信息，无论创建多少个实例对象，其依据的都是用一个Class对象。 Class类只存私有构造函数，因此对应Class对象只能有JVM创建和加载 Class类的对象的作用是运行时提供或获得某个对象的类型信息，这点对于反射技术很重要。 类字面常量 java还提供了另一种方法来生成对Class对象的引用。即使用类字面常量，就像这样：Cat.class，这样做不仅更简单，而且更安全，因为它在编译时就会受到检查(因此不需要置于try语句块中)。并且根除了对forName()方法的调用，所有也更高效。类字面量不仅可以应用于普通的类，也可以应用于接口、数组及基本数据类型。 注意：基本数据类型的Class对象和包装类的Class对象是不一样的 Class c1 = Integer.class; Class c2 = int.class; System.out.println(c1); System.out.println(c2); System.out.println(c1 == c2); /* Output class java.lang.Integer int false oop 详细解释一下，定义这样一个类： class Main { } 那么当这个类所在的文件被加载，更准确地说，这个类被ClassLoader加载到JVM中的时候，Hotspot虚拟机会为这个类在虚拟机内部创建一个叫做Klass的数据结构： class Klass : public Metadata { friend class VMStructs; protected: // note: put frequently-used fields together at start of klass structure // for better cache behavior (may not make much of a difference but sure won't hurt) enum { _primary_super_limit = 8 }; jint _layout_helper; juint _super_check_offset; Symbol* _name; Klass* _secondary_super_cache; // Array of all secondary supertypes Array* _secondary_supers; // Ordered list of all primary supertypes Klass* _primary_supers[_primary_super_limit]; // java/lang/Class instance mirroring this class oop _java_mirror; // Superclass Klass* _super; // First subclass (NULL if none); _subklass->next_sibling() is next one Klass* _subklass; // Sibling link (or NULL); links all subklasses of a klass Klass* _next_sibling; Klass* _next_link; // The VM's representation of the ClassLoader used to load this class. // Provide access the corresponding instance java.lang.ClassLoader. ClassLoaderData* _class_loader_data; jint _modifier_flags; // Processed access flags, for use by Class.getModifiers. AccessFlags _access_flags; // Access flags. The class/interface distinction is stored here. // Biased locking implementation and statistics // (the 64-bit chunk goes first, to avoid some fragmentation) jlong _last_biased_lock_bulk_revocation_time; markOop _prototype_header; // Used when biased locking is both enabled and disabled for this type jint _biased_lock_revocation_count; TRACE_DEFINE_KLASS_TRACE_ID; // Remembered sets support for the oops in the klasses. jbyte _modified_oops; // Card Table Equivalent (YC/CMS support) jbyte _accumulated_modified_oops; // Mod Union Equivalent (CMS support) .... } 这个类的完整定义，大家可以去看hotspot/src/share/vm/oops/klass.hpp。 我们这里就不再多列了。这些属性已经足够我们讲解的了。 如果Main class被加载，那么虚拟机内部就会为它创建一个Klass，它的 _name 属性就是字符串 \"Main\"。 _primary_supers代表了这个类的父类。比如，我们看IOException, 是Exception的子类，而Exception又是Throwable的子类。那么，如果你去看IOException的 _primary_supers 属性就会发现，它是这样的：[Throwable, Exception, IOException]，后面5位为空。 其他的属性我们先不看，以后有时间会慢慢再来讲。今天重点说一下oop，这个我猜是ordinary object pointer的缩写，到底是什么的缩写，其实我也不确定。但我能确定的是，这种类型代表是一个真正的Java对象。比如说， Main m = new Main(); 这行语句里创建的 m 在JVM中，就是一个oop，是一个普通的Java对象，而Main在JVM里则是一个Klass。 大家理清了这里面的关系了吗？我建议没看懂的，再多看一遍。一般地来说，我不是很鼓励新手学习JVM源代码。但是有一些核心概念，如果能加以掌握的话，还是有利于快速掌握概念的本质的。 好了。说了这么多，才刚来到我们今天的主题：java_mirror。不起眼的一行： // java/lang/Class instance mirroring this class oop _java_mirror; 注释说得很清楚了，这个属性代表的就是本class的Class对象。举例来说，如果JVM加载了Main这个类，那么除了为Main创建了一个名为\"Main\"的Klass，还默默地背后创建一个object，并且把这个object 挂到了 Klass 的 _java_mirror 属性上了。 那我们通过Java代码能不能访问到这个背后的对象呢？你肯定已经猜到了，当然能啊，这就是Main的class对象啊。有两种写法来访问它： Class m = Main.class; Class m = Class.forName(\"Main\"); 这两种方法都能访问到Main的Class object，也就是Klass上那个不起眼的_java_mirror。那么这个_java_mirror上定义的 newInstance 方法，其实最终也是通过JVM中的方法来创建真正的对象： 卸载 如果有下面的情况，类就会被卸载： 1、该类所有的实例都已经被回收，也就是java堆中不存在该类的任何实例。 2、加载该类的ClassLoader已经被回收。 3、该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。 如果以上三个条件全部满足，jvm就会在方法区垃圾回收的时候对类进行卸载，类的卸载过程其实就是在方法区中清空类信息，java类的整个生命周期就结束了。 Java中的常量池 Java中的常量池，实际上分为两种形态：静态常量池和运行时常量池。 所谓静态常量池，即*.class文件中的常量池，class文件中的常量池不仅仅包含字符串(数字)字面量，还包含类、方法的信息，占用class文件绝大部分空间。这种常量池主要用于存放两大类常量：字面量(Literal)和符号引用量(Symbolic References)，字面量相当于Java语言层面常量的概念，如文本字符串，声明为final的常量值等，符号引用则属于编译原理方面的概念，包括了如下三种类型的常量： 类和接口的全限定名 字段名称和描述符 方法名称和描述符 而运行时常量池，则是jvm虚拟机在完成类装载操作后，将class文件中的常量池载入到内存中，并保存在方法区中，我们常说的常量池，就是指方法区中的运行时常量池。 运行时常量池相对于CLass文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是并非预置入CLass文件中常量池的内容才能进入方法区运行时常量池，运行期间也可能将新的常量放入池中，这种特性被开发人员利用比较多的就是String类的intern()方法。 String的intern()方法会查找在常量池中是否存在一份equal相等的字符串,如果有则返回该字符串的引用,如果没有则添加自己的字符串进入常量池。 常量池的好处 常量池是为了避免频繁的创建和销毁对象而影响系统性能，其实现了对象的共享。 例如字符串常量池，在编译阶段就把所有的字符串文字放到一个常量池中。 （1）节省内存空间：常量池中所有相同的字符串常量被合并，只占用一个空间。 （2）节省运行时间：比较字符串时，==比equals()快。对于两个引用变量，只用==判断引用是否相等，也就可以判断实际值是否相等。 因此public static final String FIANL_VALUE = \"fianl value loading\"; 字面量 被存储到了常量池中。 但是不能说所有的静态常用访问都不需要类的加载，这里还要判断这个常量是否属于“编译期常量”，即在编译期即可确定常量值。如果常量值必须在运行时才能确定，如常量值是一个随机值，也会引起类的加载，如下： public static final int FINAL_VALUE_INT = new Random(66).nextInt(); 静态常量和常量 static+final 静态常量，编译期常量，编译时就确定值。（Java代码执行顺序，先编译为class文件，在用虚拟机加载class文件执行） 放于方法区中的静态常量池。 在编译阶段存入调用类的常量池中 如果调用此常量的类不是定义常量的类，那么不会初始化定义常量的类，因为在编译阶段通过常量传播优化，已经将常量存到调用类的常量池中了 final 常量，类加载时确定或者更靠后。 当用final作用于类的成员变量时，成员变量（注意是类的成员变量，局部变量只需要保证在使用之前被初始化赋值即可）必须在定义时或者构造器中进行初始化赋值 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改； 如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。但是它指向的对象的内容是可变的 "},"Chapter06/JvmMemoryModel.html":{"url":"Chapter06/JvmMemoryModel.html","title":"JVM内存模型","keywords":"","body":"JVM内存模型 本身随着CPU和内存的发展速度差异的问题，导致CPU的速度远快于内存，所以现在的CPU加入了高速缓存，高速缓存一般可以分为L1、L2、L3三级缓存。 基于上面的例子我们知道了这导致了缓存一致性的问题，所以加入了缓存一致性协议，同时导致了内存可见性的问题，而编译器和CPU的重排序导致了原子性和有序性的问题，JMM内存模型正是对多线程操作下的一系列规范约束，因为不可能让陈雇员的代码去兼容所有的CPU，通过JMM我们才屏蔽了不同硬件和操作系统内存的访问差异，这样保证了Java程序在不同的平台下达到一致的内存访问效果，同时也是保证在高效并发的时候程序能够正确执行。 说了半天，到底工作内存和主内存是什么？ 主内存可以认为就是物理内存，Java内存模型中实际就是虚拟机内存的一部分。而工作内存就是CPU缓存，他有可能是寄存器也有可能是L1\\L2\\L3缓存，都是有可能的。 总线风暴 在java中使用unsafe实现cas,而其底层由cpp调用汇编指令实现的，如果是多核cpu是使用lock cmpxchg指令，单核cpu 使用compxch指令。如果在短时间内产生大量的cas操作在加上 volatile的嗅探机制则会不断地占用总线带宽，导致总线流量激增，就会产生总线风暴。 总之，就是因为volatile 和CAS 的操作导致BUS总线缓存一致性流量激增所造成的影响。 总线锁 在早期处理器提供一个 LOCK# 信号，CPU1在操作共享变量的时候会预先对总线加锁，此时CPU2就不能通过总线来读取内存中的数据了，但这无疑会大大降低CPU的执行效率。 缓存一致性协议 由于总线锁的效率太低所以就出现了缓存一致性协议，Intel 的MESI协议就是其中一个佼佼者。MESI协议保证了每个缓存变量中使用的共享变量的副本都是一致的。 MESI 的核心思想 modified（修改）、exclusive（互斥）、share（共享）、invalid（无效） 如上图，CPU1使用共享数据时会先数据拷贝到CPU1缓存中,然后置为独占状态(E)，这时CPU2也使用了共享数据，也会拷贝也到CPU2缓存中。通过总线嗅探机制，当该CPU1监听总线中其他CPU对内存进行操作，此时共享变量在CPU1和CPU2两个缓存中的状态会被标记为共享状态(S)； 若CPU1将变量通过缓存回写到主存中，需要先锁住缓存行，此时状态切换为（M），向总线发消息告诉其他在嗅探的CPU该变量已经被CPU1改变并回写到主存中。接收到消息的其他CPU会将共享变量状态从（S）改成无效状态（I），缓存行失效。若其他CPU需要再次操作共享变量则需要重新从内存读取。 缓存一致性协议失效的情况： 共享变量大于缓存行大小，MESI无法进行缓存行加锁； CPU并不支持缓存一致性协议 嗅探机制 每个处理器会通过嗅探器来监控总线上的数据来检查自己缓存内的数据是否过期，如果发现自己缓存行对应的地址被修改了，就会将此缓存行置为无效。当处理器对此数据进行操作时，就会重新从主内存中读取数据到缓存行。 缓存一致性流量 通过前面都知道了缓存一致性协议，比如MESI会触发嗅探器进行数据传播。当有大量的volatile 和cas 进行数据修改的时候就会产大量嗅探消息。 解决办法 部分volatile和cas使用synchronize "},"Chapter06/GC.html":{"url":"Chapter06/GC.html","title":"垃圾回收","keywords":"","body":"垃圾回收 一个对象什么时候才能被回收 怎样判断一个对象“已死”？ 在堆里面存放着 Java 世界中几乎所有的对象实例，垃圾收集器在对堆进行回收前， 第一件事情就是要确定这些对象之中哪些还“存活”着，哪些已经“死去”（即不可能再被任何途径使用的对象）。 那么怎么判断一个对象“已死”呢，目前有两种算法可以判断对象“已死”。 引用计数算法 这个算法的判断依据是通过给对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加 1；当引用失效时，计数器值就减 1；任何时刻计数器为 0 的对象就不可能再被使用的。 客观的说，引用计数算法的实现简单，判断效率也很高，在大部分情况下它都是一个不错的算法，也有一些比较著名的应用案例，例如微软公司的 COM（Component Object Model）技术。但是，至少主流的 Java 虚拟机里面没有选用引用计算法来管理内存，其中最主要的原因是它很难解决对象之间相互循环引用的问题。 举个简单的例子，请看下面代码中的 testGC() 方法：对象 objA 和 objB 都有字段 mInstance，赋值令 objA.mInstance = objB 及 objB.mInstance = objA，除此之外，这两个对象再无任何实际上这两个对象已经不可能再被访问，但是它们因为互相引用着对方，导致它们的引用计数都不为 0，于是引用技术算法无法通知 GC 收集器回收它们。 可达性分析算法 在主流的商用程序语言（如Java）的主流实现中， 都是称通过可达性分析来判断对象是否存活的。 这个算法的基本思路就是通过一系列的称为“GC Roots”的对象作为起始点， 从这些节点开始向下搜索，搜索所走过的路径称为引用链， 当一个对象到 GC Roots 没有任何引用链相连时， 则证明此对象是不可用的。 在 Java 语言中，可作为 GC Roots 的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象。 方法区中类静态属性引用的对象。 方法区中常量引用的对象。 本地方法栈中 JNI（即一般说的 Native 方法）引用的对象。 引用的分类 无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析判断对象的引用链是否可达，判断对象是否存活都与“引用”有关。 在 JDK 1.2 之后，Java 对引用的概念进行了扩充，将引用分为强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference） 4 种，这 4 种引用强度一次逐渐减弱。 强引用就是指在程序代码之中普遍存在的， 只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用是用来描述一些还有用但并非必须的对象。 对于软引用关联着的对象，在系统将要发生内存溢出异常之前（即内存紧张）， 将会把这些对象列进垃圾回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。在 JDK1.2 之后， 提供了 SoftReference 类累实现软引用。 弱引用是用来描述非必需的对象的，但是它的强度比软引用更弱一些， 被弱引用关联的对象只能生存到下一次垃圾收集发生之前。 当垃圾收集器工作时，无论当前内存是否足够， 都会回收掉只被弱引用关联的对象。 使用 WeekReference 类来实现弱引用。 虚引用也成为幽灵引用或者幻影引用，它是最弱的一种引用。一个对象是否有虚引用的存在，完全不会对其生存周期时间构成影响， 也无法通过虚引用来取得一个对象实例。唯一的作用就是能在这个对象被收集器回收时收到一个系统通知。使用 PhantomReference 表示。 回收方法区数据 方法区的垃圾收集主要回收两部分内容：废弃常量和无用的类。 回收废弃常量与回收 Java 堆中的对象非常类似。 以常量池中字面量的回收为例， 假如一个字符串“abc”已经进入了常量池， 但是当前系统没有任何一个 String 对象是叫做 “abc”的， 换句话说，就是没有任何一个 String 对象引用常量池中的“abc”常量， 也没有其他地方引用了这个字面量，如果这时发生内存回收， 而且必要的话，这个“abc”常量就会被系统清理出常量池。 常量池中的其他类（接口）、方法、字段的符号引用也与此类似。 判定一个常量为“废弃常量”比较简单， 而要判断一个常量池中的类是否是“无用的类”条件则苛刻很多。 类需要同时满足下面 3 个条件才能称为“无用的类”： 该类所有的实例都已经被全部回收，也就是说 Java 堆中不再存在任何该类的实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方引用，并且无法再任何地方通过反射调用该类的方法。 最后做个总结 我们可以通过 引用计数器 和 可达性算法 来判断一个对象是否“已死”。 引用计数器很难解决对象之间互相循环引用的问题， 所以在主流的商用程序语言（如Java）的主流实现中， 都是称通过可达性分析来判断对象是否存活的。 对象的引用可以分为 强引用、软引用、弱引用 以及 虚引用 4 种， 其中被 强引用 引用的对象垃圾收集器永远不会回收掉；被 软引用 引用的对象， 只有当系统将要发生内存溢出时，才会去回收软引用引用的对象；只被 弱引用关联的对象， 只要发生垃圾收集事件，只被弱引用关联的对象就会被回收； 被虚引用关联的对象的唯一作用是能在这个对象被回收器回收时受到一个系统通知。 回收方法区的数据，垃圾收集器主要回收 废弃常量和无用的类 两部分内容。 Java垃圾回收机制 GC，即就是Java垃圾回收机制。 目前主流的JVM（HotSpot）采用的是分代收集算法。 作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码， 对内存泄露和溢出的问题。 与C++不同的是，Java采用的是类似于树形结构的可达性分析法来判断对象是否还存在引用。 即：从gcroot开始，把所有可以搜索得到的对象标记为存活对象。 缺点就是： 有可能不知不觉浪费了很多内存。 JVM花费过多时间来进行内存回收。 内存泄露 理解Java的垃圾回收机制， 就要从：“什么时候”，“对什么东西”，“做了什么”三个方面来具体分析。 第一：“什么时候”即就是GC触发的条件。 GC触发的条件有两种。 （1）程序调用System.gc时可以触发； （2）系统自身来决定GC触发的时机。 系统判断GC触发的依据： 根据Eden区和From Space区的内存大小来决定。 当内存大小不足时，则会启动GC线程并停止应用线程。 Minor GC触发条件：当Eden区满时，触发Minor GC。 Full GC触发条件： 调用System.gc时，系统建议执行Full GC，但是不必然执行 老年代空间不足 方法去空间不足 通过Minor GC后进入老年代的平均大小大于老年代的可用内存 由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小。 第二：“对什么东西”笼统的认为是Java对象。 但是准确来讲，GC操作的对象分为： 通过可达性分析法无法搜索到的对象和可以搜索到的对象。 对于搜索不到的方法进行标记。 第三：“做了什么”最浅显的理解为释放对象。 但是从GC的底层机制可以看出，对于可以搜索到的对象进行复制操作， 对于搜索不到的对象，调用finalize()方法进行释放。 具体过程：当GC线程启动时， 会通过可达性分析法把Eden区和From Space区的存活对象复制到To Space区， 然后把Eden Space和From Space区的对象释放掉。 当GC轮训扫描To Space区一定次数后，把依然存活的对象复制到老年代，然后释放To Space区的对象。 对于用可达性分析法搜索不到的对象， GC并不一定会回收该对象。要完全回收一个对象， 至少需要经过两次标记的过程： 第一次标记：对于一个没有其他引用的对象， 筛选该对象是否有必要执行finalize()方法， 如果没有执行必要，则意味可直接回收。 （筛选依据：是否复写或执行过finalize()方法； 因为finalize方法只能被执行一次）。 第二次标记：如果被筛选判定位有必要执行， 则会放入FQueue队列， 并自动创建一个低优先级的finalize线程来执行释放操作。 如果在一个对象释放前被其他对象引用，则该对象会被移除FQueue队列。 虚拟机中GC的过程 在初始阶段，新创建的对象被分配到Eden区，survivor的两块空间都为空。 当Eden区满了的时候，minor garbage 被触发 。 经过扫描与标记，存活的对象被复制到S0，不存活的对象被回收 在下一次的Minor GC中，Eden区的情况和上面一致， 没有引用的对象被回收，存活的对象被复制到survivor区。 然而在survivor区，S0的所有的数据都被复制到S1，需要注意的是， 在上次minor GC过程中移动到S0中的两个对象在复制到S1后其年龄要加1。 此时Eden区S0区被清空，所有存活的数据都复制到了S1区， 并且S1区存在着年龄不一样的对象，过程如下图所示 再下一次MinorGC则重复这个过程， 这一次survivor的两个区对换，存活的对象被复制到S0， 存活的对象年龄加1，Eden区和另一个survivor区被清空。 再经过几次Minor GC之后，当存活对象的年龄达到一个阈值之后 （可通过参数配置，默认是8），就会被从年轻代Promotion到老年代。 随着MinorGC一次又一次的进行，不断会有新的对象被promote到老年代。 上面基本上覆盖了整个年轻代所有的回收过程。最终，MajorGC将会在老年代发生，老年代的空间将会被清除和压缩。 从上面的过程可以看出，Eden区是连续的空间，且Survivor总有一个为空。经过一次GC和复制，一个Survivor中保存着当前还活着的对象，而Eden区和另一个Survivor区的内容都不再需要了，可以直接清空，到下一次GC时，两个Survivor的角色再互换。因此，这种方式分配内存和清理内存的效率都极高，这种垃圾回收的方式就是著名的“停止-复制（Stop-and-copy）”清理法（将Eden区和一个Survivor中仍然存活的对象拷贝到另一个Survivor中），这不代表着停止复制清理法很高效，其实，它也只在这种情况下（基于大部分对象存活周期很短的事实）高效，如果在老年代采用停止复制，则是非常不合适的。 老年代存储的对象比年轻代多得多，而且不乏大对象，对老年代进行内存清理时，如果使用停止-复制算法，则相当低效。一般，老年代用的算法是标记-压缩算法，即：标记出仍然存活的对象（存在引用的），将所有存活的对象向一端移动，以保证内存的连续。在发生Minor GC时，虚拟机会检查每次晋升进入老年代的大小是否大于老年代的剩余空间大小，如果大于，则直接触发一次Full GC，否则，就查看是否设置了-XX:+HandlePromotionFailure（允许担保失败），如果允许，则只会进行MinorGC，此时可以容忍内存分配失败；如果不允许，则仍然进行Full GC（这代表着如果设置-XX:+Handle PromotionFailure，则触发MinorGC就会同时触发Full GC，哪怕老年代还有很多内存，所以，最好不要这样做）。 关于方法区即永久代的回收，永久代的回收有两种：常量池中的常量，无用的类信息，常量的回收很简单，没有引用了就可以被回收。对于无用的类进行回收，必须保证3点： 类的所有实例都已经被回收。 加载类的ClassLoader已经被回收。 类对象的Class对象没有被引用（即没有通过反射引用该类的地方）。 永久代的回收并不是必须的，可以通过参数来设置是否对类进行回收。 "},"Chapter06/jvm指令集.html":{"url":"Chapter06/jvm指令集.html","title":"jvm指令集","keywords":"","body":"JVM指令集整理 指令码|助记符 | 说明 指令码 助记符 说明 0×00 nop 什么都不做 0×01 aconst_null 将null推送至栈顶 0×02 iconst_m1 将int型-1推送至栈顶 0×03 iconst_0 将int型0推送至栈顶 0×04 iconst_1 将int型1推送至栈顶 0×05 iconst_2 将int型2推送至栈顶 0×06 iconst_3 将int型3推送至栈顶 0×07 iconst_4 将int型4推送至栈顶 0×08 iconst_5 将int型5推送至栈顶 0×09 lconst_0 将long型0推送至栈顶 0x0a lconst_1 将long型1推送至栈顶 0x0b fconst_0 将float型0推送至栈顶 0x0c fconst_1 将float型1推送至栈顶 0x0d fconst_2 将float型2推送至栈顶 0x0e dconst_0 将double型0推送至栈顶 0x0f dconst_1 将double型1推送至栈顶 0×10 bipush 将单字节的常量值(-128~127)推送至栈顶 0×11 sipush 将一个短整型常量值(-32768~32767)推送至栈顶 0×12 ldc 将int, float或String型常量值从常量池中推送至栈顶 0×13 ldc_w 将int, float或String型常量值从常量池中推送至栈顶（宽索引） 0×14 ldc2_w 将long或double型常量值从常量池中推送至栈顶（宽索引） 0×15 iload 将指定的int型本地变量推送至栈顶 0×16 lload 将指定的long型本地变量推送至栈顶 0×17 fload 将指定的float型本地变量推送至栈顶 0×18 dload 将指定的double型本地变量推送至栈顶 0×19 aload 将指定的引用类型本地变量推送至栈顶 0x1a iload_0 将第0个int型本地变量推送至栈顶 0x1b iload_1 将第1个int型本地变量推送至栈顶 0x1c iload_2 将第2个int型本地变量推送至栈顶 0x1d iload_3 将第3个int型本地变量推送至栈顶 0x1e lload_0 将第0个long型本地变量推送至栈顶 0x1f lload_1 将第1个long型本地变量推送至栈顶 0×20 lload_2 将第2个long型本地变量推送至栈顶 0×21 lload_3 将第3个long型本地变量推送至栈顶 0×22 fload_0 将第0个float型本地变量推送至栈顶 0×23 fload_1 将第1个float型本地变量推送至栈顶 0×24 fload_2 将第2个float型本地变量推送至栈顶 0×25 fload_3 将第3个float型本地变量推送至栈顶 0×26 dload_0 将第0个double型本地变量推送至栈顶 0×27 dload_1 将第1个double型本地变量推送至栈顶 0×28 dload_2 将第2个double型本地变量推送至栈顶 0×29 dload_3 将第3个double型本地变量推送至栈顶 0x2a aload_0 将第0个引用类型本地变量推送至栈顶 0x2b aload_1 将第1个引用类型本地变量推送至栈顶 0x2c aload_2 将第2个引用类型本地变量推送至栈顶 0x2d aload_3 将第3个引用类型本地变量推送至栈顶 0x2e iaload 将int型数组指定索引的值推送至栈顶 0x2f laload 将long型数组指定索引的值推送至栈顶 0×30 faload 将float型数组指定索引的值推送至栈顶 0×31 daload 将double型数组指定索引的值推送至栈顶 0×32 aaload 将引用型数组指定索引的值推送至栈顶 0×33 baload 将boolean或byte型数组指定索引的值推送至栈顶 0×34 caload 将char型数组指定索引的值推送至栈顶 0×35 saload 将short型数组指定索引的值推送至栈顶 0×36 istore 将栈顶int型数值存入指定本地变量 0×37 lstore 将栈顶long型数值存入指定本地变量 0×38 fstore 将栈顶float型数值存入指定本地变量 0×39 dstore 将栈顶double型数值存入指定本地变量 0x3a astore 将栈顶引用型数值存入指定本地变量 0x3b istore_0 将栈顶int型数值存入第0个本地变量 0x3c istore_1 将栈顶int型数值存入第1个本地变量 0x3d istore_2 将栈顶int型数值存入第2个本地变量 0x3e istore_3 将栈顶int型数值存入第3个本地变量 0x3f lstore_0 将栈顶long型数值存入第0个本地变量 0×40 lstore_1 将栈顶long型数值存入第1个本地变量 0×41 lstore_2 将栈顶long型数值存入第2个本地变量 0×42 lstore_3 将栈顶long型数值存入第3个本地变量 0×43 fstore_0 将栈顶float型数值存入第0个本地变量 0×44 fstore_1 将栈顶float型数值存入第1个本地变量 0×45 fstore_2 将栈顶float型数值存入第2个本地变量 0×46 fstore_3 将栈顶float型数值存入第3个本地变量 0×47 dstore_0 将栈顶double型数值存入第0个本地变量 0×48 dstore_1 将栈顶double型数值存入第1个本地变量 0×49 dstore_2 将栈顶double型数值存入第2个本地变量 0x4a dstore_3 将栈顶double型数值存入第3个本地变量 0x4b astore_0 将栈顶引用型数值存入第0个本地变量 0x4c astore_1 将栈顶引用型数值存入第1个本地变量 0x4d astore_2 将栈顶引用型数值存入第2个本地变量 0x4e astore_3 将栈顶引用型数值存入第3个本地变量 0x4f iastore 将栈顶int型数值存入指定数组的指定索引位置 0×50 lastore 将栈顶long型数值存入指定数组的指定索引位置 0×51 fastore 将栈顶float型数值存入指定数组的指定索引位置 0×52 dastore 将栈顶double型数值存入指定数组的指定索引位置 a b c a b c a b c a b c a b c a b c a b c 0×53 aastore 将栈顶引用型数值存入指定数组的指定索引位置 0×54 bastore 将栈顶boolean或byte型数值存入指定数组的指定索引位置 0×55 castore 将栈顶char型数值存入指定数组的指定索引位置 0×56 sastore 将栈顶short型数值存入指定数组的指定索引位置 0×57 pop 将栈顶数值弹出 (数值不能是long或double类型的) 0×58 pop2 将栈顶的一个（long或double类型的)或两个数值弹出（其它） 0×59 dup 复制栈顶数值并将复制值压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0x5a dup_x1 复制栈顶数值并将两个复制值压入栈顶 0x5b dup_x2 复制栈顶数值并将三个（或两个）复制值压入栈顶 0x5c dup2 复制栈顶一个（long或double类型的)或两个（其它）数值并将复制值压入栈顶 0x5d dup2_x1 0x5e dup2_x2 0x5f swap 将栈最顶端的两个数值互换(数值不能是long或double类型的) 0×60 iadd 将栈顶两int型数值相加并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0×61 ladd 将栈顶两long型数值相加并将结果压入栈顶 0×62 fadd 将栈顶两float型数值相加并将结果压入栈顶 0×63 dadd 将栈顶两double型数值相加并将结果压入栈顶 0×64 isub 将栈顶两int型数值相减并将结果压入栈顶 0×65 lsub 将栈顶两long型数值相减并将结果压入栈顶 0×66 fsub 将栈顶两float型数值相减并将结果压入栈顶 0×67 dsub 将栈顶两double型数值相减并将结果压入栈顶 0×68 imul 将栈顶两int型数值相乘并将结果压入栈顶 0×69 lmul 将栈顶两long型数值相乘并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0x6a fmul 将栈顶两float型数值相乘并将结果压入栈顶 0x6b dmul 将栈顶两double型数值相乘并将结果压入栈顶 0x6c idiv 将栈顶两int型数值相除并将结果压入栈顶 0x6d ldiv 将栈顶两long型数值相除并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0x6e fdiv 将栈顶两float型数值相除并将结果压入栈顶 0x6f ddiv 将栈顶两double型数值相除并将结果压入栈顶 0×70 irem 将栈顶两int型数值作取模运算并将结果压入栈顶 0×71 lrem 将栈顶两long型数值作取模运算并将结果压入栈顶 0×72 frem 将栈顶两float型数值作取模运算并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0×73 drem 将栈顶两double型数值作取模运算并将结果压入栈顶 0×74 ineg 将栈顶int型数值取负并将结果压入栈顶 0×75 lneg 将栈顶long型数值取负并将结果压入栈顶 0×76 fneg 将栈顶float型数值取负并将结果压入栈顶 0×77 dneg 将栈顶double型数值取负并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0×78 ishl 将int型数值左移位指定位数并将结果压入栈顶 0×79 lshl 将long型数值左移位指定位数并将结果压入栈顶 0x7a ishr 将int型数值右（符号）移位指定位数并将结果压入栈顶 0x7b lshr 将long型数值右（符号）移位指定位数并将结果压入栈顶 0x7c iushr 将int型数值右（无符号）移位指定位数并将结果压入栈顶 0x7d lushr 将long型数值右（无符号）移位指定位数并将结果压入栈顶 0x7e iand 将栈顶两int型数值作“按位与”并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0x7f land 将栈顶两long型数值作“按位与”并将结果压入栈顶 0×80 ior 将栈顶两int型数值作“按位或”并将结果压入栈顶 0×81 lor 将栈顶两long型数值作“按位或”并将结果压入栈顶 0×82 ixor 将栈顶两int型数值作“按位异或”并将结果压入栈顶 0×83 lxor 将栈顶两long型数值作“按位异或”并将结果压入栈顶 0×84 iinc 将指定int型变量增加指定值，可以有两个变量，分别表示index, const，index指第index个int型本地变量，const增加的值 0×85 i2l 将栈顶int型数值强制转换成long型数值并将结果压入栈顶 0×86 i2f 将栈顶int型数值强制转换成float型数值并将结果压入栈顶 0×87 i2d 将栈顶int型数值强制转换成double型数值并将结果压入栈顶 0×88 l2i 将栈顶long型数值强制转换成int型数值并将结果压入栈顶 0×89 l2f 将栈顶long型数值强制转换成float型数值并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0x8a l2d 将栈顶long型数值强制转换成double型数值并将结果压入栈顶 0x8b f2i 将栈顶float型数值强制转换成int型数值并将结果压入栈顶 0x8c f2l 将栈顶float型数值强制转换成long型数值并将结果压入栈顶 0x8d f2d 将栈顶float型数值强制转换成double型数值并将结果压入栈顶 0x8e d2i 将栈顶double型数值强制转换成int型数值并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0x8f d2l 将栈顶double型数值强制转换成long型数值并将结果压入栈顶 0×90 d2f 将栈顶double型数值强制转换成float型数值并将结果压入栈顶 0×91 i2b 将栈顶int型数值强制转换成byte型数值并将结果压入栈顶 0×92 i2c 将栈顶int型数值强制转换成char型数值并将结果压入栈顶 0×93 i2s 将栈顶int型数值强制转换成short型数值并将结果压入栈顶 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0×94 lcmp 比较栈顶两long型数值大小，并将结果（1，0，-1）压入栈顶 0×95 fcmpl 比较栈顶两float型数值大小，并将结果（1，0，-1）压入栈顶；当其中一个数值为NaN时，将-1压入栈顶 0×96 fcmpg 比较栈顶两float型数值大小，并将结果（1，0，-1）压入栈顶；当其中一个数值为NaN时，将1压入栈顶 0×97 dcmpl 比较栈顶两double型数值大小，并将结果（1，0，-1）压入栈顶；当其中一个数值为NaN时，将-1压入栈顶 0×98 dcmpg 比较栈顶两double型数值大小，并将结果（1，0，-1）压入栈顶；当其中一个数值为NaN时，将1压入栈顶 0×99 ifeq 当栈顶int型数值等于0时跳转 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0x9a ifne 当栈顶int型数值不等于0时跳转 0x9b iflt 当栈顶int型数值小于0时跳转 0x9c ifge 当栈顶int型数值大于等于0时跳转 0x9d ifgt 当栈顶int型数值大于0时跳转 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0x9e ifle 当栈顶int型数值小于等于0时跳转 0x9f if_icmpeq 比较栈顶两int型数值大小，当结果等于0时跳转 0xa0 if_icmpne 比较栈顶两int型数值大小，当结果不等于0时跳转 0xa1 if_icmplt 比较栈顶两int型数值大小，当结果小于0时跳转 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0xa2 if_icmpge 比较栈顶两int型数值大小，当结果大于等于0时跳转 0xa3 if_icmpgt 比较栈顶两int型数值大小，当结果大于0时跳转 0xa4 if_icmple 比较栈顶两int型数值大小，当结果小于等于0时跳转 0xa5 if_acmpeq 比较栈顶两引用型数值，当结果相等时跳转 0xa6 if_acmpne 比较栈顶两引用型数值，当结果不相等时跳转 0xa7 goto 无条件跳转 | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0xa8 jsr 跳转至指定16位offset位置，并将jsr下一条指令地址压入栈顶 0xa9 ret 返回至本地变量指定的index的指令位置（一般与jsr, jsr_w联合使用） 0xaa tableswitch 用于switch条件跳转，case值连续（可变长度指令） 0xab lookupswitch 用于switch条件跳转，case值不连续（可变长度指令） 0xac ireturn 从当前方法返回int 0xad lreturn 从当前方法返回long 0xae freturn 从当前方法返回float 0xaf dreturn 从当前方法返回double 0xb0 areturn 从当前方法返回对象引用 0xb1 return 从当前方法返回void | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | 0xb2 getstatic 获取指定类的静态域，并将其值压入栈顶 0xb3 putstatic 为指定的类的静态域赋值 0xb4 getfield 获取指定类的实例域，并将其值压入栈顶 0xb5 putfield 为指定的类的实例域赋值 0xb6 invokevirtual 调用实例方法 0xb7 invokespecial 调用超类构造方法，实例初始化方法，私有方法 0xb8 invokestatic 调用静态方法 0xb9 invokeinterface 调用接口方法 0xba – | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | a | b | c | | 0xbb | new | 创建一个对象，并将其引用值压入栈顶 | | 0xbc |newarray| 创建一个指定原始类型（如int, float, char…）的数组，并将其引用值压入栈顶 | | 0xbd | anewarray | 创建一个引用型（如类，接口，数组）的数组，并将其引用值压入栈顶 | | 0xbe | arraylength | 获得数组的长度值并压入栈顶 | | 0xbf | athrow | 将栈顶的异常抛出 | | 0xc0 | checkcast | 检验类型转换，检验未通过将抛出ClassCastException ||0xc1 | instanceof | 检验对象是否是指定的类的实例，如果是将1压入栈顶，否则将0压入栈顶| | 0xc2 | monitorenter | 获得对象的锁，用于同步方法或同步块 | | 0xc3 | monitorexit | 释放对象的锁，用于同步方法或同步块 | | 0xc4 | wide | 当本地变量的索引超过255时使用该指令扩展索引宽度。 | | 0xc5 | multianewarray | create a new array of dimensions dimensions with elements of type identified by class reference in constant pool index (indexbyte1 0xc6 ifnull if value is null, branch to instruction at branchoffset (signed short constructed from unsigned bytes branchbyte1 对应英文 => https://en.wikipedia.org/wiki/Java_bytecode_instruction_listings 来几个sample: 切换到文件的对应的目录 sample1 java 1 2 3 public void sample1(){ int num = 5; } javap -c 查看字节码 1 2 3 4 5 public void sample1(); Code: 0: iconst_5 1: istore_1 2: return 解释 1 2 3 iconst_5 //将int型5推送至栈顶 istore_1 //将栈顶int型数值存入第1个本地变量 return //从当前方法返回void sample2 java 1 2 3 public int sample2(int a, int b) { return a + b; } 字节码及解释 1 2 3 4 5 6 public int sample2(int, int); Code: 0: iload_1 //将第1个int型本地变量推送至栈顶 1: iload_2 //将第2个int型本地变量推送至栈顶 2: iadd //将栈顶两int型数值相加并将结果压入栈顶 3: ireturn //从当前方法返回int sample3 java 稍稍复杂点 1 2 3 4 5 6 7 public float sample3() { float num = 0; for (int i = 0; i num *= i; } return num; } 字节码及解释 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public float sample3(); Code: 0: fconst_0 //将float型0推送至栈顶 1: fstore_1 //将栈顶float型数值存入第1个本地变量 2: iconst_0 //将int型0推送至栈顶，也就是for循环中的i = 0 3: istore_2 //将栈顶int型数值存入第2个本地变量 4: iload_2 //将第2个int型本地变量推送至栈顶 5: iconst_5 //将int型5推送至栈顶，也就是for循环中的 最大值5 6: if_icmpge 20 //比较栈顶两int型数值大小，当结果大于等于0时跳转， //也就是比较0是否大于等于5，(cpmge指compare larger equals)，如果是跳转到到第20条指令 9: fload_1 //将第1个float型本地变量推送至栈顶，也就是变量num 10: iload_2 //将第2个int型本地变量推送至栈顶，也就是for循环中的变量i 11: i2f //int型强转为float型，也就是把变量i强转成float 12: fmul //将栈顶两float型数值相乘并将结果压入栈顶，也就是i与num相乘 13: fstore_1 //将栈顶float型数值存入第1个本地变量，也就是之前i与num的乘积 14: iinc 2, 1 //将指定int型变量增加指定值，将第2个int型本地变量增加1， //可以看到，第2个int型本地变量就是之前的变量i 17: goto 4 //无条件跳转到指令4，实现循环效果 20: fload_1 //将第1个float型本地变量推送至栈顶 21: freturn //从当前方法返回float public class MainActivity extends AppCompatActivity { public MainActivity() { } public int calc() { int a = 500; int b = 200; int c = 50; return (a + b) / c; } protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); this.setContentView(2130968603); } } 他的字节码文件 javap -c public class zew.testdemo.MainActivity extends android.support.v7.app.AppCompatActivity { public zew.testdemo.MainActivity(); Code: 0: aload_0 1: invokespecial #1 // Method android/support/v7/app/AppCompatActivity.\"\":()V 4: return public int calc(); Code: 0: sipush 500 3: istore_1 4: sipush 200 7: istore_2 8: bipush 50 10: istore_3 11: iload_1 12: iload_2 13: iadd 14: iload_3 15: idiv 16: ireturn protected void onCreate(android.os.Bundle); Code: 0: aload_0 1: aload_1 2: invokespecial #2 // Method android/support/v7/app/AppCompatActivity.onCreate:(Landroid/os/Bundle;)V 5: aload_0 6: ldc #4 // int 2130968603 8: invokevirtual #5 // Method setContentView:(I)V 11: return } 这里额外要说的就是，在非静态方法中，aload_0 表示对this的操作，在static 方法中，aload_0表示对方法的第一参数的操作。 ———————————————— 版权声明：本文为CSDN博主「遥望江南2009」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/u012070360/article/details/81624854 "},"Chapter06/jstack.html":{"url":"Chapter06/jstack.html","title":"jstack","keywords":"","body":"JStack "},"Chapter06/newObject.html":{"url":"Chapter06/newObject.html","title":"new Object占多大内存","keywords":"","body":"Object o=new Object()在内存中占用多少字节 如果jvm默认开启了UseCompressedClassPointers类型指针压缩，那么首先new Object（）占用16个字节（markword占8+classpointer占4+instancedata占0+补齐4），然后Object o有一个引用，这个引用默认开启了压缩，所以是4个字节（每个引用占用4个字节），所以一共占用20个字节（byte） 如果jvm没开启CompressedClassPointers类型指针压缩，那么首先new Object（）占用8(markword)+8(class pointer)+0(instance data)+0(补齐为8的倍数)16个字节，然后加引用（因为jvm默认开启UseCompressedClassPointers类型指针压缩，所以默认引用是占4字节，但这里没启用压缩，所以为8字节）占的8个字节=24个字节 普通对象在内存中的存储布局: 普通对象（new xx（））组成 markword（8字节）：关于锁的信息，关于synchronized所有信息都存储在markword中 类型指针（jvm默认开启压缩，为4字节）：指向具体哪个类，64位系统中，默认一个类型指针占64位，8字节，但是jvm默认UseCompressedClassPointers,将其压缩为4字节，markword+类型指针class pointer=对象头（12字节） 实例数据：像int就是4字节，long就是8字节 对齐：因为jvm按8的倍数读，所以要对齐，不够的补，这样读就特别快，提升效率 数组对象组成 对象头markword,类型指针class pointer，数组长度length(4字节)，实例数据instance data，对齐padding 与普通对象相比，数组对象就是多了一个4字节的数组长度length,其余部分与数组对象保持一致。 Klass Word 这里其实是虚拟机设计的一个oop-klass model模型，这里的OOP是指Ordinary Object Pointer（普通对象指针），看起来像个指针实际上是藏在指针里的对象。 而 klass 则包含 元数据和方法信息，用来描述 Java 类。它在64位虚拟机开启压缩指针的环境下占用 32bits 空间。 Mark Word Mark Word 是我们分析的重点，这里也会设计到锁的相关知识。Mark Word 在64位虚拟机环境下占用 64bits 空间。整个Mark Word的分配有几种情况： 未锁定（Normal）： 哈希码（identity_hashcode）占用31bits，分代年龄（age）占用4 bits，偏向模式（biased_lock）占用1 bits，锁标记（lock）占用2 bits，剩余26bits 未使用(也就是全为0) 可偏向（Biased）： 线程id 占54bits，epoch 占2 bits，分代年龄（age）占用4 bits，偏向模式（biased_lock）占用1 bits，锁标记（lock）占用2 bits，剩余 1bit 未使用。 轻量锁定（Lightweight Locked）： 锁指针占用62bits，锁标记（lock）占用2 bits。 重量级锁定（Heavyweight Locked）：锁指针占用62bits，锁标记（lock）占用2 bits。 GC 标记：标记位占2bits，其余为空（也就是填充0） 以上就是我们对Java对象头内存模型的解析，只要是Java对象，那么就肯定会包括对象头，也就是说这部分内存占用是避免不了的。所以，在笔者64位虚拟机，Jdk1.8（开启了指针压缩）的环境下，任何一个对象，啥也不做，只要声明一个类，那么它的内存占用就至少是96bits，也就是至少12字节。 验证模型 首先添加maven依赖 org.openjdk.jol jol-core 0.10 "},"Chapter06/Tuning.html":{"url":"Chapter06/Tuning.html","title":"调优","keywords":"","body":"1 top， top -Hp pid ,jstack ，jmap 保存堆栈 cat stack | grep -i 34670 -C10 --color 我通过jhat 命令生成html的内存信息页面 然后输入http://localhost:7000查看 "},"Chapter06/ZGC.html":{"url":"Chapter06/ZGC.html","title":"ZGC","keywords":"","body":"Java 新一代垃圾回收器 ZGC ZGC（The Z Garbage Collector）是JDK 11中推出的一款低延迟垃圾回收器，它的设计目标包括： 停顿时间不超过10ms； 停顿时间不会随着堆的大小，或者活跃对象的大小而增加； 支持8MB~4TB级别的堆（未来支持16TB）。 GC之痛 很多低延迟高可用Java服务的系统可用性经常受GC停顿的困扰。GC停顿指垃圾回收期间STW（Stop The World），当STW时，所有应用线程停止活动，等待GC停顿结束。 以风控服务为例，部分上游业务要求风控服务65ms内返回结果，并且可用性要达到99.99%。但因为GC停顿，我们未能达到上述可用性目标。当时使用的是CMS垃圾回收器，单次Young GC 40ms，一分钟10次，接口平均响应时间30ms。通过计算可知，有（ 40ms + 30ms ) 10次 / 60000ms = 1.12%的请求的响应时间会增加0 ~ 40ms不等，其中30ms 10次 / 60000ms = 0.5%的请求响应时间会增加40ms。 可见，GC停顿对响应时间的影响较大。为了降低GC停顿对系统可用性的影响，我们从降低单次GC时间和降低GC频率两个角度出发进行了调优，还测试过G1垃圾回收器，但这三项措施均未能降低GC对服务可用性的影响。 CMS与G1停顿时间瓶颈 在介绍ZGC之前，首先回顾一下CMS和G1的GC过程以及停顿时间的瓶颈。CMS新生代的Young GC、G1和ZGC都基于标记-复制算法，但算法具体实现的不同就导致了巨大的性能差异。 标记-复制算法应用在CMS新生代（ParNew是CMS默认的新生代垃圾回收器）和G1垃圾回收器中。标记-复制算法可以分为三个阶段： 标记阶段，即从GC Roots集合开始，标记活跃对象； 转移阶段，即把活跃对象复制到新的内存地址上； 重定位阶段，因为转移导致对象的地址发生了变化，在重定位阶段，所有指向对象旧地址的指针都要调整到对象新的地址上。 下面以G1为例，通过G1中标记-复制算法过程（G1的Young GC和Mixed GC均采用该算法），分析G1停顿耗时的主要瓶颈。G1垃圾回收周期如下图所示： G1的混合回收过程可以分为标记阶段、清理阶段和复制阶段。 标记阶段停顿分析 初始标记阶段：初始标记阶段是指从GC Roots出发标记全部直接子节点的过程，该阶段是STW的。由于GC Roots数量不多，通常该阶段耗时非常短。 并发标记阶段：并发标记阶段是指从GC Roots开始对堆中对象进行可达性分析，找出存活对象。该阶段是并发的，即应用线程和GC线程可以同时活动。并发标记耗时相对长很多，但因为不是STW，所以我们不太关心该阶段耗时的长短。 再标记阶段：重新标记那些在并发标记阶段发生变化的对象。该阶段是STW的。 清理阶段停顿分析 清理阶段清点出有存活对象的分区和没有存活对象的分区，该阶段不会清理垃圾对象，也不会执行存活对象的复制。该阶段是STW的。 复制阶段停顿分析 复制算法中的转移阶段需要分配新内存和复制对象的成员变量。转移阶段是STW的，其中内存分配通常耗时非常短，但对象成员变量的复制耗时有可能较长，这是因为复制耗时与存活对象数量与对象复杂度成正比。对象越复杂，复制耗时越长。 四个STW过程中，初始标记因为只标记GC Roots，耗时较短。再标记因为对象数少，耗时也较短。清理阶段因为内存分区数量少，耗时也较短。转移阶段要处理所有存活的对象，耗时会较长。因此，G1停顿时间的瓶颈主要是标记-复制中的转移阶段STW。为什么转移阶段不能和标记阶段一样并发执行呢？主要是G1未能解决转移过程中准确定位对象地址的问题。 G1的Young GC和CMS的Young GC，其标记-复制全过程STW，这里不再详细阐述。 ZGC原理 全并发的ZGC 与CMS中的ParNew和G1类似，ZGC也采用标记-复制算法，不过ZGC对该算法做了重大改进：ZGC在标记、转移和重定位阶段几乎都是并发的，这是ZGC实现停顿时间小于10ms目标的最关键原因。 ZGC垃圾回收周期如下图所示： ZGC只有三个STW阶段：初始标记，再标记，初始转移。其中，初始标记和初始转移分别都只需要扫描所有GC Roots，其处理时间和GC Roots的数量成正比，一般情况耗时非常短；再标记阶段STW时间很短，最多1ms，超过1ms则再次进入并发标记阶段。即，ZGC几乎所有暂停都只依赖于GC Roots集合大小，停顿时间不会随着堆的大小或者活跃对象的大小而增加。与ZGC对比，G1的转移阶段完全STW的，且停顿时间随存活对象的大小增加而增加。 ZGC关键技术 ZGC通过着色指针和读屏障技术，解决了转移过程中准确访问对象的问题，实现了并发转移。大致原理描述如下：并发转移中“并发”意味着GC线程在转移对象的过程中，应用线程也在不停地访问对象。假设对象发生转移，但对象地址未及时更新，那么应用线程可能访问到旧地址，从而造成错误。而在ZGC中，应用线程访问对象将触发“读屏障”，如果发现对象被移动了，那么“读屏障”会把读出来的指针更新到对象的新地址上，这样应用线程始终访问的都是对象的新地址。那么，JVM是如何判断对象被移动过呢？就是利用对象引用的地址，即着色指针。下面介绍着色指针和读屏障技术细节。 着色指针 着色指针是一种将信息存储在指针中的技术。 ZGC仅支持64位系统，它把64位虚拟地址空间划分为多个子空间，如下图所示： 其中，[0~4TB) 对应Java堆，[4TB ~ 8TB) 称为M0地址空间，[8TB ~ 12TB) 称为M1地址空间，[12TB ~ 16TB) 预留未使用，[16TB ~ 20TB) 称为Remapped空间。 当应用程序创建对象时，首先在堆空间申请一个虚拟地址，但该虚拟地址并不会映射到真正的物理地址。ZGC同时会为该对象在M0、M1和Remapped地址空间分别申请一个虚拟地址，且这三个虚拟地址对应同一个物理地址，但这三个空间在同一时间有且只有一个空间有效。ZGC之所以设置三个虚拟地址空间，是因为它使用“空间换时间”思想，去降低GC停顿时间。“空间换时间”中的空间是虚拟空间，而不是真正的物理空间。后续章节将详细介绍这三个空间的切换过程。 与上述地址空间划分相对应，ZGC实际仅使用64位地址空间的第0~41位，而第42~45位存储元数据，第47~63位固定为0。 ZGC将对象存活信息存储在42~45位中，这与传统的垃圾回收并将对象存活信息放在对象头中完全不同。 读屏障 读屏障是JVM向应用代码插入一小段代码的技术。当应用线程从堆中读取对象引用时，就会执行这段代码。需要注意的是，仅“从堆中读取对象引用”才会触发这段代码。 读屏障示例： Object o = obj.FieldA // 从堆中读取引用，需要加入屏障 Object p = o // 无需加入屏障，因为不是从堆中读取引用 o.dosomething // 无需加入屏障，因为不是从堆中读取引用 int i = obj.FieldB //无需加入屏障，因为不是对象引用 ZGC中读屏障的代码作用：在对象标记和转移过程中，用于确定对象的引用地址是否满足条件，并作出相应动作。 ZGC并发处理演示 接下来详细介绍ZGC一次垃圾回收周期中地址视图的切换过程： 初始化：ZGC初始化之后，整个内存空间的地址视图被设置为Remapped。程序正常运行，在内存中分配对象，满足一定条件后垃圾回收启动，此时进入标记阶段。 并发标记阶段：第一次进入标记阶段时视图为M0，如果对象被GC标记线程或者应用线程访问过，那么就将对象的地址视图从Remapped调整为M0。所以，在标记阶段结束之后，对象的地址要么是M0视图，要么是Remapped。如果对象的地址是M0视图，那么说明对象是活跃的；如果对象的地址是Remapped视图，说明对象是不活跃的。 并发转移阶段：标记结束后就进入转移阶段，此时地址视图再次被设置为Remapped。如果对象被GC转移线程或者应用线程访问过，那么就将对象的地址视图从M0调整为Remapped。 其实，在标记阶段存在两个地址视图M0和M1，上面的过程显示只用了一个地址视图。之所以设计成两个，是为了区别前一次标记和当前标记。即第二次进入并发标记阶段后，地址视图调整为M1，而非M0。 着色指针和读屏障技术不仅应用在并发转移阶段，还应用在并发标记阶段：将对象设置为已标记，传统的垃圾回收器需要进行一次内存访问，并将对象存活信息放在对象头中；而在ZGC中，只需要设置指针地址的第42~45位即可，并且因为是寄存器访问，所以速度比访问内存更快。 ZGC调优实践 ZGC不是“银弹”，需要根据服务的具体特点进行调优。网络上能搜索到实战经验较少，调优理论需自行摸索，我们在此阶段也耗费了不少时间，最终才达到理想的性能。本文的一个目的是列举一些使用ZGC时常见的问题，帮助大家使用ZGC提高服务可用性。 重要参数配置样例： -Xms10G -Xmx10G -XX:ReservedCodeCacheSize=256m -XX:InitialCodeCacheSize=256m -XX:+UnlockExperimentalVMOptions -XX:+UseZGC -XX:ConcGCThreads=2 -XX:ParallelGCThreads=6 -XX:ZCollectionInterval=120 -XX:ZAllocationSpikeTolerance=5 -XX:+UnlockDiagnosticVMOptions -XX:-ZProactive -Xlog:safepoint,classhisto*=trace,age*,gc*=info:file=/opt/logs/logs/gc-%t.log:time,tid,tags:filecount=5,filesize=50m -Xms -Xmx：堆的最大内存和最小内存，这里都设置为10G，程序的堆内存将保持10G不变。 -XX:ReservedCodeCacheSize -XX:InitialCodeCacheSize: 设置CodeCache的大小， JIT编译的代码都放在CodeCache中，一般服务64m或128m就已经足够。我们的服务因为有一定特殊性，所以设置的较大，后面会详细介绍。 -XX:+UnlockExperimentalVMOptions -XX:+UseZGC：启用ZGC的配置。 -XX:ConcGCThreads：并发回收垃圾的线程。默认是总核数的12.5%，8核CPU默认是1。调大后GC变快，但会占用程序运行时的CPU资源，吞吐会受到影响。 -XX:ParallelGCThreads：STW阶段使用线程数，默认是总核数的60%。 -XX:ZCollectionInterval：ZGC发生的最小时间间隔，单位秒。 -XX:ZAllocationSpikeTolerance：ZGC触发自适应算法的修正系数，默认2，数值越大，越早的触发ZGC。 -XX:+UnlockDiagnosticVMOptions -XX:-ZProactive：是否启用主动回收，默认开启，这里的配置表示关闭。 -Xlog：设置GC日志中的内容、格式、位置以及每个日志的大小。 理解ZGC触发时机 相比于CMS和G1的GC触发机制，ZGC的GC触发机制有很大不同。ZGC的核心特点是并发，GC过程中一直有新的对象产生。如何保证在GC完成之前，新产生的对象不会将堆占满，是ZGC参数调优的第一大目标。因为在ZGC中，当垃圾来不及回收将堆占满时，会导致正在运行的线程停顿，持续时间可能长达秒级之久。 ZGC有多种GC触发机制，总结如下： 阻塞内存分配请求触发：当垃圾来不及回收，垃圾将堆占满时，会导致部分线程阻塞。我们应当避免出现这种触发方式。日志中关键字是“Allocation Stall”。 基于分配速率的自适应算法：最主要的GC触发方式，其算法原理可简单描述为”ZGC根据近期的对象分配速率以及GC时间，计算出当内存占用达到什么阈值时触发下一次GC”。自适应算法的详细理论可参考彭成寒《新一代垃圾回收器ZGC设计与实现》一书中的内容。通过ZAllocationSpikeTolerance参数控制阈值大小，该参数默认2，数值越大，越早的触发GC。我们通过调整此参数解决了一些问题。日志中关键字是“Allocation Rate”。 基于固定时间间隔：通过ZCollectionInterval控制，适合应对突增流量场景。流量平稳变化时，自适应算法可能在堆使用率达到95%以上才触发GC。流量突增时，自适应算法触发的时机可能会过晚，导致部分线程阻塞。我们通过调整此参数解决流量突增场景的问题，比如定时活动、秒杀等场景。日志中关键字是“Timer”。 主动触发规则：类似于固定间隔规则，但时间间隔不固定，是ZGC自行算出来的时机，我们的服务因为已经加了基于固定时间间隔的触发机制，所以通过-ZProactive参数将该功能关闭，以免GC频繁，影响服务可用性。日志中关键字是“Proactive”。 预热规则：服务刚启动时出现，一般不需要关注。日志中关键字是“Warmup”。 外部触发：代码中显式调用System.gc触发。日志中关键字是“System.gc”。 元数据分配触发：元数据区不足时导致，一般不需要关注。日志中关键字是“Metadata GC Threshold”。 理解ZGC日志 一次完整的GC过程，需要注意的点已在图中标出。 注意：该日志过滤了进入安全点的信息。正常情况，在一次GC过程中还穿插着进入安全点的操作。 GC日志中每一行都注明了GC过程中的信息，关键信息如下： Start：开始GC，并标明的GC触发的原因。上图中触发原因是自适应算法。 Phase-Pause Mark Start：初始标记，会STW。 Phase-Pause Mark End：再次标记，会STW。 Phase-Pause Relocate Start：初始转移，会STW。 Heap信息：记录了GC过程中Mark、Relocate前后的堆大小变化状况。High和Low记录了其中的最大值和最小值，我们一般关注High中Used的值，如果达到100%，在GC过程中一定存在内存分配不足的情况，需要调整GC的触发时机，更早或者更快地进行GC。 GC信息统计：可以定时的打印垃圾收集信息，观察10秒内、10分钟内、10个小时内，从启动到现在的所有统计信息。利用这些统计信息，可以排查定位一些异常点。 日志中内容较多，关键点已用红线标出，含义较好理解，更详细的解释大家可以自行在网上查阅资料。 理解ZGC停顿原因 我们在实战过程中共发现了6种使程序停顿的场景，分别如下： GC时，初始标记：日志中Pause Mark Start。 GC时，再标记：日志中Pause Mark End。 GC时，初始转移：日志中Pause Relocate Start。 内存分配阻塞：当内存不足时线程会阻塞等待GC完成，关键字是\"Allocation Stall\"。 安全点：所有线程进入到安全点后才能进行GC，ZGC定期进入安全点判断是否需要GC。先进入安全点的线程需要等待后进入安全点的线程直到所有线程挂起。 dump线程、内存：比如jstack、jmap命令。 "},"Chapter06/Constantpool.html":{"url":"Chapter06/Constantpool.html","title":"常量池","keywords":"","body":"常量池 在java的内存分配中，经常听到很多关于常量池的描述，我开始看的时候也是看的很模糊，网上五花八门的说法简直太多了，最后查阅各种资料，终于算是差不多理清了，很多网上说法都有问题，笔者尝试着来区分一下这几个概念。 全局字符串池（string pool也有叫做string literal pool） 全局字符串池里的内容是在类加载完成，经过验证，准备阶段之后在堆中生成字符串对象实例，然后将该字符串对象实例的引用值存到string pool中（记住：string pool中存的是引用值而不是具体的实例对象，具体的实例对象是在堆中开辟的一块空间存放的。）。 在HotSpot VM里实现的string pool功能的是一个StringTable类，它是一个哈希表，里面存的是驻留字符串(也就是我们常说的用双引号括起来的)的引用（而不是驻留字符串实例本身），也就是说在堆中的某些字符串实例被这个StringTable引用之后就等同被赋予了”驻留字符串”的身份。这个StringTable在每个HotSpot VM的实例只有一份，被所有的类共享。 class文件常量池（class constant pool） 我们都知道，class文件中除了包含类的版本、字段、方法、接口等描述信息外，还有一项信息就是常量池(constant pool table)，用于存放编译器生成的各种字面量(Literal)和符号引用(Symbolic References)。 字面量就是我们所说的常量概念，如文本字符串、被声明为final的常量值等。 符号引用是一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可（它与直接引用区分一下，直接引用一般是指向方法区的本地指针，相对偏移量或是一个能间接定位到目标的句柄）。一般包括下面三类常量： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 常量池的每一项常量都是一个表，一共有如下表所示的11种各不相同的表结构数据，这每个表开始的第一位都是一个字节的标志位（取值1-12），代表当前这个常量属于哪种常量类型。 每种不同类型的常量类型具有不同的结构，具体的结构本文就先不叙述了，本文着重区分这三个常量池的概念（读者若想深入了解每种常量类型的数据结构可以查看《深入理解java虚拟机》第六章的内容）。 运行时常量池（runtime constant pool） 当java文件被编译成class文件之后，也就是会生成我上面所说的class常量池，那么运行时常量池又是什么时候产生的呢？ jvm在执行某个类的时候，必须经过加载、连接、初始化，而连接又包括验证、准备、解析三个阶段。而当类加载到内存中后， jvm就会将class常量池中的内容存放到运行时常量池中，由此可知，运行时常量池也是每个类都有一个。在上面我也说了，class常量池中存的是字面量和符号引用， 也就是说他们存的并不是对象的实例，而是对象的符号引用值。而经过解析（resolve）之后，也就是把符号引用替换为直接引用，解析的过程会去查询全局字符串池， 也就是我们上面所说的StringTable，以保证运行时常量池所引用的字符串与全局字符串池中所引用的是一致的。 举个实例来说明一下: public class HelloWorld { public static void main(String []args) { String str1 = \"abc\"; String str2 = new String(\"def\"); String str3 = \"abc\"; String str4 = str2.intern(); String str5 = \"def\"; System.out.println(str1 == str3);//true System.out.println(str2 == str4);//false System.out.println(str4 == str5);//true } } 回到上面的那个程序，现在就很容易解释整个程序的内存分配过程了，首先，在堆中会有一个”abc”实例，全局StringTable中存放着”abc”的一个引用值，然后在运行第二句的时候会生成两个实例，一个是”def”的实例对象，并且StringTable中存储一个”def”的引用值，还有一个是new出来的一个”def”的实例对象，与上面那个是不同的实例，当在解析str3的时候查找StringTable，里面有”abc”的全局驻留字符串引用，所以str3的引用地址与之前的那个已存在的相同，str4是在运行的时候调用intern()函数，返回StringTable中”def”的引用值，如果没有就将str2的引用值添加进去，在这里，StringTable中已经有了”def”的引用值了，所以返回上面在new str2的时候添加到StringTable中的 “def”引用值，最后str5在解析的时候就也是指向存在于StringTable中的”def”的引用值，那么这样一分析之后，下面三个打印的值就容易理解了。上面程序的首先经过编译之后，在该类的class常量池中存放一些符号引用，然后类加载之后，将class常量池中存放的符号引用转存到运行时常量池中，然后经过验证，准备阶段之后，在堆中生成驻留字符串的实例对象（也就是上例中str1所指向的”abc”实例对象），然后将这个对象的引用存到全局String Pool中，也就是StringTable中，最后在解析阶段，要把运行时常量池中的符号引用替换成直接引用，那么就直接查询StringTable，保证StringTable里的引用值与运行时常量池中的引用值一致，大概整个过程就是这样了。 总结 全局常量池在每个VM中只有一份，存放的是字符串常量的引用值。 class常量池是在编译的时候每个class都有的，在编译阶段，存放的是常量的符号引用。 运行时常量池是在类加载完成之后，将每个class常量池中的符号引用值转存到运行时常量池中，也就是说，每个class都有一个运行时常量池，类在解析之后，将符号引用替换成直接引用，与全局常量池中的引用值保持一致。 class文件常量池和运行时常量池 最近一直被方法区里面存着什么东西困扰着？ 方法区里存class文件信息和class文件常量池是个什么关系。 class文件常量池和运行时常量池是什么关系。 方法区存着类的信息，常量和静态变量，即类被编译后的数据。这个说法其实是没问题的，只是太笼统了。更加详细一点的说法是方法区里存放着类的版本，字段，方法，接口和常量池。常量池里存储着字面量和符号引用。 符号引用包括： 类的全限定名， 字段名和属性， 方法名和属性。 下面一张图是方法区，class文件信息，class文件常量池和运行时常量池的关系 下面一张图用来表示方法区class文件信息包括哪些内容: 可以看到在方法区里的class文件信息包括：魔数，版本号，常量池，类，父类和接口数组，字段，方法等信息，其实类里面又包括字段和方法的信息。 下面的图表是class文件中存储的数据类型 类型 名称 数量 u4 magic 1 u2 minor_version 1 u2 major_version 1 u2 constant_pool_count 1 cp_info constant_pool constant_pool_count -1 u2 access_flags 1 下面用一张图来表示常量池里存储的内容： class文件常量池和运行时常量池的关系以及区别 class文件常量池存储的是当class文件被java虚拟机加载进来后存放在方法区的一些字面量和符号引用，字面量包括字符串，基本类型的常量。 运行时常量池是当class文件被加载完成后，java虚拟机会将class文件常量池里的内容转移到运行时常量池里，在class文件常量池的符号引用有一部分是会被转变为直接引用的，比如说类的静态方法或私有方法，实例构造方法，父类方法，这是因为这些方法不能被重写其他版本，所以能在加载的时候就可以将符号引用转变为直接引用，而其他的一些方法是在这个方法被第一次调用的时候才会将符号引用转变为直接引用的。 总结： 方法区里存储着class文件的信息和运行时常量池,class文件的信息包括类信息和class文件常量池。 运行时常量池里的内容除了是class文件常量池里的内容外，还将class文件常量池里的符号引用转变为直接引用，而且运行时常量池里的内容是能动态添加的。例如调用String的intern方法就能将string的值添加到String常量池中，这里String常量池是包含在运行时常量池里的，但在jdk1.8后，将String常量池放到了堆中 "},"Chapter07/concurrent.html":{"url":"Chapter07/concurrent.html","title":"Part VII 并发篇","keywords":"","body":"第七章 并发篇 计算机体系结构基础知识 java并发必知的计算机原理知识 java并发必知的底层CPU知识 Java线程与内核线程 Java中断的工作方式 Java多线程如何阻塞与唤醒线程？ 并发锁LockSupport原理剖析 线程的状态及其转换过程 Java线程何时放弃CPU时间片 线程的sleep操作实现原理 Happens-before原则 指令重新排序 Java锁机制 JDK的互斥锁与共享锁 AQS的自旋锁 CountDownLatch实现原理及案例 深入剖析可重入锁ReentrantLock ReadWriteLock实现原理剖析 "},"Chapter07/BasicKnowledgeOfComputerArchitecture.html":{"url":"Chapter07/BasicKnowledgeOfComputerArchitecture.html","title":"计算机体系结构基础知识","keywords":"","body":"计算机体系结构基础知识 CPU基础 CPU即处理器，是计算机中控制数据操控的电路。 它主要由三部分构成：算术/逻辑单元、控制单元和寄存器单元。 它们的作用分别为执行运算、协调机器活动以及临时存储。 CPU与主存 CPU中的寄存器分为通用寄存器和专用寄存器，通用寄存器用于临时存放CPU正在使用的数据，而专用寄存器用于CPU专有用途，比如指令寄存器和程序计数器。 CPU与主存的通过总线进行通信，CPU通过控制单元能够操作主存中的数据。 执行两个数值相加的过程大致为：从主存读取第一个值放到寄存器1 -> 从主存读取第二个值放到寄存器2 -> 两个寄存器保存的值作为输入送到加法电路 -> 将加法结果保存到寄存器3 -> 控制单元将结果放到主存中。 程序等同数据 原始的计算机并不像现代计算机一样将程序保存起来，以前的人们只对数据进行保存，而设备执行的步骤作为计算机的一部分而被内置在控制单元中。这样就很不灵活，最多只能通过重新布线来提升灵活性。将程序与数据视作相同本质是很大的思想突破，因为人们一直认为它们是不同的事物，数据应该存放在主存中而程序应该属于CPU的一部分。 将程序作为数据一样保存在主存中大有好处，控制单元能够从主存读取程序，然后对它们解码并执行。当我们要修改执行程序时可以在计算机的主存中修改，而不必对CPU更改或重新布线。 指令系统 程序包含了大量的机器指令，CPU对这些指令进行解码并执行。CPU分为两类体系：精简指令集计算机（RISC）和复杂指令集计算机(CISC)。RISC提供了最小的机器指令集，计算机效率高速度快且制造成本低。而CISC提供了强大丰富的指令集，能更方便实现复杂的软件。 机器指令分为三类：数据传输类、算术/逻辑类与控制类。 数据传输类指令用于将数据从一个地方移动到另一个地方。比如将主存单元的内容加载到寄存器的LOAD指令，反之将寄存器的内容保存到主存的STORE指令。此外，CPU与其它设备（键盘、鼠标、打印机、显示器、磁盘等）进行通信的指令被称为I/O指令。 算术/逻辑类指令用于让控制单元请求在算术/逻辑单元内执行运算。这些运算包括算术、与、或、异或和位移等。 控制类指令用于指导程序执行。比如转移（JUMP）指令，它包括无条件转移和条件转移。 指令寄存器与程序计数器 CPU将主存的指令加载进来解码并执行，其中涉及两个重要寄存器：指令寄存器与程序计数器。指令寄存器英语存储正在执行的指令，而程序计数器则保持下一个待执行的指令地址。 CPU向主存请求加载程序计数器指定的地址的指令，将其存放到指令寄存器中，加载后将程序计数器的值加2（假如指令长度为2个字节）。 指令如何执行 比如我们要计算11+22，假设过程为：将主存地址为00的内容加载到寄存器1中->将主存地址为01的内容加载到寄存器2中->将寄存器1和寄存器2的数据相加并将结果保存到寄存器3->将寄存器3的结果存储到主存地址为02的位置->停止。 这个过程CPU涉及到四个操作：加载(load)、存储(store)、加法(add)和停止(halt)。可以对这些操作进行编码，比如可以分别用1、2、3、0000表示。 1100 1201 3312 2302 0000 控制器 CPU与其他设备的通信一般通过控制器来实现，控制器可能在主板上，也可能以电路板形式查到主板。控制器本身可以看成是小型计算机，也有自己简单的CPU。 以前每连接一种外设都需要购买对应的控制器，而现在随着通用串行总线（USB）成为通用的标准，很多外设都可以直接用USB控制器作为通信接口。每个控制器都连接在总线上，通过总线进行通信。 每个控制器可能被设计成对应一组地址引用，主存则会忽略这些地址引用。当CPU往这些地址上发送消息时，其实是直接穿过主存而到控制器的，操作的是控制器而非主存。这种模式称为存储映射输入/输出。此外，这种模式的另外一种实现可以在机器指令中提供特定的操作码，专门用于与控制器通信，这样的操作码称为I/O指令。 直接存储器存取 直接存储器存取（DMA）是一种提升外设通信性能的措施，CPU并非总是需要使用总线，在总线空闲的时间里控制器能够充分利用起来。因为控制器都与总线相连接，而控制器又有执行指令的能力，所以可以将CPU的一些工作分给控制器来完成。比如在磁盘中检索数据时，CPU可以将告知控制器，然后由控制器找到数据并放到主存上，期间CPU可以去执行其他任务。这样能节省CPU资源。不过DMA会使总线通信更加复杂，而且会导致总线竞争问题。总线瓶颈源自冯诺依曼体系结构。 "},"Chapter07/JavaConcurrencyMustKnowComputerPrinciplesKnowledge.html":{"url":"Chapter07/JavaConcurrencyMustKnowComputerPrinciplesKnowledge.html","title":"java并发必知的计算机原理知识","keywords":"","body":"java并发必知的计算机原理知识 前言 Java并发为什么有这么多难点呢？实际上并不是Java语言的问题，从本质上来说是因为并发操作本身的问题， 此外还有一些问题是由计算机的体系结构引发的。为了能更好地理解Java并发过程中的问题，我们需要对计算机组成原理有一些基础的认识。 逻辑门 逻辑门是计算机的基础元件，通过它可以完成逻辑运算（也称布尔运算），这类运算输入输出都只有0和1，包含与门、或门、非门等常见逻辑门。 与门，即执行“与”操作，两个输入一个输出。只有当两个输入都为1时输出才为1，其它情况都为0。 或门，即执行“或”操作，两个输入一个输出。只要两个输入的其中一个为1则输出就为1。 非门，即执行“非”操作，一个输入一个输出，该运算用于取输入信号的对立信号。 通过以上三种基本的逻辑门就能实现所有逻辑运算，计算机的本质就是由这三种基本门实现，通过成千上万个逻辑门实现计算。 加法操作 加法是所有一切运算的基础，我们看怎么通过前面说到的逻辑门实现加法运算。计算机与人类的计算方式不同，人类常用十进制，而计算机擅长用二进制，计算机计算时以二进制进行。 异或门(XOR)由三种最基础的门组合得到，异或门结构如下。 简记为， 组合成半加器，如下， 记为， 半加器只能处理两个二进制一位数的相加，并且不能处理前面计算的进位。为了处理进位，可以将两个半加器和一个或门连接，组成全加器，如下， 全加器只能处理三个二进制一位数（其中一位是进位输入）的相加。如果要实现n位二进制数据的相加，就需要使用n个全加器连接起来。 之所以说加法是一切运算的基础，是因为数学家已经证明了能通过加法能实现其它运算，比如乘法、除法、平方、开方、对数等等。 机器指令 虽说有了逻辑运算已经能帮助人类完成计算了，但是人类直接这样使用是非常不友好而且低效的。比如要做(11+22+33)+(44+55)操作时，我们需要分别先输入三个数并相加，然后自己手动记录下中间结果，类似地需要将另外一个中间结果记录下来，然后再一次输入两个数执行加法运算。 为了让上述过程能自动化，人类搞来了内存，它可以用来存放数据，可以把内存看成很多块，每块都对应有一个地址，通过地址可以对数据进行存储、读取和修改。这里把相关数据都已放到内存中。 读取000地址的数到加法器 把001地址的数加到加法器 把002地址的数加到加法器 把加法器的数保存到003地址指向位置 读取004地址的数到加法器 把005地址的数加到加法器 把加法器的数保存到006地址指向位置 读取003地址的数到加法器 把006地址的数加到加法器 停止加法器。 这个过程涉及到四个操作，读取(load)、保存(store)、加(add)和停止(halt)。并且也可以对这些操作进行编码，比如可以分别用100、101、102、103表示。上述过程可用下面指令操作。 load 000 add 001 add 002 store 003 load 004 add 005 store 006 load 003 add 006 halt 对应指令编码为， 100 000 102 001 102 002 101 003 ... 103 有了如上指令，将它们保存到内存中，这样计算机就能够一条条往下执行，不需要人工介入，直到运行到停止指令才结束，整个过程实现自动化。 以上以加法运算过程简单介绍指令运算过程，而真正的计算机需要更多的指令集，这也需要更多的硬件来支持。对于计算机系统的指令集的设计有两种思路：①设计精简的指令集，然后复杂的计算通过编程实现。②设计复杂的指令集，直接通过硬件实现复杂的指令，这种运算速度更快，但增加了硬件的复杂度和成本。 寄存器 实际硬件设计中，因为计算过程中经常涉及到一些常用的数进行操作，于是专门设计了寄存器用来对需要中转的数据进行暂存，这类暂存方式速度远超内存方式，速度很快以至于能够很好匹CPU的执行。类似于前面加法器将计算结果保存在加法器中。 下面两张图第一张体现不同存储直接的速度比较，第二张是一些指令例子。 "},"Chapter07/JavaConcurrencyMustKnowTheUnderlyingCPUKnowledge.html":{"url":"Chapter07/JavaConcurrencyMustKnowTheUnderlyingCPUKnowledge.html","title":"java并发必知的底层CPU知识","keywords":"","body":"java并发必知的底层CPU知识 前言 Java并发为什么有这么多难点呢？实际上并不是Java语言的问题，从本质上来说是因为并发操作本身的问题，此外还有一些问题是由计算机的体系结构引发的。为了能更好地理解Java并发过程中的问题，我们应该对CPU有一些基础的认识。 CPU 在信息时代，CPU是我们耳熟能详的一个概念，大家都知道CPU就是计算机的大脑。计算机中一连串复杂的指令就是由它负责执行，而这些指令通常就是我们称之为程序的东西。那么CPU到底是什么呢？CPU即中央处理器，它是计算机中控制数据操控的电路。它主要由三部分构成：算术/逻辑单元、控制单元和寄存器单元。 控制单元是整个CPU的指挥控制中心，它的主要职责就是协调机器活动，通过向其它两个单元发送控制指令来达到控制效果。算术/逻辑单元主要的职责是负责执行运算，包括算术运算和逻辑运算，它估计控制单元发送过来的指令执行相应的运算操作。寄存器单元主要的作用就是用来临时存储数据，它保存着待处理的或已处理的数据，它的出现是为了减少CPU对内存的访问次数，提升读取数据性能，从而提升CPU的整个工作效率。 CPU与主存的协作 CPU中的寄存器分为通用寄存器和专用寄存器，通用寄存器用于临时存放CPU正在使用的数据，而专用寄存器用于CPU专有用途，比如指令寄存器和程序计数器。CPU与主存的通过总线进行通信，CPU通过控制单元能够操作主存中的数据。 执行两个数值相加的过程大致为：从主存读取第一个值放到寄存器1->从主存读取第二个值放到寄存器2->两个寄存器保存的值作为输入送到加法电路->将加法结果保存到寄存器3->控制单元将结果放到主存中。 程序等于数据 原始的计算机并不像现代计算机一样将程序保存起来，以前的人们只对数据进行保存，而设备执行的步骤作为计算机的一部分而被内置在控制单元中。这样就很不灵活，最多只能通过重新布线来提升灵活性。将程序与数据视作相同本质是很大的思想突破，因为人们一直认为它们是不同的事物，数据应该存放在主存中而程序应该属于CPU的一部分。 将程序作为数据一样保存在主存中大有好处，控制单元能够从主存读取程序，然后对它们解码并执行。当我们要修改执行程序时可以在计算机的主存中修改，而不必对CPU更改或重新布线。 指令系统 程序包含了大量的机器指令，CPU对这些指令进行解码并执行。CPU分为两类体系：精简指令集计算机（RISC）和复杂指令集计算机(CISC)。RISC提供了最小的机器指令集，计算机效率高速度快且制造成本低。而CISC提供了强大丰富的指令集，能更方便实现复杂的软件。 机器指令分为三类：数据传输类、算术/逻辑类与控制类。 数据传输类指令用于将数据从一个地方移动到另一个地方。比如将主存单元的内容加载到寄存器的LOAD指令，反之将寄存器的内容保存到主存的STORE指令。此外，CPU与其它设备（键盘、鼠标、打印机、显示器、磁盘等）进行通信的指令被称为I/O指令。 算术/逻辑类指令用于让控制单元请求在算术/逻辑单元内执行运算。这些运算包括算术、与、或、异或和位移等。 控制类指令用于指导程序执行。比如转移（JUMP）指令，它包括无条件转移和条件转移。 指令寄存器与程序计数器 CPU将主存的指令加载进来解码并执行，其中涉及两个重要寄存器：指令寄存器与程序计数器。指令寄存器用于存储正在执行的指令，而程序计数器则保持下一个待执行的指令地址。 CPU向主存请求加载程序计数器指定的地址的指令，将其存放到指令寄存器中，加载后将程序计数器的值加2（假如指令长度为2个字节）。 指令如何执行 比如我们要计算11+22，假设过程为：将主存地址为00的内容加载到寄存器1中->将主存地址为01的内容加载到寄存器2中->将寄存器1和寄存器2的数据相加并将结果保存到寄存器3->将寄存器3的结果存储到主存地址 "},"Chapter07/Thread.html":{"url":"Chapter07/Thread.html","title":"Java线程与内核线程","keywords":"","body":"Java线程与内核线程 计算机结构 正如我们熟知，现代机器可以分为硬件和软件两大块。如图所示，硬件是基础，软件提供了实现不同功能的手段。软件可以分为操作系统和应用程序，操作系统专注于对硬件的交互管理并提供一个运行环境给应用程序使用，而应用程序则是能实现若干功能的并且运行在操作系统环境中的软件。 线程模型 当我们谈到Java多线程时肯定就会涉及到Java多线程的模型，而且也将涉及Java线程与底层操作系统之间的关系。线程按照操作系统和应用程序两层次可以分为内核线程(Kernel Thread)和用户线程(User Thread)。 所谓内核线程就是直接由操作系统内核支持和管理的线程，线程的建立、启动、同步、销毁、切换等操作都由内核完成。基本所有的现代操作系统都支持内核线程。 用户线程指完全建立在用户空间的线程库上，由内核支持而无需内核管理，内核也无法感知用户线程的存在。线程的建立、启动、同步、销毁、切换完全在用户态完成，无需切换到内核。 可以将用户线程看成是更高层面的线程，而内核线程则是最底层的支持。这样一来他们之间必然存在着一定的映射关系，实际上一般存在三种常见的关系，下面将逐个介绍。 一对一模型 一对一模型可以说是最简单的映射模型。如图所示，KT为内核线程，UT为用户线程。每个用户线程都对应一个内核线程，由于每个用户线程都有各自的内核线程，所以他们互不影响。即使其中一个线程阻塞，另一个线程也可以继续执行。互不影响是该模型的优点，但同时也存在一个严重的缺陷。由于是一对一的关系，所以有多少个用户线程就必须要有多少个内核线程与之对应，这将导致内核线程的总开销大。所以，操作系统一般都会有内核线程数量的限制，因此用户线程的数量也会被限制。 多对一模型 第二种是多对一模型，如图所示，可以清晰看到多个用户线程映射到同一个内核线程上。从另一个角度来看的话，我们可以将其看成由一条内核线程实现若干个用户线程的并发功能。而且线程的管理在用户空间中进行，一般不需要切换到内核态，这样的效率较高。 对比于一对一模型，多对一模型支持的线程数量更大。但该模型存在一个致命的弱点，那就是如果一个线程阻塞了则将导致所有线程阻塞。此外，任意时刻只能有一个线程访问内核，而且对线程的所有操作都将由用户应用自己处理。所以一般除了在不支持多线程的操作系统被迫使用此模型外，该模型基本不被使用。 多对多模型 一对一模型受内核线程数的限制，而多对一模型虽然解决了内核线程数限制问题，但它存在一个线程阻塞导致所有线程阻塞的风险，同时一个内核线程只能调度一个线程也导致了并发性不强。 多对多模型的提出是为了解决前面两种模型的缺点，如图所示，多个用户线程与多个内核线程映射形成多路复用。由于多对一是多对多的子集，所以多对多具备多对一的优点，线程数不受限制。除此之外，多个内核线程可处理多个用户线程，当某个线程阻塞时，将可以调度另外一个线程执行，这从另一方面看也是增强了并发性。 轻量级进程 在实际程序中我们一般不直接使用内核线程，用户线程与内核线程之间需要一种中间数据结构，它由内核支持且是内核线程的高级抽象，这个高级接口被称为轻量级进程（Light Weight Process），下面简称LWP。 如下图，是三种模型增加了轻量级进程的示意图。从某种层面上看，LWP最多算是广义的用户线程，并非狭义定义的用户进程。LWP线程库以内核为基础，很多操作要进行内核调用，效率不高。如果要进行快速低消耗的操作则需要一个纯粹的用户线程，于是可以看到一个进程P里面一般包含若干个用户进程，而且用户进程以某种关系对应轻量级进程。一个内核线程堵塞将导致LWP也阻塞，与LWP相连的用户线程也将阻塞。 Java与操作系统 最后要谈谈Java线程与底层操作系统的关系，由于Java语言通过JVM来封装底层操作系统的差异，所以Java线程也必然会将不同操作系统的线程进行封装，并提供一个统一的并发定义。 在JDK发展历史上，Java语言开发者曾经通过一类叫“绿色线程（Green Threads）”的用户线程来实现Java线程。但从JDK1.2开始，Java线程改用操作系统原生线程模型来实现，也就是说Java线程的实现是通过不同操作系统提供的线程库来分别实现的。JVM会根据不同操作系统的线程模型对Java线程进行映射，假如Java运行在windows系统上，它通常直接使用Win32 API实现多线程。假如Java运行在linux系统则直接使用Pthread线程库实现多线程。这样一来就隐藏了线程底层的实现细节，提供给开发者就是一个统一的抽象线程语义。 "},"Chapter07/ThreadInterrupts.html":{"url":"Chapter07/ThreadInterrupts.html","title":"Java中断的工作方式","keywords":"","body":"Java中断的工作方式 关于中断 线程的定义为我们提供了并发地执行多个任务的解决方式，在大多数情况下我们会让每个任务都自行执行结束，这样才能保证事务的一致性。但是有时我们希望在任务执行中取消该任务，使线程停止，这时就会涉及到中断操作。在Java中要让线程安全、快速、可靠地停下来并不是一件容易的事，Java也没有提供任何可靠的方法终止线程的执行。 两种模式 前面我们了解了线程调度策略中有抢占式和协作式两种模式，中断机制也类似，对应着抢占模式和协作模式。抢占式中断就是直接向某个线程发起中断指令，此时不管线程处于什么状态都必须立即中断。协作式中断的核心是维护一个中断标识，当标识被标记成中断后，线程在适当的时间节点执行中断操作。本节将介绍抢占式中断。 协作式中断 经历了很长时间的发展，Java最终选择用一种协作式的中断机制实现中断。协作式中断的原理很简单，其核心使用一个中断变量作为标识，即对某线程的中断标进行识位，被标记了中断位的线程在适当的时间节点会抛出异常，我们捕获异常后做相应的处理。 中断标识 中断标识直接使用一个布尔变量即可，那么中断标识应该放在哪里呢？中断是针对线程实例而言，所以从Java层面上看，标识变量放到线程中肯定再合适不过了。从JDK开发者的角度看，而由于线程是由JVM维护的，所以中断标识具体可由本地方法维护。但如果从Java应用开发的角度看，也可以自定义中断标识变量。不管是哪种方式，其实本质都是一样的。 自定义中断标识 下面我们看看如何实现自定义中断标识变量。我们说过中断标识应当放到线程中，所以自定义一个线程类，同时定义一个变量布尔变量exit。注意，这里为了保证变量的可见性，变量应该声明为volatile。而在run方法里面是不断检测exit变量的，一旦该变量被设为true，则停止继续执行。此外，自定义线程还提供了stopThread方法用于中断操作，其实它就是简单将exit变量设为true。以上，就实现了一个自定义的中断标识。 JVM中断标识 JVM为我们提供了中断的支持，它内部也使用了一个中断标识来实现中断操作。所以我们只要学会使用它就行了，下面看看如何使用。核心逻辑都一样，不断检查线程的中断标识变量，如果没有中断则一直执行。而一旦调用interrupt方法将该线程的中断标识变量设为true后，StopThreadInterrup线程则停止执行。 三个API 在Java层面仅仅留下几个API用于操作中断标识，如下。 public class Thread{ public void interrupt() {……} public Boolean isInterrupted() {……} public static Boolean interrupted() {……} } 上面三个方法依次用于设置线程为中断状态、判断线程状态是否中断、清除当前线程中断状态并返回它之前的值。通过interrupt()方法设置中断标识，假如在非阻塞线程则仅仅只是改变了中断状态，线程将继续往下运行。但假如在可取消阻塞线程中，如正在执行sleep()、wait()、join()等方法的线程则会因为被设置了中断状态而抛出InterruptedException异常，程序对此异常捕获处理。判断线程是否处于中断状态其实很简单，只需使用Thread.interrupted()操作，如果为true则说明线程处于中断标识，并清除中断标识。 总结 协作式中断我们可以在JVM层面实现，同样也可以在Java层面实现，例如JDK并发工具的中断即是在Java层面实现，不过如果继续深究则是因为Java预留了几个API供我们操作线程的中断标识位，这才使Java层面实现中断操作得以实现。 "},"Chapter07/blockAndWakeUpThreads.html":{"url":"Chapter07/blockAndWakeUpThreads.html","title":"Java多线程如何阻塞与唤醒线程？","keywords":"","body":"Java多线程如何阻塞与唤醒线程？ 目前在Java语言层面能实现阻塞唤醒的方式一共有三种：suspend与resume组合、wait与notify组合、park与unpark组合。 其中suspend与resume因为存在无法解决的竟态问题而被Java废弃。同样地，wait与notify也存在竟态条件，wait必须在notify之前执行，假如一个线程先执行notify再执行wait将可能导致一个线程永远阻塞。 如此一来，必须要另外一种解决方案，即park与unpark组合。该方案的实现类位于juc包下，自JDK1.5版本引入，应该也是因为当时编写JUC时发现Java现有方式无法解决问题而引入的新阻塞唤醒方式。由于park与unpark的实现基于许可机制，许可只能为0或1两种状态，所以unpark与park的操作不会累加状态值。也就是说许可是一次性的，也就是说不管是先unpark一次还是一百次，只要使用了park后许可就没了，下次要得到许可就得重新unpark。此外park与unpark没有时序性要求，unpark可以在park之前执行。如unpark先执行，后面park将不阻塞。下面我们将重新回顾下阻塞与唤醒相关操作知识。 为什么需要阻塞和唤醒 对于线程的阻塞和唤醒我们已经非常熟悉了，但为什么需要阻塞和唤醒操作呢？笼统地说，就是为了控制线程在某些关键节点执行的先后顺序。因为线程之间是独立的，它们要互相通信就比较麻烦，而为了让线程之间能够通信我们就引入共享变量。比如某个线程将信息写到变量上，而另外一个线程就能够读取信息了，这样便能够实现线程之间的通信。比如下面的例子，线程一通过共享变量message与线程二进行通信。但这种方式并不总是正确，因为两个线程是独立执行的，无法控制线程二先赋值共享变量，导致偶尔线程一接收到的message是null。 为了解决上述问题，我们可以在线程一中增加一个while判断，当message变量为null时就一直循环，直到线程二将信息赋值到message。这样做确实能解决问题，但这种做法最多只能算是二流方法。 所以，最终还是需要Java从内置角度来提供阻塞与唤醒的操作，这样才能是线程之间通信更加高效。也就是suspend与resume、wait与notify、park与unpark三套组合解决方案。 wait与notify实现阻塞与唤醒 suspend与resume很早就已弃用，我们跳过，先看wait与notify组合。这两个线程通过wait与notify组合来实现阻塞与唤醒。需要注意到按要求必须使用synchronized对wait和notify进行加锁，否则将会产生异常。假如线程一先得到锁，那么message肯定为null，这时线程一会调用wait方法进去阻塞状态。而如果线程二先获得锁，那么message则已经被赋值，肯定不为空。这两种情况都没问题，保证了程序的准确性。 park与unpark实现阻塞与唤醒 对于wait与notify方式，仍然存在不足。wait必须在notify之前执行，假如先执行notify再执行wait将可能导致线程永远无法被唤醒。可以看到使用了park与unpark组合后代码也简洁很多，而且更容易理解。假如线程一执行得更快则该线程执行park操作而阻塞，等到线程二对message赋值后执行unpark操作才能让线程一继续往下执行。假如线程二执行得更快则对message赋值后会对线程一进行unpark操作，那么线程一在执行park操作时就能直接通过了。这就是park与unpark使用许可机制的优势。 另外，我们需要注意的是许可是一次性的。也就是说不管是先unpark一次还是一百次，只要使用了park后许可就没了，下次要得到许可就得重新unpark。比如下面先对当前线程unpark了五次，然后第一次park就将许可使用掉了，那么第二个park就阻塞了，主线程被阻塞无法往下执行。 阻塞唤醒核心类LockSupport Java语言层面真正意义上的并发编程应该从并发专家Doug Lea领导的JSR-166开始，该规范向JCP提交了向Java语言中添加并发编程工具的申请，即在JDK中添加java.util.concurrent工具包供开发者使用。该工具包为开发者提供了大量并发工具，从而轻松解决多线程并发过程中的各种问题。同时也可以通过该工具构建自己的同步器，而在此之前Java并发问题都只能依靠JVM内置的管程。 AQS同步器的阻塞与唤醒 JDK内置并发AQS同步器的阻塞与唤醒使用的就是LockSupport类的park与unpark方法，该类分别调用的是Unsafe类的park与unpark本地方法，本地方法依赖于不同操作系统的实现。AQS同步器的阻塞唤醒操作会在获取锁的操作中使用，即如果获取不到锁的线程进入排队队列后则需要阻塞，阻塞使用的正是LockSupport.park()方法。 对应地，对于排队队列中，前一个节点负责唤醒下一个节点包含的线程，唤醒使用的是LockSupport.unpark(某个线程)方法。 完整的逻辑为：假如一条线程参与锁竞争，首先它会先尝试获取锁。如果失败的话就创建节点并插入队列尾部，然后再次尝试获取锁，如若成功则直接返回。否则设置节点状态为待运行状态，最后使用LockSupport的park阻塞当前线程。前驱节点运行完后将尝试唤醒后继节点，使用的即是LockSupport的unpark唤醒。 "},"Chapter07/LockSupport.html":{"url":"Chapter07/LockSupport.html","title":"并发锁LockSupport原理剖析","keywords":"","body":"并发锁LockSupport原理剖析 关于LockSupport LockSupport类为构建锁和同步器提供了基本的线程阻塞唤醒原语，JDK中我们熟悉的AQS基础同步类就使用了它来控制线程的阻塞和唤醒，当然还有其他的同步器或锁也会使用它。 也许我们更加熟悉的阻塞唤醒操作是wait/notify方式，它主要以Object的角度来设计。而LockSupport提供的park/unpark则是以线程的角度来设计，真正解耦了线程之间的同步。 为了更好地理解JDK的这些并发工具，我们需要具体分析一下该类的实现。该类主要包含两种操作，分别为阻塞操作和唤醒操作。 LockSupport核心方法 通过下面的思维导图来看LockSupport类的几个核心方法，总体可以分为以park开头的方法和unpark方法。park开头的方法用于执行阻塞操作，它又分为两类：参数包含阻塞对象和参数不包含阻塞对象。下面对每个方法进行说明。 park()方法，对当前线程执行阻塞操作，直到获取到可用许可后才解除阻塞，也就相当于当前线程进入阻塞状态。 parkNanos(long)方法，对当前线程执行阻塞操作，等待获取到可用许可后才解除阻塞，最大的等待时间由传入的参数来指定，一旦超过最大时间它也会解除阻塞。 parkUntil(long)方法，，对当前线程执行阻塞操作，等待获取到可用许可后才解除阻塞，最大的等待时间为参数所指定的最后期限时间。 park(Object)方法，与park()方法同义，但它多传入的参数为阻塞对象。 parkNanos(Object,long)方法，与parkNanos(long)同义，但指定了阻塞对象。 parkUntil(Object,long)方法，与parkUntil(long)同义，但指定了阻塞对象。 unpark(Thread)方法，将指定线程的许可置为可用，也就相当于唤醒了该线程。 许可机制 在介绍核心方法时出现了一个高频的词——许可，那么什么是许可呢？其实很容易理解，线程成功拿到许可则能够往下执行，否则将进入阻塞状态。对于LockSupport使用的许可可看成是一种二元信号，该信号分有许可和无许可两种状态。 每个线程都对应一个信号变量，当线程调用park时其实就是去获取许可，如果能成功获取到许可则能够往下执行，否则则阻塞直到成功获取许可为止。而当线程调用unpark时则是释放许可，供线程去获取。如下图所示，每个线程都对应一个信号，其中有许可用和无许可分别用1和0来表示。 刚开始四个线程调用park方法，因为无许可而都处于阻塞状态。接着假如线程一和线程四的信号变为有许可，则它们都能继续往下执行。最后线程一和线程四获取许可后，其信号又变为无许可。 LockSupport例子 下面看一个通过LockSupport来进行阻塞和唤醒的例子，主线程分别创建thread1和thread2，然后先启动thread1，thread1,调用LockSupport.park()方法进入阻塞状态。接着主线程睡眠三秒后启动thread2，thread2会调用LockSupport.unpark(thread1)方法让thread1得到许可，即唤醒thread1。最终thread1被唤醒，输出\"thread2 wakes up thread1\"。 需要注意的是在调用LockSupport的park方法时一般会使用while(condition)循环体，如下方框的代码所示，这样能保证在线程被唤醒后再一次判断条件是否符合。 park与unpark的顺序 下面来探讨LockSupport的park和unpark的执行顺序的问题。正常来说一般是先park线程阻塞，然后unpark来唤醒该线程。但是如果先执行unpark再执行park，会不会park就永远没办法被唤醒了呢？在往下分析之前我们先来看看wait/notify方式的执行顺序例子。主线程分别创建了thread1和thread2，然后启动它们。由于thread1会睡眠三秒，所以先由thead2执行了notify去唤醒阻塞在锁对象的线程，而在三秒后thread1才会执行wait方法，此时它将一直阻塞在那里。所以wait/notify方式的执行顺序会影响到唤醒。 类似地，我们队park和unpark进行分析。这个例子与上面的wait/notify例子逻辑几乎相同，唯一不同的是阻塞和唤醒操作改为了park和unpark。实际上这个例子能正确运行，且最终输出了“thread2 wakes up thread1”。从此可以看出park/unpark方式的执行顺序不影响唤醒，这是因为park/unpark是使用了许可机制，如果先调用unpark去释放许可，那么调用park时就直接能获取到许可而不必等待。 park对中断的响应 park方法支持中断，也就是说一个线程调用park方法进入阻塞后，如果该线程被中断则能够解除阻塞立即返回。但需要注意的是，它不会抛出中断异常，所以我们不必去捕获InterruptedException。下面是一个中断的例子，thread1启动后调用park方法进入阻塞状态，然后主线程睡眠一秒后中断thread1，此时thread1将解除阻塞状态并输出null。接着主线程睡眠三秒后启动thread2，thread2将调用unpark，但thread1已经因中断而解除阻塞了。 park是否会释放锁 我们再思考一个问题，当线程调用LockSupport的park方法时是否会释放该线程所持有的锁资源呢？答案是不会。我们看下面的例子来理解，thread1和thread2两个线程都通过synchronized(lock)来获取锁。由于thread1先启动所以获得锁，而且它调用park方法使得thread1进入阻塞状态，但是thread1却不会释放锁lock。对于thread2这边，因为一直无法获取锁而无法进入同步块，也就没办法执行unpark操作。最终的结果就是造成了死锁，thread1在等thread2的unpark，而thread2却在等thread1释放锁。 Blocker是什么 前面介绍park相关方法时将其分为了两大类，一类是不带阻塞对象，而另一类是带阻塞对象。下面看什么是阻塞对象及其作用。阻塞对象（Blocker）就是线程调用park方法时所在的对象，它的主要作用是供监视、诊断工具使用，通过getBlocker方法获取阻塞对象从而对阻塞进行分析。下面是使用的例子，我们定义一个Blocker类，其中doPark方法调用LockSupport的park方法，并传入了this作为阻塞对象。 thread1 启动后调用Blocker的doPark方法，thread2则能够通过LockSupport的getBlocker方法来获取阻塞对象，从而进一步对阻塞对象进行分析。这里会输出“我是blocker” LockSupport的实现 实际上LockSupport的实现很简单，如果用一句话来概括就是间接调用Unsafe的方法。Unsafe类提供了很多底层和不安全的操作，主要用了它park、unpark和putObject三类方法， park和unpark方法可以看出是对底层线程进行操作，而putObject方法则是将对象设置为Thread对象里面的parkBlocker属性， getObjectVolatile方法则是获取Thread对象里面的parkBlocker属性值。 下面我们分析LockSupport类的实现，代码并非与JDK完全相同，但核心实现是一样的。该类主要包含一个Unsafe对象和long变量， 由于Java开发者无法直接通过new来实例化Unsafe对象，所以在static块中通过发射来获取Unsafe对象，而long变量表示 Thread对象parkBlocker属性的偏移，有了偏移就可以操作该属性。可以看到park、parkNanos和parkUntil三个方法都是间接调用了Unsafe对象的park方法。 继续看传入阻塞对象的情况，包括park、parkNonos和parkUntil三个方法，这三个方法都是会先通过Thread.currentThread()获取当前线程对象， 然后调用serBlocker将阻塞对象设置到Thread对象中的parkBlocker属性，接着调用Unsafe对象的park方法，最终调用serBlocker将Thread对象中的parkBlocker属性清空。 注意这里的setBlocker方法间接调用了Unsafe对象的putObject方法，该方法能够通过属性偏移来修改属性值，而gerBlocker方法则是间接调用 Unsafe对象的getObjectVolatile方法则是获取Thread对象里面的parkBlocker属性值 unpark方法则是间接调用Unsafe对象的unpark方法。 总结 本文讲解了JDK并发包中锁支持类LockSupport，它主要的功能就是实现线程的阻塞和唤醒，而且使用了许可机制。他的核心方法包括六种park方法和unpark 方法，然后通过例子我们了解了LockSupport的基本用法，接着探讨了park和unpark的执行顺序，然后继续分析了park对中断的响应和park时是否会释放锁， 同时也介绍了阻塞对象Blocker以及他的作用。最后从源码角度跑细了LockSupport类的实现，至此我们已经完全掌握了LockSupport的机制原理。 "},"Chapter07/ThreadStatus.html":{"url":"Chapter07/ThreadStatus.html","title":"线程的状态及其转换过程","keywords":"","body":"线程的状态及其转换过程 线程生命周期 线程跟人类一样拥有自己的生命周期，一条线程从创建到执行完毕的过程即是线程的生命周期。整个生命周期中线程可能在不同时刻处于不同的状态，有些线程任务简单，可能涉及的状态就很少。而有些线程任务复杂，相对应的状态也更多。那么线程到底有多少种状态？不同状态之间是如何转化的呢？ 线程的状态 实际上可以这样说，线程状态的分类并没有严格的规定，只要能正确表示状态即可。如下图，我们来看一种常见的状态分类。一个线程从创建到死亡可能会经历若干个状态，但在任意一个时间点线程只能处于其中一种状态。线程总共包含了五个状态：新建（new）、可运行（runnable）、运行（running）、非可运行（not runnable）、死亡（dead）。 五种状态 新建状态（new），一个线程被创建但未被启动的话则处于新建状态。在程序中使用new MyThread();来创建的线程实例就处于此状态。 可运行状态（runnable），创建的线程实例调用start()方法后便进入可运行状态。处于此状态的线程并不是说一定处于运行状态，我们在前面讲解线程调度策略有讲到Java多线程使用的是抢占式调度，所以每个可运行线程在队列里面等着获取CPU时间片。我们可以想象成有一个可运行线程池，start()方法把线程放进可运行线程池中，而后CPU会按一定规则去执行池里的线程。 运行（running），当可运行线程获取到CPU执行时间片则进入了运行状态。 非可运行（notrunnable）：运行中的线程因某种原因暂时放弃CPU的使用权，可能是因为执行了挂起、睡眠或等待等操作。在执行I/O操作时由于外部设备速度远低于处理器速度也可能导致线程暂时放弃CPU使用权，在获取对象的同步锁过程中如果同步锁先被别的线程占用同样可能导致线程暂时放弃CPU。 死亡（dead）：线程执行完run()方法实现的任务，或因为异常导致退出任务停止执行，线程进入死亡状态后将无法再转换成其他状态。 状态的转换 线程状态的转换可以由Java程序控制，我们可以通过某些API来达到状态转换效果。例如Thread类的start、stop、sleep、suspend、resume、wait、notify等方法，但要注意的是stop、suspend、resume等方法因为容易引起死锁问题而已被弃用。 具体转换过程 下图完整地画出了线程的状态及状态之间的转换情况，下面我们将根据该图对每个状态及其转换进行说明。其实该图的状态与前面介绍的五个状态本质是相同的，不同的地方在于这里将非可运行（not runnable）状态进行了细分。而新建、可运行、运行、死亡四个状态的定义和转化都与前面的一样。现在我们重点看看非可运行状态引申出来的三个状态：阻塞（blocked）、同步锁（locked）、等待（waiting）。 阻塞（blocked）：该状态由阻塞事件触发，线程处于阻塞状态将放弃CPU的使用权，暂时停止运行。一般如果线程执行了sleep()、join()方法，或发出了I/O请求，则线程将处于阻塞状态。一旦sleep()执行的睡眠结束、join()执行的等待中断超时、I/O请求结束，则线程将重新回到可执行状态，等待分配CPU。 同步锁（locked）：假如一个线程准备调用一个同步方法，而同步方法对应的对象正被其他线程占用，此时线程就将进入同步锁状态。实际上，Java中的每个object对象都有一个monitor。此monitor负责对同步域在并发时的独占处理，即一个线程调用某对象 "},"Chapter07/CPUTimeSlices.html":{"url":"Chapter07/CPUTimeSlices.html","title":"Java线程何时放弃CPU时间片","keywords":"","body":"Java线程何时放弃CPU时间片 关于执行时间 一个线程从启动到结束过程总，有两个时间概念我们要理解。其一是CPU时间，即线程真正执行的时间。其二是总消耗时间，即真正执行的时间加上等待的时间。如下图可以很清晰看到这两者的关系，P1得到了多个CPU执行时间，而总消耗时间则包括P1执行期间CPU分配给其它线程执行的时间，所以总消耗时间总是大于等于CPU时间。 在Java中，线程会按线程优先级来分配CPU时间片，整个过程中线程何时会放弃CPU的使用权呢？其实可以归类成以下三种情况。 线程死亡 线程死亡即线程运行结束，也就是运行完run()方法里面的任务后整个线程的生命周期结束。这种情况下放弃CPU就很好理解了，任务执行完了自然就放弃CPU。 主动放弃CPU 这种情况是运行线程主动放弃CPU的执行时间，由JVM负责放弃CPU的操作。需要注意的是，基于时间片轮转调度的JVM操作系统不会让线程永久放弃CPU，也就是说是放弃本次CPU时间片的执行权。比如当我们调用yield()方法，本来执行的线程将放弃执行，而是通知JVM线程管理器去执行线程队列里面的其他线程。 我们来看使用yield放弃CPU时的代码情况： 现在截取某段的输出，可以看到输出“主线程”比“被放弃线程”的机会更多。这是因为mt线程每次循环都把时间片让给了主线程，所以主线程执行得多。同时又因为yield操作并不会永远放弃CPU的使用，它仅仅只是放弃了此次时间片，所以它有机会在下一轮中得到执行。 阻塞放弃CPU 这种情况是指运行线程因为某些原因进入阻塞状态，从而放弃CPU的执行时间。进入阻塞状态的原因可能有很多种，比如当磁盘IO时、网络IO时、主动睡眠时、锁竞争时、执行等待时等等，都可能会导致阻塞状态。进入阻塞状态则意味着放弃了CPU的执行时间片。 下面举个阻塞放弃CPU的例子，为了节省代码量我们将使用伪代码表示，例子虽然简单但已能说明问题。程序包含了两条线程，其中ioThread每次遇到IO阻塞时就放弃CPU的执行时间片，而主线程则按JVM分配的时间片正常运行。 "},"Chapter07/JVMSchedulingofThreads.html":{"url":"Chapter07/JVMSchedulingofThreads.html","title":"JVM对线程的调度","keywords":"","body":"JVM对线程的调度 进程与线程 进程是指程序的一次动态执行过程，通常我们说计算机中正在执行的程序就是进程，每个程序都会对应着一个进程。一个进程包含了从代码加载到执行完成的一个完整过程，它是操作系统资源分配最小单元。 而线程则是比进程更小的执行单位，是CPU调度和分派的基本单位。每个进程至少有一个线程，反过来一个线程只能属于一个进程，线程可以对进程所有的资源进行调度和运算。线程既可以由操作系统内核来控制调度，也可以由用户程序进行控制调度。 根据下图可以看到，多个CPU会去执行这三个进程。其中每个进程都包含着至少一个线程，比如进程1包含了四个线程，进程2和进程3包含一个线程。此外，每个进程都有自己的资源，而进程内的所有线程都共享进程包含的资源。 线程调度器 在Java多线程环境中，为保证所有线程的执行能按照一定的规则执行，JVM需要实现一个线程调度器。这个调度器定义了线程调度的策略，同时对CPU运算的分配都进行了约定，通过特定的机制为多个线程分配CPU的使用权。 线程调度器里面包含了多种调度策略算法，由这些算法来决定CPU的任务执行。此外，每个线程还有自己的优先级，比如有高、中、低，调度算法会通过这些优先级来实现优先机制。 抢占式调度 常见的线程调度模式有两种：抢占式调度和协同式调度。 抢占式调度指的是每条线程的执行时间、线程的切换都由调度系统控制，调度系统拥有某种运行机制。调度系统可能为每条线程都分配相同的执行时间片，也可能为某些特定线程分配较长的执行时间，甚至在极端条件下还可能不给某些线程分配执行时间片，从而导致某些线程得不到执行。在抢占式调度机制下，一个线程的堵塞不会导致整个进程堵塞。 协同式调度 协同式调度指某一线程执行完后主动通知调度系统切换到另一线程上执行。这种调度模式就像接力赛一样，一个人跑完自己的路程后就把接力棒交接给下一个人，下个人继续往下跑。在这种模型下，线程的执行时间由线程本身控制，也就是说线程的切换点是可以预先知道的。所以这种模式不存在多线程同步问题，但它有一个致命弱点：如果一个线程编写得有问题，则可能导致系统运行到一半就一直阻塞了，最终将可能导致整个系统崩溃。 两种模式对比 我们通过下面的对比图来理解两种模式的不同。左边为抢占式线程调度，现在假如存在三条线程需要运行，在该模式的调度下，处理器的执行路径为：先在线程一运行一个时间片，然后强制切换到线程二运行一个时间片，最后切到线程三去执行。接着下一轮又再回到线程一，如此循环直至三条线程都执行完。而在协同式线程调度下，执行的策略则不是这样的，调度器会先将处理器分配给线程一并且一次性执行完。然后线程一再通知线程二，线程二也一次性执行完。最后线程二通知线程三，线程三一次性执行完。 Java的调度 在了解了两种线程调度模式后，现在我们来看Java使用的是哪种线程调度模式。实际上，Java的线程调度涉及到了JVM的实现，JVM规范中规定每个线程都有优先级，且优先级越高越优先执行。但优先级高并不代表能独自占用执行时间片，可能是优先级高得到越多的执行时间片。反之，优先级低的分到的执行时间少但不会分配不到执行时间。JVM的规范并没有严格地给调度策略定义，可能因为面对众多不同调度策略，JVM要封装所有细节提供一个统一的策略不太现实，于是给了一个不严格但足够统一的定义。 那么Java线程使用的是什么调度呢？其实它使用的是抢占式调度。在JVM中体现为让可运行池中优先级高的线程拥有CPU使用权，如果可运行池中线程优先级一样则随机选择线程。但我们要注意的是实际上一个绝对时间点只有一个线程在运行，这里是相对于一个CPU来说，如果你的机器是多核的还是可能多个线程同时运行的。只有线程进入非可运行状态或另一个具有更高优先级的线程进入到可运行线程池时，才会使之让出CPU的使用权，从而更高优先级的线程会抢占优先级低的线程的CPU执行。 "},"Chapter07/sleep.html":{"url":"Chapter07/sleep.html","title":"线程的sleep操作实现原理","keywords":"","body":"线程的sleep操作实现原理 前言 sleep操作是我们经常会使用的操作，特别是在开发环境中调试的时候，有时为了模拟长时间的执行就会使用sleep来模拟。该操作对应着java.lang.Thread类的sleep本地方法，该方法能使当前线程睡眠指定的时间。 例子 先通过一个简单的例子来看sleep的操作，代码如下，我们让主线程睡眠3000ms，即3秒。于是主线程先输出“是当前线程睡眠3000ms”，然后暂停三秒钟，最后再输出“睡眠结束”。 sleep方法的要点 该方法只针对当前线程，即让当前线程进入休眠状态，哪个线程调用Thread.sleep则哪个线程会睡眠。 注意sleep方法传入的睡眠时间并不精准，这个取决于操作系统的计时器和调度器。 如果在synchronized块里面进行sleep操作，或在已获得锁的线程中执行sleep操作，都不会让线程失去锁，这点与Object.wait方法不同。 当前线程执行sleep操作进入睡眠状态后，其它线程能够中断当前线程，使其解除睡眠状态并抛出InterruptedException异常。 不会释放锁 前面说到sleep操作不会使得当前线程释放锁，现在我们看一个例子，该例子通过synchronized同步块实现锁机制。主线程创建thread1和thread2两个线程，其中thread1比thread2先启动。 thread1获取锁后输出\"thread1 gets the lock.\"并开始睡眠三秒。接着thread2启动，但它会因为获取不到锁而阻塞，直到thread1睡眠结束输出\"thread1 releases the lock.\"并释放锁，thread2才能获得锁往下执行，输出\"thread2 gets the lock 3 second later.\"。 支持中断 sleep操作是支持中断机制的，即某个线程如果处于睡眠状态，那么其他线程可以将其中断。该例子中主线程创建thread1和thread2两个线程，thread1启动后输出\"thread1 sleeps for 30 seconds.\"，然后开始睡眠30秒。 主线程休眠两秒后开始启动thread2，它输出\"thread2 interrupts thread1.\"后对thread1进行中断操作。最终thread1还没休眠够30秒而被中断，捕获异常后输出\"thread1 is interrupted by thread2.\"。 关于sleep(0) sleep方法传入的时间参数值必须大于等于0，正常情况下我们都会传入大于0的值，但有时我们也会看到竟然有传入0的。0？是不是表示不睡眠呢？那不睡眠的话又为什么要执行sleep操作呢？实际上sleep(0)并非指睡眠0秒钟，它的意义是让出该轮cpu时间，也就是说它的意义与yield方法相同，而JVM实现的时候也可以用yield操作来代替。下面例子中MyThread会不断让出自己的CPU时间，而主线程则得到更多的执行时间，这个过程一共输出100次“main thread”，而仅输出几次“yield cpu time”。 sleep工作过程 我们依据前面的一张线程状态图来看sleep的工作过程，处于runnable队列的任务会按一定的策略分到CPU时间，然后线程状态为running。此时如果线程调用了sleep操作的话则会进入阻塞状态，睡眠时间到达后就会重新进入到runnable队列等待分配CPU时间。 "},"Chapter07/Happens-Before.html":{"url":"Chapter07/Happens-Before.html","title":"Happens-before原则","keywords":"","body":"一文理解Java并发中Happens-before原则 关于happens-before原则 happens-before原则实际上是一种一致性模型，它主要规定了Java内存在多线程下操作的顺序性。如果某个操作happens-before于另外一个操作，那么先发生的操作的执行结果将对后发生的操作可见。 正式点的描述就是：假设操作A和操作B由多线程执行，如果A操作happens-before于B操作，那么A操作对内存产生的影响将对于B操作可见，即执行B操作前能看到A操作对内存的改变。 如下图中，线程一执行了A操作并对内存做了一些修改，此时如果A和B满足happens-before关系，那么线程二中的B操作就能看到A操作对内存的修改。 为什么制定happens-before原则 总的来说，happens-before原则用于保证某个线程执行的某个操作能被其它线程中的某个操作可见。对于Java体系而言，它希望通过某些规范来将复杂的物理计算机底层封装到JVM里面，从而向上提供一种统一的内存模型语义。 通过happens-before原则就能够规范JVM的实现，同时又能向上层的Java开发人员描述多线程并发中的可见性问题，而且还不必关注JVM复杂的实现。 可见性问题 为了让大家更清楚了解什么是可见性问题，下面我们通过几个例子来说明可见性问题以及如何解决可见性问题。示例1中有x和y两个共享变量，线程二负责更改x和y的值，而线程一则通过while循环检测x和y是否被线程二更改过，主线程启动线程一后睡眠一秒钟再启动线程二。 原本我们期望的结果是当线程二启动后线程一应该输出\"thread1可以看到变量改变\"，但事实却是永远都不会输出该提示。 由于JVM体系非常复杂，我们写的Java层代码可能会在编译时被编译器改动，也可能会在执行时被改动。所以实际上我们无法百分之百保证上述Java例子就一定是严格的可见性例子，因为示例中的Java代码可能会被编译器优化。 而上面对可见性的分析也只是从Java层源码去分析，有两种情况可能发生。情况一是编译器将线程一中的while(true)if(x==2&&y==3)的顺序反过来了，变成了if(x==2&&y==3)while(true)，当然这是编译器的优化策略，我们无法控制。 与此同时也导致了线程一只会去内存读一次x和y，后面不再读内存，也就读不到被线程二修改的x和y的新值。情况二是编译器不会改变Java层代码逻辑，线程一一直无线循环处于繁忙状态而不去读取内存中最新的x和y值。 接着我们看示例2如何解决可见性问题，其实很简单，就是将x和y都声明为volatile便可。关于volatile我们前面有分析过，每个线程对volatile变量的读写相当于直接对主内存读写，所以能保证线程对变量的可见性。这次运行的结果则是不断地输出“thread1可以看到变量改变”。 最后这里再提一下前面讲的编译器优化问题，给大家抛出一些现象，大家可以自己去深入探索。我们在示例1的基础上分别增加三种情况的代码，第一种是调用System.out.println()，第二种是调用new Random().nextInt()，而第三种就是Thread.sleep()。 最终可以发现这三种情况都能使线程一输出“thread1可见变量改变”，这说明线程一能看到线程二对x和y的修改。这个是不是因为增加了这些代码而阻止了编译器的优化操作呢？还是因为增加了这些代码使得while循环会去读取内存中的x和y的最新值？感兴趣的朋友可以去深挖下。 常见happens-before原则 我们一般将Java并发常见的happens-before原则归类为八个，其中很多都是我们非常熟悉的了，只是我们很少从happens-before的角度去理解它。下面我们来分别介绍这八个原则。 单线程原则 单线程原则是最简单的规则，就是说在单个线程内前面的代码happens-before于后面的代码。比如下面的例子，在主线程内step-1比step-2输出更早，四个输出操作按照代码顺序执行。 锁原则 锁原则是指某个锁解锁前的操作happens-before于接下去获取该锁后的其它操作。以synchronized的锁为例，我们都知道进入和离开synchronized大括号就是加锁和解锁操作，那么假如线程一先获取锁则解锁前的所有操作都happens-before于线程二获取该锁后的其它操作。反之亦成立。 该例子可能输出以下两种情况，因为happens-before原则保证了可见性。第一种情况是线程一解锁前的操作对线程二获取锁后的操作可见，所以线程一的x=3而线程二的x=7。第二种情况是线程二解锁前的操作对线程一获取锁后的操作可见，所以线程二的x=4而线程一的x=7。 thread1 x = 3 thread2 x = 7 thread2 x = 4 thread1 x = 7 volatile原则 volatile原则是指对某个volatile变量的写操作和写操作之前的所有操作都happens-before于对这个volatile变量的读操作和读操作之后的所有操作。比如下面例子中，x是volatile变量而y为非volatile变量。线程一对y和x进行写操作，那么线程二在对x和y读操作时就能看到x和y的最新值，即输出“thread2 x,y = 4,2”。这里x写操作前的所有操作都对x读操作后的所有操作可见。 线程start原则 线程start原则是指某个线程在调用另外一个线程的start方法前的所有操作都happens-before于刚被启动的线程中的所有操作。看下面的例子，主线程中先执行x=3，然后创建thread1并调用它的start方法。运行后输出为“thread1 x = 6”，调用start方法前的x=3对于线程一中是可见的。 线程join原则 线程join原则是指如果线程A调用了线程B的join方法，那么线程B的所有动作都happens-before于线程A中join方法后面的所有动作。如下例子中，主线程启动线程一后并调用join方法等待线程一执行完 Thread B = new Thread(()->{ // 此处对共享变量 var 修改 var = 66; }); // 例如此处对共享变量修改， // 则这个修改结果对线程 B 可见 // 主线程启动子线程 B.start(); B.join() // 子线程所有对共享变量的修改 // 在主线程调用 B.join() 之后皆可见 // 此例中，var==66 此处，主线程A能看到线程B对共享变量var的操作，也就是可以看到var==66。 线程的interrupt()规则 指的是线程A调用线程B的interrupt()方法，Happens-Before 于线程B检测中断事件(也就是Thread.interrupted()方法)。这个也很容易理解。 传递性 传递性，指的是如果A Happens-Before于B，B Happens-Before于C，则A Happens-Before于C。这个是很好理解的。用白话说就是，如果A的操作结果对B可见，B操作结果对C可见，则A的操作结果对C也是可见的。 finalize()规则 指的是对象的构造函数执行、结束 Happens-Before 于finalize()方法的开始。 as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），单线程程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。 但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。为了具体说明，请看下面代码示例： 总结 Happens-Before原则非常重要，它是判断数据是否存在竞争、线程是否安全的主要一句，依靠这个原则，我们可以解决并发环境下两个操作之间是否存在冲突的所有问题。 "},"Chapter07/Orderreordering.html":{"url":"Chapter07/Orderreordering.html","title":"指令重新排序","keywords":"","body":"Java编译会打乱指令顺序？指令重新排序问题该如何解决？ 指令重排 “你看到的不一定就是你以为的！”，我觉得用这句话来描述指令重排非常贴切。当我们在语言层面写代码时，我们是按照我们的理解和习惯去编写程序，但对于编译器和CPU来说执行时的顺序却有可能与你的代码顺序不一样。 因为每个层面都有各自需要关注和考虑的事情，编译器和CPU可能会对愚蠢的人类编写的代码进行优化再执行，以此提高执行效率。 所以指令重排其实是为了提升机器执行效率而提出的一种措施。比如下图中，左边的是原来的代码，通过编译器编译后或被CPU执行时等同的代码顺序是可能为右边的代码，当然这里只是举个例子，并不是说一定会这样重排。 引入流水线之前 为了更好地理解指令重排的作用，我们先了解流水线相关技术。在没有使用流水线之前，所有指令是一条条执行的，只有当一条指令执行完后才会执行下一条指令。如下图中，一共有四个指令，假设每个指令都需要三个单位的CPU时间，那么四条指令总共耗时12个单位CPU时间。 引入流水线后 那么引入流水线技术后指令是如何执行的呢？每个指令将被拆分成若干个部分，而在同一个单位CPU时间内执行多个指令的不同部分，这样就能提升执行效率。比如下图，每个指令都被分成三部分，指令一的第二部分和指令二的第一部分可以同时被执行，最终四个指令被执行完一共花费了6个单位CPU时间。可见运行速度提升了很多。那么流水线技术为什么能这样执行呢？主要是因为一条指令被执行时不同阶段会涉及到不同的硬件部分，比如取指阶段使用指令通路和指令寄存器，译码阶段使用指令译码器，执行阶段使用执行单元和数据通路。鉴于此，可以将指令分成多个部分，并且不同指令的不同阶段能够同时被执行。总的来说，流水线的本质就是充分利用CPU的各个硬件部分，不同硬件部分执行不同指令的不同部分，看起来就像是并行的效果，从多个指令的整体效果来看执行性能大大提升。 为什么要指令重排 我们知道指令重排的原因是为了能让CPU能更快执行完所有指令，也就是说指令重排能让流水线更加高效。我们的代码生成指令后可能前后指令是相关的，比如后面一条指令依赖于前面指令执行的结果，那么在流水线的执行过程中就可能导致空等待而白白浪费CPU时间。我们看下面两个图，假设原来是按照指令一二三四的顺序执行的，而且指令二依赖于指令一的结果。那么不重排指令的情况需要等到指令一执行完才能接着执行指令二，但是如果我们将指令二当成最后一条指令则流水线能够充分执行，因为指令三和指令四不需要指令一的结果，而当要执行指令二时指令一已经执行完得到结果了。 指令重排的原则 指令重排的原则就是不能影响到程序在单线程下的准确性，就是说不管怎么重排都要保证其与重排前在单线程中执行的结果相同。比如下面的例子，尽管指令被重排了，但最终m和n的结果是与重排前相同的。 但对于多线程来说，指令重排却可能导致程序执行出现错误的结果。这也是指令重排的弊端，虽然它能让执行效率提升，但同样也会引入多线程问题。比如下面的例子，线程一调用method1方法且线程二调用method2方法，如果没有进行指令重排的话则不会输出a=2，但如果method1被重排则可能flg=true在a=1前面，则程序可能会输出a=2。 两类指令重排 指令重排主要发生在编译和运行时两个阶段，这两个阶段对应的主角分别为编译器和CPU。我们先看编译阶段重排，总的来说就是源代码经过编译器编译后成为机器指令，而机器指令可能被重排。对于Java来说，就是java源文件被javac编译后成为字节码指令。而字节码则可能被重排。 运行时阶段重排，指的是机器指令被CPU执行时可能会被CPU重排后才执行。对于Java来说，就是字节码被Java执行器执行时可能会被重排后才执行。 如何解决指令重排 以多核CPU为例（两核），我们知道CPU的速度比内存要快得多，为了弥补这个性能差异，CPU内核都会有自己的高速缓存区，当内核运行的线程执行一段代码时，首先将这段代码的指令集进行缓存行填充到高速缓存， 如果非volatil变量当CPU执行修改了此变量之后，会将修改后的值回写到高速缓存，然后再刷新到内存中。 如果在刷新会内存之前，由于是共享变量，那么CORE2中的线程执行的代码也用到了这个变量，这是变量的值依然是旧的。 volatile关键字就会解决这个问题的，如何解决呢，首先被volatile关键字修饰的共享变量在转换成汇编语言时， 会加上一个以lock为前缀的指令，当CPU发现这个指令时，立即做两件事： 将当前内核高速缓存行的数据立刻回写到内存； 使在其他内核里缓存了该内存地址的数据无效。 第一步很好理解，第二步如何做到呢？ MESI协议：在早期的CPU中，是通过在总线加LOCK#锁的方式实现的，但这种方式开销太大， 所以Intel开发了缓存一致性协议，也就是MESI协议，该解决缓存一致性的思路是： 当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本， 那么他会发出信号通知其他CPU将该变量的缓存行设置为无效状态。当其他CPU使用这个变量时， 首先会去嗅探是否有对该变量更改的信号，当发现这个变量的缓存行已经无效时， 会从新从内存中读取这个变量。 "},"Chapter07/synchronized.html":{"url":"Chapter07/synchronized.html","title":"synchronized原理","keywords":"","body":"synchronized原理 synchronized是java提供的原子性内置锁，这种内置的并且使用者看不到的锁也被称为监视器锁，使用synchronized之后，会在编译之后在同步的代码块前后加上monitorenter和monitorexit字节码指令，他依赖操作系统底层互斥锁实现。他的作用主要就是实现原子性操作和解决共享变量的内存可见性问题。 执行monitorenter指令时会尝试获取对象锁，如果对象没有被锁定或者已经获得了锁，锁的计数器+1。此时其他竞争锁的线程则会进入等待队列中。 执行monitorexit指令时则会把计数器-1，当计数器值为0时，则锁释放，处于等待队列中的线程再继续竞争锁。 synchronized是排它锁，当一个线程获得锁之后，其他线程必须等待该线程释放锁后才能获得锁，而且由于Java中的线程和操作系统原生线程是一一对应的，线程被阻塞或者唤醒时时会从用户态切换到内核态，这种转换非常消耗性能。 从内存语义来说，加锁的过程会清除工作内存中的共享变量，再从主内存读取，而释放锁的过程则是将工作内存中的共享变量写回主内存。 实际上大部分时候我认为说到monitorenter就行了，但是为了更清楚的描述，还是再具体一点。 如果再深入到源码来说，synchronized实际上有两个队列waitSet和entryList。 当多个线程进入同步代码块时，首先进入entryList 有一个线程获取到monitor锁后，就赋值给当前线程，并且计数器+1 如果线程调用wait方法，将释放锁，当前线程置为null，计数器-1，同时进入waitSet等待被唤醒，调用notify或者notifyAll之后又会进入entryList竞争锁 如果线程执行完毕，同样释放锁，计数器-1，当前线程置为null "},"Chapter07/monitor.html":{"url":"Chapter07/monitor.html","title":"管程","keywords":"","body":"管程 汤小丹等人的《计算机操作系统》书中是这样说的 在利用管程实现进程同步时，当某进程通过管程请求获得临界资源而未能满足时，管程便调用wait原语使该进程等待，并将其排在等待队列上。 仅当另一个进程访问完成并释放该资源后，管程才又调用signal原语，唤醒等待队列中的队首进程。 但是，考虑这样一种情况：当一个进程调用了管程后，在管程中时被阻塞或挂起，直到阻塞或挂起的原因解除； 在此期间，如果该进程不释放管程，则其它进程就无法进入管程，被迫长时间等待。为了解决这个问题，引入条件变量condition。 通常，一个进程被被阻塞或挂起的条件（原因）可有多个，因此在管程中设置了多个条件变量，对这些条件变量的访问智能在管程中进行。 我的理解是，调用管程的进程，在管程中可能会因为某些资源限制而被阻塞或挂起；为了不占用管程资源，同时又不能直接退出管程， 就需要在管程中按照被阻塞或挂起的原因，分别挂到不同的等待队列上，而条件变量conditions就是分门别类的队列+相应操作。 需要condition变量的原因本质上就是程序执行顺序的不确定性.管程(monitor)只是保证了同一时刻只有一个进程在管程内活动, 即管程内定义的操作在同一时刻只被一个进程调用(由编译器实现).但是这样并不能保证进程以设计的顺序执行,因此需要设置condition变量, 让进入管程而无法继续执行的进程阻塞自己.也可以这么说,由于程执行顺序的不确定性,进程在entry序列中的顺序并不一定是我们想要的, 而condition变量就是用来操作entry序列的.有了condition变量,我们就可以让在自己前驱进程之前提前进入管程的进程挂起自己, 退回到entry序列中重新排队.具体来说: 以生产者-消费者问题(也称有限缓冲问题)为例, 如果我们不使用信号量(semaphore)而简单的用整形变量count来记录buffer中的数据项数目, 用系统调用sleep()阻塞进程 和 wakeup() 唤醒进程 管程类型也包括一组变量，用于定义这一类型的实例状态，也包括操作这些变量的函数实现。管程类型的语法如下所示: monitor monitor name { /* shared variable declarations */ function P1 (...) { ... } function P2 (...){ ... } . . . function Pn ( . . . ) { ... } initialization_code (...){ ... } } 管程类型的表示不能直接由各种进程所使用。因此，只有管程内定义的函数才能访问管程内的局部声明的变量和形式参数。类似地，管程的局部变量只能为局部函数所访问。 管程结构确保每次只有一个进程在管程内处于活动状态。因此，程序员不需要明确编写同步约束. 然而，如到目前为止所定义的管程结构，在处理某些同步问题时，还不够强大。为此，我们需要定义附加的同步机制； 这些可由条件（condition)结构来提供。 当程序员需要编写定制的同步方案时，他可定义一个或多个类型为 condition 的变量： condition x, y; 对于条件变量，只有操作 wait() 和 signal() 可以调用。操作 x.wait(); 意味着调用这一操作的进程会被挂起，直到另一进程调用 x.signal(); 操作 x.signal() 重新恢复正好一个挂起进程。如果没有挂起进程，那么操作 signal() 就没有作用，即x的状态如同没有执行任何操作。 这一操作与信号量的操作 signal() 不同，后者始终影响信号量的状态。 现在，假设当操作 x.signal() 被一个进程 P 调用时，在条件变量 x 上有一个挂起进程 Q。显然，如果挂起进程 Q 允许重执行，那么进程 P 必须等待。否则，管程内有两个进程 P 和 Q 可能同时执行。 注意，从概念上说两个进程都可以继续执行。有两种可能性存在： 唤醒并等待：进程 P 等待直到 Q 离开管程，或者等待另一个条件。 唤醒并继续：进程 Q 等待直到 P 离开管程或者等待另一个条件。 对于任一选项，都有赞同理由。一方面，由于 P 已经在管程中执行，唤醒并继续的方法似乎更为合理。另一方面，如果我们允许线程 P 继续，那么 Q 等待的逻辑条件在 Q 重新启动时可能已不再成立。 Concurrent Pascal 语言采用这两种选择的折中。当进程 P 执行操作 signal 时，它立即离开管程。因此，进程 Q 立即重新执行。 "},"Chapter07/Lock.html":{"url":"Chapter07/Lock.html","title":"Java锁机制","keywords":"","body":"Java锁机制 什么是锁？ 在计算机科学中，锁(lock)或互斥(mutex)是一种同步机制，用于在有许多执行线程的环境中强制对资源的访问限制。锁旨在强制实施互斥排他、并发控制策略。 锁通常需要硬件支持才能有效实施。这种支持通常采取一个或多个原子指令的形式，如\"test-and-set\", \"fetch-and-add\" or \"compare-and-swap\"”。这些指令允许单个进程测试锁是否空闲，如果空闲，则通过单个原子操作获取锁。 锁的一个重要属性 粒度 Granularity 在引入锁粒度之前，需要了解关于锁的三个概念： 锁开销 lock overhead 锁占用内存空间、 cpu初始化和销毁锁、获取和释放锁的时间。程序使用的锁越多，相应的锁开销越大 锁竞争 lock contention 一个进程或线程试图获取另一个进程或线程持有的锁，就会发生锁竞争。锁粒度越小，发生锁竞争的可能性就越小 死锁 deadlock 至少两个任务中的每一个都等待另一个任务持有的锁的情况锁粒度是衡量锁保护的数据量大小，通常选择粗粒度的锁(锁的数量少，每个锁保护大量的数据)， 在当单进程访问受保护的数据时锁开销小，但是当多个进程同时访问时性能很差。因为增大了锁的竞争。相反，使用细粒度的锁(锁数量多，每个锁保护少量的数据)增加了锁的开销但是减少了锁竞争。例如数据库中，锁的粒度有表锁、页锁、行锁、字段锁、字段的一部分锁 相关术语 Critical Section(临界区)、 Mutex/mutual exclusion(互斥体)、 Semaphore/binary semaphore(信号量) 锁的种类 独享锁/共享锁 独享锁是指该锁一次只能被一个线程所持有。 (ReentrantLock、 Synchronized) 共享锁是指该锁可被多个线程所持有。 (ReadWriteLock) 互斥锁/读写锁 独享锁/共享锁这是广义上的说法，互斥锁/读写锁就分别对应具体的实现。在Java中如ReentrantLock就是互斥锁(独享锁)， ReadWriteLock就是读写锁(共享锁)。 独享锁与共享锁也是通过AQS来实现的 锁升级：读锁到写锁 (不支持) 锁降级：写锁到读锁 (支持) 读写锁 ReentrantReadWriteLock 低16位代表写锁，高16位代表读锁 公平锁/非公平锁 公平锁是指多个线程按照申请锁的顺序来获取锁。 非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能会造成饥饿现象。 对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。 对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的控制线程对锁的获取， 所以并没有任何办法使其变成公平锁。 可重入锁 可重入锁又名递归锁，是指同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。 ReentrantLock和Synchronized都是可重入锁。可重入锁的一个好处是可一定程度避免死锁 乐观锁/悲观锁 乐观锁/悲观锁不是指具体类型的锁，而是看待并发的角度。 悲观锁认为存在很多并发更新操作，采取加锁操作，如果不加锁一定会有问题 乐观锁认为不存在很多的并发更新操作，不需要加锁。数据库中乐观锁的实现一般采用版本号，Java中可使用CAS实现乐观锁。 乐观锁与悲观锁并不是特指某两种类型的锁，是人们定义出来的概念或思想，主要是指看待并发同步的角度。 乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS(Compare and Swap 比较并交换)实现的。 悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。比如Java里面的同步原语synchronized关键字的实现就是悲观锁。 悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。 悲观锁在Java中的使用，就是利用各种锁。 乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。 乐观锁与悲观锁是一种广义上的概念，体现了看待线程同步的不同角度。在Java和数据库中都有此概念对应的实际应用。 先说概念。对于同一个数据的并发操作，悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改。Java中，synchronized关键字和Lock的实现类都是悲观锁。 而乐观锁认为自己在使用数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作（例如报错或者自动重试）。 乐观锁在Java中是通过使用无锁编程来实现，最常采用的是CAS算法，Java原子类中的递增操作就通过CAS自旋实现的。 乐观锁 乐观锁总是认为不存在并发问题，每次去取数据的时候，总认为不会有其他线程对数据进行修改，因此不会上锁。但是在更新时会判断其他线程在这之前有没有对数据进行修改，一般会使用“数据版本机制”或“CAS操作”来实现。 数据版本机制 实现数据版本一般有两种，第一种是使用版本号，第二种是使用时间戳。以版本号方式为例。 版本号方式：一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 核心SQL代码 update table set xxx=#{xxx}, version=version+1 where id=#{id} and version=#{version}; CAS操作 CAS（Compare and Swap 比较并交换），当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS操作中包含三个操作数——需要读写的内存位置(V)、进行比较的预期原值(A)和拟写入的新值(B)。如果内存位置V的值与预期原值A相匹配，那么处理器会自动将该位置值更新为新值B，否则处理器不做任何操作。 悲观锁 悲观锁认为对于同一个数据的并发操作，一定会发生修改的，哪怕没有修改，也会认为修改。因此对于同一份数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁并发操作一定会出问题。 在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。具体响应方式由开发者根据实际需要决定。 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。 期间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。 分段锁 分段锁是一种锁的设计，并不是一种具体的锁。对于ConcuttentHashMap就是通过分段锁实现高效的并发操作。 自旋锁 自旋锁是指尝试获取锁的线程不会阻塞，而是采用循环的方式尝试获取锁。好处是减少上下文切换，缺点是一直占用CPU资源。 偏向锁/轻量级锁/重量级锁 背景知识 指令流水线 CPU的基本工作是执行存储的指令序列，即程序。程序的执行过程实际上是不断地取出指令、分析指令、执行指令的过程。 几乎所有的冯•诺伊曼型计算机的CPU，其工作都可以分为5个阶段：取指令、指令译码、执行指令、访存取数和结果写回。 现代处理器的体系结构中，采用了流水线的处理方式对指令进行处理。指令包含了很多阶段，对其进行拆解，每个阶段由专门的硬件电路、寄存器来处 理，就可以实现流水线处理。实现更高的CPU吞吐量，但是由于流水线处理本身的额外开销，可能会增加延迟。 cpu多级缓存 在计算机系统中，CPU高速缓存（CPU Cache，简称缓存）是用于减少处理器访问内存所需平均时间的部件。在金字塔式存储体系中它位于自顶向下的第二层，仅次于CPU寄存器。其容量远小于内存，但速度却可以接近处理器的频率。 当处理器发出内存访问请求时，会先查看缓存内是否有请求数据。如果存在（命中），则不经访问内存直接返回该数据；如果不存在（失效），则要先把内存中的相应数据载入缓存，再将其返回处理器。 缓存之所以有效，主要是因为程序运行时对内存的访问呈现局部性（Locality）特征。这种局部性既包括空间局部性（Spatial Locality），也包括时间局部性（Temporal Locality）。有效利用这种局部性，缓存可以达到极高的命中率。 问题引入 原子性 即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 示例方法：{i++ （i为实例变量）} 这样一个简单语句主要由三个操作组成： 读取变量i的值 进行加一操作 将新的值赋值给变量i 如果对实例变量i的操作不做额外的控制，那么多个线程同时调用，就会出现覆盖现象，丢失部分更新。 另外，如果再考虑上工作内存和主存之间的交互，可细分为以下几个操作： read 从主存读取到工作内存 （非必须） load 赋值给工作内存的变量副本（非必须） use 工作内存变量的值传给执行引擎 执行引擎执行加一操作 assign 把从执行引擎接收到的值赋给工作内存的变量 store 把工作内存中的一个变量的值传递给主内存（非必须） write 把工作内存中变量的值写到主内存中的变量（非必须） 可见性 是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值 存在可见性问题的根本原因是由于缓存的存在，线程持有的是共享变量的副本，无法感知其他线程对于共享变量的更改，导致读取的值不是最新的。 while (flag) {//语句1 doSomething();//语句2 } flag = false;//语句3 线程1判断flag标记，满足条件则执行语句2；线程2flag标记置为false，但由于可见性问题，线程1无法感知，就会一直循环处理语句2。 顺序性 即程序执行的顺序按照代码的先后顺序执行 由于编译重排序和指令重排序的存在，是的程序真正执行的顺序不一定是跟代码的顺序一致，这种情况在多线程情况下会出现问题。 if (inited == false) { context = loadContext(); //语句1 inited = true; //语句2 } doSomethingwithconfig(context); //语句3 由于语句1和语句2没有依赖性，语句1和语句2可能 并行执行 或者 语句2先于语句1执行，如果这段代码两个线程同时执行，线程1执行了语句2，而语句1还没有执行完，这个时候线程2判断inited为true，则执行语句3，但由于context没有初始化完成，则会导致出现未知的异常。 JMM内存模型 Java虚拟机规范定义了Java内存模型（Java Memory Model，JMM）来屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果（C/C++等则直接使用物理机和OS的内存模型，使得程序须针对特定平台编写），它在多线程的情况下尤其重要。 内存划分 JMM的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的底层细节。这里的变量是指共享变量，存在竞争问题的变量，如实例字段、静态字段、数组对象元素等，不包括线程私有的局部变量、方法参数等，因为私有变量不存在竞争问题。可以认为JMM包括内存划分、变量访问操作与规则两部分。 分为主内存和工作内存，每个线程都有自己的工作内存，它们共享主内存。 主内存（Main Memory）存储所有共享变量的值。 工作内存（Working Memory）存储该线程使用到的共享变量在主内存的的值的副本拷贝。 线程对共享变量的所有读写操作都在自己的工作内存中进行，不能直接读写主内存中的变量。 不同线程间也无法直接访问对方工作内存中的变量，线程间变量值的传递必须通过主内存完成。 这种划分与Java内存区域中堆、栈、方法区等的划分是不同层次的划分，两者基本没有关系。硬要联系的话，大致上主内存对应Java堆中对象的实例数据部分、工作内存对应栈的部分区域；从更低层次上说，主内存对应物理硬件内存、工作内存对应寄存器和高速缓存。 内存间交互规则 关于主内存与工作内存之间的交互协议，即一个变量如何从主内存拷贝到工作内存，如何从工作内存同步到主内存中的实现细节。Java内存模型定义了8种原子操作来完成： lock: 将一个变量标识为被一个线程独占状态 unclock: 将一个变量从独占状态释放出来，释放后的变量才可以被其他线程锁定 read: 将一个变量的值从主内存传输到工作内存中，以便随后的load操作 load: 把read操作从主内存中得到的变量值放入工作内存的变量的副本中 use: 把工作内存中的一个变量的值传给执行引擎，每当虚拟机遇到一个使用到变量的指令时都会使用该指令 assign: 把一个从执行引擎接收到的值赋给工作内存中的变量，每当虚拟机遇到一个给变量赋值的指令时，都要使用该操作 store: 把工作内存中的一个变量的值传递给主内存，以便随后的write操作 write: 把store操作从工作内存中得到的变量的值写到主内存中的变量 定义原子操作的使用规则 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步会主内存中 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或者assign）的变量。即就是对一个变量实施use和store操作之前，必须先自行assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。lock和unlock必须成对出现。 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量之前需要重新执行load或assign操作初始化变量的值。 如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作） 从上面可以看出，把变量从主内存复制到工作内存需要顺序执行read、load，从工作内存同步回主内存则需要顺序执行store、write。总结： read、load、use必须成对顺序出现，但不要求连续出现。assign、store、write同之； 变量诞生和初始化：变量只能从主内存“诞生”，且须先初始化后才能使用，即在use/store前须先load/assign； lock一个变量后会清空工作内存中该变量的值，使用前须先初始化；unlock前须将变量同步回主内存； 一个变量同一时刻只能被一线程lock，lock几次就须unlock几次；未被lock的变量不允许被执行unlock，一个线程不能去unlock其他线程lock的变量。 long和double型变量的特殊规则 Java内存模型要求前述8个操作具有原子性，但对于64位的数据类型long和double，在模型中特别定义了一条宽松的规定：允许虚拟机将没有被volatile修饰的64位数据的读写操作划分为两次32位的操作来进行。即未被volatile修饰时线程对其的读取read不是原子操作，可能只读到“半个变量”值。虽然如此，商用虚拟机几乎都把64位数据的读写实现为原子操作，因此我们可以忽略这个问题。 先行发生原则 Java内存模型具备一些先天的“有序性”，即不需要通过任何同步手段（volatile、synchronized等）就能够得到保证的有序性，这个通常也称为happens-before原则。 如果两个操作的执行次序不符合先行原则且无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 程序次序规则（Program Order Rule）：一个线程内，逻辑上书写在前面的操作先行发生于书写在后面的操作。 锁定规则（Monitor Lock Rule）：一个unLock操作先行发生于后面对同一个锁的lock操作。“后面”指时间上的先后顺序。 volatile变量规则（Volatile Variable Rule）：对一个volatile变量的写操作先行发生于后面对这个变量的读操作。“后面”指时间上的先后顺序。 传递规则（Transitivity）：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。 线程启动规则（Thread Start Rule）：Thread对象的start()方法先行发生于此线程的每个一个动作。 线程中断规则（Thread Interruption Rule）：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生（通过Thread.interrupted()检测）。 线程终止规则（Thread Termination Rule）：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行。 对象终结规则（Finaizer Rule）：一个对象的初始化完成（构造函数执行结束）先行发生于他的finalize()方法的开始。 问题解决 原子性 由JMM直接保证的原子性变量操作包括read、load、use、assign、store、write； 基本数据类型的读写（工作内存）是原子性的 由JMM的lock、unlock可实现更大范围的原子性保证，但是这是JVM需要实现支持的功能，对于开发者则是有由synchronized关键字 或者 Lock读写锁 来保证原子性。 可见性 volatile 变量值被一个线程修改后会立即同步回主内存、变量值被其他线程读取前立即从主内存刷新值到工作内存。即read、load、use三者连续顺序执行，assign、store、write连续顺序执行。 synchronized/Lock 由lock和unlock的使用规则保证 “对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）”。 *\"如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量之前需要重新执行load或assign操作初始化变量的值\" final 修饰的字段在构造器中一旦初始化完成，且构造器没有把“this”的引用传递出去，则其他线程可立即看到final字段的值。 顺序性 volatile 禁止指令重排序 synchronized/Lock “一个变量在同一个时刻只允许一条线程对其执行lock操作” 开发篇 volatile 被volatile修饰的变量能保证器顺序性和可见性 顺序性 对一个volatile变量的写操作先行发生于后面对这个变量的读操作。“后面”指时间上的先后顺序 可见性 当写一个 * volatile 变量时，JMM 会把该线程对应的工作内存中的共享变量刷新到主内存。 当读一个 * volatile 变量时，JMM 会把该线程对应的工作内存置为无效，线程接下来将从主内存中读取共享变量。 volatile相比于synchronized/Lock是非常轻量级，但是使用场景是有限制的： 对变量的写入操作不依赖于其当前值，即仅仅是读取和单纯的写入，比如操作完成、中断或者状态之类的标志 禁止对volatile变量操作指令的重排序 实现原理 volatile底层是通过cpu提供的内存屏障指令来实现的。硬件层的内存屏障分为两种：Load Barrier 和 Store Barrier即读屏障和写屏障。 内存屏障有两个作用： 阻止屏障两侧的指令重排序 强制把写缓冲区/高速缓存中的脏数据等写回主内存，让缓存中相应的数据失效 final 对于final域的内存语义，编译器和处理器要遵守两个重排序规则（内部实现也是使用内存屏障）： 写final域的重排序规则：在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 读final域的重排序规则：初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 public class FinalExample { int i;//普通域 final int j;//final域 static FinalExample obj; public FinalExample () { i = 1;//写普通域。对普通域的写操作【可能会】被重排序到构造函数之外 j = 2;//写final域。对final域的写操作【不会】被重排序到构造函数之外 } // 写线程A执行 public static void writer () {  obj = new FinalExample (); } // 读线程B执行 public static void reader () {  FinalExample object = obj;//读对象引用 int a = object.i;//读普通域。可能会看到结果为0(由于i=1可能被重排序到构造函数外，此时y还没有被初始化) int b = object.j;//读final域。保证能够看到结果为2 } } 初次读对象引用与初次读该对象包含的final域，这两个操作之间存在间接依赖关系。由于编译器遵守间接依赖关系，因此编译器不会重排序这两个操作。大多数处理器也会遵守间接依赖，也不会重排序这两个操作。但有少数处理器允许对存在间接依赖关系的操作做重排序（比如alpha处理器），这个规则就是专门用来针对这种处理器的。 对于final域是引用类型，写final域的重排序规则对编译器和处理器增加了如下约束： 在构造函数内对一个final引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 synchronized synchronized用于修饰普通方法、修饰静态方法、修饰代码块 确保代码的同步执行（即不同线程间的互斥）（原子性） 确保对共享变量的修改能够及时可见（可见性） 有效解决指令重排问题（顺序性） 实现原理 使用对象的监视器(Monitor，也有叫管程的)进行控制 进入/加锁时执行字节码指令MonitorEnter 退出/解锁时执行字节码指令MonitorExit 当执行代码有异常退出方法/代码段时，会自动解锁 使用哪个对象的监视器： 修饰对象方法时，使用当前对象的监视器 修饰静态方法时，使用类类型（Class 的对象）监视器 修饰代码块时，使用括号中的对象的监视器 必须为 Object 类或其子类的对象 MonitorEnter（加锁）： 每个对象都有一个关联的监视器。 监视器被锁住，当且仅当它有属主(Owner)时。 线程执行MonitorEnter就是为了成为Monitor的属主。 如果 Monitor 对象的记录数(Entry Count，拥有它的线程的重入次数)为 0， 将其置为 1，线程将自己置为 Monitor 对象的属主。 如果Monitor的属主为当前线程，就会重入监视器，将其记录数增一。 如果Monitor的属主为其它线程，当前线程会阻塞，直到记录数为0，才会 去竞争属主权。 MonitorExit（解锁）： 执行MonitorExit的线程一定是这个对象所关联的监视器的属主。 线程将Monitor对象的记录数减一。 如果Monitor对象的记录数为0，线程就会执行退出动作，不再是属主。 此时其它阻塞的线程就被允许竞争属主。 对于 MonitorEnter、MonitorExit 来说，有两个基本参数: 线程 关联监视器的对象 关键结构 在 JVM 中，对象在内存中的布局分为三块区域：对象头、实例数据、对齐填充。 如下: 实例变量 存放类的属性数据信息，包括父类的属性信息 如果是数组的实例变量，还包括数组的长度 这部分内存按4字节对齐 填充数据 由于虚拟机要求对象起始地址必须是8字节的整数倍 填充数据仅仅是为了字节对齐 保障下一个对象的起始地址为 8 的整数倍 长度可能为0 对象头（Object Header） 对象头由 Mark Word 、Class Metadata Address（类元数据地址） 和 数组长度（对象为数组时）组成 在 32 位和 64 位的虚拟机中，Mark Word 分别占用 32 字节和 64 字节，因此称其为 word Mark Word 存储的并非对象的 实际业务数据（如对象的字段值），属于 额外存储成本。为了节约存储空间，Mark Word 被设计为一个 非固定的数据结构，以便在尽量小的空间中存储尽量多的数据，它会根据对象的状态，变换自己的数据结构，从而复用自己的存储空间。 锁的状态共有 4 种:无锁、偏向锁、轻量级锁、重量级锁。随着竞争的增加，锁的使用情况如下: 无锁 -> 偏向锁 -> 轻量级锁 -> 重量级锁 其中偏向锁和轻量级锁是从 JDK 6 时引入的，在 JDK 6 中默认开启。 锁的升级(锁膨胀，inflate)是单向的，只能从低到高(从左到右)。不会出现 锁的降级。 偏向锁 当锁对象第一次被线程获取的时候，虚拟机将会把对象头中的标志位设为“01” （可偏向），即偏向模式。同时使用CAS操作把获取到这个锁的线程的ID记录在对象的Mark Word之中，如果CAS操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不再进行任何同步操作。 当有另外一个线程去尝试获取这个锁时，偏向模式就宣告结束。根据锁对象目前是否处于被锁定的状态，撤销偏向（Revoke Bias）后恢复到未锁定（标志位为“01”，不可偏向）或 轻量级锁定（标志位为“00”）的状态，后续的同步操作就进入轻量级锁的流程。 轻量级锁 进入到轻量级锁说明不止一个线程尝试获取锁，这个阶段会通过自适应自旋CAS方式获取锁。如果获取失败，则进行锁膨胀，进入重量级锁流程，线程阻塞。 重量级锁 重量级锁是通过系统的线程互斥锁来实现的，代价最昂贵 ContentionList，CXQ，存放最近竞争锁的线程 LIFO，单向链表 很多线程都可以把请求锁的线程放入队列中 但只有一个线程能将线程出队 EntryLis，表示胜者组 双向链表 只有拥有锁的线程才可以访问或变更 EntryLis 只有拥有锁的线程在释放锁时，并且在 EntryList 为空、ContentionList 不为 空的情况下，才能将ContentionList 中的线程全部出队，放入到EntryList 中 WaitSet，存放处于等待状态的线程 将进行 wait() 调用的线程放入WaitSet 当进行 notify()、notifyAll()调用时，会将线程放入到ContentionList或EntryList 队列中 注意： 对一个线程而言，在任何时候最多只处于三个集合中的一个 处于这三个集合中的线程，均为 BLOCKED 状态，底层使用互斥量来进行阻塞 当一个线程成功获取到锁时 对象监视器的 owner 字段从 NULL 变为非空，指向此线程 必须将自己从ContentionList或EntryList中出队 竞争型的锁传递机制 线程释放锁时，不保证后继线程一定可以获得到锁，而是后继线程去竞争锁 OnDeck，表示准备就绪的线程，保证任何时候都只有一个线程来直接竞争 锁 在获取锁时，如果发生竞争，则使用自旋锁来争用，如果自旋后仍得不 到，再放入上述队列中。 自旋可以减少ContentionList和EntryList上出队入队的操作，也就是减少了内部 维护的这些锁的争用。 OS 互斥锁 重量级锁是通过操作系统的线程互斥锁来实现的，在 Linux 下，锁所用的技术是 pthead_mutex_lock / pthead_mutex_unlock，即线程间的互斥锁。 线程互斥锁是基于 futex（Fast Userspace Mutex）机制实现的。 常规的操作系统的同步机制(如 IPC 等)，调用时都需要陷入到内核中执行，即使没有竞争也要执行一次陷入操作(int 0x80，trap)。而 futex 则是内核态和用户态的混合，无竞争时，获取锁和释放锁都不需要陷入内核。 初始分配 首先在内存分配 futex 共享变量，对线程而言，内存是共享的，直接分配(malloc)即可，为整数类型，初始值为1。 获取锁 使用CAS对 futex 变量减1，观察其结果： 如果由1变为0，表示无竞争，继续执行 如果小于 0，表示有竞争，调用 futex(..., FUTEX_WAIT, ...) 使当前线程休眠 释放锁 使用CAS给futex变量加1 如果futex变量由0变为1，表示无竞争，继续执行 如果 futex 变量变化前为负值，表示有竞争，调用 futex(..., FUTEX_WAKE, ...) 唤醒一个或多个等待线程 "},"Chapter07/XSLock.html":{"url":"Chapter07/XSLock.html","title":"JDK的互斥锁与共享锁","keywords":"","body":"JDK的互斥锁与共享锁 数据竞争 我们知道现代机器处理器几乎都是多核多线程的，引入多核多线程机制是为了尽可能提升机器整体处理性能。但是多核多线程也会带来很多并发问题，其中很重要的一个问题是数据竞争，数据竞争即多个线程同时访问共享数据而导致了数据冲突（不正确）。数据竞争如果没处理好则意味着整个业务逻辑可能出错，所以在高并发环境中我们要特别注意这点。 数据竞争产生条件 存在数据竞争的场景必须满足以下几个条件： 多个线程对某个共享数据进行访问。 这些线程同时地进行访问。 访问即是读或写数据操作。 至少有一个线程是执行写数据操作。 数据竞争例子 为更好理解数据竞争问题，下面我们举一个数据竞争的例子。下面两张图，上面的是不存在数据竞争时正确的结果。刚开始内存中i=0，线程一读取后将i加5。修改完后线程二才读取内存中的i并将其加6，最终i=11。而下面的情况则不同，线程二在线程一还没修改完就读取内存中i，此时导致最终的结果为i=6。 同步与锁 既然多个线程并发执行经常会涉及数据竞争问题，那么我们该如何解决这个问题呢？答案就是引入同步机制，通过同步机制来控制共享数据的访问，就能够解决数据竞争问题。实现同步机制可以通过锁来实现，所以AQS框架也抽象出了锁的获取操作和释放操作。而且还提供了包括独占锁和共享锁两种模式，这样对于上层的各种同步器的实现就方便很多了。 独占锁 独占锁是指该锁一次只能由一个线程持有，其它线程则无法获得，除非已持有锁的线程释放了该锁。一个线程只有在成功获取锁后才能继续往下执行，当离开竞争区域时则释放锁，释放的锁供其他即将进入数据竞争区域的线程获取。 获取独占锁和释放独占锁分别对应acquire方法和release方法。获取独占锁的主要逻辑为：先尝试获取锁，成功则往下执行，否则把线程放到等待队列中并可能将线程挂起。释放独占锁的主要逻辑为：唤醒等待队列中一个或多个线程去尝试获取锁。在AQS中可以用以下伪代码表示独占锁的获取与释放。 获取独占锁的伪代码： 释放独占锁的伪代码： 共享锁 共享锁是指该锁可以由多个线程所持有，多个线程都能同时获得该锁，而不必等到持有锁的线程释放该锁。比如一般我们所说的读锁就是共享锁，一个共享数据是可以被多个线程去读取的，只要它们都不改变共享数据就不会有数据竞争问题。 获取共享锁和释放共享锁分别对应acquireShared方法和releaseShared方法。获取共享锁的主要逻辑为： 先尝试获取锁，成功则往下执行，否则把线程放到等待队列中并可能将线程挂起。释放共享锁的主要逻辑为：唤醒等待队列中一个或多个线程去尝试获取锁。在AQS中可以用以下伪代码表示共享锁的获取与释放。 获取共享锁的伪代码 释放共享锁的伪代码 "},"Chapter07/Condition.html":{"url":"Chapter07/Condition.html","title":"Condition","keywords":"","body":"Condition 源码分析 它是一个用来多线程的协调通信的工具类，当某个线程阻塞等待某个条件时，当满足条件才会被唤醒。 public interface Condition { void await() throws InterruptedException; void awaitUninterruptibly(); boolean await(long time, TimeUnit unit) throws InterruptedException; void signal(); .... } 两个重要方法，await()和signal()。 它的实现在 AbstractQueuedSynchronizer （也就是我们平时说的aqs）和 AbstractQueuedLongSynchronizer中 await 调用此方法会使得线程进入等待队列并释放锁，线程的状态变成等待状态 public final void await() throws InterruptedException { //允许线程中断 if (Thread.interrupted()) throw new InterruptedException(); //创建一个状态为condition的节点，采用链表的形式存放数据 Node node = addConditionWaiter(); //释放当前的锁，得到锁的状态，释放等待队列中的一个线程 int savedState = fullyRelease(node); int interruptMode = 0; //判断当前节点是或否在队列上 while (!isOnSyncQueue(node)) { //挂起当前线程 LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } //acquireQueued为false就拿到了锁 //interruptMode != THROW_IE表示这个线程没有成功将 node 入队,但 signal 执行了 enq 方法让其入队了 if (acquireQueued(node, savedState) && interruptMode != THROW_IE) //将这个变量设置成 REINTERRUPT interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled //如果node节点的下一个等待者不为空，则开始进行清理，清理condition节点 unlinkCancelledWaiters(); if (interruptMode != 0) //如果线程中断了，需要抛出异常 reportInterruptAfterWait(interruptMode); } private Node addConditionWaiter() { Node t = lastWaiter; // If lastWaiter is cancelled, clean out. //如果lastWaiter不等于空并且waitStatus不为condition，把这个节点从链表中移除 if (t != null && t.waitStatus != Node.CONDITION) { unlinkCancelledWaiters(); t = lastWaiter; } //创建一个状态为condition的单向列表 Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; } final int fullyRelease(Node node) { boolean failed = true; try { //获得重入的次数 int savedState = getState(); //释放并唤醒同步队列中的线程 if (release(savedState)) { failed = false; return savedState; } else { throw new IllegalMonitorStateException(); } } finally { if (failed) node.waitStatus = Node.CANCELLED; } } final boolean isOnSyncQueue(Node node) { //判断当前节点是否在队列中，false表示不在，true表示在 if (node.waitStatus == Node.CONDITION || node.prev == null) return false; if (node.next != null) // If has successor, it must be on queue return true; /* * node.prev can be non-null, but not yet on queue because * the CAS to place it on queue can fail. So we have to * traverse from tail to make sure it actually made it. It * will always be near the tail in calls to this method, and * unless the CAS failed (which is unlikely), it will be * there, so we hardly ever traverse much. */ /从tail节点往前扫描AQS队列，如果发现AQS队列中的节点与当前节点相等，则说明节点一定存在与队列中 return findNodeFromTail(node); } signal() 调用此方法，将会唤醒在AQS队列中的节点 public final void signal() { //判断当前线程是否获得了锁 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //AQS队列的第一个节点 Node first = firstWaiter; if (first != null) doSignal(first); } private void doSignal(Node first) { do { //从condition队列中移除first节点 if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; } while (!transferForSignal(first) && (first = firstWaiter) != null); } final boolean transferForSignal(Node node) { /* * If cannot change waitStatus, the node has been cancelled. */ //更新节点状态为0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; /* * Splice onto queue and try to set waitStatus of predecessor to * indicate that thread is (probably) waiting. If cancelled or * attempt to set waitStatus fails, wake up to resync (in which * case the waitStatus can be transiently and harmlessly wrong). */ //调用 enq，把当前节点添加到AQS队列。并且返回返回按当前节点的上一个节点，也就是原tail 节点 Node p = enq(node); int ws = p.waitStatus; //如果上一个节点被取消了，尝试设置上一节点状态为SIGNAL if (ws > 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) //唤醒节点上的线程 LockSupport.unpark(node.thread); return true; } 阻塞：await()方法中，在线程释放锁资源之后，如果节点不在AQS等待队列，则阻塞当前线程，如果在等待队列，则自旋等待尝试获取锁； 释放：signal()后，节点会从condition队列移动到AQS等待队列，则进入正常锁的获取流程。 "},"Chapter07/aqs.html":{"url":"Chapter07/aqs.html","title":"AQS源码分析","keywords":"","body":"AQS源码分析 AQS是AbstractQueuedSynchronizer的简称，翻译过来就是【抽象队列同步】。AQS提供了一种实现阻塞锁和一系列依赖FIFO等待队列的同步器的框架， 如下图所示。AQS为一系列同步器依赖于一个单独的原子变量（state）的同步器提供了一个非常有用的基础。子类们必须定义改变state变量的protected方法， 这些方法定义了state是如何被获取或释放的。鉴于此，本类中的其他方法执行所有的排队和阻塞机制。子类也可以维护其他的state变量， 但是为了保证同步，必须原子地操作这些变量。 是用来构建锁或者其它同步器组件的重量级基础框架及整个JUC体系的基石，通过内置的CLH(FIFO)队列的变种来完成资源获取线程的排队工作,将每条将要去抢占资源的线程封装成一个Node节点来实现锁的分配，有一个int类变量表示持有锁的状态,通过CAS完成对status值的修改(0表示没有,1表示阻塞) AbstractQueuedSynchronizer中对state的操作是原子的，且不能被继承。所有的同步机制的实现均依赖于对改变量的原子操作。 为了实现不同的同步机制，我们需要创建一个非共有的（non-public internal）扩展了AQS类的内部辅助类来实现相应的同步逻辑。 AbstractQueuedSynchronizer并不实现任何同步接口，它提供了一些可以被具体实现类直接调用的一些原子操作方法来重写相应的同步逻辑。 AQS同时提供了互斥模式（exclusive）和共享模式（shared）两种不同的同步逻辑。 一般情况下，子类只需要根据需求实现其中一种模式，当然也有同时实现两种模式的同步类， 如ReadWriteLock。接下来将详细介绍AbstractQueuedSynchronizer的提供的一些具体实现方法。 state状态 AbstractQueuedSynchronizer维护了一个volatile int类型的变量，用户表示当前同步状态。volatile虽然不能保证操作的原子性，但是保证了当前变量state的可见性。至于volatile的具体语义，可以参考相关文章。state的访问方式有三种: getState() setState() compareAndSetState() 这三种叫做均是原子操作，其中compareAndSetState的实现依赖于Unsafe的compareAndSwapInt()方法 自定义资源共享方式 AQS定义两种资源共享方式：Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法： isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。 tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。 tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。 源码实现 接下来我们开始开始讲解AQS的源码实现。依照acquire-release、acquireShared-releaseShared的次序来。 1. acquire(int) acquire是一种以独占方式获取资源，如果获取到资源，线程直接返回，否则进入等待队列，直到获取到资源为止，且整个过程忽略中断的影响。该方法是独占模式下线程获取共享资源的顶层入口。获取到资源后，线程就可以去执行其临界区代码了。下面是acquire()的源码： /** * Acquires in exclusive mode, ignoring interrupts. Implemented * by invoking at least once {@link #tryAcquire}, * returning on success. Otherwise the thread is queued, possibly * repeatedly blocking and unblocking, invoking {@link * #tryAcquire} until success. This method can be used * to implement method {@link Lock#lock}. * * @param arg the acquire argument. This value is conveyed to * {@link #tryAcquire} but is otherwise uninterpreted and * can represent anything you like. */ public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 通过注释我们知道，acquire方法是一种互斥模式，且忽略中断。该方法至少执行一次tryAcquire(int)方法，如果tryAcquire(int)方法返回true，则acquire直接返回，否则当前线程需要进入队列进行排队。函数流程如下： 35 张图让你深入 AQS AQS实现原理 AQS中 维护了一个volatile int state（代表共享资源）和一个FIFO线程等待队列（多线程争用资源被阻塞时会进入此队列）。 这里volatile能够保证多线程下的可见性，当state=1则代表当前对象锁已经被占有，其他线程来加锁时则会失败，加锁失败的线程会被放入一个FIFO的等待队列中，比列会被UNSAFE.park()操作挂起，等待其他获取锁的线程释放锁才能够被唤醒。 另外state的操作都是通过CAS来保证其并发修改的安全性。 具体原理我们可以用一张图来简单概括： AQS 中提供了很多关于锁的实现方法， getState()：获取锁的标志state值 setState()：设置锁的标志state值 tryAcquire(int)：独占方式获取锁。尝试获取资源，成功则返回true，失败则返回false。 tryRelease(int)：独占方式释放锁。尝试释放资源，成功则返回true，失败则返回false。 这里还有一些方法并没有列出来，接下来我们以ReentrantLock作为突破点通过源码和画图的形式一步步了解AQS内部实现原理。 目录结构 文章准备模拟多线程竞争锁、释放锁的场景来进行分析AQS源码： 三个线程(线程一、线程二、线程三)同时来加锁/释放锁 目录如下： 线程一加锁成功时AQS内部实现 线程二/三加锁失败时AQS中等待队列的数据模型 线程一释放锁及线程二获取锁实现原理 通过线程场景来讲解公平锁具体实现原理 通过线程场景来讲解Condition中await()和signal()实现原理 这里会通过画图来分析每个线程加锁、释放锁后AQS内部的数据结构和实现原理 场景分析 线程一加锁成功 如果同时有三个线程并发抢占锁，此时线程一抢占锁成功，线程二和线程三抢占锁失败，具体执行流程如下： 此时AQS内部数据为： 线程二、线程三加锁失败： 有图可以看出，等待队列中的节点Node是一个双向链表，这里SIGNAL是Node中waitStatus属性，Node中还有一个nextWaiter属性，这个并未在图中画出来，这个到后面Condition会具体讲解的。 具体看下抢占锁代码实现： java.util.concurrent.locks.ReentrantLock .NonfairSync static final class NonfairSync extends Sync { final void lock() { if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); } protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } } 这里使用的ReentrantLock非公平锁，线程进来直接利用CAS尝试抢占锁，如果抢占成功state值回被改为1，且设置对象独占锁线程为当前线程。如下所示： protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } protected final void setExclusiveOwnerThread(Thread thread) { exclusiveOwnerThread = thread; } 线程二抢占锁失败 我们按照真实场景来分析，线程一抢占锁成功后，state变为1，线程二通过CAS修改state变量必然会失败。此时AQS中FIFO(First In First Out 先进先出)队列中数据如图所示： 我们将线程二执行的逻辑一步步拆解来看： java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(): public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 先看看tryAcquire()的具体实现： java.util.concurrent.locks.ReentrantLock .nonfairTryAcquire(): final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc nonfairTryAcquire()方法中首先会获取state的值，如果不为0则说明当前对象的锁已经被其他线程所占有，接着判断占有锁的线程是否为当前线程，如果是则累加state值，这就是可重入锁的具体实现，累加state值，释放锁的时候也要依次递减state值。 如果state为0，则执行CAS操作，尝试更新state值为1，如果更新成功则代表当前线程加锁成功。 以线程二为例，因为线程一已经将state修改为1，所以线程二通过CAS修改state的值不会成功。加锁失败。 线程二执行tryAcquire()后会返回false，接着执行addWaiter(Node.EXCLUSIVE)逻辑，将自己加入到一个FIFO等待队列中，代码实现如下： java.util.concurrent.locks.AbstractQueuedSynchronizer.addWaiter(): private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node; } 这段代码首先会创建一个和当前线程绑定的Node节点，Node为双向链表。此时等待对内中的tail指针为空，直接调用enq(node)方法将当前线程加入等待队列尾部 private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 第一遍循环时tail指针为空，进入if逻辑，使用CAS操作设置head指针，将head指向一个新创建的Node节点。此时AQS中数据： 执行完成之后，head、tail、t都指向第一个Node元素。 接着执行第二遍循环，进入else逻辑，此时已经有了head节点，这里要操作的就是将线程二对应的Node节点挂到head节点后面。此时队列中就有了两个Node节点： addWaiter()方法执行完后，会返回当前线程创建的节点信息。继续往后执行acquireQueued(addWaiter(Node.EXCLUSIVE), arg) 逻辑，此时传入的参数为线程二对应的Node节点信息： java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(): final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head && tryAcquire(arg)) { setHead(node); p.next = null; failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) && parkAndChecknIterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) return true; if (ws > 0) { do { node.prev = pred = pred.prev; } while (pred.waitStatus > 0); pred.next = node; } else { compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted(); } acquireQueued()这个方法会先判断当前传入的Node对应的前置节点是否为head，如果是则尝试加锁。加锁成功过则将当前节点设置为head节点，然后空置之前的head节点，方便后续被垃圾回收掉。 如果加锁失败或者Node的前置节点不是head节点，就会通过shouldParkAfterFailedAcquire方法 将head节点的waitStatus变为了SIGNAL=-1，最后执行parkAndChecknIterrupt方法，调用LockSupport.park()挂起当前线程。 此时AQS中的数据如下图： 此时线程二就静静的待在AQS的等待队列里面了，等着其他线程释放锁来唤醒它。 线程三抢占锁失败 看完了线程二抢占锁失败的分析，那么再来分析线程三抢占锁失败就很简单了，先看看addWaiter(Node mode)方法： private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node; } 此时等待队列的tail节点指向线程二，进入if逻辑后，通过CAS指令将tail节点重新指向线程三。接着线程三调用enq()方法执行入队操作，和上面线程二执行方式是一致的，入队后会修改线程二对应的Node中的waitStatus=SIGNAL。最后线程三也会被挂起。此时等待队列的数据如图： 线程一释放锁 现在来分析下释放锁的过程，首先是线程一释放锁，释放锁后会唤醒head节点的后置节点，也就是我们现在的线程二，具体操作流程如下 执行完后等待队列数据如下： 此时线程二已经被唤醒，继续尝试获取锁，如果获取锁失败，则会继续被挂起。如果获取锁成功，则AQS中数据如图： 接着还是一步步拆解来看，先看看线程一释放锁的代码： java.util.concurrent.locks.AbstractQueuedSynchronizer.release() public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null && h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } 这里首先会执行tryRelease()方法，这个方法具体实现在ReentrantLock中，如果tryRelease执行成功，则继续判断head节点的waitStatus是否为0，前面我们已经看到过，head的waitStatue为SIGNAL(-1)，这里就会执行unparkSuccessor()方法来唤醒head的后置节点，也就是我们上面图中线程二对应的Node节点。 此时看ReentrantLock.tryRelease()中的具体实现： protected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); return free; } 执行完ReentrantLock.tryRelease()后，state被设置成0，Lock对象的独占锁被设置为null。此时看下AQS中的数据： 接着执行java.util.concurrent.locks.AbstractQueuedSynchronizer.unparkSuccessor()方法，唤醒head的后置节点： private void unparkSuccessor(Node node) { int ws = node.waitStatus; if (ws 0) { s = null; for (Node t = tail; t != null && t != node; t = t.prev) if (t.waitStatus 这里主要是将head节点的waitStatus设置为0。 此时重新将head指针指向线程二对应的Node节点，且使用LockSupport.unpark方法来唤醒线程二。 被唤醒的线程二会接着尝试获取锁，用CAS指令修改state数据。 执行完成后可以查看AQS中数据： 此时线程二被唤醒，线程二接着之前被park的地方继续执行，继续执行acquireQueued()方法。 线程二唤醒继续加锁 final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head && tryAcquire(arg)) { setHead(node); p.next = null; failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 此时线程二被唤醒，继续执行for循环，判断线程二的前置节点是否为head，如果是则继续使用tryAcquire()方法来尝试获取锁，其实就是使用CAS操作来修改state值，如果修改成功则代表获取锁成功。接着将线程二设置为head节点，然后空置之前的head节点数据，被空置的节点数据等着被垃圾回收。 此时线程二获取锁成功，AQS中队列数据如下： 等待队列中的数据都等待着被垃圾回收。 线程二释放锁/线程三加锁 当线程二释放锁时，会唤醒被挂起的线程三，流程和上面大致相同，被唤醒的线程三会再次尝试加锁，具体代码可以参考上面内容。具体流程图如下： 此时AQS中队列数据如图： 公平锁实现原理 上面所有的加锁场景都是基于非公平锁来实现的，非公平锁是ReentrantLock的默认实现，那我们接着来看一下公平锁的实现原理，这里先用一张图来解释公平锁和非公平锁的区别： 非公平锁执行流程： 这里我们还是用之前的线程模型来举例子，当线程二释放锁的时候，唤醒被挂起的线程三，线程三执行tryAcquire()方法使用CAS操作来尝试修改state值，如果此时又来了一个线程四也来执行加锁操作，同样会执行tryAcquire()方法。 这种情况就会出现竞争，线程四如果获取锁成功，线程三仍然需要待在等待队列中被挂起。这就是所谓的非公平锁，线程三辛辛苦苦排队等到自己获取锁，却眼巴巴的看到线程四插队获取到了锁。 公平锁执行流程： 公平锁在加锁的时候，会先判断AQS等待队列中是存在节点，如果存在节点则会直接入队等待，具体代码如下. 公平锁在获取锁是也是首先会执行acquire()方法，只不过公平锁单独实现了tryAcquire()方法： java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(): public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 这里会执行ReentrantLock中公平锁的tryAcquire()方法 java.util.concurrent.locks.ReentrantLock.FairSync.tryAcquire(): static final class FairSync extends Sync { protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) { if (!hasQueuedPredecessors() && compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc 这里会先判断state值，如果不为0且获取锁的线程不是当前线程，直接返回false代表获取锁失败，被加入等待队列。如果是当前线程则可重入获取锁。 如果state=0则代表此时没有线程持有锁，执行hasQueuedPredecessors()判断AQS等待队列中是否有元素存在，如果存在其他等待线程，那么自己也会加入到等待队列尾部，做到真正的先来后到，有序加锁。具体代码如下： java.util.concurrent.locks.AbstractQueuedSynchronizer.hasQueuedPredecessors(): public final boolean hasQueuedPredecessors() { Node t = tail; Node h = head; Node s; return h != t && ((s = h.next) == null || s.thread != Thread.currentThread()); } 这段代码很有意思，返回false代表队列中没有节点或者仅有一个节点是当前线程创建的节点。返回true则代表队列中存在等待节点，当前线程需要入队等待。 先判断head是否等于tail，如果队列中只有一个Node节点，那么head会等于tail。 接着判断(s = h.next) == null，这种属于一种极端情况，在enq()入队操作中，此时不是原子性操作，可能存在这种情况： 在第一个红框处，例如 线程一 执行完成，此时head已经有值，而还未执行tail=head的时候，此时 线程二 判断 head != tail成立。而接着 线程一 执行完第二个红框处，此时tail = node，但是并未将head.next指向node。而这时 线程二 就会得到head.next == null成立，直接返回true。这种情况代表有节点正在做入队操作。 如果head.next不为空，那么接着判断head.next节点是否为当前线程，如果不是则返回false。大家要记清楚，返回false代表FIFO队列中没有等待获取锁的节点，此时线程可以直接尝试获取锁，如果返回true代表有等待线程，当前线程如要入队排列，这就是体现公平锁的地方。 非公平锁和公平锁的区别： 非公平锁性能高于公平锁性能。非公平锁可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量 非公平锁性能虽然优于公平锁，但是会存在导致线程饥饿的情况。在最坏的情况下，可能存在某个线程一直获取不到锁。不过相比性能而言，饥饿问题可以暂时忽略，这可能就是ReentrantLock默认创建非公平锁的原因之一了。 Condition实现原理 Condition简介 上面已经介绍了AQS所提供的核心功能，当然它还有很多其他的特性，这里我们来继续说下Condition这个组件。 Condition是在java 1.5中才出现的，它用来替代传统的Object的wait()、notify()实现线程间的协作，相比使用Object的wait()、notify()，使用Condition中的await()、signal()这种方式实现线程间协作更加安全和高效。因此通常来说比较推荐使用Condition 其中AbstractQueueSynchronizer中实现了Condition中的方法，主要对外提供awaite(Object.wait())和signal(Object.notify())调用。 Condition Demo示例 public class ReentrantLockDemo { static ReentrantLock lock = new ReentrantLock(); public static void main(String[] args) { Condition condition = lock.newCondition(); new Thread(() -> { lock.lock(); try { System.out.println(\"线程一加锁成功\"); System.out.println(\"线程一执行await被挂起\"); condition.await(); System.out.println(\"线程一被唤醒成功\"); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); System.out.println(\"线程一释放锁成功\"); } }).start(); new Thread(() -> { lock.lock(); try { System.out.println(\"线程二加锁成功\"); condition.signal(); System.out.println(\"线程二唤醒线程一\"); } finally { lock.unlock(); System.out.println(\"线程二释放锁成功\"); } }).start(); } } 这里线程一先获取锁，然后使用await()方法挂起当前线程并释放锁，线程二获取锁后使用signal唤醒线程一。 Condition实现原理图解 我们还是用上面的demo作为实例，执行的流程如下： 线程一执行await()方法： 先看下具体的代码实现，#java.util.concurrent.locks.AbstractQueuedSynchronizer.ConditionObject.await()： public final void await() throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) { LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } if (acquireQueued(node, savedState) && interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); } await()方法中首先调用addConditionWaiter()将当前线程加入到Condition队列中。 执行完后我们可以看下Condition队列中的数据： private Node addConditionWaiter() { Node t = lastWaiter; if (t != null && t.waitStatus != Node.CONDITION) { unlinkCancelledWaiters(); t = lastWaiter; } Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; } 这里会用当前线程创建一个Node节点，waitStatus为CONDITION。接着会释放该节点的锁，调用之前解析过的release()方法，释放锁后此时会唤醒被挂起的线程二，线程二会继续尝试获取锁。 接着调用isOnSyncQueue()方法是判断当前的线程节点是不是在同步队列中，因为上一步已经释放了锁，也就是说此时可能有线程已经获取锁同时可能已经调用了singal()方法，如果已经唤醒，那么就不应该park了，而是退出while方法，从而继续争抢锁。 此时线程一被挂起，线程二获取锁成功。 线程二执行signal()方法： 首先我们考虑下线程二已经获取到锁，此时AQS等待队列中已经没有了数据。 接着就来看看线程二唤醒线程一的具体执行流程： public final void signal() { if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first); } 先判断当前线程是否为获取锁的线程，如果不是则直接抛出异常。 接着调用doSignal()方法来唤醒线程。 private void doSignal(Node first) { do { if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; } while (!transferForSignal(first) && (first = firstWaiter) != null); } final boolean transferForSignal(Node node) { if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; Node p = enq(node); int ws = p.waitStatus; if (ws > 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true; } /** * Inserts node into queue, initializing if necessary. See picture above. * @param node the node to insert * @return node's predecessor */ private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 这里先从transferForSignal()方法来看，通过上面的分析我们知道Condition队列中只有线程一创建的一个Node节点，且waitStatue为CONDITION，先通过CAS修改当前节点waitStatus为0，然后执行enq()方法将当前线程加入到等待队列中，并返回当前线程的前置节点。 加入等待队列的代码在上面也已经分析过，此时等待队列中数据如下图： 接着开始通过CAS修改当前节点的前置节点waitStatus为SIGNAL，并且唤醒当前线程。此时AQS中等待队列数据为： 线程一被唤醒后，继续执行await()方法中的while循环。 public final void await() throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); int interruptMode = 0; while (!isOnSyncQueue(node)) { LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } if (acquireQueued(node, savedState) && interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); } 因为此时线程一的waitStatus已经被修改为0，所以执行isOnSyncQueue()方法会返回false。跳出while循环。 接着执行acquireQueued()方法，这里之前也有讲过，尝试重新获取锁，如果获取锁失败继续会被挂起。直到另外线程释放锁才被唤醒。 final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head && tryAcquire(arg)) { setHead(node); p.next = null; failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); } } 此时线程一的流程都已经分析完了，等线程二释放锁后，线程一会继续重试获取锁，流程到此终结。 Condition总结 我们总结下Condition和wait/notify的比较： Condition可以精准的对多个不同条件进行控制，wait/notify只能和synchronized关键字一起使用，并且只能唤醒一个或者全部的等待队列； Condition需要使用Lock进行控制，使用的时候要注意lock()后及时的unlock()，Condition有类似于await的机制，因此不会产生加锁方式而产生的死锁出现，同时底层实现的是park/unpark的机制，因此也不会产生先唤醒再挂起的死锁，一句话就是不会产生死锁，但是wait/notify会产生先唤醒再挂起的死锁。 通过AQS是如何设置链表尾巴的来理解AQS为什么效率这么高 我们的思路是什么呢？假如你要往一个链表上添加尾巴，尤其是好多线程都要往链表上添加尾巴，我们仔细想想看用普通的方法怎么做？第一点要加锁这一点是肯定的，因为多线程，你要保证线程安全，一般的情况下，我们会锁定整个链表(Sync)，我们的新线程来了以后，要加到尾巴上，这样很正常，但是我们锁定整个链表的话，锁的太多太大了，现在呢它用的并不是锁定整个链表的方法，而是只观测tail这一个节点就可以了，怎么做到的呢？compareAndAetTail(oldTail,node)，中oldTail是它的预期值，假如说我们想把当前线程设置为整个链表尾巴的过程中，另外一个线程来了，它插入了一个节点，那么仔细想一下Node oldTail = tail;的整个oldTail还等于整个新的Tail吗？不等于了吧，那么既然不等于了，说明中间有线程被其它线程打断了，那如果说却是还是等于原来的oldTail，这个时候就说明没有线程被打断，那我们就接着设置尾巴，只要设置成功了OK，compareAndAetTail(oldTail,node)方法中的参数node就做为新的Tail了，所以用了CAS操作就不需要把原来的整个链表上锁，这也是AQS在效率上比较高的核心。 "},"Chapter07/SpinLock.html":{"url":"Chapter07/SpinLock.html","title":"AQS的自旋锁","keywords":"","body":"AQS的自旋锁 先说互斥锁 互斥锁就是上节说到过的独占锁，一个锁一次只能由一个线程持有，其它线程则无法获得，除非已持有锁的线程释放了该锁。 这里为什么提互斥锁呢？其实互斥锁和自旋锁都是实现同步的方案，最终实现的效果都是相同的，但它们对未获得锁的线程的处理方式却是不同的。 对于互斥锁，当某个线程占有锁后，另外一个线程将进入阻塞状态。与互斥锁类似，自旋锁保证了公共数据在任意时刻最多只能由一条线程获取使用，不同的是在获取锁失败后自旋锁会采取自旋的处理方式。 自旋锁 自旋锁是一种非阻塞锁，它的核心机制就在自旋两个字，即用自旋操作来替代阻塞操作。某一线程尝试获取某个锁时，如果该锁已经被另一个线程占用的话，则此线程将不断循环检查该锁是否被释放，而不是让此线程挂起或睡眠。 一旦另外一个线程释放该锁后，此线程便能获得该锁。自旋是一种忙等待状态，过程中会一直消耗CPU的时间片。 为什么自旋 互斥锁有一个很大的缺点，即获取锁失败后线程会进入睡眠或阻塞状态，这个过程会涉及到用户态到内核态的调度，上下文切换的开销比较大。 假如某个锁的锁定时间很短，此时如果锁获取失败则让它睡眠或阻塞的话则有点得不偿失，因为这种开销可能比自旋的开销更大。 总结起来就是互斥锁更适合持有锁时间长的情况，而自旋锁更适合持有锁时间短的情况。 自旋锁特点 自旋锁的核心机制就是死等，所有想要获得锁的线程都在不停尝试去获取锁，当然这也会引来竞争问题。 与互斥锁一样，自旋锁也只允许一个线程获得锁。 自旋锁能提供中断机制，因为它并不会进入阻塞状态，所以能很好支持中断。 自旋锁适用于锁持有时间叫短的场景，即锁保护临界区很小的常见，这个很容易理解，如果持有锁太久，那么将可能导致大量线程都在自旋，浪费大量CPU资源。 自旋锁无法保证公平性，不保证先到先获得锁，这样就可能造成线程饥饿。 自旋锁需要保证各个本地缓存数据的一致性，在多处理器机器上，每个线程对应的处理器都对同一个变量进行读写。每次写操作都需要同步每个处理器缓存，这可能会影响性能。 自旋锁例子 下面看一个简单的自旋锁的实现，主要看lock和unlock两个方法，Unsafe仅仅是为操作提供了硬件级别的原子CAS操作。对于lock方法，假如有若干线程竞争，能成功通过CAS操作修改value值为newV的线程即是成功获取锁的线程。 它将顺利通过，而其它线程则不断在循环检测value值是否改回0，将value改为0的操作就是获取锁的线程执行完后对该锁进行释放。 对于unlock方法，用于释放锁，释放后若干线程又继续对该锁竞争。如此一来，没获得锁的线程也不会被挂起或阻塞，而是不断循环检查状态。 "},"Chapter07/ReentrantLock.html":{"url":"Chapter07/ReentrantLock.html","title":"ReentrantLock","keywords":"","body":"深入剖析可重入锁ReentrantLock 相比于synchronized，ReentrantLock需要显式的获取锁和释放锁，相对现在基本都是用JDK7和JDK8的版本，ReentrantLock的效率和synchronized区别基本可以持平了。他们的主要区别有以下几点： 等待可中断，当持有锁的线程长时间不释放锁的时候，等待中的线程可以选择放弃等待，转而处理其他的任务。 公平锁：synchronized和ReentrantLock默认都是非公平锁，但是ReentrantLock可以通过构造函数传参改变。只不过使用公平锁的话会导致性能急剧下降。 绑定多个条件：ReentrantLock可以同时绑定多个Condition条件对象。 ReentrantLock基于AQS(AbstractQueuedSynchronizer 抽象队列同步器)实现。 synchronized 是 JVM 实现的，而 ReentrantLock 是 JDK 实现的 synchronized比较简单，ReentrantLock需要lock()和unlock()来实现，较为复杂 锁的细粒度和灵活度：很明显ReenTrantLock优于Synchronized 性能的区别 在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized， 其实synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。 AQS内部维护一个state状态位，尝试加锁的时候通过CAS(CompareAndSwap)修改值，如果成功设置为1，并且把当前线程ID赋值， 则代表加锁成功，一旦获取到锁，其他的线程将会被阻塞进入阻塞队列自旋，获得锁的线程释放锁的时候将会唤醒阻塞队列中的线程， 释放锁的时候则会把state重新置为0，同时当前线程ID置为空 AbstractOwnableSynchronizer#setExclusiveOwnerThread protected final void setExclusiveOwnerThread(Thread thread) { exclusiveOwnerThread = thread; } 可重入锁 Java中的可重入锁（ReentrantLock）是用于控制并发的一种工具，它的功能类似于Java内置的Synchronized语法。 可重入锁是指一个线程可以多次对某个锁进行加锁操作，比如程序在多层调用中多次使用锁。 而对于不可重入锁来说，进行两次以及上的加锁会导致死锁的产生。 ReentrantLock是一种独占锁，在独占模式下线程只能逐一使用锁，即任意时刻仅仅只能有一条线程持有锁。 如下图，线程一首先成功调用lock方法进行加锁操作从而持有该锁，其它两个线程只能等待线程一释放锁。 因为锁是可重入的，所以线程一可多次调用lock方法对锁进行加锁操作。 当线程一调用unlock方法释放锁后，线程二调用lock方法成功获得锁。 接着线程二也调用unlock方法释放锁，然后线程三调用lock方法成功获得锁，并且其也可以多次调用lock方法对该锁进行加锁操作。 四要素 ReentrantLock类的四要素为：公平/非公平模式、lock方法、unlock方法以及newCondition方法。 公平/非公平模式表示多个线程同时去获取锁时是否按照先到先得的顺序获得锁，如果是则为公平模式，否则为非公平模式。 lock方法用于获取锁，假如锁已被其它线程占有则进入等待状态。unlock方法用于释放锁。 newCondition方法用于返回一个新创建的Condition对象，这个对象支持线程的阻塞和唤醒功能，每个Condition对象内部都有一个等待队列，具体的实现是基于AQS同步器的，我们在AQS章节会讲到具体的实现。 非公平模式的实现 ReentrantLock类的内部是基于AQS同步器来实现的，不管是公平模式还是非公平模式都是基于AQS的独占模式，只是在获取锁的操作逻辑上有些差异。 ReentrantLock的默认模式为非公平模式，我们先看非公平模式的实现。 ReentrantLock类的方法较多，为了能够更加清晰且更方便理解，这里我们分析主要的几个核心方法。 它提供了两个构造函数，无参的构造函数默认使用非公平模式，如果要指定模式则传入一个boolean参数，当其为true时表示公平模式。 其中NonfairSync为非公平同步器，而FairSync为公平同步器。lock方法用于加锁操作，其间接调用AQS同步器的acquire方法获取独占锁。 unlock方法用于释放锁操作，其间接调用AQS同步器的release方法释放独占锁。newCondition方法用于创建新的Condition对象并返回。 ReentrantLock类的内部Sync子类是公平模式FairSync类和非公平模式NonfairSync类的抽象父类。 因为ReentrantLock属于独占模式，所以这里提供了非公平的锁获取方法nonfairTryAcquire。tryRelease方法提供了释放锁的操作， 公平模式和非公平模式都通过该方法释放锁。newCondition方法用于创建AQS同步器的ConditionObject对象， 供ReentrantLock类的newCondition方法调用。 我们进一步分析nonfairTryAcquire方法，主要的逻辑是：获取AQS同步器的状态变量，其值只能为0或1，分别表示已被未被加锁和已被加锁。 如果状态变量为0则通过compareAndSetState方法进行CAS修改，将状态变量改为1，修改成功的话还要通过setExclusiveOwnerThread方法将当前线程设为锁持有线程。 如果状态变量为1且当前线程为锁持有线程，则表示正在进行锁重入操作，这时要将状态变量累加一，而且如果重入的次数超过int类型范围的话则抛Maximum lock count exceeded错误。 继续分析tryRelease方法，主要的逻辑为：首先得保证当前线程为锁持有线程，如果不是则抛IllegalMonitorStateException异常。 然后将AQS同步器的状态变量减一，如果状态变量为0的话则表示锁已经完全释放成功，并且通过setExclusiveOwnerThread方法将持有锁线程设为空。 如果状态变量不为0的话，那么只需要将状态变量减一即可。 非公平模式NonfairSync类仅需实现tryAcquire方法，它直接调用父类Sync的的nonfairTryAcquire方法即可。非公平就体现在这个方法中，大家还记得前面讲解的AQS同步器的实现逻辑吗？ 这里的tryAcquire实际上就是使用了闯入策略，即刚准备获取锁的线程是先尝试去获取锁，失败了才会进入队列。所以这就是不公平的来源。 公平模式的实现 公平模式与非公平模式的主要差异就在获取锁时的机制，非公平模式通过闯入策略来打破公平，而公平模式则所有线程完全通过队列来实现公平机制。 下面看公平模式的FairSync类，它们的差异就在tryAcquire方法，实际上不同的地方就在下图中加了方框的那行代码。 它会检查是否已经存在等待队列，如果已经有等待队列则返回false，其实就是说队列已经有其它线程了，直接放弃闯入操作。返回false则表示让AQS同步器将当前线程放进等待队列中，这种放弃闯入操作的做法则意味着公平。 Lock接口 例子一 这是一个简单的例子，三个线程启动后都执行加锁操作，独占锁的特性保证了三个线程轮流进行加锁和释放锁。某个运行的输出如下面所示，线程二调用lock方法后先成功获取锁，睡眠两秒后释放锁。 接着线程一又成功获取锁，它也睡眠两秒后释放掉锁。最后线程三成功获取锁，睡眠两秒后释放锁。注意这里使用了ReentrantLock的五参数构造函数，意味着使用了非公平模式，所以三个线程在竞争锁时都使用了闯入策略。 thread2 got the lock thread2 release the lock thread1 got the lock thread1 release the lock thread3 got the lock thread3 release the lock 例子二 接着看第二个例子，这个例子是为了说明ReentrantLock的可重入性。线程一调用lock方法成功获取锁后又一次调用lock方法获取锁，这便是锁的重入操作。 此时我们调用getHoldCount方法输出该锁被加锁的次数，输出为2。睡眠两秒后调用unlock方法进行一次锁释放操作，此时再调用getHoldCount方法得知锁被加锁的次数为1。 最后再调用一次unlock方法完全释放该锁，此时锁被加锁的次数为0。 thread1 got the lock thread1 got the lock again lock times : 2 thread1 release the lock lock times : 1 thread1 release the lock again lock times : 0 例子三 第三个例子主要看ReentrantLock的Condition的使用，通过ReentrantLock的newCondition方法能得到一个Condition对象，通过这个对象就能实现等待与唤醒操作。 ReentrantLock与Condition对象的await、signalAll方法，对应着Synchronized与Object对象的wait、notifyAll()方法。 这个例子中线程一和线程二都通过lock方法依次获得锁，但加锁后都调用了Condition对象的await方法进入等待。这里要注意，await方法会将锁释放掉后再进行等待， 所以两个线程都能依次获得锁。而当主线程中调用Condition对象的signalAll方法唤醒所有等待的线程时，线程一和线程二都会依次被唤醒。 唤醒后又会把原来的线程放到等待队列中，所以还需要unlock方法进行锁释放操作，该操作即将线程从等待队列中移除，最终才能往下执行。 thread1 waiting for signal thread2 waiting for signal thread1 release the lock thread2 release the lock 总结 本文介绍了Java中的可重入锁（ReentrantLock），包括它的核心四要素以及实现原理。ReentrantLock具有可重入性，它是一种独占锁，包括了公平模式和非公平模式。 ReentrantLock类的实现基于AQS同步器，我们对公平与非公平的实现机制都进行了深入的讲解。此外，还提供了三个例子，分别展示了ReentrantLock的普通用法、可重入性质以及Condition的使用。 "},"Chapter07/CountDownLatch.html":{"url":"Chapter07/CountDownLatch.html","title":"CountDownLatch实现原理及案例","keywords":"","body":"CountDownLatch实现原理及案例 关于闭锁 闭锁（CountDownLatch）是Java多线程并发中的一种同步器，它是JDK内置的同步器。通过它可以定义一个倒计数器，当倒计数器的值大于0时，所有调用await方法的线程都会等待。而调用countDown方法则可以让倒计数器的值减一，当倒计数器值为0时所有等待的线程都将继续往下执行。 闭锁的主要应用场景是让某个或某些线程在某个运行节点上等待N个条件都满足后才让所有线程继续往下执行，其中倒计数器的值为N，每满足一个条件倒计数器就减一。比如下图中，倒计数器初始值为3，然后三个线程调用await方法后都在等待。随后倒计数器减一为2，再减一为1，最后减一为0，所有等待的线程都往下继续执行。 三要素 闭锁的三要素为：倒计数器的初始值、await方法以及countdown方法。倒计数器的初始值在构建CountDownLatch对象时指定，它表示我们需要等待的条件个数。await方法能让线程进入等待状态，等待的条件是倒计数器的值大于0。countdown方法用于将倒计数器的值减一。 实现原理 前面我们介绍过如何基于AQS同步器实现一个自定义同步器，实际上CountdownLatch也是基于AQS来实现的，只要使用AQS的共享模式即可以轻松实现闭锁。 下面我们看详细的实现代码，CountdownLatch类的构造函数需要传入一个整型参数，表示倒计数器的初始值，对应着AQS的state状态变量。按照官方推荐的自定义同步器的做法，将继承了AQS类的子类Sync作为CountdownLatch类的内部类，而CountdownLatch同步器中相关的操作只需代理成子类中对应的方法即可。比如await方法和countDown方法分别调用Sync子类的acquireSharedInterruptibly方法和releaseShared方法。 Sync子类中需要我们实现的两个方法是tryAcquireShared和tryReleaseShared，分别用于获取共享锁和释放共享锁。先看获取共享锁的逻辑，如果状态变量等于0则返回1，当倒计数器的值减少到0的时候全部线程都可以直接尝试得到共享锁，而当倒计数器的值为非0时使之返回-1交给AQS进行入队管理。然后看释放共享锁的逻辑，主要是通过自旋来进行减一操作，getState方法获取状态变量，将其值减一后使用compareAndSetState方法进行CAS修改状态值。 例子1 第一个例子是创建一个CountdownLatch对象作为倒计数器，其值为2。然后线程一调用await方法进行等待，线程二调用countDown方法将倒计数器的值减一并往下执行。线程三再调用countDown方法将倒计数器的值再减一并往下执行，此时倒计数器的值为0，线程一停止等待并往下执行。 下面的例子输出如下，thread1调用await方法后进入等待状态，thread2睡眠两秒后调用countDown方法并往下执行，thread3睡眠四秒后调用countDown方法并往下执行，最后thread1才停止等待继续往下执行。 thread1 is waiting thread2 count down thread2 goes thread3 count down thread3 goes thread1 go "},"Chapter07/ReadWriteLock.html":{"url":"Chapter07/ReadWriteLock.html","title":"ReadWriteLock实现原理剖析","keywords":"","body":"Java多线程并发读写锁ReadWriteLock实现原理剖析 关于读写锁 前面的章节中我们分析了Java语法层面的synchronized锁和JDK内置可重入锁ReentrantLock，我们在多线程并发场景中可以通过它们来控制对资源的访问从而达到线程安全。这两种锁都属于纯粹的独占锁，也就是说这些锁任意时刻只能由一个线程持有，其它线程都得排队依次获取锁。 有些场景下为了提高并发性能我们会对纯粹的独占锁进行改造，额外引入共享锁来与独占锁共同对外构成一个锁，这种就叫读写锁。为什么叫读写锁呢？主要是因为它的使用考虑了读写场景，一般认为读操作不会改变数据所以可以多线程进行读操作，但写操作会改变数据所以只能一个线程进行写操作。读写锁在内部维护了一对锁（读锁和写锁），它通过将锁进行分离从而得到更高的并发性能。 如下图中，存在一个读写锁对象，其内部包含了读锁和写锁两个对象。假如存在五个线程，其中线程一和线程二想要获取读锁，那么两个线程是可以同时获取到读锁的。但是写锁就不可以共享，它是独占锁。比如线程三、线程四和线程五都想要持有写锁，那么只能一个个线程轮着持有。 读写锁的性质 可以多个线程同时持有读锁，某个线程成功获取读锁后其它线程仍然能成功获取读锁，即使该线程不释放读锁。 在某个线程持有读锁的情况下其它线程不能持有写锁，除非持有读锁的线程全部都释放掉读锁。 在某个线程持有写锁的情况下其它线程不能持有写锁或读锁，某个线程成功获取写锁后其它所有尝试获取读锁和写锁的线程都将进入等待状态，只有当该线程释放写锁后才其它线程能够继续往下执行。 如果我们要获取读锁则需要满足两个条件：目前没有线程持有写锁和目前没有线程请求获取写锁。 如果我们要获取写锁则需要满足两个条件：目前没有线程持有写锁和目前没有线程持有读锁。 简单的实现版本 为了加深对读写锁的理解，在分析JDK实现的读写锁之前我们先来看一个简单的读写锁实现版本。其中三个整型变量分别表示持有读锁的线程数、持有写锁的线程数以及请求获取写锁的线程数，四个方法分别对应读锁、写锁的获取和释放操作。acquireReadLock方法用于获取读锁，如果持有写锁的线程数量或请求读锁的线程数大于0则让线程进入等待状态。releaseReadLock方法用于释放读锁，将读锁线程数减一并唤醒其它线程。acquireWriteLock方法用于获取写锁，如果持有读锁的线程数量或持有写锁的线程数量大于0则让线程进入等待状态。releaseWriteLock方法用于释放写锁， 将写锁线程数减一并唤醒其它线程。 读锁升级为写锁 在某些场景下，我们希望某个已经拥有读锁的线程能够获得写锁，并将原来的读锁释放掉，这种情况就涉及到读锁升级为写锁操作。读写锁的升级操作需要满足一定的条件，这个条件就是某个线程必须是唯一拥有读锁的线程，否则将无法成功升级。如下图中，线程二已经持有读锁了，而且它是唯一的一个持有读锁的线程，所以它可以成功获得写锁。 写锁降级为读锁 与锁升级相对应的是锁降级，锁降级就是某个已经拥有写锁的线程希望能够获得读锁，并将原来的写锁释放掉。锁降级操作几乎没有什么风险，因为写锁是独占锁，持有写锁的线程肯定是唯一的，而且读锁也肯定不存在持有线程，所以写锁可以直接降级为读锁。如下图中，线程三持有写锁，此时其它线程不可能持有读锁和写锁，所以可以安全地将写锁降为读锁。 ReadWriteLock接口 ReadWriteLock实际上是一个接口，它仅仅提供了两个方法：readLock和writeLock。分别表示获取读锁对象和获取写锁对象，JDK为我们提供了一个内置的读写锁工具，那就是ReentrantReadWriteLock类，我们将对其进行深入分析。ReentrantReadWriteLock类包含的属性和方法较多，为了让分析思路清晰且方便读者理解，我们将剔除非核心源码，只对核心功能进行分析。 ReentrantReadWriteLock三要素 ReentrantReadWriteLock类的三要素为：公平/非公平模式、读锁对象和写锁对象。其中公平/非公平模式表示多个线程同时去获取锁时是否按照先到先得的顺序获得锁，如果是则为公平模式，否则为非公平模式。读锁对象负责实现读锁功能，而写锁对象负责实现写锁功能，这两个类都属于ReentrantReadWriteLock的内部类，下面会详细讲解。 ReentrantReadWriteLock实现思想 总的来说，ReentrantReadWriteLock类的内部包含了ReadLock内部类和WriteLock内部类，分别对应读锁和写锁，这两种锁都提供了公平模式和非公平模式。不管公平模式还是非公平模式、不管是读锁还是写锁都是基于AQS同步器来实现的。实现的主要难点在于只使用一个AQS同步器对象来实现读锁和写锁，这就要求读锁和写锁共用同一个共享状态变量，下面会具体讲解如何用一个状态变量来供读锁和写锁使用。 对应ReentrantReadWriteLock类的结构如下，ReentrantReadWriteLock.ReadLock和ReentrantReadWriteLock.WriteLock分别为读锁对象和写锁对象。Sync对象表示ReentrantReadWriteLock类的同步器，它基于AQS同步器，而FairSync类和NonfairSync类分别表示公平模式和非公平模式的同步器，可以看到默认情况下使用的是非公平模式。 读写锁共用状态变量 前面提到过ReentrantReadWriteLock的难点在于读锁和写锁都共用一个共享变量，下面看具体是如何共用的。我们知道AQS同步器的共享状态是整型的，即32位，那么最简单的共用方式就是读锁和写锁分别使用16位。其中高16位用于读锁的状态，而低16位则用于写锁的状态，这样便达到共用效果。但是这样设计后当我们要获取读锁和写锁的状态值时则需要一些额外的计算，比如一些移位和逻辑与操作。 ReentrantReadWriteLock的同步器共用状态变量的逻辑如下，其中SHARED_SHIFT表示移动的位数为16；SHARED_UNIT表示读锁每次加锁对应的状态值大小，1左移16位刚好对应高16位的1；MAX_COUNT表示读锁能被加锁的最大次数，值为16个1（二进制）；EXCLUSIVE_MASK表示写锁的掩码，值为16个1（二进制）。sharedCount方法用于获取读锁（高16位）的状态值，左移16位即能得到。exclusiveCount方法用于获取写锁（低16位）的状态值，通过掩码即能得到。 ReadLock与WriteLock简介 ReadLock与WriteLock是ReentrantReadWriteLock的两个要素，它们都属于ReentrantReadWriteLock的内部类。它们都实现了Lock接口，我们主要关注lock、unlock和newCondition这几个核心方法。分别表示对读锁和写锁的加锁操作、释放锁操作和创建Condition对象操作，可以看到这些方法都间接调用了ReentrantReadWriteLock的同步器的方法，需要注意的是读锁不支持创建Condition对象。我们在可重入锁ReentrantLock章节中已经讲解过Condition对象，本节将不再赘述。 公平/非公平模式 ReentrantReadWriteLock的默认模式为非公平模式，其内部类Sync是公平模式FairSync类和非公平模式NonfairSync类的抽象父类。因为ReentrantReadWriteLock的读锁使用了共享模式，而写锁使用了独占模式，所以该父类将不同模式下的公平机制抽象成readerShouldBlock和writerShouldBlock两个抽象方法，然后子类就可以各自实现不同的公平模式。换句话说，ReentrantReadWriteLock的公平机制就由这两个方法来决定了。 下面看公平模式的FairSync类，该类的readerShouldBlock和writerShouldBlock两个方法都直接返回hasQueuedPredecessors方法的结果，这个方法是AQS同步器的方法，用于判断当前线程前面是否有排队的线程。如果有排队队列就要让当前线程也加入排队队列中，这样按照队列顺序获取锁也就保证了公平性。 继续看非公平模式NonfairSync类，该类的writerShouldBlock方法直接返回false，表明不要让当前线程进入排队队列中，直接进行锁的获取竞争。readerShouldBlock方法则调用apparentlyFirstQueuedIsExclusive方法，这个方法是AQS同步器的方法，用于判断头结点的下一个节点线程是否在请求获取独占锁（写锁）。如果是则让其它线程先获取写锁，而自己则乖乖去排队。如果不是则说明下一个节点线程是请求共享锁（读锁），此时直接与之竞争读锁。 写锁WriteLock的实现 上面的介绍中我们知道WriteLock有两个核心方法：lock和unlock。它们都会间接调用了ReentrantReadWriteLock内部同步器的对应方法，在同步器中需要重写tryAcquire方法和tryRelease方法，分别用于获取写锁和释放写锁操作。 先看tryAcquire方法的逻辑，获取状态值并通过exclusiveCount方法得到低16位的写锁状态值。c!=0时有两种情况，一种是高16位的读锁状态不为0，一种是低16位的写锁状态不为0。w等于0时表示还有线程持有读锁，直接返回false表示获取写锁失败。如果持有写锁的线程为当前线程，则表示写锁重入操作，此时需要将状态变量进行累加，此外需要校验的是写锁重入状态值不能超过MAX_COUNT。通过writerShouldBlock方法判断是否需要将当前线程放入排队队列中，同时通过拥有CAS算法的compareAndSetState方法对状态变量进行累加操作，CAS失败的话也需要将当前线程放入排队队列中。对于非公平模式，这里的CAS操作就是闯入操作，即线程先尝试一次竞争写锁。最后通过setExclusiveOwnerThread设置当前线程持有写锁，该方法只是简单的设置变量方法。 继续看tryRelease方法的逻辑，先用isHeldExclusively方法检查当前线程必须为写锁持有线程。然后将状态值减去释放的值，并通过exclusiveCount得到低16位的写锁状态值，如果其值为0则表示已经没有重入可以彻底释放锁了，调用setExclusiveOwnerThread(null)设置没有线程持有写锁。最后设置新的状态值。 ReadWriteLock读写锁用法 ReadWriteLock接口定义了两个Lock对象，其中一个用于读操作，一个用于写操作。 public interface ReadWriteLock { Lock readLock(); Lock writeLock(); } 在读写锁的加锁策略中，允许多个读操作同时进行，但每次只允许一个写操作，即读读不冲突，读写、写写冲突。 ReentrantReadWriteLock实现了ReadWriteLock接口，为这两种锁都提供了可重入的加锁语义。与ReentrantLock类似，ReentrantReadWriteLock在构造时也可以选择是一个非公平锁还是一个公平的锁，默认是非公平锁。 ReentrantReadWriteLock的构造函数： public ReentrantReadWriteLock() { this(false); } public ReentrantReadWriteLock(boolean fair) { sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); } ReentrantReadWriteLock类派生出了ReadLock和WriteLock分别表示读锁和写锁。 ReadLock的加锁方法是基于AQS同步器的共享模式。 public void lock() { sync.acquireShared(1); } WriteLock的加锁方法是基于AQS同步器的独占模式。 public void lock() { sync.acquire(1); } 我们用读写锁构建线程安全的Map来熟悉它的用法： ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(); final Lock readLock = reentrantReadWriteLock.readLock(); final Lock writeLock = reentrantReadWriteLock.writeLock(); final HashMap map = new HashMap<>(); public void put(String k, Integer v) { writeLock.lock(); try { map.put(k, v); } finally { writeLock.unlock(); } } public Integer get(String k) { readLock.lock(); try { return map.get(k); } finally { readLock.unlock(); } } 从上面代码我们可能觉着读写控制分开了，提高了读的共享控制是不是读写锁的性能要比独占锁要高呢？ 但是从一些性能测试上来说，读写锁的性能并不好，而且使用不当还会引起饥饿写的问题。 饥饿写即在使用读写锁的时候，读线程的数量要远远大于写线程的数量，导致锁长期被读线程持有，写线程无法获取写操作权限而进入饥饿状态。 因此JDK1.8引入了StampedLock。 "},"Chapter07/StampedLock.html":{"url":"Chapter07/StampedLock.html","title":"StampedLock实现原理剖析","keywords":"","body":"StampedLock实现原理剖析 StampedLock在获取锁的时候会返回一个long型的数据戳，该数据戳用于稍后的锁释放参数，如果返回的数据戳为0则表示锁获取失败。 StampedLock是不可重入的，即使当前线程持有了锁再次获取锁还是会返回一个新的数据戳，所以要注意锁的释放参数，使用不小心可能会导致死锁。 StampedLock几乎具备了ReentrantReadWriteLock和ReentrantLock的功能，我们将上面的测试代码替换为StampedLock来熟悉下它的用法。 final HashMap map = new HashMap<>(); final StampedLock stampedLock = new StampedLock(); public void put2(String k, Integer v) { long stamp = stampedLock.writeLock(); try { map.put(k, v); } finally { stampedLock.unlockWrite(stamp); } } public Integer get2(String k) { long stamp = stampedLock.readLock(); try { return map.get(k); } finally { stampedLock.unlockRead(stamp); } } StampedLock还提供了乐观读模式，使用tryOptimisticRead()方法获取一个非排他锁并且不会进入阻塞状态。 该方法会返回long型的数据戳，数据戳非0表示获取锁成功，如果为0表示获取锁失败。 我们把上面测试用例的读操作换成乐观读模式。 public String optimisticGet(String k) { //尝试获取读锁，返回获取的结果，线程不会进入阻塞 long stamp = lock.tryOptimisticRead(); //对上一步获取的结果进行验证 //如果验证失败，则此时可能其他线程加了写锁，那么此时线程通过加读锁进入阻塞状态直到获取到读锁 //如果验证成功，不进行任何加锁操作直接返回共享数据，这样的话就实现了无锁读的操作，提高了读访问性能。 if (!lock.validate(stamp)) { stamp = lock.readLock(); try { return map.get(k); }finally { lock.unlockRead(stamp); } } return map.get(k); } 那么什么时候选择synchronized，什么时候选择ReentrantLock呢？ 如果你用到了ReentrantLock的高级性能，那么没办法只能ReentrantLock了。 如果不是高级性能可以优先选择synchronized，毕竟不用显示的加锁和解锁。但是我们需要考虑synchronized锁的升级不可逆的特点，如果线程并发存在比较明显的峰谷，则可以考虑选用ReentrantLock，毕竟重量级锁的性能确实不怎么好。 StampedLock的性能明显优于ReentrantReadWriteLock，且它还提供了乐观读的模式，优化了读操作的性能。StampedLock还提供了ReentrantLock的加锁解锁方式，因此如果遇到读写锁的场景那么可以考虑优先选择StampedLock。 StampedLock的特点 StampedLock的主要特点概括一下，有以下几点： 所有获取锁的方法，都返回一个邮戳（Stamp），Stamp为0表示获取失败，其余都表示成功； 所有释放锁的方法，都需要一个邮戳（Stamp），这个Stamp必须是和成功获取锁时得到的Stamp一致； StampedLock是不可重入的；（如果一个线程已经持有了写锁，再去获取写锁的话就会造成死锁） StampedLock有三种访问模式： ①Reading（读模式）：功能和ReentrantReadWriteLock的读锁类似 ②Writing（写模式）：功能和ReentrantReadWriteLock的写锁类似 ③Optimistic reading（乐观读模式）：这是一种优化的读模式。 StampedLock支持读锁和写锁的相互转换 我们知道RRW中，当线程获取到写锁后，可以降级为读锁，但是读锁是不能直接升级为写锁的。 StampedLock提供了读锁和写锁相互转换的功能，使得该类支持更多的应用场景。 无论写锁还是读锁，都不支持Conditon等待 我们知道，在ReentrantReadWriteLock中，当读锁被使用时，如果有线程尝试获取写锁，该写线程会阻塞。 但是，在Optimistic reading中，即使读线程获取到了读锁，写线程尝试获取写锁也不会阻塞，这相当于对读模式的优化，但是可能会导致数据不一致的问题。所以，当使用Optimistic reading获取到读锁时，必须对获取结果进行校验。 StampedLock原理 StampedLock虽然不像其它锁一样定义了内部类来实现AQS框架，但是StampedLock的基本实现思路还是利用CLH队列进行线程的管理，通过同步状态值来表示锁的状态和类型。 StampedLock内部定义了很多常量，定义这些常量的根本目的还是和ReentrantReadWriteLock一样，对同步状态值按位切分，以通过位运算对State进行操作： 对于StampedLock来说，写锁被占用的标志是第8位为1，读锁使用0-7位，正常情况下读锁数目为1-126，超过126时，使用一个名为readerOverflow的int整型保存超出数。 /** Number of processors, for spin control */ //cpu核数 用于控制自旋次数 private static final int NCPU = Runtime.getRuntime().availableProcessors(); /** Maximum number of retries before enqueuing on acquisition */ //尝试获取锁时如果超过该值仍未获得锁，加入等待队列 private static final int SPINS = (NCPU > 1) ? 1 1) ? 1 1) ? 1 部分常量的比特位表示如下： 另外，StampedLock相比ReentrantReadWriteLock，对多核CPU进行了优化，可以看到，当CPU核数超过1时，会有一些自旋操作 来看下writeLock方法 /** *获取写锁，如果获取失败进入阻塞 * 注意该方法不响应中断 **/ public long writeLock() { long s, next; // bypass acquireWrite in fully unlocked case only return ((((s = state) & ABITS) == 0L && ///((s = state) & ABITS) == 0L 表示读锁和写锁都为使用 U.compareAndSwapLong(this, STATE, s, next = s + WBIT)) ? //cas 操作 将第8位置1 表示写锁被占用 next : acquireWrite(false, 0L)); //获取失败则调用acquireWrite 加入等待队列 } StampedLock中大量运用了位运算，这里(s = state) & ABITS == 0L 表示读锁和写锁都未被使用，这里写锁可以立即获取成功，然后CAS操作更新同步状态值State。 操作完成后，等待队列的结构如下： 注意：StampedLock中，等待队列的结点要比AQS中简单些，仅仅三种状态。 0：初始状态 -1：等待中 1：取消 来看下readLock方法 由于ThreadA此时持有写锁，所以ThreadB获取读锁失败，将调用acquireRead方法，加入等待队列： /** * 获取读锁，如果写锁被占用，线程会阻塞 * **/ public long readLock() { long s = state, next; // bypass acquireRead on common uncontended case //whead == wtail 队列为空 且读锁数目未超限 //(s & ABITS) acquireRead方法非常复杂，用到了大量自旋操作： /** * 尝试自旋的获取读锁, 获取不到则加入等待队列, 并阻塞线程 * * @param interruptible true 表示检测中断, 如果线程被中断过, 则最终返回INTERRUPTED * @param deadline 如果非0, 则表示限时获取 * @return 非0表示获取成功, INTERRUPTED表示中途被中断过 */ private long acquireRead(boolean interruptible, long deadline) { WNode node = null, p; // node指向入队结点, p指向入队前的队尾结点 /** * 自旋入队操作 * 如果写锁未被占用, 则立即尝试获取读锁, 获取成功则返回. * 如果写锁被占用, 则将当前读线程包装成结点, 并插入等待队列（如果队尾是写结点,直接链接到队尾;否则,链接到队尾读结点的栈中） */ for (int spins = -1; ; ) { WNode h; if ((h = whead) == (p = wtail)) { // 如果队列为空或只有头结点, 则会立即尝试获取读锁 for (long m, s, ns; ; ) { if ((m = (s = state) & ABITS) = WBIT) { // 写锁被占用,以随机方式探测是否要退出自旋 if (spins > 0) { if (LockSupport.nextSecondarySeed() >= 0) --spins; } else { if (spins == 0) { WNode nh = whead, np = wtail; if ((nh == h && np == p) || (h = nh) != (p = np)) break; } spins = SPINS; } } } } if (p == null) { // p == null表示队列为空, 则初始化队列(构造头结点) WNode hd = new WNode(WMODE, null); if (U.compareAndSwapObject(this, WHEAD, null, hd)) wtail = hd; } else if (node == null) { // 将当前线程包装成读结点 node = new WNode(RMODE, p); } else if (h == p || p.mode != RMODE) { // 如果队列只有一个头结点, 或队尾结点不是读结点, 则直接将结点链接到队尾, 链接完成后退出自旋 if (node.prev != p) node.prev = p; else if (U.compareAndSwapObject(this, WTAIL, p, node)) { p.next = node; break; } } // 队列不为空, 且队尾是读结点, 则将添加当前结点链接到队尾结点的cowait链中（实际上构成一个栈, p是栈顶指针 ） else if (!U.compareAndSwapObject(p, WCOWAIT, node.cowait = p.cowait, node)) { // CAS操作队尾结点p的cowait字段,实际上就是头插法插入结点 node.cowait = null; } else { for (; ; ) { WNode pp, c; Thread w; // 尝试唤醒头结点的cowait中的第一个元素, 假如是读锁会通过循环释放cowait链 if ((h = whead) != null && (c = h.cowait) != null && U.compareAndSwapObject(h, WCOWAIT, c, c.cowait) && (w = c.thread) != null) // help release U.unpark(w); if (h == (pp = p.prev) || h == p || pp == null) { long m, s, ns; do { if ((m = (s = state) & ABITS) 0) { node = null; // throw away break; } if (deadline == 0L) time = 0L; else if ((time = deadline - System.nanoTime()) = WBIT && LockSupport.nextSecondarySeed() >= 0 && --k 我们来分析下这个方法。 该方法会首先自旋的尝试获取读锁，获取成功后，就直接返回；否则，会将当前线程包装成一个读结点，插入到等待队列。 由于，目前等待队列还是空，所以ThreadB会初始化队列，然后将自身包装成一个读结点，插入队尾，然后在下面这个地方跳出自旋： if (p == null) { // p == null表示队列为空, 则初始化队列(构造头结点) WNode hd = new WNode(WMODE, null); if (U.compareAndSwapObject(this, WHEAD, null, hd)) wtail = hd; } else if (node == null) { // 将当前线程包装成读结点 node = new WNode(RMODE, p); } else if (h == p || p.mode != RMODE) { // 如果队列只有一个头结点, 或队尾结点不是读结点, 则直接将结点链接到队尾, 链接完成后退出自旋 if (node.prev != p) node.prev = p; else if (U.compareAndSwapObject(this, WTAIL, p, node)) { p.next = node; break; //跳出自选 } } 此时，等待队列的结构如下： 跳出自旋后，ThreadB会继续向下执行，进入下一个自旋，在下一个自旋中，依然会再次尝试获取读锁，如果这次再获取不到，就会将前驱的等待状态置为WAITING, 表示我（当前线程）要去睡了（阻塞），到时记得叫醒我： if (whead == h) { if ((np = node.prev) != p) { if (np != null) (p = np).next = node; // stale } else if ((ps = p.status) == 0) // 将前驱结点的等待状态置为WAITING, 表示之后将唤醒当前结点 U.compareAndSwapInt(p, WSTATUS, 0, WAITING); //进入阻塞 else if (ps == CANCELLED) { if ((pp = p.prev) != null) { node.prev = pp; pp.next = node; } 最终, ThreadB进入阻塞状态: } else { // 阻塞当前读线程 long time; if (deadline == 0L) time = 0L; else if ((time = deadline - System.nanoTime()) 最终，等待队列的结构如下： "},"Chapter07/full.html":{"url":"Chapter07/full.html","title":"如果你提交任务时，线程池队列已满，这时会发生什么？","keywords":"","body":"如果你提交任务时，线程池队列已满，这时会发生什么？ 这里区分一下： 1、如果使用的是无界队列 LinkedBlockingQueue，也就是无界队列的话，没关 系，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎 认为是一个无穷大的队列，可以无限存放任务 2、如果使用的是有界队列比如 ArrayBlockingQueue，任务首先会被添加到 ArrayBlockingQueue 中，ArrayBlockingQueue 满了，会根据 maximumPoolSize 的值增加线程数量，如果增加了线程数量还是处理不过来， ArrayBlockingQueue 继续满，那么则会使用拒绝策略 RejectedExecutionHandler 处理满了的任务，默认是 AbortPolicy 高并发、任务执行时间短的业务怎样使用线程池？ 高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 并发不高、任务执行时间长的业务怎样使用线程池？ 并发不高、任务执行时间长的业务要区分开看： a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 并发高、业务执行时间长的业务怎样使用线程池？ 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步， 至于线程池的设置，设置参考（2）。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。 synchronized的CPU原语级别是如何实现的？ 代码片段 synchronized代码块主要是靠monitorenter和monitorexit这两个原语来实现同步的。当线程进入monitorenter获得执行代码的权利时，其他线程就不能执行里面的代码，直到锁Owner线程执行monitorexit释放锁后，其他线程才可以竞争获取锁。 普通方法 常量池中多了ACC_SYNCHRONIZED标示符。JVM就是根据该标示符来实现方法的同步的：当方法调用时会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取monitor，获取成功之后才能执行方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个monitor对象。这种方式与语句块没什么本质区别，都是通过竞争monitor的方式实现的。只不过这种方式是隐式的实现方法。 静态方法 常量池中用ACC_STATIC标志了这是一个静态方法，然后用ACC_SYNCHRONIZED标志位提醒线程去竞争monitor。由于静态方法是属于类级别的方法（即不用创建对象就可以被调用），所以这是一个类级别（XXX.class）的锁，即竞争某个类的monitor。 monitor介绍 1.每个对象有一个监视器锁（monitor）。当monitor被占用时就会处于锁定状态，线程执行monitorenter指令时尝试获取monitor的所有权，过程如下： 如果monitor的进入数为0，则该线程进入monitor，然后将进入数设置为1，该线程即为monitor的所有者。 2.如果线程已经占有该monitor，只是重新进入，则进入monitor的进入数加1.这里涉及重入锁，如果一个线程获得了monitor，他可以再获取无数次，进入的时候monito+1，退出-1，直到为0，开可以被其他线程获取 3.如果其他线程已经占用了monitor，则该线程进入阻塞状态，直到monitor的进入数为0，再重新尝试获取monitor的所有权。 什么时候触发MinorGC?什么时候触发FullGC? summerZBH123 2018-07-23 21:28:21 5172 收藏 4 分类专栏： jvm 触发MinorGC(Young GC) 虚拟机在进行minorGC之前会判断老年代最大的可用连续空间是否大于新生代的所有对象总空间 1、如果大于的话，直接执行minorGC 2、如果小于，判断是否开启HandlerPromotionFailure，没有开启直接FullGC 3、如果开启了HanlerPromotionFailure, JVM会判断老年代的最大连续内存空间是否大于历次晋升的大小，如果小于直接执行FullGC 4、如果大于的话，执行minorGC 触发FullGC 老年代空间不足 如果创建一个大对象，Eden区域当中放不下这个大对象，会直接保存在老年代当中，如果老年代空间也不足，就会触发Full GC。为了避免这种情况，最好就是不要创建太大的对象。 持久代空间不足 如果有持久代空间的话，系统当中需要加载的类，调用的方法很多，同时持久代当中没有足够的空间，就出触发一次Full GC YGC出现promotion failure promotion failure发生在Young GC, 如果Survivor区当中存活对象的年龄达到了设定值，会就将Survivor区当中的对象拷贝到老年代，如果老年代的空间不足，就会发生promotion failure， 接下去就会发生Full GC. 统计YGC发生时晋升到老年代的平均总大小大于老年代的空闲空间 在发生YGC是会判断，是否安全，这里的安全指的是，当前老年代空间可以容纳YGC晋升的对象的平均大小，如果不安全，就不会执行YGC,转而执行Full GC。 显示调用System.gc 服务器端接受多个请求时的高并发处理 多个RPC请求进来，服务器怎么处理并发呢 1.在服务器端编程处理时，可以设置一个全局的队列变量来存储从客户端传过来的参数。这个全局队列存储了多个客户端发送过来的参数，处理程序要处理时直接从这个队列中读取就可以了。服务器端接受客户端请求向共享队列中写参数作为一个线程，读取队列中的参数并进行处理作为另一个线程。两个线程独立执行来提高处理的并发性。 附：服务器的同步处理与异步处理的分析 同步服务为每个请求创建单一线程，由此线程完成整个请求的处理：接收消息，处理消息，返回数据；这种情况下服务器资源对所有入栈请求开放，服务器资源被所有入栈请求竞争使用，如果入栈请求过多就会导致服务器资源耗尽宕机，或者导致竞争加剧，资源调度频繁，服务器资源利用效率降低。 异步服务则可以分别设置两个线程队列，一个专门负责接收消息，另一个专门负责处理消息并返回数据，另有一些值守线程负责任务派发和超时监控等工作。在这种情况下无论入栈请求有多少，服务器始终依照自己的能力处理请求，服务器资源消耗始终在一个可控的范围。这种模式的一个问题就是这两个线程队列的大小如何根据机器负载情况动态调整。 对于异步服务模式： 这种情况下，虽然入栈请求以消息队列的方式被异步处理但每个请求内部却是采用阻塞的方式访问外部资源，如果外部资源访问速度过慢，可能导致请求处理队列中的所有线程均处于阻塞状态，此时CPU使用率虽然很低但是却因为队列中线程已满而无法处理消息队列中的新消息，此时若能调整线程队列最大线程数将可提高CPU利用率。但另一个问题是如果线程数被调高之后所有线程的IO处理突然结束并且接下来每个线程都将进行大量计算的话那么CPU可能出现过载。 在系统运行的每个时间点上，当时正在进行IO的线程数量和正在进行计算的线程数量是不断变化着的，那么如何才能设计出一个可以根据系统当时情况自动适应负载变化的高度自适应的系统呢？ 在这方面采用反应式计算模型确实能设计出适应负载能力很强的系统，系统利用率和吞吐量可以大幅提高，但这种系统仍然可能会出现系统局部负载过高的风险。 采用反应式计算模型，不仅系统中的入栈请求以消息队列的方式得以异步化，而且系统中所有的IO任务也必需依照此法行之，这些IO任务的处理需要采用异步模型（如NIO）。另外要考虑的就是如何划分异步IO消息并为其配置线程队列了，比如是要将所有IO任务放入统一的队列还是为某类IO任务设置单独的队列。 服务器资源虽然由系统分配但大多以线程为持有者被线程持有并使用，如线程堆栈，被线程持有的各类锁等资源。 Sentinel的工作方式: 1)：每个Sentinel以每秒钟一次的频率向它所知的Master，Slave以及其他 Sentinel 实例发送一个 PING 命令。 2)：如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel 标记为主观下线。 3)：如果一个Master被标记为主观下线，则正在监视这个Master的所有 Sentinel 要以每秒一次的频率确认Master的确进入了主观下线状态。 4)：当有足够数量的 Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态， 则Master会被标记为客观下线 。 5)：在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率向它已知的所有Master，Slave发送 INFO 命令 。 6)：当Master被 Sentinel 标记为客观下线时，Sentinel 向下线的 Master 的所有 Slave 发送 INFO 命令的频率会从 10 秒一次改为每秒一次 。 7)：若没有足够数量的 Sentinel 同意 Master 已经下线， Master 的客观下线状态就会被移除。 若 Master 重新向 Sentinel 的 PING 命令返回有效回复， Master 的主观下线状态就会被移除。 心跳检测 在命令传播阶段，从服务器默认以每秒一次的频率，向主服务器发送命令： REPLCONF ACK //replication_offset是从服务器当前的复制偏移量。 心跳检测的作用：检测主服务器的网络连接状态；辅助实现min-slaves选项；检测命令丢失。 检测主从服务器的网络连接状态 通过向主服务器发送INFO replication命令，可以列出从服务器列表，可以看出从最后一次向主发送命令距离现在过了多少秒。 lag的值应该在0或1之间跳动，如果超过1则说明主从之间的连接有故障。 辅助实现min-slaves选项 Redis可以通过配置防止主服务器在不安全的情况下执行写命令； min-slaves-to-write 3 min-slaves-max-lag 10 上面的配置表示：从服务器的数量少于3个，或者三个从服务器的延迟（lag）值都大于或等于10秒时，主服务器将拒绝执行写命令。这里的延迟值就是上面INFOreplication命令的lag值。 检测命令丢失 如果因为网络故障，主服务器传播给从服务器的写命令在半路丢失，那么当从服务器向主服务器发送REPLCONF ACK命令时，主服务器将发觉从服务器当前的复制偏移量少于自己的复制偏移量，然后主服务器就会根据从服务器提交的复制偏移量，在复制积压缓冲区里面找到从服务器缺少的数据，并将这些数据重新发送给从服务器。 主服务器向从服务器补发缺失数据这一操作的原理和部分重同步操作的原理非常相似，它们的区别在于：补发缺失数据操作在主从服务器没有断线的情况下执行，而部分重同步操作则在主从服务器断线并重连之后执行。 "},"Chapter07/face.html":{"url":"Chapter07/face.html","title":"并发面试","keywords":"","body":"面试 synchronized 的实现原理以及锁优化？ volatile 的实现原理？ Java 的信号灯？ synchronized 在静态方法和普通方法的区别？ 怎么实现所有线程在等待某个事件的发生才会去执行？ CAS？CAS 有什么缺陷，如何解决？ synchronized 和 lock 有什么区别？ Hashtable 是怎么加锁的 ？ HashMap 的并发问题？ ConcurrenHashMap 介绍？1.8 中为什么要用红黑树？ AQS 如何检测死锁？怎么预防死锁？ Java 内存模型？ 如何保证多线程下 i++ 结果正确？ 线程池的种类，区别和使用场景？ 分析线程池的实现原理和线程的调度过程？ 线程池如何调优，最大数目如何确认？ ThreadLocal原理，用的时候需要注意什么 CountDownLatch 和 CyclicBarrier 的用法，以及相互之间的差别? LockSupport工具 Condition接口及其实现原理 Fork/Join框架的理解 分段锁的原理,锁力度减小的思考 八种阻塞队列以及各个阻塞队列的特性 "},"Chapter07/ThreadLocal.html":{"url":"Chapter07/ThreadLocal.html","title":"ThreadLocal原理","keywords":"","body":"ThreadLocal原理 ThreadLocal可以理解为线程本地变量，他会在每个线程都创建一个副本，那么在线程之间访问内部副本变量就行了，做到了线程之间互相隔离，相比于synchronized的做法是用空间来换时间。 ThreadLocal有一个静态内部类ThreadLocalMap，ThreadLocalMap又包含了一个Entry数组，Entry本身是一个弱引用，他的key是指向ThreadLocal的弱引用，Entry具备了保存key value键值对的能力。 弱引用的目的是为了防止内存泄露，如果是强引用那么ThreadLocal对象除非线程结束否则始终无法被回收，弱引用则会在下一次GC的时候被回收。 但是这样还是会存在内存泄露的问题，假如key和ThreadLocal对象被回收之后，entry中就存在key为null，但是value有值的entry对象，但是永远没办法被访问到，同样除非线程结束运行。 但是只要ThreadLocal使用恰当，在使用完之后调用remove方法删除Entry对象，实际上是不会出现这个问题的。 ThreadLocal 是一个线程的本地变量，也就意味着这个变量是线程独有的，是不能与其他线程共享的，这样就可以避免资源竞争带来的多线程的问题，这种解决多线程的安全问题和lock(这里的lock 指通过synchronized 或者Lock 等实现的锁) 是有本质的区别的: lock 的资源是多个线程共享的，所以访问的时候需要加锁。 ThreadLocal 是每个线程都有一个副本，是不需要加锁的。 lock 是通过时间换空间的做法。 ThreadLocal 是典型的通过空间换时间的做法。 使用 public class ThreadLocalTest { public static void main(String[] args) { ThreadLocal threadLocal = new ThreadLocal(); threadLocal.set(\"1\"); threadLocal.get(); } } 源码分析 在分析源码之前先画一下ThreadLocal ，ThreadLocalMap 和Thread 的关系. set 方法 public void set(T value) { Thread t = Thread.currentThread(); //获取线程绑定的ThreadLocalMap ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else // 第一次设置值的时候走的这里逻辑 createMap(t, value); } createMap 方法只是在第一次设置值的时候创建一个ThreadLocalMap 赋值给Thread 对象的threadLocals 属性进行绑定，以后就可以直接通过这个属性获取到值了。从这里可以看出，为什么说ThreadLocal 是线程本地变量来的了 void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } 值真正是放在ThreadLocalMap 中存取的，ThreadLocalMap 内部类有一个Entry 类，key是ThreadLocal 对象，value 就是你要存放的值，上面的代码value 存放的就是hello word。ThreadLocalMap 和HashMap的功能类似，但是实现上却有很大的不同： HashMap 的数据结构是数组+链表 ThreadLocalMap的数据结构仅仅是数组 HashMap 是通过链地址法解决hash 冲突的问题 ThreadLocalMap 是通过开放地址法来解决hash 冲突的问题 HashMap 里面的Entry 内部类的引用都是强引用 ThreadLocalMap里面的Entry 内部类中的key 是弱引用，value 是强引用 为什么ThreadLocalMap 采用开放地址法来解决哈希冲突? jdk 中大多数的类都是采用了链地址法来解决hash 冲突，为什么ThreadLocalMap 采用开放地址法来解决哈希冲突呢？首先我们来看看这两种不同的方式 链地址法 这种方法的基本思想是将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。列如对于关键字集合{12,67,56,16,25,37, 22,29,15,47,48,34}，我们用前面同样的12为除数，进行除留余数法： 开放地址法 这种方法的基本思想是一旦发生了冲突，就去寻找下一个空的散列地址(这非常重要，源码都是根据这个特性，必须理解这里才能往下走)，只要散列表足够大，空的散列地址总能找到，并将记录存入。 比如说，我们的关键字集合为{12,33,4,5,15,25},表长为10。 我们用散列函数f(key) = key mod l0。 当计算前S个数{12,33,4,5}时，都是没有冲突的散列地址，直接存入（蓝色代表为空的，可以存放数据） 计算key = 15时，发现f(15) = 5，此时就与5所在的位置冲突。于是我们应用上面的公式f(15) = (f(15)+1) mod 10 =6。于是将15存入下标为6的位置。这其实就是房子被人买了于是买下一间的作法： 链地址法和开放地址法的优缺点 开放地址法： 容易产生堆积问题，不适于大规模的数据存储。 散列函数的设计对冲突会有很大的影响，插入时可能会出现多次冲突的现象。 删除的元素是多个冲突元素中的一个，需要对后面的元素作处理，实现较复杂。 链地址法： 处理冲突简单，且无堆积现象，平均查找长度短。 链表中的结点是动态申请的，适合构造表不能确定长度的情况。 删除结点的操作易于实现。只要简单地删去链表上相应的结点即可。 指针需要额外的空间，故当结点规模较小时，开放定址法较为节省空间。 ThreadLocalMap 采用开放地址法原因 ThreadLocal 中看到一个属性 HASH_INCREMENT = 0x61c88647 ，0x61c88647 是一个神奇的数字，让哈希码能均匀的分布在2的N次方的数组里, 即 Entry[] table，关于这个神奇的数字google 有很多解析，这里就不重复说了 ThreadLocal 往往存放的数据量不会特别大（而且key 是弱引用又会被垃圾回收，及时让数据量更小），这个时候开放地址法简单的结构会显得更省空间，同时数组的查询效率也是非常高，加上第一点的保障，冲突概率也低 弱引用 接下来我们看看ThreadLocalMap 中的存放数据的内部类Entry 的实现源码 static class Entry extends WeakReference> { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal k, Object v) { super(k); value = v; } } 我们可以知道Entry 的key 是一个弱引用，也就意味这可能会被垃圾回收器回收掉 threadLocal.get()==null 也就意味着被回收掉了 ThreadLocalMap set 方法 private void set(ThreadLocal key, Object value) { // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; //计算数组的的下标 int i = key.threadLocalHashCode & (len-1); //注意这里结束循环的条件是e！=null 这个很重要，还记得上面的开放地址法吗？ //这里遍历的逻辑是，先通过hash找到数组下标，然后寻相等的threadLocal对象 //找不到就往下一个index找 //有两种可能退出循环 //1. 找到相同的threadLocal对象 //2. 一直往数组的下一个下标查询，直到下一个下标对应的null跳出 for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal k = e.get(); //如果找到直接设置value值返回 if (k == key) { e.value = value; return; } if (k == null) { //被收回的话就需要替换掉过期的值，把新值放在这里 replaceStaleEntry(key, value, i); return; } } //到这里说明没有找到 tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) && sz >= threshold) //进行扩容 rehash(); } 还是拿上面解释开放地址法解释的例子来说明下。 比如说，我们的关键字集合为{12,33,4,5,15,25},表长为10。 我们用散列函数f(key) = key mod l0。 当计算前S个数{12,33,4,5,15,25}时，并且此时key=33,k=5 已经过期了（蓝色代表为空的，可以存放数据，红色代表key 过期，过期的key为null）： replaceStaleEntry 这个方法 private void replaceStaleEntry(ThreadLocal key, Object value, int staleSlot) { Entry[] tab = table; int len = tab.length; Entry e; // Back up to check for prior stale entry in current run. // We clean out whole runs at a time to avoid continual // incremental rehashing due to garbage collector freeing // up refs in bunches (i.e., whenever the collector runs). //这里采用的是从当前staleSlot位置向前遍历， i-- //这样的话是为了把前面所有的已经被垃圾回收的也一起释放空间 //（注意这里只是key回收，value还没有回收，entry更加没有回收，所以需要让他们回收） //同时这样也避免了存在很多过期对象的占用，导致这个时候刚好来了一个新元素达到阈值而触发一次rehash int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; // Find either the key or trailing null slot of run, whichever // occurs first //这个时候是从数组下标小的向数组下标大的方向遍历， i++，刚好和上面相反 //这俩个遍历就是为了在左边遇到第一个空的entry到右边遇到第一个空的entry之间查询所有过期对象 //注意：在右边如果找到需要设置的key，（这个例子是key-15）相同的时候开始清理，然后返回，不再继续遍历下去 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal k = e.get(); // If we find key, then we need to swap it // with the stale entry to maintain hash table order. // The newly stale slot, or any other stale slot // encountered above it, can then be sent to expungeStaleEntry // to remove or rehash all of the other entries in run. //说面之前已经存在相同的key，所以需要替换旧的值并且和前面那个过期的对象进行位置交换 // if (k == key) { e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; // Start expunge at preceding stale entry if it exists //这里的意思就是前面的第一个for循环（i--）往前查找的时候没有找到过期的，只有staleSlot这个过期 //由于前面过期对象已经通过交换位置的方式放到了index=i上，所以需要清理位置是i，而不是传过来的staleSlot if (slotToExpunge == staleSlot) slotToExpunge = i; //进行清理过期数据 cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; } // If we didn't find stale entry on backward scan, the // first stale entry seen while scanning for key is the // first still present in the run. //如果我们在第一个for循环（i--）向前遍历的时候没有找到任何过期对象 //那么我们需要把slotToExpunge设置为向后遍历（i++）的第一个过期对象的位置 //因为如果整个数组都没找到要设置的key的时候，该key会设置在改 staleSlot的位置上 //如果数组中存在要设置的key，那么上面也会通过交换位置的时候把有效值移到staleSlot位置上 //综上所述，staleSlot位置上不管怎么样，存放的都是有效值，所以不需要清除 if (k == null && slotToExpunge == staleSlot) slotToExpunge = i; } // If key not found, put new entry in stale slot //如果key在数组中没存在，那么直接新建一个新的放进去就可以 tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); //如果有其它已经过期的对象 那么需要清理它 // If there are any other stale entries in run, expunge them if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); } 第一个for 循环是向前遍历数据的，直到遍历到空的entry 就停止（这个是根据开放地址的线性探测法）,这里的例子就是遍历到index=1就停止了。 向前遍历的过程同时会找出过期的key,这个时候找到的是下标index=3 的为过期，进入到 if (e.get() == null) slotToExpunge = i; 注意此时slotToExpunge=3，staleSlot=5 第二个for 循环是从index=staleSlot开始，向后编列的，找出是否有和当前匹配的key,有的话进行清理过期的对象和重新设置当前的值。 这个例子遍历到index=6 的时候，匹配到key=15的值， 先进行数据交换，注意此时slotToExpunge=3，staleSlot=5，i=6。这里就是把5 和6 的位置的元素进行交换，并且设置新的value=new,交换后的图是这样的 为什么要交换 这里解释下为什么交换，我们先来看看如果不交换的话，经过设置值和清理过期对象，会是以下这张图 这个时候如果我们再一次设置一个key=15,value=new2 的值，通过f(15)=5,这个时候由于上次index=5是过期对象，被清空了，所以可以存在数据，那么就直接存放在这里了 你看，这样整个数组就存在两个key=15 的数据了，这样是不允许的，所以一定要交换数据 expungeStaleEntry private int expungeStaleEntry(int staleSlot) { Entry[] tab = table; int len = tab.length; // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal k = e.get(); if (k == null) { //这里设置成null方便让GC回收 e.value = null; tab[i] = null; size--; } else { //这里的主要作用是由于使用的是开放地址法，所以删除的元素是多个冲突元素中的一个，需要对后面的元素做处理 //可以简单的理解就是让后面的元素往前移动 //为什么这么做？ //主要是开放地址寻找元素的时候遇到null就停止寻找了，你前面k==null的时候已经设置了entry==null，不移动的话，那么后面的元素永远访问不了 int h = k.threadLocalHashCode & (len - 1); //他们不相等，说明经过hash是有冲突的 if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i; } 接下来我们详细模拟下整个过程 根据我们的例子，key=5,15,25 都是冲突的，并且k=5的值已经过期，经过replaceStaleEntry 方法，在进入expungeStaleEntry 方法之前，数据结构是这样的 此时传进来的参数staleSlot=6， if (k == null) { //这里设置成null方便让GC回收 e.value = null; tab[i] = null; size--; } 这个时候会把index=6设置为null,数据结构变成下面的情况 接下来我们会遍历到i=7，经过int h = k.threadLocalHashCode & (len - 1) (实际上对应我们的举例的函数int h= f(25)); 得到的h=5,而25实际存放在index=7 的位置上，这个时候我们需要从h=5的位置上重新开始编列，直到遇到空的entry 为止 int h = k.threadLocalHashCode & (len - 1); //他们不相等，说明经过hash是有冲突的 if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } 这个时候h=6，并把k=25 的值移到index=6 的位置上，同时设置index=7 为空，如下图 其实目的跟replaceStaleEntry 交换位置的原理是一样的，为了防止由于回收掉中间那个冲突的值，导致后面冲突的值没办法找到（因为e==null 就跳出循环了） ThreadLocal 内存溢出问题 通过上面的分析，我们知道expungeStaleEntry() 方法是帮助垃圾回收的，根据源码，我们可以发现 get 和set 方法都可能触发清理方法expungeStaleEntry()， 所以正常情况下是不会有内存溢出的 但是如果我们没有调用get 和set 的时候就会可能面临着内存溢出，养成好习惯不再使用的时候调用remove(),加快垃圾回收，避免内存溢出 退一步说，就算我们没有调用get 和set 和remove 方法,线程结束的时候，也就没有强引用再指向ThreadLocal 中的ThreadLocalMap了， 这样ThreadLocalMap 和里面的元素也会被回收掉，但是有一种危险是，如果线程是线程池的， 在线程执行完代码的时候并没有结束，只是归还给线程池，这个时候ThreadLocalMap 和里面的元素是不会回收掉的 InheritableThreadLocal——父线程传递本地变量到子线程的解决方式及分析 可以看出ThreadLocal是相对于每一个线程自己使用的本地变量，但是在实际的开发中，有这样的一种需求：父线程生成的变量需要传递到子线程中进行使用， 那么在使用ThreadLocal似乎就解决不了这个问题，难道这个业务就没办法使用这个本地变量了吗？ 答案肯定是否定的，ThreadLocal有一个子类InheritableThreadLocal就是为了解决这个问题而产生的， 使用这个变量就可以轻松的在子线程中依旧使用父线程中的本地变量。 InheritableThreadLocal继承自ThreadLocal，并且重写了父类的方法：createMap（),getMap(),childValue(); 其次当主线程中对该变量进行set操作的时候，和ThreadLocal一样会初始化一个ThreadLocalMap对实际的变量值进行存储，以ThreadLocal为key，值为value，如果有多个ThreadLocal变量也都是存储在这个Map中。 该Map使用的是HashMap的原理进行数据的存储，但是和ThreadLocal有一点差别，因为其覆写了createMap的方法。 再将目光转移到线程Thread类中，可以看出Thread类维护了两个成员变量，ThreadLocal以及InheritableThreadLocal，数据类型都是ThreadLocalMap.这也就解释了为什么这个变量是线程私有的。但是如果要知道为什么父子线程的变量传递，那就继续看一下源码。当我们在主线程中开一个新的子线程的时候，开始会new一个新的Thread，具体的实现也就是在这里。 public Thread(ThreadGroup group, Runnable target) { init(group, target, \"Thread-\" + nextThreadNum(), 0); } 经过调用init方法可以看出，方法中存在对InheritableThreadLocal的操作。获取的父线程也就是当前实际开辟线程的主线程。 private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) { if (name == null) { throw new NullPointerException(\"name cannot be null\"); } this.name = name; // 这里 获取的父线程也就是当前实际开辟线程的主线程。 Thread parent = currentThread(); SecurityManager security = System.getSecurityManager(); if (g == null) { /* Determine if it's an applet or not */ /* If there is a security manager, ask the security manager what to do. */ if (security != null) { g = security.getThreadGroup(); } /* If the security doesn't have a strong opinion of the matter use the parent thread group. */ if (g == null) { g = parent.getThreadGroup(); } } /* checkAccess regardless of whether or not threadgroup is explicitly passed in. */ g.checkAccess(); /* * Do we have the required permissions? */ if (security != null) { if (isCCLOverridden(getClass())) { security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); } } g.addUnstarted(); this.group = g; this.daemon = parent.isDaemon(); this.priority = parent.getPriority(); if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext(); this.target = target; setPriority(priority); if (inheritThreadLocals && parent.inheritableThreadLocals != null) //当父线程中的inheritableThreadLocal被赋值时，会将当前线程的inheritableThreadLocal变量进行createInheritedMap(),看一下这个方法的具体实现，它会继续调用ThreadLocalMap(parentMap),主要的目的是父线程的变量值赋值给子线程。这里直接改变的是Entry[],因为ThreadLocalMap只是一个类名，具体数据存储和操作是使用内部的数组搭配Hash算法和Entry内部类实现。 this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); /* Stash the specified stack size in case the VM cares */ this.stackSize = stackSize; /* Set thread ID */ tid = nextThreadID(); } 其实就是把父线程的变量复制到了子线程 InheritableThreadLocal和线程池搭配使用存在的问题： 因为线程池中是缓存使用过的线程，当线程被重复调用的时候并没有再重新初始化init()线程，而是直接使用已经创建过的线程，所以这里的值并不会被再次操作。因为实际的项目中线程池的使用频率非常高，每一次从线程池中取出线程不能够直接使用之前缓存的变量，所以要解决这一个问题，网上大部分是推荐使用alibaba的开源项目transmittable-thread-local。具体可以自行了解，但是笔者认为需要弄清楚线程池和主线程以及本地变量直接的详细关系才可以更好的对这个项目有所理解。 "},"Chapter07/AtomicReference.html":{"url":"Chapter07/AtomicReference.html","title":"AtomicReference","keywords":"","body":"AtomicReference和AtomicStampedReference AtomicReference底层： AtomicReference原子应用类，可以保证你在修改对象引用时的线程安全性，比较时可以按照偏移量进行 怎样使用AtomicReference： AtomicReference ar = new AtomicReference(); ar.set(\"hello\"); //CAS操作更新 ar.compareAndSet(\"hello\", \"hello1\"); AtomicReference的成员变量： private static final long serialVersionUID = -1848883965231344442L; //unsafe类,提供cas操作的功能 private static final Unsafe unsafe = Unsafe.getUnsafe(); //value变量的偏移地址,说的就是下面那个value,这个偏移地址在static块里初始化 private static final long valueOffset; //实际传入需要原子操作的那个类实例 private volatile V value; 类装载的时候初始化偏移地址： static { try { valueOffset = unsafe.objectFieldOffset (AtomicReference.class.getDeclaredField(\"value\")); } catch (Exception ex) { throw new Error(ex); } } compareAndSet方法： //就是调用Unsafe的cas操作,传入对象,expect值,偏移地址,需要更新的值,即可,如果更新成功,返回true,如果失败,返回false public final boolean compareAndSet(V expect, V update) { return unsafe.compareAndSwapObject(this, valueOffset, expect, update); } 对于String变量来说,必须是对象相同才视为相同,而不是字符串的内容相同就可以相同: AtomicReference ar = new AtomicReference(); ar.set(\"hello\"); System.out.println(ar.compareAndSet(new String(\"hello\"), \"hello1\"));//false 这里的compareAndSet方法即cas操作本身是原子的，但是在某些场景下会出现异常场景 此处说一下ABA问题： 什么意思呢？就是说一个线程把数据A变为了B，然后又重新变成了A。此时另外一个线程读取的时候，发现A没有变化，就误以为是原来的那个A。这就是有名的ABA问题。ABA问题会带来什么后果呢？我们举个例子。 一个小偷，把别人家的钱偷了之后又还了回来，还是原来的钱吗，你老婆出轨之后又回来，还是原来的老婆吗？ABA问题也一样，如果不好好解决就会带来大量的问题。最常见的就是资金问题，也就是别人如果挪用了你的钱，在你发现之前又还了回来。但是别人却已经触犯了法律。 public class AtomicStampedReferenceTest { private static AtomicInteger ai = new AtomicInteger(10); public static void main(String[] args) throws InterruptedException { Thread thread1 = new Thread(()->{ ai.compareAndSet(10, 11); ai.compareAndSet(11, 10); System.out.println(Thread.currentThread().getName() + \": 10 - 11 - 10\"); }, \"张三\"); Thread thread2 = new Thread(()->{ try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } boolean success = ai.compareAndSet(10, 12); }, \"李四\"); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(ai.get()); } } 如何去解决这个ABA问题呢，就是使用今天所说的AtomicStampedReference。 AtomicStampedReference与AtomicReference的区别： AtomicStampedReference它内部不仅维护了对象值，还维护了一个时间戳（我这里把它称为时间戳，实际上它可以使任何一个整数，它使用整数来表示状态值）。 当AtomicStampedReference对应的数值被修改时，除了更新数据本身外，还必须要更新时间戳。当AtomicStampedReference设置对象值时， 对象值以及时间戳都必须满足期望值，写入才会成功。因此，即使对象值被反复读写，写回原值，只要时间戳发生变化，就能防止不恰当的写入。 public class AtomicStampedReferenceTest { private static AtomicStampedReference asr = new AtomicStampedReference<>(10, 1); public static void main(String[] args) throws InterruptedException { Thread thread1 = new Thread(() -> { int version = asr.getStamp(); asr.compareAndSet(10, 11, version, version + 1); asr.compareAndSet(11, 10, version + 1, version + 2); System.out.println(Thread.currentThread().getName() + \": 10 - 11 - 10\"); }, \"张三\"); Thread thread2 = new Thread(() -> { try { int version = asr.getStamp(); TimeUnit.SECONDS.sleep(2); boolean success = asr.compareAndSet(10, 12, version, version + 1); } catch (InterruptedException e) { e.printStackTrace(); } }, \"李四\"); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(asr.getReference()); } } 源码分析 public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) { Pair current = pair; return expectedReference == current.reference && expectedStamp == current.stamp && ((newReference == current.reference && newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp))); } 刚刚这四个参数的意思已经说了，我们主要关注的就是实现，首先我们看到的就是这个Pair，因此想要弄清楚，我们再看看这个Pair是什么， private static class Pair { final T reference; final int stamp; private Pair(T reference, int stamp) { this.reference = reference; this.stamp = stamp; } static Pair of(T reference, int stamp) { return new Pair(reference, stamp); } } 在这里我们会发现Pair里面只是保存了值reference和时间戳stamp。 在compareAndSet方法中最后还调用了casPair方法，从名字就可以看到，主要是使用CAS机制更新新的值reference和时间戳stamp。我们可以进入这个方法中看看。 return UNSAFE.compareAndSwapObject(this, pairOffset, cmp, val); } AtomicReference即可解决volatile不具有原子性(i++问题) private static volatile Integer num1 = 0; private static AtomicReference ar=new AtomicReference(num1); public void dfasd111() throws InterruptedException{ for (int i = 0; i 类似i++这样的\"读-改-写\"复合操作(在一个操作序列中, 后一个操作依赖前一次操作的结果), 在多线程并发处理的时候会出现问题, 因为可能一个线程修改了变量, 而另一个线程没有察觉到这样变化, 当使用原子变量之后, 则将一系列的复合操作合并为一个原子操作,从而避免这种问题, i++=>i.incrementAndGet() 总结 其实除了AtomicStampedReference类，还有一个原子类也可以解决，就是AtomicMarkableReference，它不是维护一个版本号，而是维护一个boolean类型的标记，用法没有AtomicStampedReference灵活。因此也只是在特定的场景下使用。 "},"Chapter07/ConcurrentHashmap.html":{"url":"Chapter07/ConcurrentHashmap.html","title":"ConcurrentHashmap","keywords":"","body":"ConcurrentHashmap 多线程环境可以使用Collections.synchronizedMap同步加锁的方式，还可以使用HashTable，但是同步的方式显然性能不达标，而ConurrentHashMap更适合高并发场景使用。 ConcurrentHashmap在JDK1.7和1.8的版本改动比较大，1.7使用Segment+HashEntry分段锁的方式实现，1.8则抛弃了Segment，改为使用CAS+synchronized+Node实现，同样也加入了红黑树，避免链表过长导致性能的问题。 1.7分段锁 从结构上说，1.7版本的ConcurrentHashMap采用分段锁机制，里面包含一个Segment数组，Segment继承与ReentrantLock，Segment则包含HashEntry的数组，HashEntry本身就是一个链表的结构，具有保存key、value的能力能指向下一个节点的指针。 实际上就是相当于每个Segment都是一个HashMap，默认的Segment长度是16，也就是支持16个线程的并发写，Segment之间相互不会受到影响。 put流程 其实发现整个流程和HashMap非常类似，只不过是先定位到具体的Segment，然后通过ReentrantLock去操作而已，后面的流程我就简化了，因为和HashMap基本上是一样的。 计算hash，定位到segment，segment如果是空就先初始化 使用ReentrantLock加锁，如果获取锁失败则尝试自旋，自旋超过次数就阻塞获取，保证一定获取锁成功 遍历HashEntry，就是和HashMap一样，数组中key和hash一样就直接替换，不存在就再插入链表，链表同样 get流程 get也很简单，key通过hash定位到segment，再遍历链表定位到具体的元素上，需要注意的是value是volatile的，所以get是不需要加锁的。 1.8CAS+synchronized 1.8抛弃分段锁，转为用CAS+synchronized来实现，同样HashEntry改为Node，也加入了红黑树的实现。主要还是看put的流程 put流程 首先计算hash，遍历node数组，如果node是空的话，就通过CAS+自旋的方式初始化 如果当前数组位置是空则直接通过CAS自旋写入数据 如果hash==MOVED，说明需要扩容，执行扩容 如果都不满足，就使用synchronized写入数据，写入数据同样判断链表、红黑树，链表写入和HashMap的方式一样，key hash一样就覆盖，反之就尾插法，链表长度超过8就转换成红黑树 get查询 get很简单，通过key计算hash，如果key hash相同就返回，如果是红黑树按照红黑树获取，都不是就遍历链表获取 "},"Chapter07/Future.html":{"url":"Chapter07/Future.html","title":"Callable/Future 使用及原理分析","keywords":"","body":"Callable/Future 使用及原理分析 线程池的执行任务有两种方法，一种是 submit、一种是 execute；这两个方法是有区别的，那么基于这个区别我们再来看看。 execute 和 submit 区别 execute 只可以接收一个 Runnable 的参数 execute 如果出现异常会抛出 execute 没有返回值 submit 可以接收 Runable 和 Callable 这两种类型的参数， 对于 submit 方法，如果传入一个 Callable，可以得到一个 Future 的返回值 submit 方法调用不会抛异常，除非调用 Future.get 这里，我们重点了解一下 Callable/Future，可能很多同学知道他是一个带返回值的线程，但是具体的实现可能不清楚。 Callable/Future 案例演示 Callable/Future 和 Thread 之类的线程构建最大的区别在于，能够很方便的获取线程执行完以后的结果。首先来看一个简单的例子 public class CallableDemo implements Callable { public String call() throws Exception { Thread.sleep(3000);//阻塞案例演示 return \"hello world\"; } public static void main(String[] args) throws ExecutionException, InterruptedException { CallableDemo callableDemo = new CallableDemo(); FutureTask futureTask = new FutureTask(callableDemo); new Thread(futureTask).start(); System.out.println(futureTask.get()); } } 想一想我们为什么需要使用回调呢？那是因为结果值是由另一线程计算的，当前线程是不知道结果值什么时候计算完成，所以它传递一个回调接口给计算线程，当计算完成时，调用这个回调接口，回传结果值。 这个在很多地方有用到，比如 Dubbo 的异步调用，比如消息中间件的异步通信等等… 利用 FutureTask、 Callable、 Thread 对耗时任务（如查询数据库）做预处理，在需要计算结果之前就启动计算。 所以我们来看一下 Future/Callable 是如何实现的 Callable/Future 原理分析 在刚刚实现的 demo 中，我们用到了两个 api，分别是 Callable 和 FutureTask。Callable 是一个函数式接口，里面就只有一个 call 方法。子类可以重写这个方法，并且这个方法会有一个返回值 @FunctionalInterface public interface Callable { /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception; } FutureTask FutureTask 的类关系图如下，它实现 RunnableFuture 接口，那么这个 RunnableFuture 接口的作用是什么呢。 在讲解 FutureTask 之前，先看看 Callable, Future, FutureTask 它们之间的关系图，如下： public interface RunnableFuture extends Runnable, Future { /** * Sets this Future to the result of its computation * unless it has been cancelled. */ void run(); } RunnableFuture 是一个接口，它继承了 Runnable 和 Future 这两个接口， Runnable 太熟悉了， 那么 Future 是什么呢？ Future 表示一个任务的生命周期，并提供了相应的方法来判断是否已经完成或取消，以及获取任务的结果和取消任务等。 public interface Future { boolean cancel(boolean mayInterruptIfRunning); // 当前的 Future 是否被取消，返回 true 表示已取消 boolean isCancelled(); // 当前 Future 是否已结束。包括运行完成、抛出异常以及取消，都表示当前 Future 已结束 boolean isDone(); // 获取 Future 的结果值。如果当前 Future 还没有结束，那么当前线程就等待， // 直到 Future 运行结束，那么会唤醒等待结果值的线程的。 V get() throws InterruptedException, ExecutionException; // 获取 Future 的结果值。与 get()相比较多了允许设置超时时间 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; } 分析到这里我们其实有一些初步的头绪了， FutureTask 是 Runnable 和 Future 的结合，如果我们把 Runnable 比作是生产者， Future 比作是消费者，那么 FutureTask 是被这两者共享的，生产者运行 run 方法计算结果，消费者通过 get 方法获取结果。 作为生产者消费者模式，有一个很重要的机制，就是如果生产者数据还没准备的时候，消费者会被阻塞。当生产者数据准备好了以后会唤醒消费者继续执行。 这个有点像我们上次可分析的阻塞队列，那么在 FutureTask 里面是基于什么方式实现的呢？ state 的含义 表示 FutureTask 当前的状态，分为七种状态 private static final int NEW = 0; // NEW 新建状态，表示这个 FutureTask还没有开始运行 // COMPLETING 完成状态， 表示 FutureTask 任务已经计算完毕了 // 但是还有一些后续操作，例如唤醒等待线程操作，还没有完成。 private static final int COMPLETING = 1; // FutureTask 任务完结，正常完成，没有发生异常 private static final int NORMAL = 2; // FutureTask 任务完结，因为发生异常。 private static final int EXCEPTIONAL = 3; // FutureTask 任务完结，因为取消任务 private static final int CANCELLED = 4; // FutureTask 任务完结，也是取消任务，不过发起了中断运行任务线程的中断请求 private static final int INTERRUPTING = 5; // FutureTask 任务完结，也是取消任务，已经完成了中断运行任务线程的中断请求 private static final int INTERRUPTED = 6; run 方法 public void run() { // 如果状态 state 不是 NEW，或者设置 runner 值失败 // 表示有别的线程在此之前调用 run 方法，并成功设置了 runner 值 // 保证了只有一个线程可以运行 try 代码块中的代码。 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try { Callable c = callable; //只有c不为null且状态state为NEW的情况 if (c != null && state == NEW) { V result; boolean ran; try { //调用callable的call方法，并获得返回结果 result = c.call(); //运行成功 ran = true; } catch (Throwable ex) { result = null; ran = false; setException(ex); } if (ran) //设置结果 set(result); } } finally { // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s >= INTERRUPTING) handlePossibleCancellationInterrupt(s); } } 其实 run 方法作用非常简单，就是调用 callable 的 call 方法返回结果值 result，根据是否发生异常，调用 set(result)或 setException(ex)方法表示 FutureTask 任务完结。 不过因为 FutureTask 任务都是在多线程环境中使用，所以要注意并发冲突问题。注意在 run方法中，我们没有使用 synchronized 代码块或者 Lock 来解决并发问题，而是使用了 CAS 这个乐观锁来实现并发安全，保证只有一个线程能运行 FutureTask 任务。 get 方法 get 方法就是阻塞获取线程执行结果，这里主要做了两个事情 判断当前的状态，如果状态小于等于 COMPLETING，表示 FutureTask 任务还没有完结，所以调用 awaitDone 方法，让当前线程等待。 report 返回结果值或者抛出异常 public V get() throws InterruptedException, ExecutionException { int s = state; if (s awaitDone 如果当前的结果还没有被执行完，把当前线程线程和插入到等待队列 private int awaitDone(boolean timed, long nanos) throws InterruptedException { final long deadline = timed ? System.nanoTime() + nanos : 0L; FutureTask.WaitNode q = null; boolean queued = false;// 节点是否已添加 for (;;) { // 如果当前线程中断标志位是 true， // 那么从列表中移除节点 q，并抛出 InterruptedException 异常 if (Thread.interrupted()) { removeWaiter(q); throw new InterruptedException(); } int s = state; // 当状态大于 COMPLETING 时，表示FutureTask任务已结束 if (s > COMPLETING) { if (q != null) // 将节点 q 线程设置为 null，因为线程没有阻塞等待 q.thread = null; return s; } // 表示还有一些后序操作没有完成，那么当前线程让出执行权 else if (s == COMPLETING) // cannot time out yet Thread.yield(); //表示状态是 NEW，那么就需要将当前线程阻塞等待。 // 就是将它插入等待线程链表中 else if (q == null) q = new FutureTask.WaitNode(); // 使用 CAS 函数将新节点添加到链表中，如果添加失败，那么queued 为 false， // 下次循环时，会继续添加，知道成功。 else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); // timed 为 true 表示需要设置超时 else if (timed) { nanos = deadline - System.nanoTime(); if (nanos 被阻塞的线程，会等到 run 方法执行结束之后被唤醒 report report 方法就是根据传入的状态值 s，来决定是抛出异常，还是返回结果值。 这个两种情况都表示 FutureTask 完结了 private V report(int s) throws ExecutionException { Object x = outcome;//表示 call 的返回值 if (s == NORMAL) // 表示正常完结状态，所以返回结果值 return (V)x; // 大于或等于 CANCELLED，都表示手动取消 FutureTask 任务， // 所以抛出 CancellationException 异常 if (s >= CANCELLED) throw new CancellationException(); // 否则就是运行过程中，发生了异常，这里就抛出这个异常 throw new ExecutionException((Throwable)x); } 线程池对于 Future/Callable 的执行 我们现在再来看线程池里面的 submit 方法，就会很清楚了。 public class CallableDemo implements Callable { public String call() throws Exception { Thread.sleep(3000);//阻塞案例演示 return \"hello world\"; } public static void main(String[] args) throws ExecutionException, InterruptedException { ExecutorService es= Executors.newFixedThreadPool(1); CallableDemo callableDemo = new CallableDemo(); FutureTask futureTask = new FutureTask(callableDemo); Future future=es.submit(callableDemo); System.out.println(futureTask.get()); } } AbstractExecutorService.submit 调用抽象类中的 submit 方法，这里其实相对于 execute 方法来说，只多做了一步操作，就是封装了一个 RunnableFuture public Future submit(Callable task) { if (task == null) throw new NullPointerException(); RunnableFuture ftask = newTaskFor(task); execute(ftask); return ftask; } newTaskFor更简单 protected RunnableFuture newTaskFor(Callable callable) { return new FutureTask(callable); } 然后调用 execute 方法，这里面的逻辑前面分析过了，会通过 worker 线程来调用过 ftask 的run 方法。而这个 ftask 其实就是 FutureTask 里面最终实现的逻辑。 Future .get()为啥能等待呢？ "},"Chapter08/plugin.html":{"url":"Chapter08/plugin.html","title":"Part VIII 中间件篇","keywords":"","body":"第八章 中间件 RabbitMq Kafka Zookeeper Redis高并发 Redis 6.0之前为什么一直不使用多线程？ 为什么使用nginx？ Tomcat如何接收请求 "},"Chapter08/Master.html":{"url":"Chapter08/Master.html","title":"分布式集群中为什么会有 Master","keywords":"","body":"分布式集群中为什么会有 Master 在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机 器可以共享这个结果， 这样可以大大减少重复计算，提高性能，于是就需要进行leader 选举。 "},"Chapter08/RabbitmqInstallation.html":{"url":"Chapter08/RabbitmqInstallation.html","title":"RabbitMq","keywords":"","body":"RabbitMq的安装 "},"Chapter08/KafkaInstallation.html":{"url":"Chapter08/KafkaInstallation.html","title":"Kafka","keywords":"","body":"Kafka 安装 http://kafka.apache.org/quickstart 测试 配置端口：2182 管理目录： /opt/soft/kafka/kafka_2.12-2.3.0 启动命令：/opt/soft/kafka/kafka_2.12-2.3.0/bin/kafka-server-start.sh /opt/soft/kafka/kafka_2.12-2.3.0/config/server.properties & 异常 Exception in thread \"main\" java.lang.UnsupportedClassVersionError: kafka/Kafka : Unsupported major.minor version 52.0 at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631) at java.lang.ClassLoader.defineClass(ClassLoader.java:615) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141) at java.net.URLClassLoader.defineClass(URLClassLoader.java:283) at java.net.URLClassLoader.access$000(URLClassLoader.java:58) at java.net.URLClassLoader$1.run(URLClassLoader.java:197) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) Could not find the main class: kafka.Kafka. Program will exit. 原因很明显是jdk版本不对 修改/opt/soft/kafka/kafka_2.12-2.3.0/bin/kafka-server-start.sh 在顶部添加如下值： ```shell script export JAVA_HOME=\"/opt/soft/jdk/jdk1.8.0_65/\" ###### 单机模式 Kafka使用ZooKeeper，因此如果您还没有ZooKeeper服务器，则需要先启动它。 您可以使用与kafka一起打包的便捷脚本来获得快速且脏的单节点ZooKeeper实例 ```shell script > bin/zookeeper-server-start.sh config/zookeeper.properties [2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig) ... 现在启动Kafka服务器 ```shell script bin/kafka-server-start.sh config/server.properties [2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties) [2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties) ``` 集群模式 修改zookeeper配置 /opt/soft/kafka/kafka_2.12-2.3.0/config/server.properties 旧值 zookeeper.connect=localhost:2181 # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 新值 # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. zookeeper.connect=192.168.66.8:2182 # Timeout in ms for connecting to zookeeper zookeeper.connection.timeout.ms=6000 注意 server.properties里broker.id的值;不同的实例应该设置不同的值 "},"Chapter08/ZookeeperInstallation.html":{"url":"Chapter08/ZookeeperInstallation.html","title":"Zookeeper","keywords":"","body":"ZooKeeper 安装 https://zookeeper.apache.org/doc/r3.5.5/zookeeperStarted.html 测试 配置端口：2182 管理目录： /opt/soft/zookeeper/zookeeper-3.4.14 配置文件：/opt/soft/zookeeper/zookeeper-3.4.14/conf/zoo.cfg 启动命令: /opt/soft/zookeeper/zookeeper-3.4.14/bin/zkServer.sh start 停止命令: /opt/soft/zookeeper/zookeeper-3.4.14/bin/zkServer.sh stop 连接命令：/opt/soft/zookeeper/zookeeper-3.4.14/bin/zkCli.sh -server 127.0.0.1:2182 "},"Chapter08/nginx.html":{"url":"Chapter08/nginx.html","title":"为什么使用nginx？","keywords":"","body":"nginx 为什么使用nginx？ 限制其托管应用程序的对外公开部分 提供附加的配置和防御层 方便与现有基础架构更好地集成 简化负载平衡和安全通信（HTTPS）配置。只有反向代理服务器需要 X.509 证书，并且该服务器可以使用 HTTP 与内部网络上的应用服务器进行通信。 Nginx 的进程模型 Nginx 服务器，正常运行过程中： 多进程：一个 Master 进程、多个 Worker 进程 Master 进程：管理 Worker 进程 对外接口：接收外部的操作（信号） 对内转发：根据外部的操作的不同，通过信号管理 Worker 监控：监控 worker 进程的运行状态，worker 进程异常终止后，自动重启 worker 进程 Worker 进程：所有 Worker 进程都是平等的 实际处理：网络请求，由 Worker 进程处理； Worker 进程数量：在 nginx.conf 中配置，一般设置为核心数，充分利用 CPU 资源，同时，避免进程数量过多，避免进程竞争 CPU 资源，增加上下文切换的损耗。 思考 请求是连接到 Nginx，Master 进程负责处理和转发？ 如何选定哪个 Worker 进程处理请求？请求的处理结果，是否还要经过 Master 进程？ HTTP 连接建立和请求处理过程 Nginx 启动时，Master 进程，加载配置文件 Master 进程，初始化监听的 socket Master 进程，fork 出多个 Worker 进程 Worker 进程，竞争新的连接，获胜方通过三次握手，建立 Socket 连接，并处理请求 Nginx 高性能、高并发 Nginx 采用：多进程 + 异步非阻塞方式（IO 多路复用 epoll） 请求的完整过程： 建立连接 读取请求：解析请求 处理请求 响应请求 请求的完整过程，对应到底层，就是：读写 socket 事件 Nginx 的事件处理模型 request：Nginx 中 http 请求。 基本的 HTTP Web Server 工作模式： 接收请求：逐行读取请求行和请求头，判断段有请求体后，读取请求体 处理请求 返回响应：根据处理结果，生成相应的 HTTP 请求（响应行、响应头、响应体） Nginx 也是这个套路，整体流程一致。 模块化体系结构 nginx的模块根据其功能基本上可以分为以下几种类型： event module: 搭建了独立于操作系统的事件处理机制的框架，及提供了各具体事件的处理。包括ngx_events_module， ngx_event_core_module和ngx_epoll_module等。nginx具体使用何种事件处理模块，这依赖于具体的操作系统和编译选项。 phase handler: 此类型的模块也被直接称为handler模块。主要负责处理客户端请求并产生待响应内容，比如ngx_http_static_module模块，负责客户端的静态页面请求处理并将对应的磁盘文件准备为响应内容输出。 output filter: 也称为filter模块，主要是负责对输出的内容进行处理，可以对输出进行修改。例如，可以实现对输出的所有html页面增加预定义的footbar一类的工作，或者对输出的图片的URL进行替换之类的工作。 upstream: upstream模块实现反向代理的功能，将真正的请求转发到后端服务器上，并从后端服务器上读取响应，发回客户端。upstream模块是一种特殊的handler，只不过响应内容不是真正由自己产生的，而是从后端服务器上读取的。 load-balancer: 负载均衡模块，实现特定的算法，在众多的后端服务器中，选择一个服务器出来作为某个请求的转发服务器。 IO 多路复用 和 多线程 的适用场景？ IO 多路复用：单个连接的请求处理速度没有优势，适合 IO 密集型 场景，事件驱动 大并发量：只使用一个线程，处理大量的并发请求，降低上下文环境切换损耗，也不需要考虑并发问题，相对可以处理更多的请求； 消耗更少的系统资源（不需要线程调度开销） 适用于长连接的情况（多线程模式长连接容易造成线程过多，造成频繁调度） 阻塞IO + 多线程：实现简单，可以不依赖系统调用，适合 CPU 密集型 场景 每个线程，都需要时间和空间； 线程数量增长时，线程调度开销指数增长 Nginx 最大连接数 基础背景： Nginx 是多进程模型，Worker 进程用于处理请求； 单个进程的连接数（文件描述符 fd），有上限（nofile）：ulimit -n Nginx 上配置单个 worker 进程的最大连接数：worker_connections 上限为 nofile Nginx 上配置 worker 进程的数量：worker_processes 因此，Nginx 的最大连接数： Nginx 的最大连接数：Worker 进程数量 x 单个 Worker 进程的最大连接数 上面是 Nginx 作为通用服务器时，最大的连接数 Nginx 作为反向代理服务器时，能够服务的最大连接数：（Worker 进程数量 x 单个 Worker 进程的最大连接数）/ 2。 Nginx 反向代理时，会建立 Client 的连接和后端 Web Server 的连接，占用 2 个连接 思考： 每打开一个 socket 占用一个 fd 为什么，一个进程能够打开的 fd 数量有限制？ IO 模型 场景： 处理多个请求时，可以采用：IO 多路复用 或者 阻塞 IO +多线程 IO 多路服用：一个 线程，跟踪多个 socket 状态，哪个就绪，就读写哪个； 阻塞 IO + 多线程：每一个请求，新建一个服务线程 思考：IO 多路复用 和 多线程 的适用场景？ IO 多路复用：单个连接的请求处理速度没有优势 大并发量：只使用一个线程，处理大量的并发请求，降低上下文环境切换损耗，也不需要考虑并发问题，相对可以处理更多的请求； 消耗更少的系统资源（不需要线程调度开销） 适用于长连接的情况（多线程模式长连接容易造成线程过多，造成频繁调度） 阻塞IO + 多线程：实现简单，可以不依赖系统调用。 每个线程，都需要时间和空间； 线程数量增长时，线程调度开销指数增长 select/poll 和 epoll 比较 详细内容，参考： select poll epoll三者之间的比较 select/poll 系统调用： // select 系统调用 int select(int maxfdp,fd_set *readfds,fd_set *writefds,fd_set *errorfds,struct timeval *timeout); // poll 系统调用 int poll(struct pollfd fds[], nfds_t nfds, int timeout)； select 查询 fd_set 中，是否有就绪的 fd，可以设定一个超时时间，当有 fd (File descripter) 就绪或超时返回； fd_set 是一个位集合，大小是在编译内核时的常量，默认大小为 1024 特点： 连接数限制，fd_set 可表示的 fd 数量太小了； 线性扫描：判断 fd 是否就绪，需要遍历一边 fd_set； 数据复制：用户空间和内核空间，复制连接就绪状态信息 poll： 解决了连接数限制： poll 中将 select 中的 fd_set 替换成了一个 pollfd 数组 解决 fd 数量过小的问题 数据复制：用户空间和内核空间，复制连接就绪状态信息 epoll： event 事件驱动 事件机制：避免线性扫描 为每个 fd，注册一个监听事件 fd 变更为就绪时，将 fd 添加到就绪链表 fd 数量：无限制（OS 级别的限制，单个进程能打开多少个 fd） select，poll，epoll： I/O多路复用的机制； I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。 监视多个文件描述符 但select，poll，epoll本质上都是同步I/O： 用户进程负责读写（从内核空间拷贝到用户空间），读写过程中，用户进程是阻塞的； 异步 IO，无需用户进程负责读写，异步IO，会负责从内核空间拷贝到用户空间； Nginx 的并发处理能力 关于 Nginx 的并发处理能力： 并发连接数，一般优化后，峰值能保持在 1~3w 左右。（内存和 CPU 核心数不同，会有进一步优化空间） "},"Chapter08/Tomcat.html":{"url":"Chapter08/Tomcat.html","title":"Tomcat如何接收请求","keywords":"","body":"Tomcat 在阅读tomcat源码之前，有必要了解一下tomcat的多层容器设计，这样读起来会省很多力气。 上图就是tomcat的基本结构，可以很明显的看出层层嵌套的架构设计。简易启动一个tomcat的代码如下: public static void main(String[] args) throws LifecycleException { Tomcat tomcat = new Tomcat(); //设置路径 tomcat.setBaseDir(\"d:tomcat/dir\"); Connector connector = new Connector(); //设置端口 connector.setPort(8080); tomcat.getService().addConnector(connector); Context context = new StandardContext(); //设置context路径 context.setPath(\"\"); context.addLifecycleListener(new Tomcat.FixContextListener()); tomcat.getHost().addChild(context); //添加servlet tomcat.addServlet(\"\", \"homeServlet\", new HomeServlet()); //设置servlet路径 context.addServletMappingDecoded(\"/\", \"homeServlet\"); tomcat.start(); tomcat.getServer().await(); } 下面就来分析一下这段启动代码的逻辑。首先进入tomcat的start方法。 public void start() throws LifecycleException { getServer(); server.start(); } 可以看到先是通过getServer方法判断是否存在server容器。如果不存在则新建。server容器就是结构图的最外层容器了。随后进入server的start方法。进入之后会发现server继承了LifecycleBase这个抽象类，而LifecycleBase实现了Lifecycle接口。这个接口就是控制容器生命周期的接口。查看接口方法可以看到有init，start和stop方法。我们先看看LifecycleBase的start方法 public final synchronized void start() throws LifecycleException { ... if (state.equals(LifecycleState.NEW)) { init(); } ... try { ... startInternal(); ... } catch (Throwable t) { ... } } 可以看到在start方法中，根据容器的状态控制了容器的启动流程。在启动时候任一点出错的话，可以安全的退出。这样的话每个容器就只需要关注自己的init方法和startInternal方法就可以了。现在查看一下server的startInternal方法。 protected void startInternal() throws LifecycleException { ... // Start our defined Services synchronized (servicesLock) { for (int i = 0; i 可以看到server的startInternal方法主要任务就是启动service。这也正好是开始那个tomcat结构图的结构，外层控制内层容器的启动。接下来看看service容器的启动。直接查看StandardService的init方法和start方法。 protected void initInternal() throws LifecycleException { if (engine != null) { engine.init(); } ... mapperListener.init(); synchronized (connectorsLock) { for (Connector connector : connectors) { connector.init(); } } } 在service的init方法中，首先初始化了engine，然后初始化mapper，最后初始化connector。这也很好理解，因为只有内部启动之后才能对外提供服务。 接下来先看看engine的启动，在看engine启动前，需要先了解下engine里面各层容器的作用。 Context 表示一个 Web 应用程序；Wrapper 表示一个 Servlet，一个 Web 应用程序中可能会有多个 Servlet；Host 代表的是一个虚拟主机，或者说一个站点，可以给 Tomcat 配置多个虚拟主机地址，而一个虚拟主机下可以部署多个 Web 应用程序；Engine 表示引擎，用来管理多个虚拟站点，一个 Service 最多只能有一个 Engine。 他们对外提供服务的路由是这样的。 在搞清楚这层关系之后，再看engine的启动就很简单了。engine管理的几个容器host，context和wrapper比之前多了一个继承就是 ContainerBase。他们的实现都继承了这个类。所以要研究的话直接看下这个类的实现。 在ContainerBase里，子类是以map的形式存在hash表中 protected final HashMap children = new HashMap<>(); 看一下ContainerBase的start方法。 protected synchronized void startInternal() throws LifecycleException { ... Container children[] = findChildren(); List> results = new ArrayList<>(); for (int i = 0; i result : results) { try { result.get(); } catch (Throwable e) { ... } } ... } 可以看到在父容器的启动方法中，逐个的启动了子类容器。这样的话tomcat的大体启动流程就差不多了解了。接下来要做的就是根据具体功能，分析单个组件。 Tomcat如何接收请求 创建过程 首先看一下Connector的创建过程。 Connector connector = new Connector(); public Connector() { //默认的连接器协议是nio的http 1.1协议 this(\"org.apache.coyote.http11.Http11NioProtocol\"); //创建过程只是简单调用一下构造函数 } //所以可以直接看Http11NioProtocol的创建过程 public Http11NioProtocol() { super(new NioEndpoint()); } //可以看到在Http11NioProtocol中，自己新建了一个EndPoint这也正好对应了上一节tomcat的结构图 启动过程 创建过程看完了下面来看看启动过程，因为上一节已经说过了tomcat的容器结构，所以启动过程可以直接从Connector的init方法开始 protected void initInternal() throws LifecycleException { ... try { protocolHandler.init(); } catch (Exception e) { ... } } Connector的init方法，除了设置一些初始化值外，就是调用protocolHandler的init方法了。所以查看protocolHandler的init方法， //查看AbstractProtocol中的init方法 public void init() throws Exception { ... String endpointName = getName(); endpoint.setName(endpointName.substring(1, endpointName.length()-1)); endpoint.setDomain(domain); endpoint.init(); } 在进行了一些初始化设置之后调用了endpoint的init方法。进入endpoint查看 //进入AbstractEndpoint的init方法 public final void init() throws Exception { if (bindOnInit) { bindWithCleanup(); bindState = BindState.BOUND_ON_INIT; } ... } private void bindWithCleanup() throws Exception { try { bind(); } catch (Throwable t) { ... } } public void bind() throws Exception { //初始化ServerSocket initServerSocket(); ... selectorPool.open(); } //这边可以看到初始化ServerSocket的方法，还有初始化了一个selector，但是这个连接器不是在接收连接时候用的，暂时先放一放 protected void initServerSocket() throws Exception { if (!getUseInheritedChannel()) { //绑定端口的方法和我们平时开发都一样。 serverSock = ServerSocketChannel.open(); socketProperties.setProperties(serverSock.socket()); InetSocketAddress addr = new InetSocketAddress(getAddress(), getPortWithOffset()); serverSock.socket().bind(addr,getAcceptCount()); } else { ... } serverSock.configureBlocking(true); //mimic APR behavior } init过程到这个就结束了，这个时候端口就已经绑定了，下一步去看看接收的socket如何处理。 查看Connector的start方法。 protected void startInternal() throws LifecycleException { //... try { protocolHandler.start(); } catch (Exception e) { ... } } public void start() throws Exception { //... endpoint.start(); //... } public final void start() throws Exception { if (bindState == BindState.UNBOUND) { //刚刚init的时候已经绑定了，所以这边不会在绑定一次 bindWithCleanup(); bindState = BindState.BOUND_ON_START; } //直接会进入start方法 startInternal(); } public void startInternal() throws Exception { if (!running) { running = true; paused = false; //一些缓存类的初始化 processorCache = new SynchronizedStack<>(SynchronizedStack.DEFAULT_SIZE, socketProperties.getProcessorCache()); eventCache = new SynchronizedStack<>(SynchronizedStack.DEFAULT_SIZE, socketProperties.getEventCache()); nioChannels = new SynchronizedStack<>(SynchronizedStack.DEFAULT_SIZE, socketProperties.getBufferPool()); // Create worker collection if ( getExecutor() == null ) { //初始化线程池，这个待会再说 createExecutor(); } initializeConnectionLatch(); //先看一下Poller线程和Acceptor线程 pollers = new Poller[getPollerThreadCount()]; for (int i=0; i 在endpoint启动的过程中，我们看到他启动了两个不同名称的线程一个叫Poller，一个叫Acceptor。这两个线程从名字也可以看出，一个是接收socket的，另一个是分发任务的。分别查看他们代码。 //虽然启动是先启动poller，但是我们需要先查看acceptor public Acceptor(AbstractEndpoint endpoint) { //初始化放入endpoint this.endpoint = endpoint; } //因为继承了Runnable类，所以查看run方法 public void run() { int errorDelay = 0; while (endpoint.isRunning()) { //... try { //如果达到最大连接就阻塞 endpoint.countUpOrAwaitConnection(); //... try { //... //接收新建立的socket socket = endpoint.serverSocketAccept(); } catch (Exception ioe) { //... } //... if (endpoint.isRunning() && !endpoint.isPaused()) { //... //在set方法中新socket会被推给poller处理 if (!endpoint.setSocketOptions(socket)) { endpoint.closeSocket(socket); } } else { endpoint.destroySocket(socket); } } catch (Throwable t) { ... } } state = AcceptorState.ENDED; } acceptor的方法主要分为几步。 判断连接数量是否超过限制 新建立连接并设置相关属性 将接收到的连接推给poller 下面接着看是怎么推送给poller的 protected boolean setSocketOptions(SocketChannel socket) { try { //...设置一些属性值 //在这边会吧任务注册到poller getPoller0().register(channel); } catch (Throwable t) { //... } return true; } public void register(final NioChannel socket) { //注册的过程也很简单，往poller的队列里添加了一个任务 addEvent(r); } 接下来就看看poller怎么处理了 //因为poller也是实现了Runnable接口，所以也直接查看run方法 public void run() { // Loop until destroy() is called while (true) { boolean hasEvents = false; try { if (!close) { 处理任务队列的任务 hasEvents = events(); ... } if (close) { ... } } catch (Throwable x) { ... } Iterator iterator = keyCount > 0 ?selector.selectedKeys().iterator() : null; while (iterator != null && iterator.hasNext()) { SelectionKey sk = iterator.next(); NioSocketWrapper attachment = (NioSocketWrapper)sk.attachment(); if (attachment == null) { iterator.remove(); } else { iterator.remove(); processKey(sk, attachment); } } timeout(keyCount,hasEvents); } getStopLatch().countDown(); poller任务分两部，一是处理任务队列，二是处理注册的socket 先看处理任务队列 public boolean events() { boolean result = false; PollerEvent pe = null; for (int i = 0, size = events.size(); i 此时刚刚accpetor推送过来的socket已经被注册到poller上了。接下来看看poller对socket的处理 protected void processKey(SelectionKey sk, NioSocketWrapper attachment) { try { if ( close ) { cancelledKey(sk); } else if ( sk.isValid() && attachment != null ) { if (sk.isReadable() || sk.isWritable() ) { if ( attachment.getSendfileData() != null ) { processSendfile(sk,attachment, false); } else { unreg(sk, attachment, sk.readyOps()); boolean closeSocket = false; //主要看看processSocket方法 if (sk.isReadable()) { if (!processSocket(attachment, SocketEvent.OPEN_READ, true)) { closeSocket = true; } } if (!closeSocket && sk.isWritable()) { if (!processSocket(attachment, SocketEvent.OPEN_WRITE, true)) { closeSocket = true; } } if (closeSocket) { cancelledKey(sk); } } } } else { //invalid key cancelledKey(sk); } } catch ( CancelledKeyException ckx ) { ... } } public boolean processSocket(SocketWrapperBase socketWrapper, SocketEvent event, boolean dispatch) { try { if (socketWrapper == null) { return false; } //封装了socket任务 SocketProcessorBase sc = processorCache.pop(); if (sc == null) { sc = createSocketProcessor(socketWrapper, event); } else { sc.reset(socketWrapper, event); } //将socket任务丢给线程池执行。 Executor executor = getExecutor(); if (dispatch && executor != null) { executor.execute(sc); } else { sc.run(); } } catch (RejectedExecutionException ree) { ... } catch (Throwable t) { ... } return true; } 分析到这儿的话，tomcat对于接收连接的处理就差不多了。我们已经了解了acceptor和poller是如何协作的。最后在看看tomcat中线程池。 回到刚刚线程池创建的地方 public void createExecutor() { internalExecutor = true; //这个队列就是对LinkedBlockingQueue的简单封装 TaskQueue taskqueue = new TaskQueue(); TaskThreadFactory tf = new TaskThreadFactory(getName() + \"-exec-\", daemon, getThreadPriority()); //线程池也是对于jdk线程池的封装，不同的是在启动是，就已创建好了全部核心线程。 executor = new ThreadPoolExecutor(getMinSpareThreads(), getMaxThreads(), 60, TimeUnit.SECONDS,taskqueue, tf); taskqueue.setParent( (ThreadPoolExecutor) executor); } public void execute(Runnable command, long timeout, TimeUnit unit) { submittedCount.incrementAndGet(); try { super.execute(command); } catch (RejectedExecutionException rx) { //在使用tomcat线程池时候执行上面有这个逻辑 if (super.getQueue() instanceof TaskQueue) { final TaskQueue queue = (TaskQueue)super.getQueue(); try { //如果初次提交任务被拒绝，则会调用TaskQueue的force方法在尝试一次，如果还是失败的话才会抛出异常。 if (!queue.force(command, timeout, unit)) { submittedCount.decrementAndGet(); throw new RejectedExecutionException(sm.getString(\"threadPoolExecutor.queueFull\")); } } catch (InterruptedException x) { submittedCount.decrementAndGet(); throw new RejectedExecutionException(x); } } else { submittedCount.decrementAndGet(); throw rx; } } } "},"Chapter08/TomcatWork.html":{"url":"Chapter08/TomcatWork.html","title":"tomcat的三种工作模式","keywords":"","body":"tomcat的三种工作模式 Tomcat 的连接器有两种：HTTP和AJP AJP(Apache JServ Protocol): AJP是面向数据包的基于TCP/IP的协议，它在Apache和Tomcat的实例之间提供了一个专用的通信信道 主要有以下特征： 1) 在快速网络有着较好的性能表现，支持数据压缩传输； 2) 支持SSL，加密及客户端证书； 3) 支持Tomcat实例集群； 4) 支持在apache和tomcat之间的连接的重用； Tomcat Connector(连接器)有三种运行模式：bio nio apr 一、bio(blocking I/O) 即阻塞式I/O操作，表示Tomcat使用的是传统的Java I/O操作(即java.io包及其子包)。是基于JAVA的HTTP/1.1连接器，Tomcat7以下版本在默认情况下是以bio模式运行的。一般而言，bio模式是三种运行模式中性能最低的一种。我们可以通过Tomcat Manager来查看服务器的当前状态。（Tomcat7 或以下，在 Linux 系统中默认使用这种方式） 一个线程处理一个请求，缺点：并发量高时，线程数较多，浪费资源 二、nio(new I/O) 是Java SE 1.4及后续版本提供的一种新的I/O操作方式(即java.nio包及其子包)。Java nio是一个基于缓冲区、并能提供非阻塞I/O操作的Java API，因此nio也被看成是non-blocking I/O的缩写。它拥有比传统I/O操作(bio)更好的并发运行性能。要让Tomcat以nio模式来运行只需要在Tomcat安装目录/conf/server.xml 中将对应的中protocol的属性值改为 org.apache.coyote.http11.Http11NioProtocol即可 利用 Java 的异步请求 IO 处理，可以通过少量的线程处理大量的请求 注意： Tomcat8 以上版本在 Linux 系统中，默认使用的就是NIO模式，不需要额外修改 ，Tomcat7必须修改Connector配置来启动 三、apr(Apache Portable Runtime/Apache可移植运行时) （ 安装配置过程相对复杂） Tomcat将以JNI的形式调用Apache HTTP服务器的核心动态链接库来处理文件读取或网络传输操作，从而大大地提高Tomcat对静态文件的处理性能。Tomcat apr也是在Tomcat上运行高并发应用的首选模式。从操作系统级别来解决异步的IO问题 APR是使用原生C语言编写的非堵塞I/O，利用了操作系统的网络连接功能，速度很快。 但是需先安装apr和native，若直接启动就支持apr，能大幅度提升性能，不亚于魔兽开局爆高科技兵种，威力强大 "},"Chapter08/RabbitMQACK.html":{"url":"Chapter08/RabbitMQACK.html","title":"RabbitMQ如何保证消息的可靠投递","keywords":"","body":"RabbitMQ如何保证消息的可靠投递 Spring Boot针对消息ack的方式和原生api针对消息ack的方式有点不同 原生api消息ack的方式 消息的确认方式有2种 自动确认（autoAck=true） 手动确认（autoAck=false） 消费者在消费消息的时候，可以指定autoAck参数 String basicConsume(String queue, boolean autoAck, Consumer callback) autoAck=false: RabbitMQ会等待消费者显示回复确认消息后才从内存（或者磁盘）中移出消息 autoAck=true: RabbitMQ会自动把发送出去的消息置为确认，然后从内存（或者磁盘）中删除，而不管消费者是否真正的消费了这些消息 手动确认的方法如下，有2个参数 basicAck(long deliveryTag, boolean multiple) deliveryTag: 用来标识信道中投递的消息。RabbitMQ 推送消息给Consumer时，会附带一个deliveryTag，以便Consumer可以在消息确认时告诉RabbitMQ到底是哪条消息被确认了。 RabbitMQ保证在每个信道中，每条消息的deliveryTag从1开始递增 multiple=true: 消息id myltiple=false: 消息id=deliveryTag的消息，都会被确认 消息一直不确认会发生啥？ 如果队列中的消息发送到消费者后，消费者不对消息进行确认，那么消息会一直留在队列中，直到确认才会删除。 如果发送到A消费者的消息一直不确认，只有等到A消费者与rabbitmq的连接中断，rabbitmq才会考虑将A消费者未确认的消息重新投递给另一个消费者 Spring Boot中针对消息ack的方式 有三种方式，定义在AcknowledgeMode枚举类中 spring boot针对消息默认的ack的方式为AUTO。 在实际场景中，我们一般都是手动ack。 application.yaml的配置改为如下 spring: rabbitmq: host: myhost port: 5672 username: guest password: guest virtual-host: / listener: simple: acknowledge-mode: manual # 手动ack，默认为auto 相应的消费者代码改为 @Slf4j @Component public class LogListenerManual { /** * 接收info级别的日志 */ @RabbitListener( bindings = @QueueBinding( value = @Queue(value = \"${log.info.queue}\", durable = \"true\"), exchange = @Exchange(value = \"${log.exchange}\", type = ExchangeTypes.TOPIC), key = \"${log.info.binding-key}\" ) ) public void infoLog(Message message, Channel channel) throws Exception { String msg = new String(message.getBody()); log.info(\"infoLogQueue 收到的消息为: {}\", msg); try { // 这里写各种业务逻辑 channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); } catch (Exception e) { channel.basicNack(message.getMessageProperties().getDeliveryTag(), false, false); } } } 我们上面用到的注解，作用如下 RabbitMQ如何保证消息的可靠投递 一个消息往往会经历如下几个阶段 所以要保证消息的可靠投递，只需要保证这3个阶段的可靠投递即可 生产阶段 这个阶段的可靠投递主要靠ConfirmListener（发布者确认）和ReturnListener（失败通知） 前面已经介绍过了，一条消息在RabbitMQ中的流转过程为 producer -> rabbitmq broker cluster -> exchange -> queue -> consumer ConfirmListener可以获取消息是否从producer发送到broker ReturnListener可以获取从exchange路由不到queue的消息 我用Spring Boot Starter 的api来演示一下效果 发布者确认回调 @Component public class ConfirmCallback implements RabbitTemplate.ConfirmCallback { @Autowired private MessageSender messageSender; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) { String msgId = correlationData.getId(); String msg = messageSender.dequeueUnAckMsg(msgId); if (ack) { } else { // 可以加一些重试的逻辑 } } } 失败通知回调 @Component public class ReturnCallback implements RabbitTemplate.ReturnCallback { @Override public void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) { String msg = new String(message.getBody()); } } @Configuration public class RabbitMqConfig { @Bean public ConnectionFactory connectionFactory( @Value(\"${spring.rabbitmq.host}\") String host, @Value(\"${spring.rabbitmq.port}\") int port, @Value(\"${spring.rabbitmq.username}\") String username, @Value(\"${spring.rabbitmq.password}\") String password, @Value(\"${spring.rabbitmq.virtual-host}\") String vhost) { CachingConnectionFactory connectionFactory = new CachingConnectionFactory(host); connectionFactory.setPort(port); connectionFactory.setUsername(username); connectionFactory.setPassword(password); connectionFactory.setVirtualHost(vhost); connectionFactory.setPublisherConfirms(true); connectionFactory.setPublisherReturns(true); return connectionFactory; } @Bean public RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory, ReturnCallback returnCallback, ConfirmCallback confirmCallback) { RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory); rabbitTemplate.setReturnCallback(returnCallback); rabbitTemplate.setConfirmCallback(confirmCallback); // 要想使 returnCallback 生效，必须设置为true rabbitTemplate.setMandatory(true); return rabbitTemplate; } } 这里我对RabbitTemplate做了一下包装，主要就是发送的时候增加消息id，并且保存消息id和消息的对应关系，因为RabbitTemplate.ConfirmCallback只能拿到消息id，并不能拿到消息内容，所以需要我们自己保存这种映射关系。在一些可靠性要求比较高的系统中，你可以将这种映射关系存到数据库中，成功发送删除映射关系，失败则一直发送 @Component public class MessageSender { @Autowired private RabbitTemplate rabbitTemplate; public final Map unAckMsgQueue = new ConcurrentHashMap<>(); public void convertAndSend(String exchange, String routingKey, String message) { String msgId = UUID.randomUUID().toString(); CorrelationData correlationData = new CorrelationData(); correlationData.setId(msgId); rabbitTemplate.convertAndSend(exchange, routingKey, message, correlationData); unAckMsgQueue.put(msgId, message); } public String dequeueUnAckMsg(String msgId) { return unAckMsgQueue.remove(msgId); } } 存储阶段 这个阶段的高可用还真没研究过，毕竟集群都是运维搭建的，后续有时间的话会把这快的内容补充一下 消费阶段 消费阶段的可靠投递主要靠ack来保证。 前文已经介绍了原生api ack的方式和Spring Boot框架ack的方式 总而言之，在生产环境中，我们一般都是单条手动ack，消费失败后不会重新入队（因为很大概率还会再次失败），而是将消息重新投递到死信队列，方便以后排查问题 总结一下各种情况 ack后消息从broker中删除 nack或者reject后，分为如下2种情况 (1) reque=true，则消息会被重新放入队列(2) reque=fasle，消息会被直接丢弃，如果指定了死信队列的话，会被投递到死信队列 "},"Chapter08/redis.html":{"url":"Chapter08/redis.html","title":"Redis中删除过期Key的三种策略","keywords":"","body":"Redis删除过期key的策略 在Redis中，假如我们设置了100w个key，这些key设置了只能存活2个小时，那么在2个小时后，redis是如何来删除这些key的？ 答案：定期删除 and 惰性删除。 那什么是定期删除？什么的惰性删除？靠这两种策略就可以删除掉redis中过期的key吗？ 定期删除：redis默认每隔100ms随机抽取一些key，检查是否有过期的key，有过期的key则删除。需要注意的是redis不是每隔100ms就将所有的key检查一次，而是随机抽取一些key来检查是否过期的key。如果每100ms，就将redis的所有key（假设有1000w的key）都检查一遍，那么会给CPU带来很大的负载，redis就会卡死了。因此，如果只采用定期删除策略，会导致很多key到时间还没有被删除。 惰性删除：定期删除策略可能会导致很多过期的key到了时间也还没有被删除掉；为了解决这个问题，redis增加了惰性删除策略；对于那些过期的key，靠定期删除策略没有被删除掉，还保留在内存中，这时候如果系统去主动查询这个key，redis判断已经过期了，才会把这个过期的key删除掉。 靠这两种策略就可以删除掉redis中过期的key吗？ 仅仅靠通过设置过期时间还是存在着问题的。由于定期删除策略是随机抽取的，因此很有可能漏掉很多过期的key，这时候我们也没有主动去查询这些过期的key，因此也就没有使用惰性删除策略了，这时候如果有大量的过期key堆积，会导致内存被消耗完。要解决这个问题：可以使用 redis 内存淘汰机制。 Redis 内存淘汰策略： noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个比较常用） allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除 Redis中删除过期Key的三种策略 Redis对于过期键有三种清除策略： 被动删除：当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期key 主动删除：由于惰性删除策略无法保证冷数据被及时删掉，所以Redis会定期主动淘汰一批已过期的key,当前已用内存超过maxmemory限定时，触发主动清理策略 被动删除 只有key被操作时(如GET)，REDIS才会被动检查该key是否过期，如果过期则删除之并且返回NIL。 1、这种删除策略对CPU是友好的，删除操作只有在不得不的情况下才会进行，不会对其他的expire key上浪费无谓的CPU时间。 2、但是这种策略对内存不友好，一个key已经过期，但是在它被操作之前不会被删除，仍然占据内存空间。如果有大量的过期键存在但是又很少被访问到，那会造成大量的内存空间浪费。expireIfNeeded(redisDb db, robj key)函数位于src/db.c。 但仅是这样是不够的，因为可能存在一些key永远不会被再次访问到，这些设置了过期时间的key也是需要在过期后被删除的，我们甚至可以将这种情况看作是一种内存泄露—-无用的垃圾数据占用了大量的内存，而服务器却不会自己去释放它们，这对于运行状态非常依赖于内存的Redis服务器来说，肯定不是一个好消息。 主动删除 先说一下时间事件，对于持续运行的服务器来说， 服务器需要定期对自身的资源和状态进行必要的检查和整理， 从而让服务器维持在一个健康稳定的状态， 这类操作被统称为常规操作（cron job） 在 Redis 中， 常规操作由 redis.c/serverCron 实现， 它主要执行以下操作 更新服务器的各类统计信息，比如时间、内存占用、数据库占用情况等。 清理数据库中的过期键值对。 对不合理的数据库进行大小调整。 关闭和清理连接失效的客户端。 尝试进行 AOF 或 RDB 持久化操作。 如果服务器是主节点的话，对附属节点进行定期同步。 如果处于集群模式的话，对集群进行定期同步和连接测试。 Redis 将 serverCron 作为时间事件来运行， 从而确保它每隔一段时间就会自动运行一次， 又因为 serverCron 需要在 Redis 服务器运行期间一直定期运行， 所以它是一个循环时间事件： serverCron 会一直定期执行，直到服务器关闭为止。 在 Redis 2.6 版本中， 程序规定 serverCron 每秒运行 10 次， 平均每 100 毫秒运行一次。 从 Redis 2.8 开始， 用户可以通过修改 hz选项来调整 serverCron 的每秒执行次数， 具体信息请参考 redis.conf 文件中关于 hz 选项的说明也叫定时删除，这里的“定期”指的是Redis定期触发的清理策略，由位于src/redis.c的activeExpireCycle(void)函数来完成。 serverCron是由redis的事件框架驱动的定位任务，这个定时任务中会调用activeExpireCycle函数，针对每个db在限制的时间REDIS_EXPIRELOOKUPS_TIME_LIMIT内迟可能多的删除过期key，之所以要限制时间是为了防止过长时间 的阻塞影响redis的正常运行。这种主动删除策略弥补了被动删除策略在内存上的不友好。 因此，Redis会周期性的随机测试一批设置了过期时间的key并进行处理。测试到的已过期的key将被删除。典型的方式为,Redis每秒做10次如下的步骤： 随机测试100个设置了过期时间的key 删除所有发现的已过期的key 若删除的key超过25个则重复步骤1 这是一个基于概率的简单算法，基本的假设是抽出的样本能够代表整个key空间，redis持续清理过期的数据直至将要过期的key的百分比降到了25%以下。这也意味着在任何给定的时刻已经过期但仍占据着内存空间的key的量最多为每秒的写操作量除以4. Redis-3.0.0中的默认值是10，代表每秒钟调用10次后台任务。 除了主动淘汰的频率外，Redis对每次淘汰任务执行的最大时长也有一个限定，这样保证了每次主动淘汰不会过多阻塞应用请求，以下是这个限定计算公式： define ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC 25 / CPU max % for keys collection / ...timelimit = 1000000*ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC/server.hz/100; hz调大将会提高Redis主动淘汰的频率，如果你的Redis存储中包含很多冷数据占用内存过大的话，可以考虑将这个值调大，但Redis作者建议这个值不要超过100。我们实际线上将这个值调大到100，观察到CPU会增加2%左右，但对冷数据的内存释放速度确实有明显的提高（通过观察keyspace个数和used_memory大小）。 可以看出timelimit和server.hz是一个倒数的关系，也就是说hz配置越大，timelimit就越小。换句话说是每秒钟期望的主动淘汰频率越高，则每次淘汰最长占用时间就越短。这里每秒钟的最长淘汰占用时间是固定的250ms（1000000*ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC/100），而淘汰频率和每次淘汰的最长时间是通过hz参数控制的。 从以上的分析看，当redis中的过期key比率没有超过25%之前，提高hz可以明显提高扫描key的最小个数。假设hz为10，则一秒内最少扫描200个key（一秒调用10次*每次最少随机取出20个key），如果hz改为100，则一秒内最少扫描2000个key；另一方面，如果过期key比率超过25%，则扫描key的个数无上限，但是cpu时间每秒钟最多占用250ms。 当REDIS运行在主从模式时，只有主结点才会执行上述这两种过期删除策略，然后把删除操作”del key”同步到从结点。 maxmemory 当前已用内存超过maxmemory限定时，触发主动清理策略 volatile-lru：只对设置了过期时间的key进行LRU（默认值） allkeys-lru ： 删除lru算法的key volatile-random：随机删除即将过期key allkeys-random：随机删除 volatile-ttl ： 删除即将过期的 noeviction ： 永不过期，返回错误 当mem_used内存已经超过maxmemory的设定，对于所有的读写请求，都会触发redis.c/freeMemoryIfNeeded(void)函数以清理超出的内存。注意这个清理过程是阻塞的，直到清理出足够的内存空间。所以如果在达到maxmemory并且调用方还在不断写入的情况下，可能会反复触发主动清理策略，导致请求会有一定的延迟。 清理时会根据用户配置的maxmemory-policy来做适当的清理（一般是LRU或TTL），这里的LRU或TTL策略并不是针对redis的所有key，而是以配置文件中的maxmemory-samples个key作为样本池进行抽样清理。 maxmemory-samples在redis-3.0.0中的默认配置为5，如果增加，会提高LRU或TTL的精准度，redis作者测试的结果是当这个配置为10时已经非常接近全量LRU的精准度了，并且增加maxmemory-samples会导致在主动清理时消耗更多的CPU时间，建议： 尽量不要触发maxmemory，最好在mem_used内存占用达到maxmemory的一定比例后，需要考虑调大hz以加快淘汰，或者进行集群扩容。 如果能够控制住内存，则可以不用修改maxmemory-samples配置；如果Redis本身就作为LRU cache服务（这种服务一般长时间处于maxmemory状态，由Redis自动做LRU淘汰），可以适当调大maxmemory-samples。 这里提一句，实际上redis根本就不会准确的将整个数据库中最久未被使用的键删除，而是每次从数据库中随机取5个键并删除这5个键里最久未被使用的键。上面提到的所有的随机的操作实际上都是这样的，这个5可以用过redis的配置文件中的maxmemeory-samples参数配置。 Replication link和AOF文件中的过期处理 为了获得正确的行为而不至于导致一致性问题，当一个key过期时DEL操作将被记录在AOF文件并传递到所有相关的slave。也即过期删除操作统一在master实例中进行并向下传递，而不是各salve各自掌控。 这样一来便不会出现数据不一致的情形。当slave连接到master后并不能立即清理已过期的key（需要等待由master传递过来的DEL操作），slave仍需对数据集中的过期状态进行管理维护以便于在slave被提升为master会能像master一样独立的进行过期处理。 后台删除之lazyfree机制 为了解决redis使用del命令删除大体积的key，或者使用flushdb、flushall删除数据库时，造成redis阻塞的情况，在redis 4.0引入了lazyfree机制，可将删除操作放在后台，让后台子线程(bio)执行，避免主线程阻塞。 lazy free的使用分为2类：第一类是与DEL命令对应的主动删除，第二类是过期key删除、maxmemory key驱逐淘汰删除。 主动删除 UNLINK命令是与DEL一样删除key功能的lazy free实现。唯一不同时，UNLINK在删除集合类键时，如果集合键的元素个数大于64个(详细后文），会把真正的内存释放操作，给单独的bio来操作。 127.0.0.1:7000> UNLINK mylist (integer) 1 FLUSHALL/FLUSHDB ASYNC 127.0.0.1:7000> flushall async //异步清理实例数据 被动删除 lazy free应用于被动删除中，目前有4种场景，每种场景对应一个配置参数； 默认都是关闭。 lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no slave-lazy-flush n lazyfree-lazy-eviction 针对redis内存使用达到maxmeory，并设置有淘汰策略时；在被动淘汰键时，是否采用lazy free机制； 因为此场景开启lazy free, 可能使用淘汰键的内存释放不及时，导致redis内存超用，超过maxmemory的限制。此场景使用时，请结合业务测试。 lazyfree-lazy-expire 针对设置有TTL的键，达到过期后，被redis清理删除时是否采用lazy free机制； 此场景建议开启，因TTL本身是自适应调整的速度。 lazyfree-lazy-server-del 针对有些指令在处理已存在的键时，会带有一个隐式的DEL键的操作。如rename命令，当目标键已存在,redis会先删除目标键，如果这些目标键是一个big key,那就会引入阻塞删除的性能问题。 此参数设置就是解决这类问题，建议可开启。 slave-lazy-flush 针对slave进行全量数据同步，slave在加载master的RDB文件前，会运行flushall来清理自己的数据场景， 参数设置决定是否采用异常flush机制。如果内存变动不大，建议可开启。可减少全量同步耗时，从而减少主库因输出缓冲区爆涨引起的内存使用增长。 expire及evict优化 redis在空闲时会进入activeExpireCycle循环删除过期key，每次循环都会率先计算一个执行时间，在循环中并不会遍历整个数据库，而是随机挑选一部分key查看是否到期，所以有时时间不会被耗尽（采取异步删除时更会加快清理过期key），剩余的时间就可以交给freeMemoryIfNeeded来执行。 "},"Chapter08/Redis6.html":{"url":"Chapter08/Redis6.html","title":"Redis 6.0之前为什么一直不使用多线程？","keywords":"","body":"Redis 6.0之前为什么一直不使用多线程？ Redis 6.0在5.2号这个美好的日子里悄无声息的发布了，这次发布在IT圈犹如一颗惊雷一般，因为这是redis最大的一次改版，首次加入了多线程。 作者Antirez在RC1版本发布时在他的博客写下： the most “enterprise” Redis version to date // 最”企业级”的 the largest release of Redis ever as far as I can tell // 最大的 the one where the biggest amount of people participated // 参与人数最多的 这次改变，性能有个飞速的提升~ 先po出新版和旧版性能图： 从上面可以看到 GET/SET 命令在 4 线程 IO 时性能相比单线程是几乎是翻倍了。另外，这些数据只是为了简单验证多线程 IO 是否真正带来性能优化，并没有针对严谨的延时控制和不同并发的场景进行压测。 数据仅供验证参考而不能作为线上指标，且只是目前的 unstble分支的性能，不排除后续发布的正式版本的性能会更好。 Redis 6.0 之前的版本真的是单线程吗？ Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。 因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。 一般来说 Redis 的瓶颈并不在 CPU，而在内存和网络。如果要使用 CPU 多核，可以搭建多个 Redis 实例来解决。 其实，Redis 4.0 开始就有多线程的概念了，比如 Redis 通过多线程方式在后台删除对象、以及通过 Redis 模块实现的阻塞命令等。 Redis 6.0 之前为什么一直不使用多线程？ 使用了单线程后，可维护性高。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。 Redis 通过 AE 事件模型以及 IO 多路复用等技术，处理性能非常高，因此没有必要使用多线程。 单线程机制使得 Redis 内部实现的复杂度大大降低，Hash 的惰性 Rehash、Lpush 等等 “线程不安全” 的命令都可以无锁进行。 Redis 6.0 为什么要引入多线程呢？ 之前的段落说了，Redis 的瓶颈并不在 CPU，而在内存和网络。 内存不够的话，可以加内存或者做数据结构优化和其他优化等，但网络的性能优化才是大头，网络 IO 的读写在 Redis 整个执行期间占用了大部分的 CPU 时间，如果把网络处理这部分做成多线程处理方式，那对整个 Redis 的性能会有很大的提升。 优化方向： 提高网络 IO 性能，典型的实现比如使用 DPDK 来替代内核网络栈的方式。 使用多线程充分利用多核，典型的实现比如 Memcached。 所以总结起来，Redis 支持多线程主要就是两个原因： 可以充分利用服务器 CPU 资源，目前主线程只能利用一个核。 多线程任务可以分摊 Redis 同步 IO 读写负荷。 Redis 6.0 默认是否开启了多线程？ 否，在conf文件进行配置 io-threads-do-reads yes io-threads 线程数 官方建议：4 核的机器建议设置为 2 或 3 个线程，8 核的建议设置为 6 个线程，线程数一定要小于机器核数，尽量不超过8个。 Redis 6.0 多线程的实现机制？ 流程简述如下： 主线程负责接收建立连接请求，获取 Socket 放入全局等待读处理队列。 主线程处理完读事件之后，通过 RR（Round Robin）将这些连接分配给这些 IO 线程。 主线程阻塞等待 IO 线程读取 Socket 完毕。 主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行。 主线程阻塞等待 IO 线程将数据回写 Socket 完毕。 解除绑定，清空等待队列。 该设计有如下特点： IO 线程要么同时在读 Socket，要么同时在写，不会同时读或写。 IO 线程只负责读写 Socket 解析命令，不负责命令处理。 开启多线程后，是否会存在线程并发安全问题？ 不会，Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行。 Redis 线程中经常提到 IO 多路复用，如何理解？ 这是 IO 模型的一种，即经典的 Reactor 设计模式，有时也称为异步阻塞 IO。 多路指的是多个 Socket 连接，复用指的是复用一个线程。多路复用主要有三种技术：Select，Poll，Epoll。 epoll 是最新的也是目前最好的多路复用技术。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快（内存内的操作不会成为这里的性能瓶颈），主要以上两点造就了 Redis 具有很高的吞吐量。 "},"Chapter08/Redlock.html":{"url":"Chapter08/Redlock.html","title":"Redlock","keywords":"","body":"Redlock 原理分析 最低保证分布式锁的有效性及安全性的要求如下： 互斥；任何时刻只能有一个client获取锁 释放死锁；即使锁定资源的服务崩溃或者分区，仍然能释放锁 容错性；只要多数redis节点（一半以上）在使用，client就可以获取和释放锁 网上讲的基于故障转移实现的redis主从无法真正实现Redlock: 因为redis在进行主从复制时是异步完成的，比如在clientA获取锁后，主redis复制数据到从redis过程中崩溃了，导致没有复制到从redis中，然后从redis选举出一个升级为主redis,造成新的主redis没有clientA 设置的锁，这是clientB尝试获取锁，并且能够成功获取锁，导致互斥失效； 思考题：这个失败的原因是因为从redis立刻升级为主redis，如果能够过TTL时间再升级为主redis（延迟升级）后，或者立刻升级为主redis但是过TTL的时间后再执行获取锁的任务，就能成功产生互斥效果；是不是这样就能实现基于redis主从的Redlock; redis单实例中实现分布式锁的正确方式（原子性非常重要） 设置锁时，使用set命令，因为其包含了setnx,expire的功能，起到了原子操作的效果，给key设置随机值，并且只有在key不存在时才设置成功返回True,并且设置key的过期时间（最好用毫秒） SET key_name my_random_value NX PX 30000 # NX 表示if not exist 就设置并返回True，否则不设置并返回False PX 表示过期时间用毫秒级， 30000 表示这些毫秒时间后此key过期 在获取锁后，并完成相关业务后，需要删除自己设置的锁（必须是只能删除自己设置的锁，不能删除他人设置的锁）； 删除原因：保证服务器资源的高利用效率，不用等到锁自动过期才删除； 删除方法：最好使用Lua脚本删除（redis保证执行此脚本时不执行其他操作，保证操作的原子性），代码如下；逻辑是 先获取key，如果存在并且值是自己设置的就删除此key;否则就跳过； if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end 多节点redis实现的分布式锁算法(RedLock):有效防止单点故障 假设有5个完全独立的redis主服务器 1.获取当前时间戳 2.client尝试按照顺序使用相同的key,value获取所有redis服务的锁，在获取锁的过程中的获取时间比锁过期时间短很多，这是为了不要过长时间等待已经关闭的redis服务。并且试着获取下一个redis实例。 比如：TTL为5s,设置获取锁最多用1s，所以如果一秒内无法获取锁，就放弃获取这个锁，从而尝试获取下个锁 3.client通过获取所有能获取的锁后的时间减去第一步的时间，这个时间差要小于TTL时间并且至少有3个redis实例成功获取锁，才算真正的获取锁成功 4.如果成功获取锁，则锁的真正有效时间是 TTL减去第三步的时间差 的时间；比如：TTL 是5s,获取所有锁用了2s,则真正锁有效时间为3s(其实应该再减去时钟漂移); 5.如果客户端由于某些原因获取锁失败，便会开始解锁所有redis实例；因为可能已经获取了小于3个锁，必须释放，否则影响其他client获取锁 算法示意图如下： RedLock算法是否是异步算法 可以看成是同步算法；因为 即使进程间（多个电脑间）没有同步时钟，但是每个进程时间流速大致相同；并且时钟漂移相对于TTL叫小，可以忽略，所以可以看成同步算法；（不够严谨，算法上要算上时钟漂移，因为如果两个电脑在地球两端，则时钟漂移非常大） RedLock失败重试 当client不能获取锁时，应该在随机时间后重试获取锁；并且最好在同一时刻并发的把set命令发送给所有redis实例；而且对于已经获取锁的client在完成任务后要及时释放锁，这是为了节省时间； RedLock释放锁 由于释放锁时会判断这个锁的value是不是自己设置的，如果是才删除；所以在释放锁时非常简单，只要向所有实例都发出释放锁的命令，不用考虑能否成功释放锁； RedLock注意点（Safety arguments） 先假设client获取所有实例，所有实例包含相同的key和过期时间(TTL) ,但每个实例set命令时间不同导致不能同时过期，第一个set命令之前是T1,最后一个set命令后为T2,则此client有效获取锁的最小时间为TTL-(T2-T1)-时钟漂移; 对于以N/2+ 1(也就是一半以 上)的方式判断获取锁成功，是因为如果小于一半判断为成功的话，有可能出现多个client都成功获取锁的情况， 从而使锁失效 一个client锁定大多数事例耗费的时间大于或接近锁的过期时间，就认为锁无效，并且解锁这个redis实例(不执行业务) ;只要在TTL时间内成功获取一半以上的锁便是有效锁;否则无效 系统有活性的三个特征 能够自动释放锁 在获取锁失败（不到一半以上），或任务完成后 能够自动释放锁，不用等到其自动过期 在client重试获取哦锁前（第一次失败到第二次重试时间间隔）大于第一次获取锁消耗的时间； 重试获取锁要有一定次数限制 RedLock性能及崩溃恢复的相关解决方法 如果redis没有持久化功能，在clientA获取锁成功后，所有redis重启，clientB能够再次获取到锁，这样违法了锁的排他互斥性; 如果启动AOF永久化存储，事情会好些， 举例:当我们重启redis后，由于redis过期机制是按照unix时间戳走的，所以在重启后，然后会按照规定的时间过期，不影响业务;但是由于AOF同步到磁盘的方式默认是每秒-次，如果在一秒内断电，会导致数据丢失，立即重启会造成锁互斥性失效;但如果同步磁盘方式使用Always(每一个写命令都同步到硬盘)造成性能急剧下降;所以在锁完全有效性和性能方面要有所取舍; 有效解决既保证锁完全有效性及性能高效及即使断电情况的方法是redis同步到磁盘方式保持默认的每秒，在redis无论因为什么原因停掉后要等待TTL时间后再重启(学名:延迟重启) ;缺点是 在TTL时间内服务相当于暂停状态; 总结： TTL时长 要大于正常业务执行的时间+获取所有redis服务消耗时间+时钟漂移 获取redis所有服务消耗时间要 远小于TTL时间，并且获取成功的锁个数要 在总数的一般以上:N/2+1 尝试获取每个redis实例锁时的时间要 远小于TTL时间 尝试获取所有锁失败后 重新尝试一定要有一定次数限制 在redis崩溃后（无论一个还是所有），要延迟TTL时间重启redis 在实现多redis节点时要结合单节点分布式锁算法 共同实现 "},"Chapter08/RedisHighConcurrency.html":{"url":"Chapter08/RedisHighConcurrency.html","title":"Redis高并发","keywords":"","body":"Redis高并发 Redis 为单进程单线程模式，采用队列模式将并发访问变为串行访问。 Redis 本身没有锁的概念，Redis 对于多个客户端连接并不存在竞争， 但是在业务客户端对 Redis 进行并发访问时会发生连接超时、数据转换错误、阻塞、客户端关闭连接等问题， 这些问题均是由于客户端连接混乱造成 以及今天要谈到的Redis并发竞争问题，这里的并发指的是多个redis的client同时set key引起的并发问题。 比如：多客户端同时并发写一个key，一个key的值是1，本来按顺序修改为2,3,4，最后是4， 但是由于并发设置的原因，最后顺序变成了4,3,2，最后变成的key值成了2。 采用分布式锁+数据修改的时间戳 方案来解决。 ①想要向缓存中写入数据时，必须要获得分布式锁，只有获得锁了才可以去进行缓存数据的写入，写入结束释放锁。就可以保证同时只有一个客户端去写缓存。 ②可是并不能保证每个客户端获取锁的顺序。但是我们要写入缓存的数据都是从数据库查询出来的，数据库都是有这种数据的创建时间的，所以可以在更新之前，先去对比自己的这条数据的时间和缓存中数据的时间，谁更新，如果自己更新则写入覆盖，否则直接放弃本次操作。 这样就可以保证并发操作时的数据顺序问题。 "},"Chapter08/mysqlFace.html":{"url":"Chapter08/mysqlFace.html","title":"mysqlFace","keywords":"","body":"mysql 面试 事务四大特性（ACID）原子性、一致性、隔离性、持久性？ 事务的并发？事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别？ MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别？ MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景？ 查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序？ 什么是临时表，临时表什么时候删除? MySQL B+Tree索引和Hash索引的区别？ sql查询语句确定创建哪种类型的索引？如何优化查询？ 聚集索引和非聚集索引区别？ 有哪些锁（乐观锁悲观锁），select 时怎么加排它锁？ 非关系型数据库和关系型数据库区别，优势比较？ 数据库三范式，根据某个场景设计数据表？ 数据库的读写分离、主从复制，主从复制分析的 7 个问题？ 使用explain优化sql和索引？ MySQL慢查询怎么解决？ 什么是 内连接、外连接、交叉连接、笛卡尔积等？ mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决？ varchar和char的使用场景？ mysql 高并发环境解决方案？ 数据库崩溃时事务的恢复机制（REDO日志和UNDO日志）？ mysql 的快照读和当前读 21条MySQL性能调优经验 为查询缓存优化你的查询 EXPLAIN你的SELECT查询 当只要一行数据时使用LIMIT 1 为搜索字段建索引 在Join表的时候使用相当类型的例，并将其索引 千万不要 ORDER BY RAND() 避免 SELECT * 永远为每张表设置一个 ID 使用 ENUM 而不是 VARCHAR 从 PROCEDURE ANALYSE() 取得建议 尽可能的使用 NOT NULL Prepared Statements 无缓冲的查询 把 IP 地址存成 UNSIGNED INT 固定长度的表会更快 垂直分割 拆分大的 DELETE 或 INSERT 语句 越小的列会越快 选择正确的存储引擎 使用一个对象关系映射器(Object Relational Mapper) 小心“永久链接” "},"Chapter08/intset.html":{"url":"Chapter08/intset.html","title":"Redis 数据结构之整数集合","keywords":"","body":"Redis 数据结构之整数集合 整数集合（intset）是集合键的底层实现之一，当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis 就会使用整数集合作为集合键的底层实现 整数集合的实现 整数集合是 Redis 用于保存整数值的集合抽象数据结构，它可以保存类型为 int16_t、int32_t 或 int64_t 的整数值，并保证集合中不会出现重复元素 contents 数组是整数集合的底层实现：整数集合的每个元素都是 contents 数组的一个数组项（item），各个项在数组中按值得大小从小到大有序排序，且数组中不包含任何重复项 虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组，但实际上 contents 数组并不保存任何 int8_t 类型的值，contents 数组的真正类型取决于 encoding 属性的值 升级 每当要将一个新元素添加到整数集合中，并且新元素的类型比整数集合现有所有元素的类型都长时，整数集合需要先升级，然后才能将新元素添加到整数集合内 升级整数集合并添加新元素共分三步进行： 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性不变 将新元素添加到底层数组中 升级的好处 提升灵活性: 因为整数集合可以通过自动升级底层数组来适应新元素，所以可以将不同类型的整数添加到集合中，而不必担心出现类型错误 节约内存: 升级可以让集合同时保持 int16_t、int32_t 和 int64_t 三种不同类型的值，又可以确保升级只会在有必要时进行，这可以尽量节约内存（否则就是直接使用 int64_t 类型的数组）。例如，如果一直想整数集合添加 int16_t 类型的值，那么整数集合的底层实现就会一直是 int16_t 类型的数组，只有将 int32_t 或 int64_t 类型的值添加到集合时，才会升级 降级 整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态 "},"Chapter08/zskiplist.html":{"url":"Chapter08/zskiplist.html","title":"Redis 数据结构之跳跃表","keywords":"","body":"跳跃表 与跳表相关结构定义在一起的还有一个有序集合结构，很多人会说 redis 中的有序集合是跳表实现的，这句话不错，但有失偏驳。 typedef struct zset { dict *dict; zskiplist *zsl; } zset; 准确来说，redis 中的有序集合是由我们之前介绍过的字典加上跳表实现的，字典中保存的数据和分数 score 的映射关系，每次插入数据会从字典中查询，如果已经存在了，就不再插入，有序集合中是不允许重复数据。 下面我们看看 redis 中跳表的相关代码的实现情况。 跳表初始化 跳表初始化 redis 中初始化一个跳表的代码如下： zskiplistNode *zslCreateNode(int level, double score, sds ele) { zskiplistNode *zn = zmalloc(sizeof(*zn)+level*sizeof(struct zskiplistLevel)); zn->score = score; zn->ele = ele; return zn; } /* Create a new skiplist. */ zskiplist *zslCreate(void) { int j; zskiplist *zsl; //分配内存空间 zsl = zmalloc(sizeof(*zsl)); //默认只有一层索引 zsl->level = 1; //0 个节点 zsl->length = 0; //1、创建一个 node 节点，这是个哨兵节点 //2、为 level 数组分配 ZSKIPLIST_MAXLEVEL=32 内存大小 //3、也即 redis 中支持索引最大 32 层 zsl->header = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL); //为哨兵节点的 level 初始化 for (j = 0; j header->level[j].forward = NULL; zsl->header->level[j].span = 0; } zsl->header->backward = NULL; zsl->tail = NULL; return zsl; } zslCreate 用于初始化一个跳表，比较简单，我也给出了基本的注释，这里不再赘述了，强调一点的是，redis 中实现的跳表最高允许 32 层索引，这么做也是一种性能与内存之间的衡量，过多的索引层必然占用更多的内存空间，32 是一个比较合适值。 2、插入一个节点 "},"Chapter08/Cluster.html":{"url":"Chapter08/Cluster.html","title":"Redis-Cluster集群","keywords":"","body":"Redis-Cluster集群 redis最开始使用主从模式做集群，若master宕机需要手动配置slave转为master；后来为了高可用提出来哨兵模式，该模式下有一个哨兵监视master和slave，若master宕机可自动将slave转为master，但它也有一个问题，就是不能动态扩充；所以在3.x提出cluster集群模式。 redis-cluster设计 Redis-Cluster采用无中心结构，每个节点保存数据和整个集群状态,每个节点都和其他所有节点连接。 其结构特点： 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。 节点的fail是通过集群中超过半数的节点检测失效时才生效。 客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。 redis-cluster把所有的物理节点映射到[0-16383]slot上（不一定是平均分配）,cluster 负责维护nodeslotvalue。 Redis集群预分好16384个桶，当需要在 Redis 集群中放置一个 key-value 时，根据 CRC16(key) mod 16384的值，决定将一个key放到哪个桶中。 redis cluster节点分配 现在我们是三个主节点分别是：A, B, C 三个节点，它们可以是一台机器上的三个端口，也可以是三台不同的服务器。那么，采用哈希槽 (hash slot)的方式来分配16384个slot 的话，它们三个节点分别承担的slot 区间是： 节点A覆盖0－5460; 节点B覆盖5461－10922; 节点C覆盖10923－16383. 获取数据: 如果存入一个值，按照redis cluster哈希槽的算法： CRC16('key')384 = 6782。 那么就会把这个key 的存储分配到 B 上了。同样，当我连接(A,B,C)任何一个节点想获取'key'这个key时，也会这样的算法，然后内部跳转到B节点上获取数据 新增一个主节点: 新增一个节点D，redis cluster的这种做法是从各个节点的前面各拿取一部分slot到D上，我会在接下来的实践中实验。大致就会变成这样： 节点A覆盖1365-5460 节点B覆盖6827-10922 节点C覆盖12288-16383 节点D覆盖0-1364,5461-6826,10923-12287 同样删除一个节点也是类似，移动完成后就可以删除这个节点了。 Redis Cluster主从模式 redis cluster 为了保证数据的高可用性，加入了主从模式，一个主节点对应一个或多个从节点，主节点提供数据存取，从节点则是从主节点拉取数据备份，当这个主节点挂掉后，就会有这个从节点选取一个来充当主节点，从而保证集群不会挂掉 上面那个例子里, 集群有ABC三个主节点, 如果这3个节点都没有加入从节点，如果B挂掉了，我们就无法访问整个集群了。A和C的slot也无法访问。 所以我们在集群建立的时候，一定要为每个主节点都添加了从节点, 比如像这样, 集群包含主节点A、B、C, 以及从节点A1、B1、C1, 那么即使B挂掉系统也可以继续正确工作。 B1节点替代了B节点，所以Redis集群将会选择B1节点作为新的主节点，集群将会继续正确地提供服务。 当B重新开启后，它就会变成B1的从节点。 不过需要注意，如果节点B和B1同时挂了，Redis集群就无法继续正确地提供服务了。 Redis 哈希槽的概念 Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数， 这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大 致均等的将哈希槽映射到不同的节点。 Redis 集群没有使用一致性hash, 而是引入了哈希槽的概念。 Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽。这种结构很容易添加或者删除节点，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。 使用哈希槽的好处就在于可以方便的添加或移除节点。 当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了； 当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了； 在这一点上，我们以后新增或移除节点的时候不用先停掉所有的 redis 服务。 用了哈希槽的概念，而没有用一致性哈希算法，不都是哈希么？这样做的原因是为什么呢 Redis Cluster是自己做的crc16的简单hash算法，没有用一致性hash。Redis的作者认为它的crc16(key) mod 16384的效果已经不错了，虽然没有一致性hash灵活，但实现很简单，节点增删时处理起来也很方便。 为了动态增删节点的时候，不至于丢失数据么？ 节点增删时不丢失数据和hash算法没什么关系，不丢失数据要求的是一份数据有多个副本。 还有集群总共有2的14次方，16384个哈希槽，那么每一个哈希槽中存的key 和 value是什么？ 当你往Redis Cluster中加入一个Key时，会根据crc16(key) mod 16384计算这个key应该分布到哪个hash slot中，一个hash slot中会有很多key和value。你可以理解成表的分区，使用单节点时的redis时只有一个表，所有的key都放在这个表里；改用Redis Cluster以后会自动为你生成16384个分区表，你insert数据时会根据上面的简单算法来决定你的key应该存在哪个分区，每个分区里有很多key 为什么Redis集群有16384个槽 ps:CRC16算法产生的hash值有16bit，该算法可以产生2^16-=65536个值。换句话说，值是分布在0~65535之间。那作者在做mod运算的时候，为什么不mod65536，而选择mod16384？ 其实我当初第一次思考这个问题的时候，我心里是这么想的，作者应该是觉得16384就够了，然后我就开始查这方面资料。 很幸运的是，这个问题，作者是给出了回答的！ 地址如下: https://github.com/antirez/redis/issues/2576 作者原版回答如下: The reason is: Normal heartbeat packets carry the full configuration of a node, that can be replaced in an idempotent way with the old in order to update an old config. This means they contain the slots configuration for a node, in raw form, that uses 2k of space with16k slots, but would use a prohibitive 8k of space using 65k slots. At the same time it is unlikely that Redis Cluster would scale to more than 1000 mater nodes because of other design tradeoffs. So 16k was in the right range to ensure enough slots per master with a max of 1000 maters, but a small enough number to propagate the slot configuration as a raw bitmap easily. Note that in small clusters the bitmap would be hard to compress because when N is small the bitmap would have slots/N bits set that is a large percentage of bits set. 在握手成功后，连个节点之间会定期发送ping/pong消息，交换数据信息，如下图所示。 在这里，我们需要关注三个重点。 (1)交换什么数据信息 (2)数据信息究竟多大 (3)定期的频率什么样 到底在交换什么数据信息？ 交换的数据信息，由消息体和消息头组成。 消息体无外乎是一些节点标识啊，IP啊，端口号啊，发送时间啊。这与本文关系不是太大，我不细说。 另外，消息头里面有个myslots的char数组，长度为16383/8，这其实是一个bitmap,每一个位代表一个槽，如果该位为1，表示这个槽是属于这个节点的。 到底数据信息究竟多大？ 在消息头中，最占空间的是myslots[CLUSTER_SLOTS/8]。这块的大小是: 16384÷8÷1024=2kb 那在消息体中，会携带一定数量的其他节点信息用于交换。 那这个其他节点的信息，到底是几个节点的信息呢？ 约为集群总节点数量的1/10，至少携带3个节点的信息。 这里的重点是:节点数量越多，消息体内容越大。 消息体大小是10个节点的状态信息约1kb。 那定期的频率是什么样的？ redis集群内节点，每秒都在发ping消息。规律如下 (1)每秒会随机选取5个节点，找出最久没有通信的节点发送ping消息 (2)每100毫秒(1秒10次)都会扫描本地节点列表，如果发现节点最近一次接受pong消息的时间大于cluster-node-timeout/2 则立刻发送ping消息 因此，每秒单节点发出ping消息数量为 数量=1+10*num（node.pong_received>cluster_node_timeout/2） 回答 (1)如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。 如上所述，在消息头中，最占空间的是myslots[CLUSTER_SLOTS/8]。 当槽位为65536时，这块的大小是: 65536÷8÷1024=8kb 因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽。 (2)redis的集群主节点数量基本不可能超过1000个。 如上所述，集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者，不建议redis cluster节点数量超过1000个。 那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了。没有必要拓展到65536个。 (3)槽位越小，节点少的情况下，压缩比高 Redis主节点的配置信息中，它所负责的哈希槽是通过一张bitmap的形式来保存的，在传输过程中，会对bitmap进行压缩，但是如果bitmap的填充率slots / N很高的话(N表示节点数)，bitmap的压缩率就很低。 如果节点数很少，而哈希槽数量很多的话，bitmap的压缩率就很低。 ps：文件压缩率指的是，文件压缩前后的大小比。 综上所述，作者决定取16384个槽，不多不少，刚刚好！ "},"Chapter08/RedisPersistence.html":{"url":"Chapter08/RedisPersistence.html","title":"Redis持久化","keywords":"","body":"Redis持久化 redis aof文件过大问题 BGREWRITEAOF 执行一个 AOF文件 重写操作。重写会创建一个当前 AOF 文件的体积优化版本。 即使 BGREWRITEAOF 执行失败，也不会有任何数据丢失，因为旧的 AOF 文件在 BGREWRITEAOF 成功之前不会被修改。 重写操作只会在没有其他持久化工作在后台执行时被触发，也就是说： 如果 Redis 的子进程正在执行快照的保存工作，那么 AOF 重写的操作会被预定(scheduled)，等到保存工作完成之后再执行 AOF 重写。在这种情况下， BGREWRITEAOF 的返回值仍然是 OK ，但还会加上一条额外的信息，说明 BGREWRITEAOF 要等到保存操作完成之后才能执行。在 Redis 2.6 或以上的版本，可以使用 INFO 命令查看 BGREWRITEAOF 是否被预定。 如果已经有别的 AOF 文件重写在执行，那么 BGREWRITEAOF 返回一个错误，并且这个新的 BGREWRITEAOF 请求也不会被预定到下次执行。 从 Redis 2.4 开始， AOF 重写由 Redis 自行触发， BGREWRITEAOF 仅仅用于手动触发重写操作。 1、在重写期间，由于主进程依然在响应命令，为了保证最终备份的完整性；因此它依然会写入旧的AOF file中，如果重写失败，能够保证数据不丢失。 2、为了把重写期间响应的写入信息也写入到新的文件中，因此也会为子进程保留一个buf，防止新写的file丢失数据。 3、重写是直接把当前内存的数据生成对应命令，并不需要读取老的AOF文件进行分析、命令合并。 4、AOF文件直接采用的文本协议，主要是兼容性好、追加方便、可读性高可认为修改修复。 无论是 RDB 还是 AOF 都是先写入一个临时文件，然后通过 rename 完成文件的替换工作。 "},"Chapter08/redisMaster.html":{"url":"Chapter08/redisMaster.html","title":"redis主从架构重新选举master带来的问题","keywords":"","body":"redis主从架构重新选举master带来的问题 主从模式好是好，他也有自己的缺点，比如数据的一致性问题，假如主数据库写操作完成，那么他的数据会被复制到从数据库，若是还没有即使复制到从数据库，读请求又来了，此时读取的数据就不是最新的数据。 若是从主同步的过程网络出故障了，导致主从同步失败，也会出现问题数据一致性的问题。 主从模式不具备自动容错和恢复的功能，一旦主数据库，从节点晋升为主数据库的过程需要人为操作，维护的成本就会升高，并且主节点的写能力、存储能力都会受到限制。 redis如何实现主从数据的同步 Redis的主从同步机制可以确保redis的master和slave之间的数据同步。按照同步内容的多少可以分为全同步和部分同步；按照同步的时机可以分为slave刚启动时的初始化同步和正常运行过程中的数据修改同步；本文将对这两种机制的流程进行分析。 全备份过程中，在slave启动时，会向其master发送一条SYNC消息，master收到slave的这条消息之后，将可能启动后台进程进行备份，备份完成之后就将备份的数据发送给slave，初始时的全同步机制是这样的： （1）slave启动后向master发送同步指令SYNC，master接收到SYNC指令之后将调用该命令的处理函数syncCommand（）进行同步处理； （2）在函数syncCommand中，将调用函数rdbSaveBackground启动一个备份进程用于数据同步，如果已经有一个备份进程在运行了，就不会再重新启动了。 （3）备份进程将执行函数rdbSave（）完成将redis的全部数据保存为rdb文件。 （4）在redis的时间事件函数serverCron（redis的时间处理函数是指它会定时被redis进行操作的函数）中，将对备份后的数据进行处理，在serverCron函数中将会检查备份进程是否已经执行完毕，如果备份进程已经完成备份，则调用函数backgroundSaveDoneHandler完成后续处理。 （5）在函数backgroundSaveDoneHandler中，首先更新master的各种状态，例如，备份成功还是失败，备份的时间等等。然后调用函数updateSlavesWaitingBgsave，将备份的rdb数据发送给等待的slave。 （6）在函数updateSlavesWaitingBgsave中，将遍历所有的等待此次备份的slave，将备份的rdb文件发送给每一个slave。另外，这里并不是立即就把数据发送过去，而是将为每个等待的slave注册写事件，并注册写事件的响应函数sendBulkToSlave，即当slave对应的socket能够发送数据时就调用函数sendBulkToSlave（），实际发送rdb文件的操作都在函数sendBulkToSlave中完成。 （7）sendBulkToSlave函数将把备份的rdb文件发送给slave。 上述函数调用过程如下图1所示： 二、数据修改操作的同步 Redis的正常部署中一般都是一个master用于写操作，若干个slave用于读操作，另外定期的数据备份操作也是单独选址一个slave完成，这样可以最大程度发挥出redis的性能。在部署完成，各master\\slave程序启动之后，首先进行第一阶段初始化时的全同步操作，全同步操作完成之后，后续所有写操作都是在master上进行，所有读操作都是在slave上进行，因此用户的写操作需要及时扩散到所有的slave以便保持数据最大程度上的同步。Redis的master-slave进程在正常运行期间更新操作（包括写、删除、更改操作）的同步方式如下： （1）master接收到一条用户的操作后，将调用函数call函数来执行具体的操作函数（此过程可参考另一文档《redis命令执行流程分析》），在该函数中首先通过proc执行操作函数，然后将判断操作是否需要扩散到各slave，如果需要则调用函数propagate（）来完成此操作。 （2）propagate（）函数完成将一个操作记录到aof文件中或者扩散到其他slave中；在该函数中通过调用feedAppendOnlyFile（）将操作记录到aof中，通过调用replicationFeedSlaves（）将操作扩散到各slave中。 （3）函数feedAppendOnlyFile（）中主要保存操作到aof文件，在该函数中首先将操作转换成redis内部的协议格式，并以字符串的形式存储，然后将字符串存储的操作追加到aof文件后。 （4）函数replicationFeedSlaves（）主要将操作扩散到每一个slave中；在该函数中将遍历自己下面挂的每一个slave，以此对每个slave进行如下两步的处理：将slave的数据库切换到本操作所对应的数据库（如果slave的数据库id与当前操作的数据id不一致时才进行此操作）；将命令和参数按照redis的协议格式写入到slave的回复缓存中。写入切换数据库的命令时将调用addReply，写入命令和参数时将调用addReplyMultiBulkLen和addReplyBulk，函数addReplyMultiBulkLen和addReplyBulk最终也将调用函数addReply。 （5）在函数addReply中将调用prepareClientToWrite（）设置slave的socket写入事件处理函数sendReplyToClient（通过函数aeCreateFileEvent进行设置），这样一旦slave对应的socket发送缓存中有空间写入数据，即调用sendReplyToClient进行处理。 （6）函数sendReplyToClient（）的主要功能是将slave中要发送的数据通过socket发出去。 图中的序号表示调用的先后关系，同级之间的序号才有意义。 "},"Chapter08/RedisString.html":{"url":"Chapter08/RedisString.html","title":"Redis String底层实现","keywords":"","body":"Redis String底层实现 Redis使用自己的简单动态字符串(simple dynamic string, SDS)的抽象类型。Redis中，默认以SDS作为自己的字符串表示。 只有在一些字符串不可能出现变化的地方使用C字符串。 SDS的定义如下： struct sdshdr { // 用于记录buf数组中使用的字节的数目 // 和SDS存储的字符串的长度相等 int len; // 用于记录buf数组中没有使用的字节的数目 int free; // 字节数组，用于储存字符串 char buf[]; //buf的大小等于len+free+1，其中多余的1个字节是用来存储’\\0’的。 }; SDS除了用来保存数据库中的字符串之外，SDS还被用作缓冲区（buffer），如AOF模块中的AOF缓冲区，以及客户端状态中的输入缓冲区 SDS 的存储示例： 使用SDS而不使用c语言的string的好处： 常数复杂度获取字符串长度 C语言中:字符串只是简单的字符的数组，当使用strlen获取字符串长度的时候，内部其实是直接顺序遍历数组的内容，找到对应的’\\0’对应的字符，从而计算出字符串的长度。即O(N)。 SDS:只需要访问SDS的len属性就能得到字符串的长度，复杂度为O(1)。 杜绝缓冲区溢出 Redis是C语言编写的，并没有方便的数据类型来进行内存的分配和释放（C++ STL String），必须手动进行内存分配和释放。 对于字符串的拼接、复制等操作，C语言开发者必须确保目标字符串的空间足够大，不然就会出现溢出的情况。 当使用SDS的API对字符串进行修改的时候， API内部第一步会检测字符串的大小是否满足。 如果空间已经满足要求，那么就像C语言一样操作即可。如果不满足，则拓展buf的空间 之后再进行操作。每次操作之后，len和free的值会做相应的修改。 扩展buf空间策略： 修改之后总长度len 修改之后总长度len>=1MB: 总空间为len+1MB+1。 换句话说，预分配的空间上限是1MB，尽量为len。 减少修改字符串时带来的内存重分配次数 Redis主要通过以下两种策略来处理内存问题。 字符串长度增加操作时，进行空间预分配 字符串长度减少操作时，惰性空间释放 当执行字符串长度缩短的操作的时候，SDS并不直接重新分配多出来的字节，而是修改len和free的值（len相应减小，free相应增大，buf的空间大小不变化），避免内存重分配。 SDS也提供直接释放未使用空间的API，在需要的时候，也能真正的释放掉多余的空间。 二进制安全 C字符串除了末尾之外不能出现空字符，否则会被程序认为是字符串的结尾。这就使得C字符串只能存储文本数据，而不能保存图像，音频等二进制数据。 使用SDS就不需要依赖控制符，而是用len来指定存储数据的大小，所有的SDS API都会以处理二进制的方式来处理SDS的buf的数据。程序不会对buf的数据做任何限制、过滤或假设，数据写入的时候是什么，读取的时候依然不变。 兼容部分C字符串函数 SDS的buf的定义（字符串末尾为’\\0’)和C字符串完全相同，因此很多的C字符串的操作都是适用于SDS->buf的。比如当buf里面存的是文本字符串的时候，大多数通过调用C语言的函数就可以。 总结 C字符串 SDS 获取字符串长度的复杂度为O(N) 可以使用所有库中的函数 API是不安全的，可能会造成缓冲区溢出 API是安全的，不会造成缓冲区溢出 修改字符串长度N次必然需要执行N次内存重分配 修改字符串长度N次最多需要执行N次内存重分配 只能保存文本数据 可以保存文本或者二进制数据 可以使用所有库中的函数 可以使用一部分库的函数 "},"Chapter09/db.html":{"url":"Chapter09/db.html","title":"Part IX 数据库篇","keywords":"","body":"第九章 数据库篇 mysql时区 innodb线程 /g/G mysql命令集 聚集索引和非聚集索引 OLTP&OLAP 术语 优化方案 Mybatis鸡肋的缓存体系 内存结构 MySQL一次insert刷几次盘 "},"Chapter09/MysqlServerTimezone.html":{"url":"Chapter09/MysqlServerTimezone.html","title":"mysql时区","keywords":"","body":"mysql时区 查看时区 mysql> show variables like '%time_zone%'; +------------------+--------+ | Variable_name | Value | +------------------+--------+ | system_time_zone | EDT | | time_zone | SYSTEM | +------------------+--------+ 2 rows in set (0.00 sec) system_time_zone 表示系统使用的时区是 EDT即北美的东部夏令时(-4h) time_zone 表示 MySQL 采用的是系统的时区。也就是说，如果在连接时没有设置时区信息，就会采用这个时区配置。 修改时区 # 仅修改当前会话的时区，停止会话失效 set time_zone = '+8:00'; # 修改全局的时区配置 set global time_zone = '+8:00'; flush privileges;  当然，也可以通过修改配置文件(my.cnf)的方式来实现配置，不过需要重启服务。 # vim /etc/my.cnf ##在[mysqld]区域中加上 default-time_zone = '+8:00' # /etc/init.d/mysqld restart ##重启mysql使新时区生效 对于线上数据库不能操作怎么办? datasource: url: jdbc:mysql://aaa:2600/test?useUnicode=true&characterEncoding=UTF-8&serverTimezone=GMT%2B8 关键点就是 &serverTimezone=GMT%2B8 当然也可以更换为 &serverTimezone=Asia/Shanghai 或者 &serverTimezone=Asia/Chongqing 解决了什么问题? 我们在用java代码插入到数据库时间时。 比如在java代码里面插入的时间为：2018-06-24 17:29:56 但是在数据库里面显示的时间却为：2018-06-24 09:29:56 有了8个小时的时差。这就是时区不同造成的。 注意：虽然数据库显示是 2018-06-24 09:29:56 但是还用原来的java逻辑读取出来时间还会恢复正常及：2018-06-24 17:29:56 "},"Chapter09/innodbThread.html":{"url":"Chapter09/innodbThread.html","title":"innodb线程","keywords":"","body":"innodb线程 后台线程的主要作用是负责刷新内存池中的数据，保证缓冲池中的内存缓存是最近的数据。此外，将已修改的数据文件刷新到磁盘文件中，同时保证出现异常时能够恢复到正常状态 InnoDB是多线程的模型，后台的线程主要有几大类 Master Thread Master Thread是一个核心的后台线程， 主要负责将缓冲池中的数据异步刷新到磁盘， 保证数据的一致性，包括脏页的刷新，合并插入缓冲，undo页的回收 IO Thread 在InnoDb存储引擎中大量使用Async IO来处理IO的请求， 可以极大提高数据库的性能。 而IO Thread的主要工作是负责IO请求的回调处理（call back）。 较早之前版本有4个IO Thread 分别是write , read , insert buffer,log IO Thread Purge Thread 事务被提交后，其所使用的undolog可能不再需要， 因此需要Purge Thread来及时回收已经分配的undo页。 innodb 1.1 之前 purge操作仅在master thread 中完成。 innodb 1.2开始 innodb支持设置多个purge thread，这样做的目的 是为了加快undo页的回收。 注意：设置了purge thread 并不代表 master thread 不再回收undo页。 [mysqld] innodb_purge_threads = 1 Page Cleaner Thread Page Cleaner Thread为高版本InnoDB引擎引入， 其作用是将之前版本的脏页刷新操作都放入单独的线程来完成。 其目的为了减轻Master Thread的工作及对于用户查询线程的阻塞， 从而进一步提高InnoDB存储引擎的性能 注意：Page Cleaner Thread之后master thread 不在进行脏读页的刷新操作 innodb 内存结构 "},"Chapter09/gG.html":{"url":"Chapter09/gG.html","title":"/g/G","keywords":"","body":"mysql-\\g和\\G的作用 \\g 的作用是分号和在sql语句中写’;’是等效的 \\G 的作用是将查到的结构旋转90度变成纵向 \\g的使用例子：查找一个表的创建语句 mysql> create table mytable(id int)\\g Query OK, 0 rows affected (0.21 sec) mysql> show create table mytable \\g +---------+-------------------------------------------------------------------------------------------+ | Table | Create Table | +---------+-------------------------------------------------------------------------------------------+ | mytable | CREATE TABLE `mytable` ( `id` int(11) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +---------+-------------------------------------------------------------------------------------------+ 1 row in set (0.04 sec) 上面的查找的表的创建语句看着很别扭，那么可以使用\\G,试一下就知道它的用途了 mysql> show create table mytable \\G *************************** 1. row *************************** Table: mytable Create Table: CREATE TABLE `mytable` ( `id` int(11) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.00 sec) 这个时候用着\\G感觉使结果很清晰 mysql> show variables like 'innodb_version'; +----------------+--------+ | Variable_name | Value | +----------------+--------+ | innodb_version | 8.0.19 | +----------------+--------+ 1 row in set (0.09 sec) mysql> show variables like 'innodb_version'\\g; +----------------+--------+ | Variable_name | Value | +----------------+--------+ | innodb_version | 8.0.19 | +----------------+--------+ 1 row in set (0.00 sec) ERROR: No query specified mysql> show variables like 'innodb_version'\\G; *************************** 1. row *************************** Variable_name: innodb_version Value: 8.0.19 1 row in set (0.00 sec) ERROR: No query specified "},"Chapter09/CommandSet.html":{"url":"Chapter09/CommandSet.html","title":"mysql命令集","keywords":"","body":"mysql命令集 显示密码策略 mysql> SHOW VARIABLES LIKE 'validate_password%'; +--------------------------------------+--------+ | Variable_name | Value | +--------------------------------------+--------+ | validate_password.check_user_name | ON | | validate_password.dictionary_file | | | validate_password.length | 8 | | validate_password.mixed_case_count | 1 | | validate_password.number_count | 1 | | validate_password.policy | MEDIUM | | validate_password.special_char_count | 1 | +--------------------------------------+--------+ 7 rows in set (0.13 sec) /*修改密码策略*/ mysql> set global validate_password.policy = LOW; Query OK, 0 rows affected (0.01 sec) 修改密码 mysql> set password for 'fire'@'%'= 'fire1234'; Query OK, 0 rows affected (0.03 sec) mysql> ALTER USER 'root'@'localhost' IDENTIFIED BY '123456'; 新建用户 mysql->create user 'test'@'localhost' identified by '123456'; mysql->create user 'test'@'192.168.1.11' identified by '123456'; mysql->create user 'test'@'%' identified by '123456'; 把某给表授权个某个用户 /*授予用户通过外网IP对于该数据库的全部权限*/ mysql> grant all privileges on `activiti`.* to 'fire'@'%' ; Query OK, 0 rows affected (0.02 sec) /*授予用户在本地服务器对该数据库的全部权限*/ mysql>grant select on test.* to 'user1'@'localhost'; /*给予查询权限*/ mysql>grant insert on test.* to 'user1'@'localhost'; /*添加插入权限*/ mysql>grant delete on test.* to 'user1'@'localhost'; /*添加删除权限*/ mysql>grant update on test.* to 'user1'@'localhost'; /*添加权限*/ mysql> flush privileges; /*刷新权限*/ 查询innodb版本 mysql> show variables like 'innodb_version'; +----------------+--------+ | Variable_name | Value | +----------------+--------+ | innodb_version | 8.0.19 | +----------------+--------+ 1 row in set (0.09 sec) mysql 配置文件位置 [fire@localhost ~]$ mysql --help | grep my.cnf order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf 查询innodb IO Thread 线程数 mysql> show variables like 'innodb_%io_threads'; +-------------------------+-------+ | Variable_name | Value | +-------------------------+-------+ | innodb_read_io_threads | 4 | | innodb_write_io_threads | 4 | +-------------------------+-------+ 2 rows in set (0.01 sec) 查询innodb中的IO Thread mysql> show engine innodb status\\G; *************************** 1. row *************************** Type: InnoDB Name: Status: ===================================== 2020-03-16 10:54:55 0x7fe1f415e700 INNODB MONITOR OUTPUT ===================================== Per second averages calculated from the last 8 seconds ----------------- BACKGROUND THREAD ----------------- srv_master_thread loops: 1 srv_active, 0 srv_shutdown, 3887 srv_idle srv_master_thread log flush and writes: 0 ---------- SEMAPHORES ---------- OS WAIT ARRAY INFO: reservation count 0 OS WAIT ARRAY INFO: signal count 0 RW-shared spins 0, rounds 0, OS waits 0 RW-excl spins 0, rounds 0, OS waits 0 RW-sx spins 0, rounds 0, OS waits 0 Spin rounds per wait: 0.00 RW-shared, 0.00 RW-excl, 0.00 RW-sx ------------ TRANSACTIONS ------------ Trx id counter 3592 Purge done for trx's n:o 可以看到IO Thread 0 为 insert buffer thread. IO thread 1 为 log thread。 之后就是根据innodb_read_io_threads 及 innodb_write_io_threads 来设置的读写线程。 并且读线程的ID总是小于写线程 查询innodb缓存池大小 mysql> show variables like 'innodb_buffer_pool_size'; +-------------------------+-----------+ | Variable_name | Value | +-------------------------+-----------+ | innodb_buffer_pool_size | 134217728 | +-------------------------+-----------+ 1 row in set (0.09 sec) 展示LRU算法的midpoint位置 mysql> show variables like 'innodb_old_blocks_pct'\\G; *************************** 1. row *************************** Variable_name: innodb_old_blocks_pct Value: 37 1 row in set (0.13 sec) ERROR: No query specified 查看lRU列表及FREE列表的使用情况和运行状态 mysql> show engine innodb status\\G; *************************** 1. row *************************** Type: InnoDB Name: Status: ===================================== 2020-03-19 11:00:52 0x7f18440fe700 INNODB MONITOR OUTPUT ===================================== Per second averages calculated from the last 23 seconds ----------------- BACKGROUND THREAD ----------------- srv_master_thread loops: 2 srv_active, 0 srv_shutdown, 190 srv_idle srv_master_thread log flush and writes: 0 ---------- SEMAPHORES ---------- OS WAIT ARRAY INFO: reservation count 0 OS WAIT ARRAY INFO: signal count 0 RW-shared spins 0, rounds 0, OS waits 0 RW-excl spins 0, rounds 0, OS waits 0 RW-sx spins 0, rounds 0, OS waits 0 Spin rounds per wait: 0.00 RW-shared, 0.00 RW-excl, 0.00 RW-sx ------------ TRANSACTIONS ------------ Trx id counter 4104 Purge done for trx's n:o 慢sql阈值 mysql> show variables like 'long_query_time'\\G; *************************** 1. row *************************** Variable_name: long_query_time Value: 10.000000 1 row in set (0.00 sec) ERROR: No query specified 记录慢sql设置 mysql> show variables like 'log_slow_queries'\\G; Empty set (0.00 sec) ERROR: No query specified 无索引sql mysql> show variables like 'log_queries_not_using_indexes'\\G; *************************** 1. row *************************** Variable_name: log_queries_not_using_indexes Value: OFF 1 row in set (0.00 sec) ERROR: No query specified 每分钟允许记录到slow log 的且未使用索引的sql语句次数 mysql> show variables like 'log_throttle_queries_not_using_indexes'\\G; *************************** 1. row *************************** Variable_name: log_throttle_queries_not_using_indexes Value: 0 1 row in set (0.01 sec) ERROR: No query specified /默认是0表示没有限制/ 分析慢sql日志 [root@localhost etc]# mysqldumpslow nh122-190-slow.log Reading mysql slow query log from nh122-190-slow.log 执行时间最长的10条sql语句 [root@localhost log]# mysqldumpslow -s al -n 10 david.log Reading mysql slow query log from david.log 查询表独立空间开启 mysql> show variables like 'innodb_file_per_table'\\G; *************************** 1. row *************************** Variable_name: innodb_file_per_table Value: ON 1 row in set (0.01 sec) ERROR: No query specified 开启表独立空间。用户可以将每个基于innodb存储引擎的表产生一个独立表空间 。独立表空间的命名规则为：表明.ibd。 自动提交 /*关闭自动提交*/ mysql> set autocommit = 0; Query OK, 0 rows affected (0.00 sec) /*打开自动提交*/ mysql> set autocommit = 1; Query OK, 0 rows affected (0.00 sec) "},"Chapter09/ClusteredIndex.html":{"url":"Chapter09/ClusteredIndex.html","title":"聚集索引和非聚集索引","keywords":"","body":"聚集索引和非聚集索引 声明：本文中提到的索引都是以B+ tree组织的 MySQL的Innodb存储引擎的索引分为聚集索引和非聚集索引两大类， 理解聚集索引和非聚集索引可通过对比汉语字典的索引。 汉语字典提供了两类检索汉字的方式，第一类是拼音检索（前提是知道该汉字读音）， 比如拼音为cheng的汉字排在拼音chang的汉字后面， 根据拼音找到对应汉字的页码（因为按拼音排序，二分查找很快就能定位），这就是我们通常所说的字典序； 第二类是部首笔画检索，根据笔画找到对应汉字，查到汉字对应的页码。 拼音检索就是聚集索引，因为存储的记录（数据库中是行数据、字典中是汉字的详情记录）是按照该索引排序的； 笔画索引，虽然笔画相同的字在笔画索引中相邻，但是实际存储页码却不相邻。 正文内容按照一个特定维度排序存储，这个特定的维度就是聚集索引； Innodb存储引擎中行记录就是按照聚集索引维度顺序存储的，Innodb的表也称为索引表； 因为行记录只能按照一个维度进行排序，所以一张表只能有一个聚集索引。 非聚集索引索引项顺序存储，但索引项对应的内容却是随机存储的； 举个例子说明下： create table student ( `id` INT UNSIGNED AUTO_INCREMENT, `name` VARCHAR(255), PRIMARY KEY(`id`), KEY(`name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 该表中主键id是该表的聚集索引、name为非聚集索引；表中的每行数据都是按照聚集索引id排序存储的； 比如要查找name='Arla'和name='Arle'的两个同学，他们在name索引表中位置可能是相邻的， 但是实际存储位置可能差的很远。name索引表节点按照name排序，检索的是每一行数据的主键。 聚集索引表按照主键id排序，检索的是每一行数据的真实内容。 也就是说查询name='Arle'的记录时，首相通过name索引表查找到Arle的主键id（可能有多个主键id，因为有重名的同学）， 再根据主键id的聚集索引找到相应的行记录； 聚集索引一般是表中的主键索引，如果表中没有显示指定主键，则会选择表中的第一个不允许为NULL的唯一索引，如果还是没有的话， 就采用Innodb存储引擎为每行数据内置的6字节ROWID作为聚集索引。 每张表只有一个聚集索引，因为聚集索引在精确查找和范围查找方面良好的性能表现（相比于普通索引和全表扫描）， 聚集索引就显得弥足珍贵，聚集索引选择还是要慎重的（一般不会让没有语义的自增id充当聚集索引）。 从宏观上分析下聚集索引和普通索引的性能差异，还是针对上述student表： （1）select * from student where id >5000 and id （2）select * from student where name > 'Alie' and name 第一条SQL语句根据id进行范围查询，因为(5000, 20000)范围内的记录在磁盘上按顺序存储， 顺序读取磁盘很快就能读到这批数据。 第二条SQL语句查询（'Alie', 'John'）范围内的记录，主键id分布可能是离散的1，100，20001，5000.....； 增加了随机读取数据页几率；所以普通索引的范围查询效率被聚集索引甩开几条街都不止； 非聚集索引的精确查询效率还是可以的，比聚集索引查询只增加了一次IO开销。 聚集索引表记录的排列顺序与索引的排列顺序一致，优点是查询速度快，因为一旦具有第一个索引值的纪录被找到，具有连续索引值的记录也一定物理的紧跟其后。 聚集索引的缺点是对表进行修改速度较慢，这是为了保持表中的记录的物理顺序与索引的顺序一致，而把记录插入到数据页的相应位置，必须在数据页中进行数据重排，降低了执行速度。 非聚集索引指定了表中记录的逻辑顺序，但记录的物理顺序和索引的顺序不一致， 聚集索引和非聚集索引都采用了B+树的结构， 但非聚集索引的叶子层并不与实际的数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针的方式。 非聚集索引比聚集索引层次多，添加记录不会引起数据顺序的重组。 聚集索引:物理存储按照索引排序 非聚集索引:物理存储不按照索引排序 聚集索引在插入数据时速度要慢（时间花费在“物理存储的排序”上，也就是首先要找到位置然后插入）,但查询数据比非聚集数据的速度快 索引是通过二叉树的数据结构来描述的，我们可以这么理解聚簇索引：索引的叶节点就是数据节点。而非聚簇索引的叶节点仍然是索引节点，只不过有一个指针指向对应的数据块。 "},"Chapter09/OLTP&OLAP.html":{"url":"Chapter09/OLTP&OLAP.html","title":"OLTP&OLAP","keywords":"","body":"OLTP与OLAP 为了完全理解OLTP(联机事务处理)和OLAP(联机分析处理)，有必要结合一些行业背景来谈。 在软件技术发展的早期，数据通常存储在一个文件中。 随后，IT需要解决的问题越来越大，关系数据库管理系统(DBMS)开始席卷市场。 在接下来的几十年里，它几乎成为了每个企业的数据存储解决方案。 随着网络的出现，一切都发生了巨大的变化。搜索引擎和社交网络如今会在某些网域中对数据进行建模，这些数据之间的关系不容易识别，有时甚至根本不需要(例如搜索引擎索引文档)。 不过，一些传统的术语今天仍然在使用，用现代的方法来看待它们是很重要的。 其中两个就是OLTP和OLAP，下面的图片显示了OLTP和OLAP之间的关系。 OLTP是一个在线交易系统；OLAP是一个在线检索和分析系统 上图强调的是，OLTP和OLAP并非解决同一问题，不存在竞争关系，而是相互补充的过程。下面是更深入的解释。 关于OLTP OLTP，即联机事务处理，它通常会涉及到那些存储和管理与系统/公司日常运营有关数据的数据库。在过去，OLTP通常与正在运行的关系数据库相关联，其主要关注点是从给定环境正在发生的事情中收集数据。 简而言之:OLTP用于存储和管理日常运营的数据。 由于存储在OLTP数据仓库上的信息通常对业务来说至关重要，所以往往需要付出巨大的努力来确保数据的原子性、一致性、隔离性和持久性(即ACID)。根据这四个原则存储的数据会被标记为与ACID兼容，这就是关系数据库管理系统的优势所在。 但是拥有一个与ACID兼容的数据仓库并不意味着我们不需要做任何额外的工作来保证数据符合这些原则。我们处理数据的方式很重要，例如，如果在数据存储中允许冗余，我们如何保证数据的一致性? 如果我们要存储客户的地址，重要的是要确保当客户移动到另一个地方时，这个地址在任何地方都是更新的。但是，将地址存储在多个位置会使数据很难保持一致的状态。这就是为什么关系数据库常常被设计成与第五种范式(5NF)相匹配——一种避免冗余的关系数据设计方法。 正如前面所说，自从定义OLTP术语以来，世界已经发生了变化，现在很容易在非关系数据库中存储数据。这些数据存储大部分只符合ACID的四个原则中的一部分。但是，根据用例的不同，可以适当放宽这些原则中的一两个，以换取其他收益(速度、可伸缩性等)。 例如，如果我们在存储社交网络的帖子中“赞”的数量时，保证“赞”数量100%准确真的很重要吗?或者说，可不可以显示995个赞而不是准确的998个，以换取对数百万用户的更快响应? 由于OLTP指的是联机事务处理，所以我们看到它并不局限于关系数据库，甚至不完全附从ACID数据库。它只是指使用这些数据存储的方式，例如，如果我们使用文档数据存储(例如MongoDB)来存储和处理来自社交应用程序日常运营的数据(例如注册用户、存储点赞等)，我们也可以说它是OLTP。 关于OLAP OLAP，即联机分析处理，通常涉及到那些存储和管理与分析和决策相关的数据的数据库。 OLAP与商业智能(BI)紧密相关，BI是一种专门的软件开发模式，用于交付业务分析应用程序。换句话说，BI的目标是允许高层管理人员在没有IT人员参与的情况下查询和研究数据。 简而言之:OLAP用于分析数据并做出决策。 这个领域带来的最大进步是实时生成报告的能力，它不需要另外再请IT部门来定制报告，而是自动生成特定的报告。BI系统可以回答开发人员不需要提前知道的问题。 BI系统可以通过以一种名叫Hypercube(超立方体)的形式组织数据来实现，此形式会探索数据的多个维度，并允许用户通过操纵立方体的大小维度来聚合或下钻数据。 有趣的是，有了正确的接口，高层管理人员就可以在没有帮助的情况下动态生成报告。 OLAP系统可以使用关系数据库实现，这种技术通常称为ROLAP(关系OLAP)。但为此，我们需要用第三范式而不是第五范式来设计数据库。 在分析数据时，我们可以处理冗余数据，真正重要的是浏览数据维度的能力。这就是ROLAP的亮点，因为第三范式的数据库模式适合于聚合和下钻。 将它们相结合 当第一次遇到OLTP和OLAP这两个术语时，很容易产生疑问：哪个更好?实际上，我们应该问的是：他们如何互相补充? 我们现在知道: OLTP用于存储和管理日常操作的数据; OLAP用于分析这些数据。 这正是它们在现有业务中使用的方式。 OLTP和OLAP协同工作 上述示例中上部的数据(HR数据库、CRM、计费系统)一般是通过一个萃取、转置和加载(Extract, Transform and Load, ETL)的过程进行批量处理(通常是在夜间)。它用于从多个OLTP源收集数据并将其放入OLAP数据仓库(允许跨系统分析)。在图的下半部分，您可以看到数据在OLAP立方体中得到了正确的存储和组织。 这样，进行分析的人员就可以处理最新的信息，并及时做出决定，而不会中断操作。 "},"Chapter09/term.html":{"url":"Chapter09/term.html","title":"术语","keywords":"","body":"术语 逻辑门 逻辑门（Logic Gates)是在集成电路(Integrated Circuit)上的基本组件。 简单的逻辑门可由晶体管组成。 这些晶体管的组合可以使代表两种信号的高低电平在通过它们之后产生高电平或者低电平的信号。 高、低电平可以分别代表逻辑上的“真”与“假”或二进制当中的1和0，从而实现逻辑运算。 逻辑门又称“数字逻辑电路基本单元”。执行“或”、“与”、“非”、“或非”、“与非”等逻辑运算的电路。 任何复杂的逻辑电路都可由这些逻辑门组成。广泛用于计算机、通信、控制和数字化仪表。 逻辑门的作用是通过控制高、低电平（分别代表逻辑上的“真”与“假”或二进制当中的“1”和“0”），从而实现逻辑运算。 TTL TTL是 Time To Live的缩写，该字段指定IP包被路由器丢弃之前允许通过的最大网段数量。 TTL是IPv4报头的一个8 bit字段。 注意：TTL与DNS TTL有区别。二者都是生存时间，前者指ICMP包的转发次数（跳数），后者指域名解析信息在DNS中的存在时间。 TTL的作用是限制IP数据包在计算机网络中的存在的时间。TTL的最大值是255，TTL的一个推荐值是64。 虽然TTL从字面上翻译，是可以存活的时间，但实际上TTL是IP数据包在计算机网络中可以转发的最大跳数。TTL字段由IP数据包的发送者设置，在IP数据包从源到目的的整个转发路径上，每经过一个路由器，路由器都会修改这个TTL字段值，具体的做法是把该TTL的值减1，然后再将IP包转发出去。如果在IP包到达目的IP之前，TTL减少为0，路由器将会丢弃收到的TTL=0的IP包并向IP包的发送者发送 ICMP time exceeded消息。 TTL的主要作用是避免IP包在网络中的无限循环和收发，节省了网络资源，并能使IP包的发送者能收到告警消息。 TTL 是由发送主机设置的，以防止数据包不断在IP互联网络上永不终止地循环。转发IP数据包时，要求路由器至少将 TTL 减小 1 生存时间，就是一条域名解析记录在DNS服务器中的存留时间。当各地的DNS服务器接受到解析请求时，就会向域名指定的DNS服务器(权威域名服务器）发出解析请求从而获得解析记录；在获得这个记录之后，记录会在DNS服务器(各地的缓存服务器，也叫递归域名服务器）中保存一段时间，这段时间内如果再接到这个域名的解析请求，DNS服务器将不再向NS服务器发出请求，而是直接返回刚才获得的记录；而这个记录在DNS服务器上保留的时间，就是TTL值。 扇入和扇出 扇出（fan-out）是一个定义单个逻辑门能够驱动的数字信号输入最大量的专业术语。大多数的TTL逻辑门能够为10个其他数字门或驱动器提供信号。所以，一个典型的TTL逻辑门有10个扇出信号。 在软件工程中的定义：该模块直接调用的下级模块的个数。在面向对象编程中，扇出应用于继承。 一个模块被其他模块调用的个数，称为该模块的扇入。扇入大些，一般不会影响问题的复杂性，而且扇入越大，说明该模块的复用性越好。 在软件设计中，扇入和扇出的概念是指应用程序模块之间的层次调用情况。 按照结构化设计方法，一个应用程序是由多个功能相对独立的模块所组成。 扇入：是指直接调用该模块的上级模块的个数。扇入大表示模块的复用程度高。 扇出：是指该模块直接调用的下级模块的个数。扇出大表示模块的复杂度高， 需要控制和协调过多的下级模块；但扇出过小（例如总是1）也不好。 扇出过大一般是因为缺乏中间层次，应该适当增加中间层次的模块。 扇出太小时可以把下级模块进一步分解成若干个子功能模块，或者合并到它的上级模块中去。 设计良好的软件结构，通常顶层扇出比较大，中间扇出小，底层模块则有大扇入。 索引的填充因子 创建索引时，可以指定一个填充因子，以便在索引的每个叶级页上留出额外的间隙和保留一定百分比的空间，供将来表的数据存储容量进行扩充和减少页拆分的可能性。填充因子的值是从 0 到 100 的百分比数值，指定在创建索引后对数据页的填充比例。值为 100 时表示页将填满，所留出的存储空间量最小。只有当不会对数据进行更改时（例如，在只读表中）才会使用此设置。值越小则数据页上的空闲空间越大，这样可以减少在索引增长过程中对数据页进行拆分的需要，但需要更多的存储空间。当表中数据会发生更改时，这种设置更为适当。 填充因子越大,意味着一个索引页包含的索引记录越多,空闲空间越小.一般来说查询的效率越高,因为这样查询的时候,可以减少读取索引页的工作量和减少内存使用 但这样导致的结果是数据变动导致的索引维护的工作量增加,因为索引页的空闲空间小,如果不能在本页内完成索引调整,就会引起调整其他索引页 所以一般的选择是,数据基本不变化的（例如OLAP的场景，数据只用来分析的）,将填充因子设置到足够大,数据经常变化的（例如OLTP的场景）,将填充因子设置为足够小 填充因子是页大小的1/16，页默认16k就是是预留1k的空间，类似oracle的pct free，为以后的Update留出空闲的空间，避免类似行迁移的问题，使每次访问增加了IO操作。 查看参数 17:57:45 mysql>show variables like ‘%fill%’; ±-------------------±------+ | Variable_name | Value | ±-------------------±------+ | innodb_fill_factor | 100 | ±-------------------±------+ 100的话表预留1K的空间，80的话表示预留20%的空间。 这个参数很少使用，如果更新很多的话可以考虑小这个参数。 "},"Chapter09/optimization.html":{"url":"Chapter09/optimization.html","title":"优化方案","keywords":"","body":"mysql优化 引擎的选择 linux版本mysql5.5后默认引擎是innodb。 MyISAM不支持事务，如果表中绝大多数都只是读查询，可以考虑MyISAM。比如码表。 字段的选择 选择合适的字段类型，填充默认值，非空处理 long类型替换datetime，如果可以用int类型替换datetime 逆范式 逆范式是指打破范式，通过增加冗余或重复的数据来提高数据库的性能。 多做单边查询,少做联合查询 索引 不要过度索引。索引越多，占用空间越大，反而性能变慢； 只对WHERE子句中频繁使用的建立索引； 列独立 左原则 假如业务逻辑上出现: field like '%keywork%'类似查询，需要使用全文索引。 复合索引：一个索引关联多个字段，仅仅针对左边字段有效果。 OR的使用: 必须要保证 OR 两端的条件都存在可以用的索引，该查询才可以使用索引。 索引排序: 如果order by 排序需要的字段上存在索引，则可能使用到索引。 前缀索引: 前缀索引是建立索引关键字一种方案。通常会使用字段的整体作为索引关键字。有时，即使使用字段前部分数据，也可以去识别某些记录。就比如一个班级里，我要找王xx，假如姓王的只有1个人，那么就可以建一个前缀索引，就是王。 全文索引: 全文索引几乎不用，因为它不支持中文 加大缓存池 32位系统最大缓存池是4G; 64位系统理论上是 sql优化 对于并发性的SQL 少用（不用）多表操作（子查询，联合查询），而是将复杂的SQL拆分多次执行。如果查询很原子（很小），会增加查询缓存的利用率。 Limit 的使用 慢查询日志的使用 善于利用Explain 命令 分区分表 分库分表 "},"Chapter09/Mybatis.html":{"url":"Chapter09/Mybatis.html","title":"Mybatis鸡肋的缓存体系","keywords":"","body":"Mybatis鸡肋的缓存体系 众所周知mybatis缓存体系分为一级缓存和二级缓存，所以今天就分别聊聊这两级缓存。 一级缓存 一级缓存的使用是不需要任何配置的，直接使用session就可以使用一级缓存。代码如下: public List selectList(String statement, Object parameter, RowBounds rowBounds) { try { MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); }... } } public List query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException { BoundSql boundSql = ms.getBoundSql(parameter); CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql); return query(ms, parameter, rowBounds, resultHandler, key, boundSql); } public List query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { ... list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql); ... return list; } private List queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { List list; localCache.putObject(key, EXECUTION_PLACEHOLDER); try { list = doQuery(ms, parameter, rowBounds, resultHandler, boundSql); } finally { localCache.removeObject(key); } localCache.putObject(key, list); if (ms.getStatementType() == StatementType.CALLABLE) { localOutputParameterCache.putObject(key, parameter); } return list; } localCache就是所谓的一级缓存。 this.localCache = new PerpetualCache(\"LocalCache\"); public class PerpetualCache implements Cache { private final String id; private Map cache = new HashMap(); public PerpetualCache(String id) { this.id = id; } } 从代码中可以看出localCache是一个PerpetualCache的实现类。在该类里面，存了一个hashmap,这便是一级缓存。看到这儿大家会发现一级缓存是如此简单，以至于没有任何的淘汰，过期策略! 如果一直执行查询的话，一级缓存会不断增加。那么什么时候一级缓存会清空呢？两种情况: 执行增，删，改，之类的写操作的时候，一级缓存会清空。 当session执行commit的时候，一级缓存会清空。 看到这儿大家会发现，一级缓存并不好用。 首先每次修改数据库的时候，缓存都会被清空 对，是清空! 其次，因为没有过期，淘汰之类的策略，长时间的查询会导致缓存变得异常庞大。只能通过session的commit操作来清空缓存，对又是清空! 这边还有个细节就是当mybatis与spring整合的时候，mybatis在SqlSessionTemplate类中给session封装了一层SqlSessionInterceptor，而这个类中有这样一个逻辑。 private class SqlSessionInterceptor implements InvocationHandler { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ... Object result = method.invoke(sqlSession, args); if (!isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) { sqlSession.commit(true); } return result; ... } } 如果该线程没有开启事务则执行sqlSession.commit(true)。也就是清空一级缓存。所以大家在spring中使用mybatis的话，是不用担心一级缓存的问题的，因为每次操作后都会删掉。当然在spring中你也别想使用一级缓存。 二级缓存 二级缓存使用需要开启一下配置，首先在mapper文件中添加cache配置。 其次数据bean还需要实现Serializable接口。这样就可以开启二级缓存了。二级缓存代码如下: public List selectList(String statement, Object parameter, RowBounds rowBounds) { try { MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } ... } } public List query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException { BoundSql boundSql = ms.getBoundSql(parameterObject); CacheKey key = createCacheKey(ms, parameterObject, rowBounds, boundSql); return query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); } public List query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException { Cache cache = ms.getCache(); ... List list = (List) tcm.getObject(cache, key); if (list == null) { list = delegate. query(ms, parameterObject, rowBounds, resultHandler, key, boundSql); tcm.putObject(cache, key, list); } return list; } ... } 这个地方cache就是二级缓存。当然二级缓存在使用上和一级缓存稍有区别。一级缓存执行session.commit()之后，缓存就清空了。二级缓存则必须执行session.commit()数据才会被真正的缓存下来。这边可以看一下 tcm.putObject(cache, key, list); 这个方法 public void putObject(Cache cache, CacheKey key, Object value) { getTransactionalCache(cache).putObject(key, value); } public void putObject(Object key, Object object) { 数据仅仅是被存到了entriesToAddOnCommit这个里面 entriesToAddOnCommit.put(key, object); } session.commit(); public void commit(boolean force) { try { executor.commit(isCommitOrRollbackRequired(force)); dirty = false; } catch (Exception e) { throw ExceptionFactory.wrapException(\"Error committing transaction. Cause: \" + e, e); } finally { ErrorContext.instance().reset(); } } public void commit(boolean required) throws SQLException { delegate.commit(required); //这个方法才是真正存数据的方法 tcm.commit(); } public void commit() { for (TransactionalCache txCache : transactionalCaches.values()) { txCache.commit(); } } public void commit() { if (clearOnCommit) { delegate.clear(); } flushPendingEntries(); reset(); } private void flushPendingEntries() { for (Map.Entry entry : entriesToAddOnCommit.entrySet()) { delegate.putObject(entry.getKey(), entry.getValue()); } for (Object entry : entriesMissedInCache) { if (!entriesToAddOnCommit.containsKey(entry)) { delegate.putObject(entry, null); } } } 二级缓存是带有过期和淘汰策略的。进入XMLMapperBuilder的cacheElement方法: private void cacheElement(XNode context) throws Exception { if (context != null) { String type = context.getStringAttribute(\"type\", \"PERPETUAL\"); Class typeClass = typeAliasRegistry.resolveAlias(type); String eviction = context.getStringAttribute(\"eviction\", \"LRU\"); Class evictionClass = typeAliasRegistry.resolveAlias(eviction); Long flushInterval = context.getLongAttribute(\"flushInterval\"); Integer size = context.getIntAttribute(\"size\"); boolean readWrite = !context.getBooleanAttribute(\"readOnly\", false); boolean blocking = context.getBooleanAttribute(\"blocking\", false); Properties props = context.getChildrenAsProperties(); builderAssistant.useNewCache(typeClass, evictionClass, flushInterval, size, readWrite, blocking, props); } } 这个地方可以看到和缓存相关的配置以及默认值。这边逻辑比较简单大家自己看看就好。 那么相比于一级缓存而言，拥有了过期，淘汰等策略，还可以自定义的二级缓存是不是就好用一点了呢？至少我觉得不是，因为翻看代码可以发现，在默认情况下，每次执行增删改等操作的时候，二级缓存也会被清空。这等于手动触发缓存雪崩啊。当然你可以自己定义，但是因为CacheKey是固定生成模式，想要自己定义，得自己解析相应的key或者结果集，还是比较麻烦的。 "},"Chapter09/memory.html":{"url":"Chapter09/memory.html","title":"内存结构","keywords":"","body":"内存结构 "},"Chapter09/IndexMerge.html":{"url":"Chapter09/IndexMerge.html","title":"mysql索引合并:一条sql可以使用多个索引","keywords":"","body":"mysql索引合并:一条sql可以使用多个索引 前言 mysql的索引合并并不是什么新特性。早在mysql5.0版本就已经实现。之所以还写这篇博文，是因为好多人还一直保留着一条sql语句只能使用一个索引的错误观念。本文会通过一些示例来说明如何使用索引合并。 什么是索引合并 下面我们看下mysql文档中对索引合并的说明： The Index Merge method is used to retrieve rows with several range scans and to merge their results into one. The merge can produce unions, intersections, or unions-of-intersections of its underlying scans. This access method merges index scans from a single table; it does not merge scans across multiple tables. 根据官方文档中的说明，我们可以了解到： 索引合并是把几个索引的范围扫描合并成一个索引。 索引合并的时候，会对索引进行并集，交集或者先交集再并集操作，以便合并成一个索引。 这些需要合并的索引只能是一个表的。不能对多表进行索引合并。 使用索引合并有啥收益 简单的说，索引合并，让一条sql可以使用多个索引。对这些索引取交集，并集，或者先取交集再取并集。从而减少从数据表中取数据的次数，提高查询效率。 怎么确定使用了索引合并 在使用explain对sql语句进行操作时，如果使用了索引合并，那么在输出内容的type列会显示 index_merge，key列会显示出所有使用的索引。如下 在explain的extra字段中会以下几种： Using union 索引取并集 Using sort_union 先对取出的数据按rowid排序，然后再取并集 Using intersect 索引取交集 你会发现并没有 sort_intersect，因为根据目前的实现，想索引取交集，必须保证通过索引取出的数据顺序和rowid顺序是一致的。所以，也就没必要sort了。 "},"Chapter10/net.html":{"url":"Chapter10/net.html","title":"Part X 网络篇","keywords":"","body":"第十章 网络篇 OSI模型概念 网络层详解 网际报文控制协议ICMP TCP协议 HTTP协议详解 HTTP是如何使用TCP连接 HTTP2 HTTPS协议详解 WebSocket IM "},"Chapter10/OSI.html":{"url":"Chapter10/OSI.html","title":"OSI模型概念","keywords":"","body":"OSI模型概念 应用层:客户操作浏览器(7层) GRPC协议 表示层:SSL\\TSL转换 会话层:概念层session 传输层： 网络层: 路由器;IP协议保证广域网中数据传输 INTERNET 数据链路层: 网桥，交换机;局域网中通过MAC地址链接到相应的交换机、路由器将报文传输到另外一个主机上 物理层: 网卡、网线、集线器、中继器、调制解调器等介质 集线器是物理层设备,采用广播的形式来传输信息。 交换机就是用来进行报文交换的机器。多为链路层设备(二层交换机)，能够进行地址学习，采用存储转发的形式来交换报文.。 路由器的一个作用是连通不同的网络，另一个作用是选择信息传送的线路。选择通畅快捷的近路，能大大提高通信速度，减轻网络系统通信负荷，节约网络系统资源，提高网络系统畅通率。 交换机的工作原理 交换机拥有一条很高带宽的内部总线和内部交换矩阵。交换机的所有的端口都挂接在这条总线上，控制电路收到数据包以后，处理端口会查找内存中的地址对照表以确定目的MAC（网卡的硬件地址）的NIC（网卡）挂接在哪个端口上，通过内部交换矩阵迅速将数据包传送到目的端口，目的MAC若不存在则广播到所有的端口，接收端口回应后交换机会“学习”新的地址，并把它添加入内部MAC地址表中。 使用交换机也可以把网络“分段”，通过对照MAC地址表，交换机只允许必要的网络流量通过交换机。通过交换机的过滤和转发，可以有效的隔离广播风暴，减少误包和错包的出现，避免共享冲突。 交换机在同一时刻可进行多个端口对之间的数据传输。每一端口都可视为独立的网段，连接在其上的网络设备独自享有全部的带宽，无须同其他设备竞争使用。当节点A向节点D发送数据时，节点B可同时向节点C发送数据，而且这两个传输都享有网络的全部带宽，都有着自己的虚拟连接。 总之，交换机是一种基于MAC地址识别，能完成封装转发数据包功能的网络设备。交换机可以\"学习\"MAC地址，并把其存放在内部地址表中，通过在数据帧的始发者和目标接收者之间建立临时的交换路径，使数据帧直接由源地址到达目的地址。 集线器 集线器的英文称为“Hub”。集线器的主要功能是对接收到的信号进行再生整形放大，以扩大网络的传输距离，同时把所有节点集中在以它为中心的节点上。 它 工作于OSI(开放系统互联参考模型)参考模型第一层，即“物理层”。 集线器与网卡、网线等传输介质一样，属于局域网中的基础设备，采用 CSMA/CD（即带冲突检测的载波监听多路访问技术)介质访问控制机制。 集线器每个接口简单的收发比特，收到1就转发1，收到0就转发0，不进行碰撞检 测。 集线器属于纯硬件网络底层设备，基本上不具有类似于交换机的\"智能记忆\"能力和\"学习\"能力。它也不具备交换机所具有的MAC地址表，所以它发送数据 时都是没有针对性的，而是采用广播方式发送。 也就是说当它要向某节点发送数据时，不是直接把数据发送到目的节点，而是把数据包发送到与集线器相连的所有节 点。 HUB是一个多端口的转发器，当以HUB为中心设备时，网络中某条线路产生了故障，并不影响其它线路的工作。所以HUB在局域网中得到了广泛的应用。 大多数的时候它用在星型与树型网络拓扑结构中。 集线器的交换机的区别 首先说HUB,也就是集线器。它的作用可以简单的理解为将一些机器连接起来组成一个局域网。而交换机（又名交换式集线器）作用与集线器大体相同。但是两者在性能上有区别：集线器采用的式共享带宽的工作方式，而交换机是独享带宽。这样在机器很多或数据量很大时，两者将会有比较明显的。 工作位置不同。集线器工作在物理层，而交换机工作在数据链路层。 工作方式不同。集线器是一种广播方式，当集线器的某个端口工作时其他端口都能收听到信息。交换机工作时端口互不影响。 带宽不同。集线器是所有端口共享一条带宽，在同一时刻只能有两个端口传输数据；而交换机每个端口独占一条带宽。 性能不同。交换机以MAC地址进行寻址，有一定额外的寻址开销；集线器以广播方式传输数据，流量小时性能下降不明显，适用于共享总线的局域网。 路由器与交换机的区别 总的来说，路由器与交换机的主要区别体现在以下几个方面： 工作层次不同。最初的的交换机是工作在数据链路层，而路由器一开始就设计工作在网络层。由于交换机工作在数据链路层，所以它的工作原理比较简单，而路由器工作在网络层，可以得到更多的协议信息，路由器可以做出更加智能的转发决策。 数据转发所依据的对象不同。交换机是利用物理地址或者说MAC地址来确定转发数据的目的地址。而路由器则是利用IP地址来确定数据转发的地址。IP地址是在软件中实现的，描述的是设备所在的网络。MAC地址通常是硬件自带的，由网卡生产商来分配的，而且已经固化到了网卡中去，一般来说是不可更改的。而IP地址则通常由网络管理员或系统自动分配。 传统的交换机只能分割冲突域，不能分割广播域；而路由器可以分割广播域。由交换机连接的网段仍属于同一个广播域，广播数据包会在交换机连接的所有网段上传播，在某些情况下会导致通信拥挤和安全漏洞。连接到路由器上的网段会被分配成不同的广播域，广播数据不会穿过路由器。虽然第三层以上交换机具有VLAN功能，也可以分割广播域，但是各子广播域之间是不能通信交流的，它们之间的交流仍然需要路由器。 交换机负责同一个网段的通信，而路由器负责不同网段的通信。路由器提供了防火墙的服务。路由器仅仅转发特定地址的数据包，不传送不支持路由协议的数据包传送和未知目标网络数据包的传送，从而可以防止广播风暴。 各层的作用 物理层：比特 在OSI参考模型中，物理层（Physical Layer）是参考模型的最低层。物理层的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。 “透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。 主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由1、0转化为电流强弱来进行传输,到达目的地后在转化为1、0，也就是我们常说的数模转换与模数转换）。这一层的数据叫做比特。 　　 物理层(physical layer)：在物理层上所传数据的单位是比特。物理层的任务就是透明地传送比特流。 数据链路层：帧 数据链路层（Data Link Layer）是OSI模型的第二层，负责建立和管理节点间的链路。该层的主要功能是：通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。在计算机网络中由于各种干扰的存在，物理链路是不可靠的。因此，这一层的主要功能是在物理层提供的比特流的基础上，通过差错控制、流量控制方法，使有差错的物理线路变为无差错的数据链路，即提供可靠的通过物理介质传输数据的方法。数据链路层的具体工作是接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层；并且，还负责处理接收端发回的确认帧的信息，以便提供可靠的数据传输。 定义了如何让格式化数据以进行传输，以及如何让控制对物理介质的访问。这一层通常还提供错误检测和纠正，以确保数据的可靠传输。 　 数据链路层(data link layer)：常简称为链路层，我们知道，两个主机之间的数据传输，总是在一段一段的链路上传送的，也就是说，在两个相邻结点之间传送数据是直接传送的(点对点)，这时就需要使用专门的链路层的协议。 在两个相邻结点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧(framing)，在两个相邻结点之间的链路上“透明”地传送帧中的数据。 每一帧包括数据和必要的控制信息(如同步信息、地址信息、差错控制等)。典型的帧长是几百字节到一千多字节。 注：”透明”是一个很重要的术语。它表示，某一个实际存在的事物看起来却好像不存在一样。”在数据链路层透明传送数据”表示无力什么样的比特组合的数据都能够通过这个数据链路层。因此，对所传送的数据来说，这些数据就“看不见”数据链路层。或者说，数据链路层对这些数据来说是透明的。 在接收数据时，控制信息使接收端能知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提取出数据部分，上交给网络层。 控制信息还使接收端能检测到所收到的帧中有无差错。如发现有差错，数据链路层就简单地丢弃这个出了差错的帧，以免继续传送下去白白浪费网络资源。如需改正错误，就由运输层的TCP协议来完成。 网络层 ：数据报 网络层（Network Layer）是OSI模型的第三层，它是OSI参考模型中最复杂的一层。它在下两层的基础上向资源子网提供服务。其主要任务是：通过路由选择算法，为报文或分组通过通信子网选择最适当的路径。具体地说，数据链路层的数据在这一层被转换为数据包，然后通过路径选择、分段组合、顺 序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。一般地，数据链路层是解决同一网络内节点之间的通信，而网络层主要解决不同子网间的通信。例如在广域网之间通信时，必然会遇到路由（即两节点间可能有多条路径）选择问题。 在位于不同地理位置的网络中的两个主机系统之间提供连接和路径选择。Internet的发展使得从世界各站点访问信息的用户数大大增加，而网络层正是管理这种连接的层。 网络层(network layer)主要包括以下两个任务： 负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组或包进行传送。在TCP/IP体系中，由于网络层使用IP协议，因此分组也叫做IP数据报，或简称为数据报。 选中合适的路由，使源主机运输层所传下来的分组，能够通过网络中的路由器找到目的主机。 协议：IP,ICMP,IGMP,ARP,RARP 传输层：报文段/用户数据报 传输层（Transport Layer）是OSI模型的第4层。因此该层是通信子网和资源子网的接口和桥梁，起到承上启下的作用。该层的主要任务是：向用户提供可靠的端到端的差错和流量控制，保证报文的正确传输。传输层的作用是向高层屏蔽下层数据通信的细节，即向用户透明地传送报文。该层常见的协议：TCP/IP中的TCP协议和UDP协议。传输层提供会话层和网络层之间的传输服务，这种服务从会话层获得数据，并在必要时，对数据进行分割。然后，传输层将数据传递到网络层，并确保数据能正确无误地传送到网络层。因此，传输层负责提供两节点之间数据的可靠传送，当两节点的联系确定之后，传输层则负责监督工作。综上，传输层的主要功能如下：监控服务质量。 定义了一些传输数据的协议和端口号（WWW端口80等），如： TCP（transmission control protocol –传输控制协议，传输效率低，可靠性强，用于传输可靠性要求高，数据量大的数据） UDP（user datagram protocol–用户数据报协议，与TCP特性恰恰相反，用于传输可靠性要求不高，数据量小的数据，如QQ聊天数据就是通过这种方式传输的）。 主要是将从下层接收的数据进行分段和传输，到达目的地址后再进行重组。常常把这一层数据叫做段。 　 运输层(transport layer)：负责向两个主机中进程之间的通信提供服务。由于一个主机可同时运行多个进程，因此运输层有复用和分用的功能 复用，就是多个应用层进程可同时使用下面运输层的服务。 分用，就是把收到的信息分别交付给上面应用层中相应的进程。 运输层主要使用以下两种协议： 传输控制协议TCP(Transmission Control Protocol)：面向连接的，数据传输的单位是报文段，能够提供可靠的交付。 用户数据包协议UDP(User Datagram Protocol)：无连接的，数据传输的单位是用户数据报，不保证提供可靠的交付，只能提供“尽最大努力交付”。 会话层 会话层（Session Layer）是OSI模型的第5层，是用户应用程序和网络之间的接口，主要任务是：向两个实体的表示层提供建立和使用连接的方法。将不同实体之间的表示层 的连接称为会话。因此会话层的任务就是组织和协调两个会话进程之间的通信，并对数据交换进行管理。 用户可以按照半双工、单工和全双工的方式建立会话。当建立会话时，用户必须提供他们想要连接的远程地址。而这些地址与MAC（介质访问控制子层）地址或网络层的逻辑地址不同，它们是为用户专门设计的，更便于用户记忆。 通过运输层（端口号：传输端口与接收端口）建立数据传输的通路。主要在你的系统之间发起会话或者接受会话请求（设备之间需要互相认识可以是IP也可以是MAC或者是主机名） 　　 表示层 表示层（Presentation Layer）是OSI模型的第六层，它对来自应用层的命令和数据进行解释，对各种语法赋予相应的含义，并按照一定的格式传送给会话层。其主要功能是“处理用户信息的表示问题，如编码、数据格式转换和加密解密”等。 可确保一个系统的应用层所发送的信息可以被另一个系统的应用层读取。例如，PC程序与另一台计算机进行通信，其中一台计算机使用扩展二一十进制交换码（EBCDIC），而另一台则使用美国信息交换标准码（ASCII）来表示相同的字符。如有必要，表示层会通过使用一种通格式来实现多种数据格式之间的转换。 　　 应用层：报文 应用层（Application Layer）是OSI参考模型的最高层，它是计算机用户，以及各种应用程序和网络之间的接口，其功能是直接向用户提供服务，完成用户希望在网络上完成的各种工作。它在其他6层工作的基础上，负责完成网络中应用程序与网络操作系统之间的联系，建立与结束使用者之间的联系，并完成网络用户提出的各种网络服务及 应用所需的监督、管理和服务等各种协议。此外，该层还负责协调各个应用程序间的工作。 应用层(application layer)：是体系结构中的最高。直接为用户的应用进程（例如电子邮件、文件传输和终端仿真）提供服务。 在因特网中的应用层协议很多，如支持万维网应用的HTTP协议，支持电子邮件的SMTP协议，支持文件传送的FTP协议，DNS，POP3，SNMP，Telnet等等。 七层协议和四层协议 路由表 路由表是指路由器或者其他互联网网络设备上存储的一张路由信息表，该表中存有到达特定网络终端的路径，在某些情况下，还有一些与这些路径相关的度量。路由器的主要工作就是为经过路由器的每个数据包寻找一条最佳的传输路径，并将该数据有效地传送到目的站点。由此可见，选择最佳路径的策略即路由算法是路由器的关键所在。为了完成这项工作，在路由器中保存着各种传输路径的相关数据——路由表（Routing Table），供路由选择时使用，表中包含的信息决定了数据转发的策略。路由表可以是由系统管理员固定设置好的，也可以由系统动态修改，可以由路由器自动调整，也可以由主机控制。 静态路由表：由系统管理员事先设置好固定的路由表称之为静态（static）路由表，一般是在系统安装时就根据网络的配置情况预先设定的，它不会随未来网络结构的改变而改变。 动态路由表：动态（Dynamic）路由表是路由器根据网络系统的运行情况而自动调整的路由表。路由器根据路由选择协议（Routing Protocol）提供的功能，自动学习和记忆网络运行情况，在需要时自动计算数据传输的最佳路径。 路由器通常依靠所建立及维护的路由表来决定如何转发。路由表能力是指路由表内所容纳路由表项数量的极限。路由表中的表项内容包括： destination mask pre costdestination：目的地址，用来标识IP包的目的地址或者目的网络。 mask：网络掩码，与目的地址一起标识目的主机或者路由器所在的网段的地址。 pre：标识路由加入IP路由表的优先级。可能到达一个目的地有多条路由，但是优先级的存在让他们先选择优先级高的路由进行利用。 cost：路由开销，当到达一个目的地的多个路由优先级相同时，路由开销最小的将成为最优路由。 interface：输出接口，说明IP包将从该路由器哪个接口转发。 nexthop：下一跳IP地址，说明IP包所经过的下一个路由器。 报文头部详解： bit：网络传输的都是二进制数据BIT流 frame:数据链路层 我们用wareshark抓包工具来查看 张各层协议图 网络协议图 对网络分层的理解 许多所谓的网络课程都是从教你记住OSI模型中的每一个层的名字和这个模型中包含的每一个协议开始的。 这样做是不必要的。甚至第5层和第6层是完全可以忽略的。 国际标准组织(ISO)制定了OSI模型。 这个模型把网络通信的工作分为7层。1至4层被认为是低层，这些层与数据移动密切相关。5至7层是高层，包含 应用程序级的数据。 每一层负责一项具体的工作，然后把数据传送到下一层。 物理层(也即OSI模型中的第一层)在课堂上经常是被忽略的。它看起来似乎很简单。但是，这一层的某些方面有时需要特别留意。 物理层实际上就是布线、光纤、网卡和其它用来把两台网络通信设备连接在一起的东西。甚至一个信鸽也可以被认为是一个1层设备(参见RFC 1149)。 网络故障的排除经常涉及到1层问题。我们不能忘记用五类线在整个一层楼进行连接的传奇故事。由于办公室的椅子经常从电缆线上压过，导致网络连接出现断断续续的情况。 遗憾的是，这种故障是很常见的，而且排除这种故障需要耗费很长时间。 第2层是以太网等协议。最重要的是应该理解网桥是什么。交换机可以看成网桥，人们现在都这样称呼它。 网桥都在2层工作，仅关注以太网上的MAC地址。如果你在谈论有关MAC地址、交换机或者网卡和驱动程序，你就是在第2层的范畴。集线器属于第1层的领域，因为它们只是电子设备，没有2层的知识。 第2层的相关问题在本网络讲座中有自己的一部分，因此现在先不详细讨论这个问题的细节。现在只需要知道第2层把数据帧转换成二进制位供1层处理就可以了。在往下讲之间，你应该回过头来重新阅读一下上面的内容，因为经验不足的网络管理员经常混淆2层和3层的区别。 如果你在谈论一个IP地址，那么你是在处理第3层的问题，这是“数据包”问题，而不是第2层的“帧”。IP是第3层问题的一部分，此外还有一些路由协议和 地址解析协议(ARP)。 有关路由的一切事情都在第3层处理。地址解析和路由是3层的重要目的。 第4层是处理信息的传输层。第4层的 数据单元也称作数据包（packets）。 但是，当你谈论TCP等具体的协议时又有特殊的叫法，TCP的数据单元称为“ 段（segments）”而UDP的数据单元称为“ 数据报（datagrams）”。 这个层负责获取全部信息，因此，它必须跟踪数据单元碎片、乱序到达的数据包和其它在传输过程中可能发生的危险。理解第4层的另一种方法是，第4层提供端对端的通信管理。 像TCP等一些协议非常善于保证通信的可靠性。有些协议并不在乎一些数据包是否丢失，UDP协议就是一个主要例子。 第5层和第6层的功能。 有一些应用程序和协议在5层和6层。但是，对于理解网络问题来说，谈论这些问题没有任何益处。请大家注意，第7层是“一切”。7层称作“应用层”，是专门用于应用程序的。 如果你的程序需要一种具体格式的数据，你可以发明一些你希望能够把数据发送到目的地的格式，并且创建一个第7层协议。 SMTP、DNS和FTP都是7层协议。 学习OSI模型中最重要的事情是它实际代表什么意思。 https://blog.csdn.net/yaopeng_2005/article/details/7064869 "},"Chapter10/NetworkLayer.html":{"url":"Chapter10/NetworkLayer.html","title":"网络层详解","keywords":"","body":"网络层详解 网络层提供的两种服务 虚电路服务：当两个计算机进行通信时，先建立连接，以保证双方通信所需的一切网络资源。 数据报服务：网络层只向上提供简单灵活的、无连接的、尽最大努力交付的数据报服务。网络在发送分组时不建立连接，每一个分组独立发送，与其前后分组无关，也不提供服务质量承诺。 IP协议 IP协议配套使用的还包括ARP（地址解析协议）、RARP（逆地址解析协议）、ICMP（网际控制报文协议）、IGMP（网际组管理协议）。 虚拟互连网络 将网络互连需要通过一些中间设备：转发器物理层（）、网桥或桥接器（数据链路层）、路由器（网络层）和网关（网络层以上）。当中间设备是转发器或网桥时仅仅是把一个网络扩大，一般不称为网络互连。 虚拟专用网：逻辑互联网。意思是互连起来的各种物理网络的异构性是客观存在的，但是使用IP协议可以使这些性能各异的网络在网络层看起来是一个统一的网络。 分类的IP地址 IP地址就是给因特网上的每一个主机（或路由器）的每一个端口分配一个唯一的32为标识符。IP地址的编制方法经历了三个阶段： 分类的IP地址 子网划分 构成超网 IP地址划分为若干个固定的类，每一个地址都有两个固定长度的字段组成，其中第一个字段是网络号，第二个字段是主机号。 A类地址：网络地址(1Byte) + 主机地址(3Byte)，且网络地址的首位必须是0；则网络地址的取值范围是0~127,网络号全为0的地址是保留地址，而127也是保留的地址，并且是用于测试环回用的。因此A类地址的范围其实是从1-126之间。 可用的A类网络有126个，每个网络能容纳1亿多个主机(2的24次方减2的主机数目)。 子网掩码：255.0.0.0。A类IP范围：首位为0；1.0.0.1~~126.255.255.254；主机号24位 B类地址：网络地址(2Byte) + 主机地址(2Byte)，且网络地址的前两位是10；则网络地址的取值范围是128~ ~191, 可用的B类网络有2的14方减1个，每个网络能容纳6万多个主机 (2的16次方减0和广播地址)。 子网掩码：255.255.0.0。B类IP范围：前两位为10 ；128.1.0.1~~191.255.255.254；主机号16位 C类地址：网络地址(3Byte)+主机地址(1Byte),且网络地址的前三位是110；则网络地址的取值范围是192.0.1~ ~223,可用的C类网络有2的21次方减1，可达209万余个，每个网络能容纳254个主机。子网掩码：255.255.255.0。 C类IP范围：前三位为110；192.0.0.1~~223.255.255.254；主机号8位 D类地址：此类地址称为多播地址，也叫做组播地址，前四位必须是1110；网络地址的取值范围是224~ ~239,D类IP范围：前四位为1110；224.0.0.1~~239.255.255.254 E类地址：此类地址是保留地址，留作将来使用，前五位必须是11110；网络地址的取值范围是240~ ~254,E类IP范围：前五位为11110；240.0.0.1~~254.255.255.254 IP地址的特点： 每一个IP地址都有网络号+主机号两部分组成。 从这个意义上讲，IP地址是一种分等级的地址结构。分等级带来的好处: a、IP地址管理机构在分配IP地址时，只分配网络号，而剩下的主机号则由得到该网络号的单位自行分配。这样便于管理； b、路由器仅根据目的主机所连接的网络号来转发分组(不许用考虑主机号)，这样就可以使用路由表中的项目数大幅度减少，从而减少路由表所占用的内存，检查查找路由所使用的时间。 实际上一个IP地址用来标志 一个主机（或 路由器）和一条链路的接口。当一个主机同时连接到两个网络时(如：实验室的代理服务器)。该主机必须具有两个相应的IP地址。这是的主机成为多归属主机 交换机是数据链路层上的设备，用它连接起来的主机仍然属于同一个网络。而不同局域网的主机由于网络号不同，必须通过路由器连接起来。 所有的网络号是对等的。 IP地址与硬件地址 物理地址是数据链路层和物理层使用的地址，而IP地址是网络层和以上各层使用的地址，是一种逻辑地址（IP地址是用软件实现的）。 在发送数据时，数据从高层下到底层，然后才到通信链路上传输。使用IP地址的IP数据报一旦交给了数据链路层，就被封装成MAC帧。MAC帧在传送时使用的源地址和目的地址都是硬件地址，这两个硬件地址都写在MAC帧的首部中。 连接在通信链路上的设备（主机或路由器）在接收MAC帧时，其根据是MAC帧首部中的硬件地址。在数据链路层看不见隐藏在MAC帧的数据中的IP地址。只有在剥去MAC帧的首部和尾部后把MAC层的数据上交给网络层后，网络层才能在IP数据报的首部中找到源IP地址和目的IP地址。 总之，IP地址放在IP数据报的首部，而硬件地址则放在MAC帧的首部。在网络层和网络层以上使用的是IP地址，而数据链路层以及以下使用的是硬件地址。 在上图中知道，当IP数据报放入数据链路层的MAC帧中以后，整个的IP数据报就成为MAC帧的数据，因而在数据链路层看不见数据报的IP地址。 举例：a画的是三个局域网用两个路由器R1和R2互连起来。现在主机H1要和主机H2通信。这两个主机的IP地址分别是IP1和IP2，而它们的硬件地址分别是HA1和HA2（HA表示hardware address）。通信路径是：H1->经过R1转发->再经过R2转发->H2。路由器R1因同时连接到两个局域网上，因此它由两个硬件地址，即HA3和HA4。同理，路由器R2也有两个硬件地址HA5和HA6。 这里注意： 在IP层抽象的互联网上只能看到IP数据报。虽然IP数据报要经过路由器R1和R2的两次转发，但在它的首部中的源地址和目的地址始终分别是IP1和 IP2。图中的数据报上写的从IP1到IP2就表示前者是源地址而后知是目的地址。数据报中间经过的两个路由器的IP地址并不出现在IP数据报的首部中。 虽然在IP数据报首部有源站IP地址，但路由器只根据目的站的IP地址的网络号进行路由选择。 在局域网的链路层，只能看见MAC帧。IP数据报被封装在MAC帧中。MAC帧在不同网络上传送时，其MAC帧首部中的源地址和目的地址要发生变化。 尽管互连在一起的网络的硬件地址体系各不相同，但IP层抽象的互联网却屏蔽了下层这些很复杂的细节。只要我们在网络层上讨论问题，就能够使用统一的，抽象的IP地址研究主机和主机或路由器直接的通信。 地址解析协议 网络层使用的是IP地址，但在实际网络的链路上传送数据帧时，最终还是必须使用该网络的硬件地址。但IP地址和下面的网络的硬件地址之间由于格式不同而不存在简单的映射关系（IP地址有32位，而局域网硬件地址是48位）。此外，在一个网络上可能经常会有新的主机加入进来，或撤走一些主机。更换网络适配器也会使主机的硬件地址改变。地址解析协议ARP解决这个问题的方法是在主机ARP高速缓存中存放一个从IP地址到硬件地址的映射表，并且这个映射表还经常动态更新（新增或超时删除）。 每一个主机都设有一个ARP高速缓存，里面有本局域网上的各主机和路由器的IP地址到硬件地址的映射表，这些都是该主机目前知道的一些地址。那么主机怎样知道这些地址呢？ 当主机A要向本局域网上的某个主机B发送IP数据报时，就先在其ARP高速缓存中查看有无主机B的IP地址。如有，就在ARP高速缓存中查出其对应的硬件地址，再把这个硬件地址写入MAC帧，然后通过局域网把这个MAC帧发往此硬件地址。也有可能查不到主机B的IP地址的项目，这可能是主机B才入网，也可 能是主机A刚开机，其高速缓存还是空的。这种情况下，主机A就自动运行ARP，然后按以下步骤找出主机B的硬件地址。 ARP进程在本局域网上广播发送一个ARP请求分组。ARP请求分组的主要内容是表明：我的IP地址是209.0.0.5，硬件地址是00-00-C0-15-AD-18。我想知道IP地址为209.0.0.6的主机的硬件地址。 在本局域网上的所有主机上运行的ARP进程都收到此ARP请求分组。 主机B在ARP请求分组中见到自己的IP地址，就向主机A发送ARP响应分组，并写入自己的硬件地址。其余的所有主机都不理睬这个ARP请求分组。ARP响应分组的主要内容：我的IP地址是209.0.0.6，我的硬件地址是08-00-2B-00-EE-0A。注意，虽然ARP请求分组是广播发送的，但 ARP响应分组是普通的单播，即从一个源地址发送到一个目的地址。 主机A收到主机B的ARP响应分组后，就在其ARP高速缓存中写入主机B的IP地址到硬件地址的映射。 可见ARP高速缓存非常有用。如果不适用ARP高速缓存，那么任何一个主机只要进行一次通信，就必须在网络上用广播方式发送ARP请求分组，这就使网络上的 通信量大大增加。ARP把已经得到的地址映射保存在高速缓存中，这样就使得该主机下次再和具有同样目的地址的主机通信时，可以直接从高速缓存中找到所需的 硬件地址而不必再用广播方式发送ARP请求分组。ARP是解决同一个局域网上的主机或路由器的IP地址和硬件地址的映射问题。 归纳下使用ARP的4种情况： 发送方是主机，要把IP数据报发送到本网络上的另一个主机。这时用ARP找到目的主机的硬件地址。 发送方是主机，要把IP数据报发送到另一个网络上的一个主机。这时用ARP找到本网络上的一个路由器的硬件地址。剩下的工作交给这个路由器来完成。 发送方是路由器，要把IP数据报转发到本网络上的一个主机。这时用ARP找到目的的主机的硬件地址。 发送方是路由器，要把IP数据报转发到另一个网络上的一个主机。这时用ARP找到本网络上的一个路由器的硬件地址。剩下的工作交给这个路由器来完成。 IP数据报格式 在TCP/IP标准中，各种数据格式常常以32位为单位来描述。一个IP数据报由首部和数据两部分组成。首部的前一部分是固定产度，共20字节，是所有IP数据报都必须具有的。在首部的固定部分的后面是一些可选字段，其长度是可变的。 版本：占4位，指IP协议的版本。通信双方使用的IP协议的版本必须一致。目前广泛使用的IP协议版本号为4（即IPv4）。以后应该会使用IPV6 首部长度 :占4位，可表示的最大十进制数值是15。注意，这个字段所表示数的单位是32位字（1个32位字是4字节），因此首部最大长度为60字节。当IP分组的首部长度不是4字节的整数倍时，必须利用组后的填充字段加以填充。 区分服务 ：占8位，用来获得更好的服务。 总长度：总长度指首部和数据之和的长度，单位为字节。总长度字段为16位，因此数据报的最大长度为2^16-1=65535字节。虽然使用尽可能长的数据报会使传输效率提高，但由于以太网的普遍应用，所以实际上使用的数据报长度很少有超过1500字节的。当数据报长度超过网络所允许的最大传送单元MTU时，就必须把过长的数据包进行分片后才能在网络上发送。 标识:占16位。IP软件在存储器中维持一个计数器，每产生一个数据报，计数器就+1，并将此值赋给标识字段。但这个标识不是序号，因为IP是无连接服务，数据报不存在按序接收的问题。当数据包由于长度超过网络的MTU而必须分片时，这个标识字段的值就被复制到所有的数据报片的标识字段中。相同的标识字段的值使分片后的个数据报片最后能正确地重装成为原来的数据报。 标志 :占3位 ,目前只有2位有意义。 标志字段中的最低位记为MF（more fragment)。MF=1即标识后面还有分片的数据报。MF=0标识这已是若干数据报片中的最后一个。标志字段中间的以为记为DF（don't fragment)，意思是不能分片。只有当DF=0时才允许分片。 片偏移：13位。片偏移指出：较长的分组在分片后，某片在原分组中的相对位置。也就是说，相对于用户数据字段的起点，该片从何处开始。片偏移以8个字节为偏移单位。这就是说，每个分片的长度一定是8字节的整数倍。 举例： 一个数据报总长度3820字节，其数据部分3800字节（使用固定首部）,需要分片为长度不超过1420字节的数据报片。因固定首部长度为20字节，因此每 个数据报片的数据部分长度不能超过1400字节。于是分为3个数据报片，其数据部分长度分别为1400,1400和1000字节。原始数据报首部被复制为 各个数据报片的首部，但必须修改有关字段的值。 生存时间TTL（time to live）：占8位，表示数据报在网络中的寿命。由发出数据报的源点设置这个字段。其目的是防止无法交付的数据报无限制地在因特网中兜圈子，白白消耗网络资源。路由器在转发数据报之前就把TTL值-1.若TTL值减小到0，就丢弃这个数据报，不再转发。因此TTL的单位是跳数。TTL的意义是指明数据报在因特网中至少可经过多少个路由器。显然，8位对应255，所以数据报能在因特网经过的路由器最大是255个。若把TTL的初始值设置为1，就表示这个数据报只能在本局域网中传送。因为这个数据报一传送到局域网上的某个路由器，在被转发前TTL值就减到0了，因而就会被这个路由器丢弃。 协议：占8位，协议字段指出数据报携带的数据使用何种协议，以便使目的主机的IP层知道应该将数据部分上交给哪个处理过程。 首部检验和 占16位。这个字段只检验数据报的首部，不包括数据部分。这是因为数据报每经过一个路由器，路由器都要重新计算一下首部检验和（一些字段，如生存时间，标志，片偏移等都可能发生变化）。不检验数据部分可减少计算的工作量。 源地址：32位 目的地址：32位 IP 数据报首部的可变部分就是一个选项字段。用来支持排错，测量一级安全等措施，内容很丰富。此段长度可变，从1个字节到40个字节不等，取决于所选择的项目。增加首部的可变部分是为了增加IP数据报的功能，但这同时也使得IP数据报的首部长度成为可变。这就增加了每一个路由器处理数据报的开销。实际上这些选项很少备用。新的IPv6版本就把IP数据报的首部长度做成固定的。 IP层分组转发的流程 从数据报的首部提取目的主机的IP地址D，计算出目的主机的网络地址N。 若N就是与此路由器直接相连的某个网络的网络地址。则直接进行交付，不需要经过其他路由器，而是直接将IP数据报交付给目的主机。(注意，直接交付时，路由器需要将目的主机地址D转换为具体的硬件地址，把数据报封装在MAC帧，在发送此帧。) 若N不是与此路由器直接相连的网络，就进行间接交付。执行3. 若路由表中有目的地址为D的特定主机路由，则把数据报传送给路由表中所指明的下一跳路由器；否则，执行4。 若路由表中有到达网络N的路由，则把数据报传送给路由表中所指明的下一跳路由器；否则，执行5。 如果3和4都没能将IP数据报转发出去，若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认；否则执行6。 报告转发分组出错。 划分子网 从两级IP地址到三级IP地址 随着因特网的普及和技术的发展，早期ARPANET的设计之缺陷显露无疑： IP地址空间的利用率有时会很低：一个A类IP地址网络可连接超过1000万台主机，而每个B类IP地址网络可连接超过6万台。可是有些网络对连接在网络上的主机数有限制，甚至远小于这样的数量。 给每个物理网络分配一个网络号会使路由表变得太大，降低网络性能。 两级IP地址不够灵活：只能在申请完IP地址后才能进行下一步工作，而无法按自己的需求变更。 为了解决这个问题，从1985年起，在IP地址中又增加了一个\"子网号字段\"，使两级IP地址变为三级IP地址。这种做法叫作划分子网，或子网寻址或子网路由选择。 划分子网的基本思路 一个拥有许多物理网络的单位，可将所属的物理网络划分为若干个子网（subnet）。划分子网只是单位内部的事情，本单位以为无法得知这个网络具体的子网数及组成，这个单位对外仍然只表现为一个网络。 划分子网的方法是从网络的主机号借用若干位作为子网号subnet-id，与此同时主机号也减少相应位数。由此两级IP地址可变为三级IP地址： IP地址 ::= {,,} 从外部网络发送给本单位某主机的IP数据报仍根据目的网络号找到连接在本单位网络上的路由器。但随后在本网络内部，路由器根据目的网络号和子网号找到目的子网，将IP数据报交付目的主机。 注意：划分子网只是把IP地址的主机号这部分进行再划分，并不改变IP地址原来的网络号 子网掩码 子网掩码是干什么的呢？我们知道，从IP数据报的首部无法看出源主机和目的主机所连接的网络是否进行了子网划分，所以想到了子网掩码（subnet mask）的方法：将三级IP地址的子网掩码（根据子网划分而变）和收到的数据报的目的IP地址逐位\"与\"（AND），就可得出所要找的子网的网络地址。子网掩码是一个网络或一个子网的重要属性，路由器的路由表中除了有目的网络地址，还有该网络的子网掩码，这是现在因特网的标准规定。 A类地址的默认子网掩码是255.0.0.0； B类地址的默认子网掩码是255.255.0.0； C类地址的默认子网掩码是255.255.255.0。 注意：划分子网增加了灵活性，却减少了能连接在网络上的主机数. 使用子网划分后，路由表必须包含以下三项内容：目的网络地址、子网掩码和下一跳地址。 在划分子网的情况下，路由转发分组的算法如下： 从收到的数据报的首部提取目的IP地址D。 先判断是否为直接交付。对路由器直接相连的网络逐个进行检查：用各网络的子网掩码和D逐位相\"与\"（AND操作），看结果是否和相应的网络地址匹配。若匹配，则把分组进行直接交付（还需把D转换成物理地址，把数据报封装成帧发送出去），转发任务结束。否则就是间接交付，执行（3）。 若路由表中有目的地址为D的特定主机路由，则把数据报传送给路由表中所指明的下一跳路由器；否则，执行（4）。 对路由表中的每一行（目的网络地址、子网掩码、下一跳地址），用其中的子网掩码和D逐位相“与”（AND），其结果为N。若N与该行的目的网络地址匹配，则把数据报传送给该行指明的下一跳路由器；否则，执行（5）。 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器；否则，执行（6）。 报告转发分组出错。 可以和没划分子网的情况比较下，发现大体的流程是不变的：提取IP地址、判断是否在同一网络、判断是否有特定路由、判断间接交付、判断是否有默认路由。 "},"Chapter10/ICMP.html":{"url":"Chapter10/ICMP.html","title":"网际报文控制协议ICMP","keywords":"","body":"网际报文控制协议ICMP 为了更有效地转发IP数据报和提高交付成功的机会，在网际层使用了网际控制报文协议ICMP。ICMP允许主机或路由器报告差错情况和提供有关异常情况的报告。ICMP是因特网的标准协议。但ICMP不是高层协议，而是IP层的协议。ICMP报文作为IP层数据报的数据，加上数据报的首部，组成IP数据报发送出去。 ICMP报文分为了两类类：ICMP差错报告报文，ICMP询问报文，通知类。ICMP报文包括8个字节的报头和长度可变的数据部分。对于不同的报文类型，报头的格式一般是不相同的，但是前3个字段(4个字节)对所有的ICMP报文都是相同的。 ICMP差错报文种类： 终点不可达:当路由器或主机不能交付数据报时就像源点发送终点不可达报文。 源点抑制:源点抑制提供了拥塞控制,当路由器或者主机因拥塞丢弃数据报时,每个被丢弃的数据报都要向源点发送源点抑制报文；源点抑制作用有:1:告诉源点数据报被丢弃2:要求源点放慢发包速度. 超时:有两种情况:生存时间(实际是跳数)递减为0,数据报被丢弃,向源点发送ICMP超时报文,这种报文只有可能是路由器发送。当目的主机收到一个分片时,就会启动一个分片计时器,如果计时器内分片没完全到达,则发送超时报文并丢弃已经收到的所有分片 参数问题:数据报首部出现错误或者首部缺少一些选项发送此报文,主机和路由器都可能发送此报文。 改变路由(重定向):路由器要经常更新自己的路由表,网络上主机的数量远远大于路由器的数量,如果主机也动态更新,将产生无法忍受的通信量,所以主机使用静态路由选择,一般情况下,开始时主机只知道默认路由地址,IP数据报将被发送到默认路由器,但也许此数据报应该被发到另外的路由器,默认路由器知道这种情况后,转发此数据报,并向源点发送改变路由ICMP, 让主机刷新自己的路由表,主机的路由表通过这种方式进行更新。 ICMP询问报文种类： 回送请求和回答：一般用于源主机或源路由器判断目的主机或目的路由器能否与其通信,主机和路由器都能发送此报文,此报文包括了回送请求报文和回送回答报文,ping命令便是此报文。 时间戳请求和回答：括时间戳请求报文和时间戳回答报文,它能够确定IP数据报在两台机器的往返时间,即使两个路由器本地时间不同步,但他们的往返时间仍然是精确的。 "},"Chapter10/TCP.html":{"url":"Chapter10/TCP.html","title":"TCP协议","keywords":"","body":"TCP协议 TCP协议的特点 TCP是面向连接的运输层协议 每一条TCP连接只能有两个端点。TCP只能进行点对点通信 TCP提供可靠交付的服务。通过TCP连接传输的数据，无差错、不丢失、不重复、并且按序到达 TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP通信的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据。 面向字节流。 面向字节流的含义是：虽然应用程序和TCP的交互是一个一个的数据块，但是TCP把应用程序交下的数据看成仅仅是一串的无结构的字节流。TCP并不知道所传送的字节流的含义。TCP不保证接收方应用程序所收到的数据块和发送方应用程序所发出的数据块具有对用关系。 TCP根据对方给出的窗口值和当前网络拥塞的程度来决定一个报文段应包含多少个字节。如果应用程序传递到TCP缓存的数据块太长，TCP可以把它划分短一些再传递。如果应用程序发来的数据块太短，TCP也可以等待积累足够多的字节后再构成报文段发送。 可靠传输的工作原理 停止等待协议 \"停止等待\"：就是每发送完一个分组就停止发送，等待对方的确认。在收到确认后再发送下一个分组。 无差错的情况 A发送分组M1，发送完就暂停发送，等待B确认。B收到M1后就向A发送确认。A收到对M1的确认，继续发送下一个分组。 出现差错 B接收M1出现差错，就丢弃M1，其他什么也不做。A只要超过一段时间没有收到确认，就认为刚才发送的分组丢失，因此重传前面发送的分组。这就叫做超时重传. 以上应该注意一下三点： A发送完一个分组后，必须暂时保留已发送的分组的副本。只有在收到相应的确认后才能清楚保留的副本。 分组和确认必须进行编号。这样才能明确是哪一个发出去的分组收到了确认。 超时计时器设置的重传时间应当比数据在分组传输的平均往返时间更长一些。 确认丢失和确认迟到 如上图a.B所发送的对M1的确认丢失。A在设定超时重传时间内没有收到确认，但并无法知道是自己发送的分组出错、丢失、或者B发送的确认丢失。因此A在超时计时器到期后就要重传M1.现在B需要采取如下两个动作。 丢弃这个重复的分组M1，不向上层交付。 向A发送确认。 如图b。传输过程没有出现差错，但B对分组M1的确认迟到。A回收到重复的确认。对重复的确认的处理很简单：收下后丢弃。B任然会收到重复的M1，并且同样丢弃重复的M1。 连续ARQ协议 滑动窗口协议是TCP协议的精髓所在。 如图a发送方维持发送窗口，它的意义是：位于发送窗口内的5个分组可连续发送出去，而不需要等待对方的确认。连续的ARQ协议规定，发送当收到一个确认，就把发送窗口向前滑动一个分组的位置。接收方一般都是采用累积确认的方式。就是接收方不必对收到的分组逐个确认，而是可以收到几个分组后对按序到达的最后一个分组发送确认，这样就表示：到这个分组为止的所有分组都已经收到。 TCP数据包格式 顺序号(32位)：用来标识从TCP源端向TCP目的端发送的数据字节流，它表示在这个报文段中的第一个数据字节的顺序号。如果将字节流看作在两个应用程序间的单向流动，则TCP用顺序号对每个字节进行计数。序号是32bit的无符号数，序号到达2^32－1后又从0开始。当建立一个新的连接时，SYN标志为1(该报文段不携带数据，但是要消耗一个序号)，顺序号字段包含由这个主机选择的该连接的初始顺序号ISN(Initial Sequence Number)。 确号(32位)：包含发送确认的一端所期望收到的下一个顺序号。因此，确认序号应当是上次已成功收到数据字节顺序号加1。只有ACK标志为1时确认序号字段才有效。TCP为应用层提供全双工服务，这意味数据能在两个方向上独立地进行传输。因此，连接的每一端必须保持每个方向上的传输数据顺序号。 TCP报头长度(4位)：给出报头中32bit字的数目，它实际上指明数据从哪里开始。需要这个值是因为任选字段的长度是可变的。这个字段占4bit ，因此TCP最多有60字节的首部。然而，没有任选字段，正常的长度是20字节。 保留位(6位)：保留给将来使用，目前必须置为0。 控制位(control flags ，6位)：在TCP 报头中有6个标志比特，它们中的多个可同时被设置为 1 。依次为：　 ACK ：为1表示确认号有效，为0表示报文中不包含确认信息，忽略确认号字段。 PSH ：为1表示是带有PUSH标志的数据，指示接收方应该尽快将这个报文段交给应用层而不用等待缓冲区装满。 RST :用于复位由于主机崩溃或其他原因而出现错误的连接。它还可以用于拒绝非法的报文段和拒绝连接请求。一般情况下，如果收到一个RST为1的报文，那么一定发生了某些问题。 SYN ：同步序号，为1表示连接请求，用于建立连接和使顺序号同步。 FIN ：用于释放连接，为1表示发送方已经没有数据发送了，即关闭本方数据流。 窗口大小(16位)：数据字节数，表示从确认号开始，本报文的源方可以接收的字节数，即源方接收窗口大小。窗口大小是一个16bit字段，因而窗口大小最大为65535字节。 校验和(16位)：此校验和是对整个的TCP报文段，包括TCP头部和TCP数据，以 16 位字进行计算所得。这是一个强制性的字段，一定是由发送端计算和存储，并由接收端进行验证。 紧急指针(16位)：只有当URG标志置1时紧急指针才有效。紧急指针是一个正的偏移量，和顺序号字段中的值相加表示紧急数据最后一个字节的序号。 TCP 的紧急方式是发送端向另一端发送紧急数据的一种方式。 选项：最常见的可选字段是最长报文大小，又称为MSS(Maximum Segment Size) 。每个连接方通常都在通信的第一个报文（为建立连接而设置 SYN 标志的那个段）中指明这个选项，它指明本端所能接收的最大长度的报文段。选项长度不一定是 32 位字的整数倍，所以要加填充位，使得报头长度成为整字数。 数据：TCP 报文段中的数据部分是可选的。在一个连接建立和一个连接终止时，双方交换的报文段仅有TCP首部。如果一方没有数据要发送，也使用没有任何数据的首部来确认收到的数据。在处理超时的许多情况中，也会发送不带任何数据的报文段。 TCP头格式 注意以下几点： TCP的包是没有IP地址的，那是IP层上的事。但是有源端口和目标端口。 一个TCP连接需要四个元组来表示是同一个连接（src_ip, src_port, dst_ip, dst_port）准确说是五元组，还有一个是协议。但因为这里只是说TCP协议，所以，这里我只说四元组。 注意上图中的四个非常重要的东西： Sequence Number是包的序号，用来解决网络包乱序（reordering）问题。 Acknowledgement Number就是ACK——用于确认收到，用来解决不丢包的问题。 Window又叫Advertised-Window，也就是著名的滑动窗口（Sliding Window），用于解决流控的。 TCP Flag，也就是包的类型，主要是用于操控TCP的状态机的。 关于其它的东西，可以参看下面的图示 TCP三次握手（建立连接） 所谓三次握手(Three-way Handshake)，是指建立一个TCP连接时，需要客户端和服务器总共发送3个包。三次握手的目的是连接服务器指定端口，建立TCP连接,并同步连接双方的序列号和确认号并交换TCP窗口大小信息. TCP连接建立的过程： 服务器B的TCP进程先创建传输控制块TCB，准备接受客户进程的连接请求。然后服务器B进入LISTEN状态，等待客户端的连接请求。 A的TCP进程首先创建传输控制块TCB，然后向B发出连接请求报文段，这时首部中的同部位SYN=1，同时选择一个合适的初始序号seq=x。TCP规定SYN报文段不能携带数据。这时TCP客户进程进入SYN-SEND状态。 B接收到请求报文段后，如果同意建立连接，则向A发送确认。在报文段中把SYN位和ACK位设置为1，确认号ack=x+1，同时选择一个初始序号seq=y。这个报文段也不能携带数据，但同样要消耗一个序号。这时服务为进入SYN-RCVD状态。 TCP客户进程收到B的确认后，还要向B发出确认。确认报文段ACK=1，确认号ack=y+1，而自己的选号seq=x+1。TCP规定ACK报文段可以携带数据。但是如果不携带数据则不消耗序列号，在这种情况下，下一个报文段的seq=x+1。这时TCP连接已经建立，A进入ESTABLISHED阶段。 当B收到A的确认后，也进入ESTABLISHED阶段。 SYN攻击 在三次握手过程中，服务器发送SYN-ACK之后，收到客户端的ACK之前的TCP连接称为半连接(half-open connect).此时服务器处于Syn_RECV状态.当收到ACK后，服务器转入ESTABLISHED状态.Syn攻击就是攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送syn包，服务器回复确认包，并等待客户的确认，由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，目标系统运行缓慢，严重者引起网络堵塞甚至系统瘫痪。 Syn攻击是一个典型的DDOS攻击。检测SYN攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击.在Linux下可以如下命令检测是否被Syn攻击netstat -n -p TCP | grep SYN_RECV 一般较新的TCP/IP协议栈都对这一过程进行修正来防范Syn攻击，修改tcp协议实现。主要方法有SynAttackProtect保护机制、SYN cookies技术、增加最大半连接和缩短超时时间等.但是不能完全防范syn攻击。 四次挥手（关闭连接） TCP的连接的拆除需要发送四个包，因此称为四次挥手(four-way handshake)。客户端或服务器均可主动发起挥手动作。 TCP释放连接的过程： A的应用进程向TCP发出释放连接报文段，并停止发送数据，主动关闭TCP连接。A把释放连接报文段首部的FIN设置为1，其序列号seq=u,它等于前面已传送过的数据的最后一个字节的序号加1.这是A进入FIN-WAIT-1状态，等待B的确认。TCP规定FIN报文段即使不携带任何数据也要消耗一个序号。 B收到连接释放报文段后即发出确认，确认号是ack=u+1,而这个报文段自己的序号是v，v于B前面已经发送过的数据的最后一个字节的序号加1.然后B就进入CLOSE_WAIT状态。TCP进程服务器这时通知高层应用进程，因而从A到B的这个方向的连接就释放了，这时TCP连接处于半关闭(half_close)状态。即A已经没有数据要发送了，但B如有数据发送A仍可以接受。 A收到来自B的确认后，进入FIN-WAIT-2状态，等待B发出连接释放报文段。 若B已经没有要向A发送的数据，其应用进程就通知TCP释放连接。这时B发出的连接释放报文FIN=1.现在假定序号为w（在半关闭状态B可能又发送了一些数据）。B还必须重复上次已经发送的确认号ack=u+1。这时B就进入LAST-ACK状态。 A在收到B的释放报文段后，发出确认。在确认报文段把ACK设置为1，确认号ack=w+1，而自己的序列号seq=u+1。然后进入TIME-WAIT状态。请注意现在连接还没有释放。必须经过时间等待计时器设置的时间2MSL后，A进入CLOSED状态。MSL（最长报文段寿命）。 CLOSED: 这个没什么好说的了，表示初始状态。 LISTEN: 这个也是非常容易理解的一个状态，表示服务器端的某个SOCKET处于监听状态，可以接受连接了。 SYN_RCVD: 这个状态表示接受到了SYN报文，在正常情况下，这个状态是服务器端的SOCKET在建立TCP连接时的三次握手会话过程中的一个中间状态，很短暂，基本上用netstat你是很难看到这种状态的，除非你特意写了一个客户端测试程序，故意将三次TCP握手过程中最后一个ACK报文不予发送。因此这种状态时，当收到客户端的ACK报文后，它会进入到ESTABLISHED状态。 SYN_SENT: 这个状态与SYN_RCVD遥想呼应，当客户端SOCKET执行CONNECT连接时，它首先发送SYN报文，因此也随即它会进入到了SYN_SENT状态，并等待服务端的发送三次握手中的第2个报文。SYN_SENT状态表示客户端已发送SYN报文。 ESTABLISHED：这个容易理解了，表示连接已经建立了。 FIN_WAIT_1: 这个状态要好好解释一下，其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。 FIN_WAIT_2：上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你，稍后再关闭连接。 TIME_WAIT: 表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FIN_WAIT_1状态下，收到了对方同时带 FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。 CLOSING: 这种状态比较特殊，实际情况中应该是很少见，属于一种比较罕见的例外状态。正常情况下，当你发送FIN报文后，按理来说是应该先收到（或同时收到）对方的ACK报文，再收到对方的FIN报文。但是CLOSING状态表示你发送FIN报文后，并没有收到对方的ACK报文，反而却也收到了对方的FIN报文。什 么情况下会出现此种情况呢？其实细想一下，也不难得出结论：那就是如果双方几乎在同时close一个SOCKET的话，那么就出现了双方同时发送FIN报文的情况，也即会出现CLOSING状态，表示双方都正在关闭SOCKET连接。 CLOSE_WAIT: 这种状态的含义其实是表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。 LAST_ACK: 这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。 为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？ 这是因为服务端的LISTEN状态下的SOCKET当收到SYN报文的建连请求后，它可以把ACK和SYN（ACK起应答作用，而SYN起同步 作用）放在一个报文里来发送。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了， 所以你可以未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里 的ACK报文和FIN报文多数情况下都是分开发送的。 为什么TIME_WAIT状态还需要等2MSL后才能返回到CLOSED状态？ 这是因为：虽然双方都同意关闭连接了，而且握手的4个报文也都协调和发送完毕，按理可以直接回到CLOSED状态（就好比从SYN_SEND状 态到ESTABLISH状态那样）；但是因为我们必须要假想网络是不可靠的，你无法保证你最后发送的ACK报文会一定被对方收到，因此对方处于 LAST_ACK状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT状态的作用就是用来重发可能丢失的 ACK报文。 TCP状态机 其实，网络上的传输是没有连接的，包括TCP也是一样的。而TCP所谓的“连接”，其实只不过是在通讯的双方维护一个“连接状态”，让它看上去好像有连接一样。所以，TCP的状态变换是非常重要的。下面是：“TCP协议的状态机” 和 “TCP建链接”、“TCP断链接”、“传数据” 的对照图，我把两个图并排放在一起，这样方便在你对照着看。 很多人会问，为什么建链接要3次握手，断链接需要4次挥手？ 对于建链接的3次握手，主要是要初始化Sequence Number 的初始值。通信的双方要互相通知对方自己的初始化的Sequence Number（缩写为ISN：Inital Sequence Number）——所以叫SYN，全称Synchronize Sequence Numbers。也就上图中的 x 和 y。这个号要作为以后的数据通信的序号，以保证应用层接收到的数据不会因为网络上的传输的问题而乱序（TCP会用这个序号来拼接数据）。 对于4次挥手，因为TCP是全双工的，所以，发送方和接收方都需要Fin和 Ack。只不过，有一方是被动的，所以看上去就成了所谓的4次挥手。如果两边同时断连接，那就会就进入到CLOSING状态，然后到达TIME_WAIT 状态。下图是双方同时断连接的示意图（你同样可以对照着TCP状态机看）： 另外，有几个事情需要注意一下： 关于建连接时SYN超时。试想一下，如果server端接到了clien发的SYN后回了SYN- ACK后client掉线了，server端没有收到client回来的ACK，那么，这个连接处于一个中间状态，即没成功，也没失败。于 是，server端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次翻倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s就知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 2^6 -1 = 63s，TCP才会把断开这个连接。 关于SYN Flood攻击。一些恶意的人就为此制造了SYN Flood攻击——给服务器发了一个SYN后，就下线了，于是服务器需要默认等63s才会断开连接，这样，攻击者就可以把服务器的syn连接的队列耗尽，让正常的连接请求不能处理。于是，Linux下给了一个叫tcp_syncookies的参数来应对这个事——当SYN队列满了后，TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发回去（又叫cookie），如果是攻击者则不会有响应，如果是正常连接，则会把这个 SYN Cookie发回来，然后服务端可以通过cookie建连接（即使你不在SYN队列中）。请注意，请先千万别用tcp_syncookies来处理正常的大负载的连接的情况。 因为，synccookies是妥协版的TCP协议，并不严谨。对于正常的请求，你应该调整三个TCP参数可供你选择，第一个 是：tcp_synack_retries 可以用他来减少重试次数；第二个是：tcp_max_syn_backlog，可以增大SYN连接数；第三个 是：tcp_abort_on_overflow 处理不过来干脆就直接拒绝连接了。 关于ISN的初始化。ISN是不能hard code的，不然会出问题的——比如：如果连接建好后始终用1来做ISN，如果client发了30个segment过去，但是网络断了，于是 client重连，又用了1做ISN，但是之前连接的那些包到了，于是就被当成了新连接的包，此时，client的Sequence Number 可能是3，而Server端认为client端的这个号是30了。全乱了。RFC793中 说，ISN会和一个假的时钟绑在一起，这个时钟会在每4微秒对ISN做加一操作，直到超过2^32，又从0开始。这样，一个ISN的周期大约是4.55个小时。因为，我们假设我们的TCP Segment在网络上的存活时间不会超过Maximum Segment Lifetime（缩写为MSL），所以，只要MSL的值小于4.55小时，那么，我们就不会重用到ISN。 关于MSL和TIME_WAIT。通过上面的ISN的描述，相信你也知道MSL是怎么来的了。我们注意到，在TCP的状态图中，从TIME_WAIT状态到CLOSED状态，有一个超时设置，这个超时设置是 2*MSL（RFC793定义了MSL为2分钟，Linux设置成了30s）为什么要这有TIME_WAIT？为什么不直接给转成CLOSED状态呢？主要有两个原 因：1）TIME_WAIT确保有足够的时间让对端收到了ACK，如果被动关闭的那方没有收到Ack，就会触发被动端重发Fin，一来一去正好2个MSL，2）有足够的时间让这个连接不会跟后面的连接混在一起（你要知道，有些自做主张的路由器会缓存IP数据包，如果连接被重用了，那么这些延迟收到的包就有可能会跟新连接混在一起）。 关于TIME_WAIT数量太多。从上面的描述我们可以知道，TIME_WAIT是个很重要的状态，但是如果在大并发的短链接下，TIME_WAIT 就会太多，这也会消耗很多系统资源。只要搜一下，你就会发现，十有八九的处理方式都是教你设置两个参数，一个叫tcp_tw_reuse，另一个叫tcp_tw_recycle的参数，这两个参数默认值都是被关闭的，后者recyle比前者resue更为激进，resue要温柔一些。另外，如果使用tcp_tw_reuse，必需设置tcp_timestamps=1，否则无效。这里，你一定要注意，打开这两个参数会有比较大的坑——可能会让TCP连接出一些诡异的问题（因为如上述一样，如果不等待超时重用连接的话，新的连接可能会建不上。正如官方文档上说的一样“It should not be changed without advice/request of technical experts”）。 关于tcp_tw_reuse。官方文档上说tcp_tw_reuse 加上tcp_timestamps（又叫PAWS, for Protection Against Wrapped Sequence Numbers）可以保证协议的角度上的安全，但是你需tcp_timestamps在两边都被打开。 关于tcp_tw_recycle。如果是tcp_tw_recycle被打开了话，会假设 对端开启了tcp_timestamps，然后会去比较时间戳，如果时间戳变大了，就可以重用。但是，如果对端是一个NAT网络的话（如：一个公司只用一 个IP出公网）或是对端的IP被另一台重用了，这个事就复杂了。建链接的SYN可能就被直接丢掉了（你可能会看到connection time out的错误）。 关于tcp_max_tw_buckets。这个是控制并发的TIME_WAIT的数量，默 认值是180000，如果超限，那么，系统会把多的给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow），官网文档说这个参数是用来对抗DDoS攻击的。也说的默认值180000并不小。这个还是需要根据实际情况考虑。 Again，使用tcp_tw_reuse和tcp_tw_recycle来解决TIME_WAIT的问题是非常非常危险的，因为这两个参数违反了TCP协议（RFC1122） 其实，TIME_WAIT表示的是你主动断连接，所以，这就是所谓的“不作死不会死”。试想，如果让对端断连接，那么这个破问题就是对方的了，呵呵。另外，如果你的服务器是于HTTP服务器，那么设置一个HTTP的KeepAlive有多重要（浏览器会重用一个TCP连接来处理多个HTTP请求），然后让客户端去断链接（你要小心，浏览器可能会非常贪婪，他们不到万不得已不会主动断连接）。 数据传输中的Sequence Number http://www.52im.net/thread-275-1-1.html "},"Chapter10/http.html":{"url":"Chapter10/http.html","title":"HTTP协议详解","keywords":"","body":"Http协议规范 什么是HTTP协议 协议是指计算机通信网络中两台计算机之间进行通信所必须共同遵守的规定或规则，超文本传输协议(HTTP)是一种通信协议，它允许将超文本标记语言(HTML)文档从Web服务器传送到客户端的浏览器 。 目前我们使用的是HTTP/1.1 版本。 Http协议是无状态协议。在Http/1.1使用了继续连接。所谓的持续连接就是万维网服务器在发送响应后任然在一段时间内保持这条连接，使同一个客户和该服务器可以继续在这条连接上传送后续的HTTP请求报文和响应报文。 Http/1.1协议的持续连接有两种工作方式：非流水线方式和流水线方式。非流水线方式：是客户在收到前一个响应后才能发出下一个请求。流水线方式：是客户在收到Http的响应报文之前就能够接着发送新的请求报文。 HTTP是一个属于应用层的面向对象的协议，由于其简捷、快速的方式，适用于分布式超媒体信息系统。它于1990年提出，经过几年的使用与发展，得到不断地完善和扩展。目前在WWW中使用的是HTTP/1.0的第六版，HTTP/1.1的规范化工作正在进行之中，而且HTTP-NG(Next Generation of HTTP)的建议已经提出。 HTTP协议的主要特点 支持客户/服务器模式。 简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。 灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 Web服务器，浏览器,代理服务器 当我们打开浏览器，在地址栏中输入URL，然后我们就看到了网页。 原理是怎样的呢？ 实际上我们输入URL后，我们的浏览器给Web服务器发送了一个Request, Web服务器接到Request后进行处理，生成相应的Response，然后发送给浏览器， 浏览器解析Response中的HTML,这样我们就看到了网页，过程如下图所示 我们的Request 有可能是经过了代理服务器，最后才到达Web服务器的。过程如下图所示 代理服务器就是网络信息的中转站，有什么功能呢？ 提高访问速度， 大多数的代理服务器都有缓存功能。 突破限制， 也就是FQ了 隐藏身份。 HTTP协议详解之URL http（超文本传输协议）是一个基于请求与响应模式的、无状态的、应用层的协议，常基于TCP的连接方式，HTTP1.1版本中给出一种持续连接的机制，绝大多数的Web开发，都是构建在HTTP协议之上的Web应用。 HTTP URL (URL是一种特殊类型的URI，包含了用于查找某个资源的足够的信息)的格式如下： URL(Uniform Resource Locator) 地址用于描述一个网络上的资源, 基本格式如下 schema://host[:port#]/path/.../[?query-string][#anchor] scheme: 指定低层使用的协议(例如：http, https, ftp) host: HTTP服务器的IP地址或者域名 port#: HTTP服务器的默认端口是80，这种情况下端口号可以省略。如果使用了别的端口，必须指明，例如 http://www.cnblogs.com:8080/ path: 访问资源的路径 query-string: 发送给http服务器的数据 anchor-: 锚 HTTP协议详解之请求 http请求由三部分组成，分别是：请求行、消息报头、请求正文 1、请求行以一个方法符号开头，以空格分开，后面跟着请求的URI和协议的版本，格式如下：Method Request-URI HTTP-Version CRLF 其中 Method表示请求方法；Request-URI是一个统一资源标识符；HTTP-Version表示请求的HTTP协议版本； CRLF表示回车和换行（除了作为结尾的CRLF外，不允许出现单独的CR或LF字符）。 请求方法（所有方法全为大写）有多种，各个方法的解释如下： GET 请求获取Request-URI所标识的资源 POST 在Request-URI所标识的资源后附加新的数据 HEAD 请求获取由Request-URI所标识的资源的响应消息报头 PUT 请求服务器存储一个资源，并用Request-URI作为其标识 DELETE 请求服务器删除Request-URI所标识的资源 TRACE 请求服务器回送收到的请求信息，主要用于测试或诊断 CONNECT 保留将来使用 OPTIONS 请求查询服务器的性能，或者查询与资源相关的选项和需求 Get和Post方法的区别 Http协议定义了很多与服务器交互的方法，最基本的有4种，分别是GET,POST,PUT,DELETE. 一个URL地址用于描述一个网络上的资源，而HTTP中的GET, POST, PUT, DELETE就对应着对这个资源的查，改，增，删4个操作。 我们最常见的就是GET和POST了。GET一般用于获取/查询资源信息，而POST一般用于更新资源信息.我们看看GET和POST的区别 GET提交的数据会放在URL之后，以?分割URL和传输数据，参数之间以&相连，如EditPosts.aspx?name=test1&id=123456. POST方法是把提交的数据放在HTTP包的Body中. GET提交的数据大小有限制（因为浏览器对URL的长度有限制），而POST方法提交的数据没有限制. GET方式需要使用Request.QueryString来取得变量的值，而POST方式通过Request.Form来获取变量的值。 GET方式提交数据，会带来安全问题，比如一个登录页面，通过GET方式提交数据时，用户名和密码将出现在URL上，如果页面可以被缓存或者其他人可以访问这台机器，就可以从历史记录获得该用户的账号和密码. 应用举例： GET方法：在浏览器的地址栏中输入网址的方式访问网页时，浏览器采用GET方法向服务器获取资源，eg:GET /form.html HTTP/1.1 (CRLF) POST方法要求被请求服务器接受附在请求后面的数据，常用于提交表单。 eg：POST /reg.jsp HTTP/ (CRLF) Accept:image/gif,image/x-xbit,... (CRLF) ... HOST:www.guet.edu.cn (CRLF) Content-Length:22 (CRLF) Connection:Keep-Alive (CRLF) Cache-Control:no-cache (CRLF) (CRLF) //该CRLF表示消息报头已经结束，在此之前为消息报头 user=jeffrey&pwd=1234 //此行以下为提交的数据 HEAD方法与GET方法几乎是一样的，对于HEAD请求的回应部分来说，它的HTTP头部中包含的信息与通过GET请求所得到的信息是相同的。利用这个方法，不必传输整个资源内容，就可以得到Request-URI所标识的资源的信息。该方法常用于测试超链接的有效性，是否可以访问，以及最近是否更新。 2、请求报头后述 3、请求正文(略) HTTP协议详解之响应 在接收和解释请求消息后，服务器返回一个HTTP响应消息。 HTTP响应也是由三个部分组成，分别是：状态行、消息报头、响应正文 1、状态行格式如下： HTTP-Version Status-Code Reason-Phrase CRLF 其中，HTTP-Version表示服务器HTTP协议的版本；Status-Code表示服务器发回的响应状态代码；Reason-Phrase表示状态代码的文本描述。 状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值： 1xx：指示信息--表示请求已接收，继续处理 2xx：成功--表示请求已被成功接收、理解、接受 3xx：重定向--要完成请求必须进行更进一步的操作 4xx：客户端错误--请求有语法错误或请求无法实现 5xx：服务器端错误--服务器未能实现合法的请求 常见状态代码、状态描述、说明： 200 OK //客户端请求成功 400 Bad Request //客户端请求有语法错误，不能被服务器所理解 401 Unauthorized //请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden //服务器收到请求，但是拒绝提供服务 404 Not Found //请求资源不存在，eg：输入了错误的URL 500 Internal Server Error //服务器发生不可预期的错误 503 Server Unavailable //服务器当前不能处理客户端的请求，一段时间后可能恢复正常 eg：HTTP/1.1 200 OK （CRLF） 2、响应报头后述 3、响应正文就是服务器返回的资源的内容 HTTP协议详解之消息报头篇 HTTP消息由客户端到服务器的请求和服务器到客户端的响应组成。请求消息和响应消息都是由开始行（对于请求消息，开始行就是请求行，对于响应消息，开始行就是状态行），消息报头（可选），空行（只有CRLF的行），消息正文（可选）组成。 HTTP消息报头包括普通报头、请求报头、响应报头、实体报头。 每一个报头域都是由名字+“：”+空格+值 组成，消息报头域的名字是大小写无关的。 1、普通报头 在普通报头中，有少数报头域用于所有的请求和响应消息，但并不用于被传输的实体，只用于传输的消息。 eg： Cache-Control 用于指定缓存指令，缓存指令是单向的（响应中出现的缓存指令在请求中未必会出现），且是独立的（一个消息的缓存指令不会影响另一个消息处理的缓存机制），HTTP1.0使用的类似的报头域为Pragma。 请求时的缓存指令包括：no-cache（用于指示请求或响应消息不能缓存）、no-store、max-age、max-stale、min-fresh、only-if-cached; 响应时的缓存指令包括：public、private、no-cache、no-store、no-transform、must-revalidate、proxy-revalidate、max-age、s-maxage. eg：为了指示IE浏览器（客户端）不要缓存页面，服务器端的JSP程序可以编写如下：response.sehHeader(\"Cache-Control\",\"no-cache\"); //response.setHeader(\"Pragma\",\"no-cache\");作用相当于上述代码，通常两者//合用 这句代码将在发送的响应消息中设置普通报头域：Cache-Control:no-cache Date普通报头域表示消息产生的日期和时间 Connection普通报头域允许发送指定连接的选项。例如指定连接是连续，或者指定“close”选项，通知服务器，在响应完成后，关闭连接 2、请求报头 请求报头允许客户端向服务器端传递请求的附加信息以及客户端自身的信息。 常用的请求报头 Accept Accept请求报头域用于指定客户端接受哪些类型的信息。eg：Accept：image/gif，表明客户端希望接受GIF图象格式的资源；Accept：text/html，表明客户端希望接受html文本。 Accept-Charset Accept-Charset请求报头域用于指定客户端接受的字符集。eg：Accept-Charset:iso-8859-1,gb2312.如果在请求消息中没有设置这个域，缺省是任何字符集都可以接受。 Accept-Encoding Accept-Encoding请求报头域类似于Accept，但是它是用于指定可接受的内容编码。eg：Accept-Encoding:gzip.deflate.如果请求消息中没有设置这个域服务器假定客户端对各种内容编码都可以接受。 Accept-Language Accept-Language请求报头域类似于Accept，但是它是用于指定一种自然语言。eg：Accept-Language:zh-cn.如果请求消息中没有设置这个报头域，服务器假定客户端对各种语言都可以接受。 Authorization Authorization请求报头域主要用于证明客户端有权查看某个资源。当浏览器访问一个页面时，如果收到服务器的响应代码为401（未授权），可以发送一个包含Authorization请求报头域的请求，要求服务器对其进行验证。 Host（发送请求时，该报头域是必需的） Host请求报头域主要用于指定被请求资源的Internet主机和端口号，它通常从HTTP URL中提取出来的，eg： 我们在浏览器中输入：http://www.guet.edu.cn/index.html 浏览器发送的请求消息中，就会包含Host请求报头域，如下： Host：www.guet.edu.cn 此处使用缺省端口号80，若指定了端口号，则变成：Host：www.guet.edu.cn:指定端口号 User-Agent 我们上网登陆论坛的时候，往往会看到一些欢迎信息，其中列出了你的操作系统的名称和版本，你所使用的浏览器的名称和版本，这往往让很多人感到很神奇，实际上，服务器应用程序就是从User-Agent这个请求报头域中获取到这些信息。User-Agent请求报头域允许客户端将它的操作系统、浏览器和其它属性告诉服务器。不过，这个报头域不是必需的，如果我们自己编写一个浏览器，不使用User-Agent请求报头域，那么服务器端就无法得知我们的信息了。 请求报头举例： GET /form.html HTTP/1.1 (CRLF) Accept:image/gif,image/x-xbitmap,image/jpeg,application/x-shockwave-flash,application/vnd.ms-excel,application/vnd.ms-powerpoint,application/msword,/ (CRLF) Accept-Language:zh-cn (CRLF) Accept-Encoding:gzip,deflate (CRLF) If-Modified-Since:Wed,05 Jan 2007 11:21:25 GMT (CRLF) If-None-Match:W/\"80b1a4c018f3c41:8317\" (CRLF) User-Agent:Mozilla/4.0(compatible;MSIE6.0;Windows NT 5.0) (CRLF) Host:www.guet.edu.cn (CRLF) Connection:Keep-Alive (CRLF) (CRLF) 3、响应报头 响应报头允许服务器传递不能放在状态行中的附加响应信息，以及关于服务器的信息和对Request-URI所标识的资源进行下一步访问的信息。 常用的响应报头 Location Location响应报头域用于重定向接受者到一个新的位置。Location响应报头域常用在更换域名的时候。 Server Server响应报头域包含了服务器用来处理请求的软件信息。与User-Agent请求报头域是相对应的。下面是 Server响应报头域的一个例子： Server：Apache-Coyote/1.1 WWW-Authenticate WWW-Authenticate响应报头域必须被包含在401（未授权的）响应消息中，客户端收到401响应消息时候，并发送Authorization报头域请求服务器对其进行验证时，服务端响应报头就包含该报头域。 eg：WWW-Authenticate:Basic realm=\"Basic Auth Test!\" //可以看出服务器对请求资源采用的是基本验证机制。 4、实体报头 请求和响应消息都可以传送一个实体。一个实体由实体报头域和实体正文组成，但并不是说实体报头域和实体正文要在一起发送，可以只发送实体报头域。实体报头定义了关于实体正文（eg：有无实体正文）和请求所标识的资源的元信息。 常用的实体报头 Content-Encoding Content-Encoding实体报头域被用作媒体类型的修饰符，它的值指示了已经被应用到实体正文的附加内容的编码，因而要获得Content-Type报头域中所引用的媒体类型，必须采用相应的解码机制。Content-Encoding这样用于记录文档的压缩方法，eg：Content-Encoding：gzip Content-Language Content-Language实体报头域描述了资源所用的自然语言。没有设置该域则认为实体内容将提供给所有的语言阅读 者。eg：Content-Language:da Content-Length Content-Length实体报头域用于指明实体正文的长度，以字节方式存储的十进制数字来表示。 Content-Type Content-Type实体报头域用语指明发送给接收者的实体正文的媒体类型。eg： Content-Type:text/html;charset=ISO-8859-1 Content-Type:text/html;charset=GB2312 Last-Modified Last-Modified实体报头域用于指示资源的最后修改日期和时间。 Expires Expires实体报头域给出响应过期的日期和时间。为了让代理服务器或浏览器在一段时间以后更新缓存中(再次访问曾访问过的页面时，直接从缓存中加载，缩短响应时间和降低服务器负载)的页面，我们可以使用Expires实体报头域指定页面过期的时间。eg：Expires：Thu，15 Sep 2006 16:23:12 GMT HTTP1.1的客户端和缓存必须将其他非法的日期格式（包括0）看作已经过期。eg：为了让浏览器不要缓存页面，我们也可以利用Expires实体报头域，设置为0，jsp中程序如下：response.setDateHeader(\"Expires\",\"0\"); 利用telnet观察http协议的通讯过程 实验目的及原理： 利用MS的telnet工具，通过手动输入http请求信息的方式，向服务器发出请求，服务器接收、解释和接受请求后，会返回一个响应，该响应会在telnet窗口上显示出来，从而从感性上加深对http协议的通讯过程的认识。 实验步骤： 1、打开telnet 1.1 打开telnet 运行-->cmd-->telnet 1.2 打开telnet回显功能 set localecho 2、连接服务器并发送请求 2.1 open www.guet.edu.cn 80 //注意端口号不能省略 HEAD /index.asp HTTP/1.0 Host:www.guet.edu.cn /我们可以变换请求方法,请求桂林电子主页内容,输入消息如下/ open www.guet.edu.cn 80 GET /index.asp HTTP/1.0 //请求资源的内容 Host:www.guet.edu.cn 2.2 open www.sina.com.cn 80 //在命令提示符号下直接输入telnet www.sina.com.cn 80 HEAD /index.asp HTTP/1.0 Host:www.sina.com.cn 3 实验结果： 3.1 请求信息2.1得到的响应是: HTTP/1.1 200 OK //请求成功 Server: Microsoft-IIS/5.0 //web服务器 Date: Thu,08 Mar 200707:17:51 GMT Connection: Keep-Alive Content-Length: 23330 Content-Type: text/html Expries: Thu,08 Mar 2007 07:16:51 GMT Set-Cookie: ASPSESSIONIDQAQBQQQB=BEJCDGKADEDJKLKKAJEOIMMH; path=/ Cache-control: private //资源内容省略 3.2 请求信息2.2得到的响应是: HTTP/1.0 404 Not Found //请求失败 Date: Thu, 08 Mar 2007 07:50:50 GMT Server: Apache/2.0.54 Last-Modified: Thu, 30 Nov 2006 11:35:41 GMT ETag: \"6277a-415-e7c76980\" Accept-Ranges: bytes X-Powered-By: mod_xlayout_jh/0.0.1vhs.markII.remix Vary: Accept-Encoding Content-Type: text/html X-Cache: MISS from zjm152-78.sina.com.cn Via: 1.0 zjm152-78.sina.com.cn:80 X-Cache: MISS from th-143.sina.com.cn Connection: close 失去了跟主机的连接 按任意键继续... 4 .注意事项：1、出现输入错误，则请求不会成功。 2、报头域不分大小写。 3、更深一步了解HTTP协议，可以查看RFC2616，在http://www.letf.org/rfc上找到该文件。 4、开发后台程序必须掌握http协议 HTTP协议是无状态的和Connection: keep-alive的区别 无状态是指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。 从另一方面讲，打开一个服务器上的网页和你之前打开这个服务器上的网页之间没有任何联系HTTP是一个无状态的面向连接的协议，无状态不代表HTTP不能保持TCP连接，更不能代表HTTP使用的是UDP协议（无连接）。 从HTTP/1.1起，默认都开启了Keep-Alive，保持连接特性，简单地说，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。 Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。 "},"Chapter10/httpUseTcp.html":{"url":"Chapter10/httpUseTcp.html","title":"HTTP是如何使用TCP连接","keywords":"","body":"HTTP是如何使用TCP连接 TCP 连接的基本知识 TCP 是可靠的数据管道 TCP 会按序、无差错地承载 HTTP 数据，TCP 为 HTTP 提供了一条可靠的比特传输管道。从 TCP 连接一端填入的字节会从另一端 以原有的顺序、正确地传送出来。 TCP 流是分段的、由 IP 分组传送 TCP 的数据是通过名为 IP 分组（或 IP 数据报）的小数据块来发送的。 这样的话，如图HTTP 就是“HTTP over TCP over IP”这个“协议栈”中的最顶层了。其安全版本 HTTPS 就是在 HTTP 和 TCP 之间插入了一个（称为 TLS 或 SSL 的）密码加密层(安全层)，就是在图中的右半部分。 HTTP 要传送一条报文时，会以流的形式将报文数据的内容通过一条打开的 TCP 连接按 序传输。TCP 收到数据流之后，会将数据流砍成被称作段的小数据块，并将段封装在 IP 分组中，通过因特网进行传输，如下图中大家看到的内容： 每个 TCP 段都是由 IP 分组承载，从一个 IP 地址发送到另一个 IP 地址的。 而每个 IP 分组中都包括： 一个 IP 分组首部（通常为 20 字节）； 一个 TCP 段首部（通常为 20 字节）； 一个 TCP 数据块（0 个或多个字节）。 IP 首部包含了源和目的 IP 地址、长度和其他一些标记。TCP 段的首部包含了 TCP 端口 号、TCP 控制标记，以及用于数据排序和完整性检查的一些数字值。 保持 TCP 连接的持续不间断地运行 在任意时刻计算机都可以有几条 TCP 连接处于打开状态。TCP 是通过端口号来保持所有 这些连接的正确运行的。 端口号和雇员使用的电话分机号很类似。 这就和我之前举得例子是一样的，公司的总机和你自己的座机一样，公司的总机号码能将你接到前台，而分机号 可以将你接到正确的雇员位置一样，IP 地址可以将你连接到正确的计算机，而端口号则 可以将你连接到正确的应用程序上去。TCP 连接是通过 4 个值来识别的： 源IP 地址、源端口号、目的IP 地址、目的端口号 这 4 个值一起唯一地定义了一条连接。两条不同的 TCP 连接不能拥有 4 个完全相同的地 址组件值（但不同连接的部分组件可以拥有相同的值）。 这里需要我们注意的是，有些连接共享了相同的目的端口号，有些连接使用了相同的源 IP 地址，有些使用了相同的目的 IP 地址，但没有两个不同连接所有的 4 个值都一样。 TCP 套接字 操作系统提供了一些操纵其 TCP 连接的工具。为了更具体地说明问题，我们来看一个 TCP 编程接口，这些套接字我就不一一介绍了，我给大家一个表格，大家可以理解一下 套接字API调用描 述s = socket()创建一个新的、未命名、未关联的套接字bind(s,)向套接字赋一个本地端口号和接口connect(s, )创建一条连接本地套接字与远程主机及端口的连接listen(s,…)标识一个本地套接字，使其可以合法接受连接s2 = accept(s)等待某人建立一条到本地端口的连接 套接字 API 允许用户创建 TCP 的端点数据结构，将这些端点与远程服务器的 TCP 端点进 行连接，并对数据流进行读写。TCP API 隐藏了所有底层网络协议的握手细节，以及 TCP 数据流与 IP 分组之间的分段和重装细节。 TCP 客户端和服务器是如何通过 TCP 套接字接口进行通信的 上图中说明了可以怎样通过套接字 API 来凸显客户端和服务器在实现 HTTP 事务时所应执行的步骤。 TCP 连接的握手 TCP 连接握手需要经过以下几个步骤。 如图所示： 请求新的 TCP 连接时，客户端要向服务器发送一个小的 TCP 分组（通常是 40 ～ 60 个字节）。这个分组中设置了一个特殊的 SYN 标记，说明这是一个连接请求。 如果服务器接受了连接，就会对一些连接参数进行计算，并向客户端回送一个 TCP 分组，这个分组中的 SYN 和 ACK 标记都被置位，说明连接请求已被接受。 最后，客户端向服务器回送一条确认信息，通知它连接已成功建立 我们永远不会看到这些分组——这些分组都由 TCP/IP 软件管理，对其是不可见 的。HTTP 程序员看到的只是创建 TCP 连接时存在的时延。 在这里我们需要注意的就是 TCP 连接的握手时延，通常 HTTP 事务都不会交换太多数据，此时，SYN/SYN+ACK 握手（参见图中的 a 段 和图中的 b 段）会产生一个可测量的时延。TCP 连接的 ACK 分组（参见图中的 c 段）通常都足够大，可以承载整个 HTTP 请求报文，而且很多 HTTP 服务器响应报文都可 以放入一个 IP 分组 中去（比如，响应是包含了装饰性图片的小型 HTML 文件，或者是对浏览器高速缓存请求产生的 304 Not Modified 响应）。 TCP 慢启动 TCP 数据传输的性能还取决于 TCP 连接的使用期（age）。TCP 连接会随着时间进行自 我“调谐”，起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输 的速度。这种调谐被称为 TCP 慢启动（slow start），用于防止因特网的突然过载和拥 塞。 TCP 慢启动限制了一个 TCP 端点在任意时刻可以传输的分组数。简单来说，每成功接收 一个分组，发送端就有了发送另外两个分组的权限。如果某个 HTTP 事务有大量数据要发 送，是不能一次将所有分组都发送出去的。必须发送一个分组，等待确认；然后可以发送 两个分组，每个分组都必须被确认，这样就可以发送四个分组了，以此类推。这种方式被 称为“打开拥塞窗口”。 由于存在这种拥塞控制特性，所以新连接的传输速度会比已经交换过一定量数据的、“已 调谐”连接慢一些。由于已调谐连接要更快一些，所以 HTTP 中有一些可以重用现存连接 的工具。 HTTP 连接的处理 前面我们说了 TCP 连接，我们重新来分析一下 HTTP ，之前我也说过在 HTTP 1.0的时候和1.1之后，有 Keep-Alive ，关于 Keep-Alive 不懂的请翻看前面的公众号的文章内容，接下来我分几个内容给大家讲述 HTTP 对连接上的处理。 并行连接1 通过多条 TCP 连接发起并发的 HTTP 请求。 持久连接1 重用 TCP 连接，以消除连接及关闭时延。 管道化连接1 通过共享的 TCP 连接发起并发的 HTTP 请求。 我们来看一下串行： 每个事务都需要（串行地建立）一条 新的连接，那么连接时延和慢启动时延就会叠加起来 并行连接就是说 HTTP 允许客户端打开多条连接，并行的去执行多个　HTTP 的事务，就会出现多条线路平行的情况。 其实并行连接并没有说是页面的传输速度，是因为多个对象同时在进展，所以，他的速度要比叠加起来，让你在感觉上快不少。 持久连接 HTTP 1.1 允许 HTTP 设备在事务处理结束之后 将 TCP 连接保持在打开状态，以便为未来的 HTTP 请求重用现存的连接。在事务处理结束之后仍然保持在打开状态的 TCP 连接被称为持久连接。非持久连接会在每个事务结束之后关闭。持久连接会在不同事务之间保持打开状态，直到客户端或服务器决定将其关闭为止。 管道化连接(也有人称之为管线化) HTTP/1.1 允许在持久连接上可选地使用请求管道。这是相对于 keep-alive 连接的又一性能优化。在响应到达之前，可以将多条请求放入队列。当第一条请求通过网络流向地球另一端的服务器时，第二条和第三条请求也可以开始发送了。在高时延网络条件下，这样做可以降低网络的环回时间，提高性能。 其实管道化说白了就是 传送过程中不需先等待服务端的回应，然后又发了几条，浏览器将 HTTP 要求大批提交可大幅缩短页面的加载时间，特别是在传输延迟（lag/latency）较高的情况下（如卫星连接）。此技术之关键在于多个 HTTP 的要求消息可以同时塞入一个 TCP 分组中，所以只提交一个分组即可同时发出多个要求，借此可减少网络上多余的分组并降低线路负载。 "},"Chapter10/http2.html":{"url":"Chapter10/http2.html","title":"HTTP2","keywords":"","body":"HTTP2.0 前言 Http协议各版本的对比和优缺点 Http2.0协议相关的SPDY协议、二进制分帧协议、多路复用、首部压缩、服务推送等基本原理 Http协议各版本的对比 http各个版本的基本情况 http协议经过20多年的演进出现过 0.9、1.0、1.1、2.0、3.0 五个主要版本，笔者画了张图看下： Http0.9版本 0.9是鼻祖版本，它的主要特点包括： 请求方法支持有限只支持GET请求方式，不支持其他请求方式 因此客户端向服务端传输信息的量非常有限，也就是现在常用的Post请求无法使用 不支持请求头header不能在请求中指定版本号，服务端只具有返回HTML字符串的能力 响应即关闭服务端相响应之后，立即关闭TCP连接 Http1.0版本 1.0版本主要是对0.9版本的强化，效果也比较明显，主要特性和缺点包括： 丰富请求方法请求方式新增了POST，DELETE，PUT，HEADER等方式，提高了客户端向服务端发送信息的量级 增加请求头和响应头增添了请求头和响应头的概念，可以在通信中指定了HTTP协议版本号，以及其他header信息，使得C/S交互更加灵活方便 丰富数据传输内容扩充了传输内容格式包括： 图片、音视频资源、二进制 等都可以进行传输，相比0.9的只能传输html内容让http的应用场景更多 链接复用性差1.0版本中每个TCP连接只能发送一个请求，数据发送完毕连接就关闭，如果还要请求其他资源，就必须重新建立连接。 TCP为了保证正确性和可靠性需要客户端和服务器三次握手和四次挥手，因此建立连接成本很高，基于拥塞控制开始时发送速率较慢，所以1.0版本的 性能并不理想 。 无状态无连接的弊端1.0版本是 无状态且无连接的，换句话说就是服务器不跟踪不记录请求过的状态，客户端每次请求都需要建立tcp连接不能复用，并且1.0规定在前一个请求响应到达之后下一个请求才能发送，如果前一个阻塞后面的请求就会被阻塞。 丢包和乱序问题和高成本的链接过程让复用和队头阻塞产生很多问题，所以无连接无状态是1.0版本的一个弱肋 。 Http1.1版本 1.1版本在1.0版本发布后大约1年就推出了，是 对1.0版本的优化和完善 ，1.1版本的主要特点包括： 增加长连接新增Connection字段，可以设置keep-alive值保持连接不断开，即TCP连接默认不关闭，可以被多个请求复用，这也是1.1版本很重要的优化，但是在S端服务器只有处理完一个回应，才会进行下一个回应。 要是前面的回应特别慢，后面就会有许多请求排队等着，仍然存在队头阻塞问题。 管道化在长连接的基础上，管道化可以不等第一个请求响应继续发送后面的请求，但响应的顺序还是按照请求的顺序返回，即在同一个TCP连接中，客户端可以同时发送多个请求，进一步改进了HTTP协议的传输效率。 更多的请求方法增加了 PUT、PATCH、OPTIONS、DELETE 等请求方式。 host字段Host字段用来指定服务器的域名，这样就可以将多种请求发往同一台服务器上的不同网站，提高了机器的复用，这个也是重要的优化 Http2.0版本 2.0版本是个里程碑式的版本，相比1.x版本有了非常多的优化去适应当前的网络场景，其中几个重要功能点包括： 二进制格式1.x是文本协议，然而2.0是以二进制帧为基本单位，可以说是一个二进制协议，将所有传输的信息分割为消息和帧，并采用二进制格式的编码，一帧中包含数据和标识符，使得网络传输变得高效而灵活。 多路复用这是一个非常重要的改进，1.x中建立多个连接的消耗以及效率都存在问题，2.0版本的多路复用多个请求共用一个连接，多个请求可以同时在一个TCP连接上并发，主要借助于二进制帧中的标识进行区分实现链路的复用。 头部压缩2.0版本使用使用HPACK算法对头部header数据进行压缩，从而减少请求的大小提高效率，这个非常好理解，之前每次发送都要带相同的header，显得很冗余，2.0版本对头部信息进行增量更新有效减少了头部数据的传输。 服务端推送这个功能有点意思，之前1.x版本服务端都是收到请求后被动执行，在2.0版本允许服务器主动向客户端发送资源，这样在客户端可以起到加速的作用。 Http2.0 详解 前面对比了几个版本的演进和优化过程，接下来深入研究下2.0版本的一些特性及其基本实现原理。 从对比来看2.0版本并不是在1.1版本上的一些优化而是革新，因为2.0背负了更多的性能目标任务，1.1虽然增加了长连接和管道化，但是从根本上并没有实现真正的高性能。 2.0的设计目标是在 兼容1.x语义和操作的基础上，给用户带来更快捷、更简单、更安全的体验高效地利用当前的网络带宽，为此2.0做了很多调整主要包括： 二进制化分帧、多路复用、头部压缩等。 akamai做了http2.0和http1.1在加载过程中的对比效果( 实验中加载379个小片段 在笔者的电脑上的加载时间是0.99s VS 5.80s )： https://http2.akamai.com/demo SPDY协议 要说2.0版本标准和新特性就必须提谷歌的SPDY协议 ，看一下百度百科: SPDY是Google开发的基于TCP的会话层协议，用以最小化网络延迟，提升网络速度，优化用户的网络使用体验。SPDY并不是一种用于替代HTTP的协议，而是对HTTP协议的增强。 新协议的功能包括 数据流的多路复用、请求优先级以及HTTP报头压缩 。谷歌表示引入SPDY协议后，在实验室测试中页面加载速度比原先快64%。 随后SPDY协议得到 Chrome、Firefox 等大型浏览器的支持，在一些大型网站和小型网站种部署，这个高效的协议引起了HTTP工作组的注意，在此基础上制定了官方Http2.0标准 。 之后几年SPDY和Http2.0继续演进相互促进，Http2.0让服务器、浏览器和网站开发者在新协议中获得更好的体验，很快被大众所认可。 二进制分帧层 二进制分帧层 binary framing layer 在不修改请求方法和语义的基础上，重新设计了编码机制 ，如图为http2.0分层结构( 图片来自参考4 )： 二进制编码机制使得通信可以在 单个TCP连接 上进行，该连接在整个对话期间一直处于活跃状态。 二进制协议将通信 数据分解为更小的帧 ，数据帧充斥在C/S之间的双向数据流中，就像双向多车道的高速路，来往如织川流不息： 要理解二进制分帧层需要知道四个概念： 链接Link就是指一条C/S之间的TCP链接，这是个基础的链路数据的高速公路 数据流Stream已建立的TCP连接内的双向字节流，TCP链接中可以承载一条或多条消息 消息Message消息属于一个数据流，消息就是逻辑请求或响应消息对应的完整的一系列帧，也就是帧组成了消息 帧Frame帧是通信的最小单位，每个帧都包含帧头和消息体，标识出当前帧所属的数据流 四者是 一对多的 包含 关系，笔者画了一张图： 再来看一下HeadersFrame头部帧的结构： 再来看一下HeadersFrame头部帧的结构：从各个域可以看到长度、类型、标志位、 流标识符、 数据净荷等，感兴趣可以阅读rfc7540相关文档。 https://httpwg.org/specs/rfc7540.html 总之 2.0版本将通信数据分解为二进制编码帧进行交换，每个帧对应着特定数据流中的特定消息，所有帧和流都在一个TCP连接内复用，二进制分帧协议是2.0其他功能和性能优化的重要基础。 多路复用 1.1版本中存在队首阻塞问题 ，因此如果客户端要发起多个并行请求来提升性能，必须使用多个TCP连接 ，这样就要承受更大延时和建链拆链成本 ，不能有效利用TCP链接。 由于2.0版本中使用新的二进制分帧协议突破了1.0的诸多限制，从根本上实现了真正的请求和响应多路复用 。 客户端和服务器将交互数据分解为 相互独立的帧 ，互不影响地交错传输 ，最后再在对端根据帧头中的流标识符 把它们重新组装起来，从而实现了TCP链接的多路复用。 如图展示了2.0版本的基于帧的消息通信过程 ( 图片来自参考4 ) ： 首部压缩 Header冗余传输 我们都知道http请求都有header部分，每个包都有并且相对于一条链接而言 大部分的包的header部分都是相同的，这样的话每次传输相同的部分确实非常浪费 。 现代网络中每个网页平均包含100多个http请求，每个请求头平均有300-500字节，总数据量达到几十KB以上，这样可能造成数据延时，尤其复杂的WiFi环境或者蜂窝网络 ，这样只能看到手机在转圈，但是这些请求头之间通常几乎没有变化，在本已经拥挤的链路中多次传输相同的数据部分确实不是高效做法。 基于TCP设计的拥塞控制具有线增积减AIMD特性 ，如果发生丢包那么传输速率将大幅度下降，这样在拥挤的网络环境中大的包头意味着只能加剧拥塞控制造成的低速率传输 。 Http压缩和犯罪攻击 在2.0版本的HPACK算法之前，http压缩使用gzip去压缩，后来提出的SPDY算法对Headers进行特殊设计，但是它依旧使用的是DEFLATE算法 。 在后面的一些实际应用中发现DEFLATE和SPDY都有被攻击的危险 ，因为DEFLATE算法使用后向 字符串匹配和动态Huffman编码 ，攻击者可以控制部分请求头部通过修改请求部分然后看压缩之后大小改变多少，如果变小了攻击者就知道注入的文本和请求中的某些内容有重复。 这个过程有点像俄罗斯方块的消除过程 ，这样经过一段时间的尝试数据内容就可能被全部搞清楚，由于这种风险的存在才研发出更安全的压缩算法。 HPACK算法 2.0版本中HPACK算法在C/S中使用 首部表 来存储之前发送的键值对，对于相同的数据通信期间几乎不会改变的通用键值对只需发送一次即可。 极端情况如果请求头每次没有变化，那么传输中则不包含首部，也就是首部开销就是零字节。如果首部键值对发生变化了，也只需要发送变化的数据，并且将新增或修改的首部帧会被追加到首部表 ，首部表在链接存活期始终存在, 并且由客户端和服务器共同更新和维护 。 简单说就是客户端和服务端共同维护了一个 key-value 的结构，发生变化时则更新传输，否则就不传输，这样相当于 首次全量传输之后增量更新传输 即可，这个思想在日常开发中也非常普遍，不用想的太复杂。 如图展示了首部表的更新过程 ( 图片来自参考4 ) ： hpack算法的相关文档： https://tools.ietf.org/html/draft-ietf-httpbis-header-compression-12 服务端推送 服务端推送是2.0版本新增的一个强大功能，和一般的 一问一答 式的C/S交互不同， 推送式交互中服务器可以对客户端的一个请求发送多个响应 ，除了对最初请求的响应外还向客户端推送额外资源，无需客户端明确地请求也可以推送。 举个栗子： 想象一下你去餐厅吃饭，服务好的快餐厅在你点好一份牛肉面之后，还会给你送上餐巾纸、筷子、勺子甚至调料等，这样主动式的服务，节约了客人的时间并且提高了用餐体验。 在实际的C/S交互中这种 主动推送额外资源 的方法很有效，因为几乎每个网络应用都会包含多种资源，客户端需要全部逐个获取它们，此时如果让服务器提前推送这些资源，从而可以 有效减少额外的延迟时 间 ，因为服务器可以知道客户端下一步要请求什么资源。 如图为服务端推送的简单过程 ( 图片来自参考4 ) ： "},"Chapter10/https.html":{"url":"Chapter10/https.html","title":"HTTPS协议详解","keywords":"","body":"tomcat 配置https HTTPS HTTPS（全称：Hyper Text Transfer Protocol over Secure Socket Layer 或 Hypertext Transfer Protocol Secure，超文本传输安全协议），是以安全为目标的HTTP通道，简单讲是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 它是一个URI scheme（抽象标识符体系），句法类同http:体系。用于安全的HTTP数据传输。https:URL表明它使用了HTTP，但HTTPS存在不同于HTTP的默认端口及一个加密/身份验证层（在HTTP与TCP之间）。这个系统的最初研发由网景公司(Netscape)进行，并内置于其浏览器Netscape Navigator中，提供了身份验证与加密通讯方法。现在它被广泛用于万维网上安全敏感的通讯，例如交易支付方面。 HTTPS和HTTP的区别 超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息。HTTP协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此HTTP协议不适合传输一些敏感信息，比如信用卡号、密码等。 为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS。为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL协议，SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。 HTTPS和HTTP的区别主要为以下四点： 一、https协议需要到ca申请证书，一般免费证书很少，需要交费。 二、http是超文本传输协议，信息是明文传输，https 则是具有安全性的ssl加密传输协议。 三、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 四、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 注意：在微信小程序里面都限制只能有https协议、搜索引擎排名都对https优先收录 HTTPS信任主机的问题 采用https的服务器必须从CA （Certificate Authority）申请一个用于证明服务器用途类型的证书。该证书只有用于对应的服务器的时候，客户端才信任此主机。所以所有的银行系统网站，关键部分应用都是https 的。客户通过信任该证书，从而信任了该主机。其实这样做效率很低，但是银行更侧重安全。这一点对局域网对内提供服务处的服务器没有任何意义。局域网中的服务器，采用的证书不管是自己发布的还是从公众的地方发布的，其客户端都是自己人，所以该局域网中的客户端也就肯定信任该服务器。 HTTPS通讯过程中的数据的泄密和被篡改 1． 一般意义上的https，就是服务器有一个证书。 a) 主要目的是保证服务器就是他声称的服务器，这个跟第一点一样。 b)服务端和客户端之间的所有通讯，都是加密的。 i. 具体讲，是客户端产生一个对称的密钥，通过服务器的证书来交换密钥，即一般意义上的握手过程。 ii. 接下来所有的信息往来就都是加密的。第三方即使截获，也没有任何意义，因为他没有密钥，当然篡改也就没有什么意义了。 2． 少许对客户端有要求的情况下，会要求客户端也必须有一个证书。 a) 这里客户端证书，其实就类似表示个人信息的时候，除了用户名/密码，还有一个CA 认证过的身份。因为个人证书一般来说是别人无法模拟的，所有这样能够更深的确认自己的身份。 b) 目前大多数个人银行的专业版是这种做法，具体证书可能是拿U盘（即U盾）作为一个备份的载体。 HTTPS-ssl SSL(Secure Sockets Layer 安全套接层),及其继任者传输层安全(Transport Layer Security，TLS)是为网络通信提供安全及数据完整性的一种安全协议。TLS与SSL在传输层对网络连接进行加密。 SSL (Secure Socket Layer)为Netscape所研发，用以保障在Internet上数据传输之安全，利用数据加密(Encryption)技术，可确保数据在网络上之传输过程中不会被截取及窃听。目前一般通用之规格为40 bit之安全标准，美国则已推出128 bit之更高安全标准，但限制出境。只要3.0版本以上之I.E.或Netscape浏览器即可支持SSL。 当前版本为3.0。它已被广泛地用于Web浏览器与服务器之间的身份认证和加密数据传输。 SSL协议位于TCP/IP协议与各种应用层协议之间，为数据通讯提供安全支持。SSL协议可分为两层：SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。 HTTPS-SSL协议提供的服务主要有哪些 1）认证用户和服务器，确保数据发送到正确的客户机和服务器 2）加密数据以防止数据中途被窃取 3）维护数据的完整性，确保数据在传输过程中不被改变。 SSL协议的工作流程 服务器认证阶段： 1）客户端向服务器发送一个开始信息“Hello”以便开始一个新的会话连接； 2）服务器根据客户的信息确定是否需要生成新的主密钥，如需要则服务器在响应客户的“Hello”信息时将包含生成主密钥所需的信息； 3）客户根据收到的服务器响应信息，产生一个主密钥，并用服务器的公开密钥加密后传给服务器； 4）服务器恢复该主密钥，并返回给客户一个用主密钥认证的信息，以此让客户认证服务器。 HTTPS用户认证阶段 在此之前，服务器已经通过了客户认证，这一阶段主要完成对客户的认证。经认证的服务器发送一个提问给客户，客户则返回（数字）签名后的提问和其公开密钥，从而向服务器提供认证。 从SSL 协议所提供的服务及其工作流程可以看出，SSL协议运行的基础是商家对消费者信息保密的承诺，这就有利于商家而不利于消费者。在电子商务初级阶段，由于运作电子商务的企业大多是信誉较高的大公司，因此这问题还没有充分暴露出来。但随着电子商务的发展，各中小型公司也参与进来，这样在电子支付过程中的单一认证问题就越来越突出。虽然在SSL3.0中通过数字签名和数字证书可实现浏览器和Web服务器双方的身份验证，但是SSL协议仍存在一些问题，比如，只能提供交易中客户与服务器间的双方认证，在涉及多方的电子交易中，SSL协议并不能协调各方间的安全传输和信任关系。在这种情况下，Visa和MasterCard两大信用卡公组织制定了SET协议，为网上信用卡支付提供了全球性的标准。 使用使用java自带keytool创建本地密钥库 创建本地密钥库命令: keytool -genkey -alias testkey -keyalg RSA -keystore \"C:\\Program Files\\Java\\Testkey\" or keytool -genkey -alias caskeystore -keypass 123456 -keyalg RSA -keystore thekeystore -alias ： 这里的testkey是别名，根据个人自定义 -keystore ：这里是声明生成key的位置（稍后会用到） 输入上述命令后，会让你输入密钥库口令（记住密码） HTTPS用的是对称加密还是非对称加密？ HTTPS灵魂拷问 随着 HTTPS 建站的成本下降，现在大部分的网站都已经开始用上 HTTPS 协议。大家都知道 HTTPS 比 HTTP 安全，也听说过与 HTTPS 协议相关的概念有 SSL 、非对称加密、 CA证书等。 但对于以下灵魂三拷问可能就答不上了： 1）为什么用了 HTTPS 就是安全的？ 2）HTTPS 的底层原理如何实现？ 3）用了 HTTPS 就一定安全吗？ 不用担心，本文将在解答“HTTPS到底用的是对称加密还是非对称加密？”的同时层层深入，从原理上把 HTTPS 的安全性讲透，您也将同时理解上述问题。 HTTPS 的实现原理 大家可能都听说过 HTTPS 协议之所以是安全的是因为 HTTPS 协议会对传输的数据进行加密，而加密过程是使用了非对称加密实现。但其实：HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。 HTTPS的整体过程分为证书验证和数据传输阶段，具体的交互过程如下： 证书验证阶段： 浏览器发起 HTTPS 请求； 服务端返回 HTTPS 证书； 客户端验证证书是否合法，如果不合法则提示告警。 数据传输阶段： 当证书验证合法后，在本地生成随机数； 通过公钥加密随机数，并把加密后的随机数传输到服务端； 服务端通过私钥对随机数进行解密； 服务端通过客户端传入的随机数构造对称加密算法，对返回结果内容进行加密后传输。 为什么数据传输是用对称加密？ 首先：非对称加密的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的。 另外：在 HTTPS 的场景中只有服务端保存了私钥，一对公私钥只能实现单向的加解密，所以 HTTPS 中内容传输加密采取的是对称加密，而不是非对称加密。 为什么需要 CA 认证机构颁发证书？ HTTP 协议被认为不安全是因为传输过程容易被监听者勾线监听、伪造服务器，而 HTTPS 协议主要解决的便是网络传输的安全性问题。 首先我们假设不存在认证机构，任何人都可以制作证书，这带来的安全风险便是经典的“中间人攻击”问题。 “中间人攻击”的具体过程如下： 如上图所以，过程原理如下： 本地请求被劫持（如DNS劫持等），所有请求均发送到中间人的服务器； 中间人服务器返回中间人自己的证书； 客户端创建随机数，通过中间人证书的公钥对随机数加密后传送给中间人，然后凭随机数构造对称加密对传输内容进行加密传输； 中间人因为拥有客户端的随机数，可以通过对称加密算法进行内容解密； 中间人以客户端的请求内容再向正规网站发起请求； 因为中间人与服务器的通信过程是合法的，正规网站通过建立的安全通道返回加密后的数据； 中间人凭借与正规网站建立的对称加密算法对内容进行解密； 中间人通过与客户端建立的对称加密算法对正规内容返回的数据进行加密传输； 客户端通过与中间人建立的对称加密算法对返回结果数据进行解密。 由于缺少对证书的验证，所以客户端虽然发起的是 HTTPS 请求，但客户端完全不知道自己的网络已被拦截，传输内容被中间人全部窃取。 浏览器是如何确保 CA 证书的合法性？ 证书包含什么信息？ 颁发机构信息； 公钥； 公司信息； 域名； 有效期； 指纹； ...... 证书的合法性依据是什么？ 首先：权威机构是要有认证的，不是随便一个机构都有资格颁发证书，不然也不叫做权威机构； 另外：证书的可信性基于信任制，权威机构需要对其颁发的证书进行信用背书，只要是权威机构生成的证书，我们就认为是合法的。 所以权威机构会对申请者的信息进行审核，不同等级的权威机构对审核的要求也不一样，于是证书也分为免费的、便宜的和贵的。 浏览器如何验证证书的合法性？ 浏览器发起 HTTPS 请求时，服务器会返回网站的 SSL 证书，浏览器需要对证书做以下验证： 验证域名、有效期等信息是否正确：证书上都有包含这些信息，比较容易完成验证； 判断证书来源是否合法：每份签发证书都可以根据验证链查找到对应的根证书，操作系统、浏览器会在本地存储权威机构的根证书，利用本地根证书可以对对应机构签发证书完成来源验证（如下图所示）： 判断证书是否被篡改：需要与 CA 服务器进行校验； 判断证书是否已吊销：通过CRL（Certificate Revocation List 证书注销列表）和 OCSP（Online Certificate Status Protocol 在线证书状态协议）实现，其中 OCSP 可用于第3步中以减少与 CA 服务器的交互，提高验证效率。 以上任意一步都满足的情况下浏览器才认为证书是合法的。 这里插一个我想了很久的但其实答案很简单的问题 既然证书是公开的，如果要发起中间人攻击，我在官网上下载一份证书作为我的服务器证书，那客户端肯定会认同这个证书是合法的，如何避免这种证书冒用的情况？ 其实这就是非加密对称中公私钥的用处，虽然中间人可以得到证书，但私钥是无法获取的，一份公钥是不可能推算出其对应的私钥，中间人即使拿到证书也无法伪装成合法服务端，因为无法对客户端传入的加密数据进行解密。 只有认证机构可以生成证书吗？ 如果需要浏览器不提示安全风险，那只能使用认证机构签发的证书。但浏览器通常只是提示安全风险，并不限制网站不能访问，所以从技术上谁都可以生成证书，只要有证书就可以完成网站的 HTTPS 传输。 例如早期的 12306 采用的便是手动安装私有证书的形式实现 HTTPS 访问： 本地随机数被窃取怎么办？ 证书验证是采用非对称加密实现，但是传输过程是采用对称加密，而其中对称加密算法中重要的随机数是由本地生成并且存储于本地的，HTTPS 如何保证随机数不会被窃取？ 其实 HTTPS 并不包含对随机数的安全保证，HTTPS 保证的只是传输过程安全，而随机数存储于本地，本地的安全属于另一安全范畴，应对的措施有安装杀毒软件、反木马、浏览器升级修复漏洞等。 用了 HTTPS 会被抓包吗？ HTTPS 的数据是加密的，常规下抓包工具代理请求后抓到的包内容是加密状态，无法直接查看。 但是，正如前文所说，浏览器只会提示安全风险，如果用户授权仍然可以继续访问网站，完成请求。因此，只要客户端是我们自己的终端，我们授权的情况下，便可以组建中间人网络，而抓包工具便是作为中间人的代理。通常 HTTPS 抓包工具的使用方法是会生成一个证书，用户需要手动把证书安装到客户端中，然后终端发起的所有请求通过该证书完成与抓包工具的交互，然后抓包工具再转发请求到服务器，最后把服务器返回的结果在控制台输出后再返回给终端，从而完成整个请求的闭环。 既然 HTTPS 不能防抓包，那 HTTPS 有什么意义？ HTTPS 可以防止用户在不知情的情况下通信链路被监听，对于主动授信的抓包操作是不提供防护的，因为这个场景用户是已经对风险知情。要防止被抓包，需要采用应用级的安全防护，例如采用私有的对称加密，同时做好移动端的防反编译加固，防止本地算法被破解。 "},"Chapter10/WebSocket.html":{"url":"Chapter10/WebSocket.html","title":"WebSocket","keywords":"","body":"WebSocket 什么是Socket？什么是WebSocket？ 对于第1次听说WebSocket技术的人来说，两者有什么区别？websocket是仅仅将socket的概念移植到浏览器中的实现吗？ 我们知道，在网络中的两个应用程序（进程）需要全双工相互通信（全双工即双方可同时向对方发送消息），需要用到的就是socket，它能够提供端对端通信，对于程序员来讲，他只需要在某个应用程序的一端（暂且称之为客户端）创建一个socket实例并且提供它所要连接一端（暂且称之为服务端）的IP地址和端口，而另外一端（服务端）创建另一个socket并绑定本地端口进行监听，然后客户端进行连接服务端，服务端接受连接之后双方建立了一个端对端的TCP连接，在该连接上就可以双向通讯了，而且一旦建立这个连接之后，通信双方就没有客户端服务端之分了，提供的就是端对端通信了。我们可以采取这种方式构建一个桌面版的im程序，让不同主机上的用户发送消息。从本质上来说，socket并不是一个新的协议，它只是为了便于程序员进行网络编程而对tcp/ip协议族通信机制的一种封装。 websocket是html5规范中的一个部分，它借鉴了socket这种思想，为web应用程序客户端和服务端之间（注意是客户端服务端）提供了一种全双工通信机制。同时，它又是一种新的应用层协议，websocket协议是为了提供web应用程序和服务端全双工通信而专门制定的一种应用层协议，通常它表示为：ws://echo.websocket.org/?encoding=text HTTP/1.1，可以看到除了前面的协议名和http不同之外，它的表示地址就是传统的url地址。 可以看到，websocket并不是简单地将socket这一概念在浏览器环境中的移植。 WebSocket是HTML5出的东西（协议），也就是说HTTP协议没有变化，或者说没关系，但HTTP是不支持持久连接的（长连接，循环连接的不算）首先HTTP有1.1、1.0、2.0、3.0之说，但是Websocket其实是一个新协议，跟HTTP协议基本没有关系，只是为了兼容现有浏览器的握手规范而已，也就是说它是HTTP协议上的一种补充。它们之间的关系可以通过这样一张图理解,有交集，但是并不是全部。 Websocket是什么样的协议，具体有什么优点 首先，Websocket是一个持久化的协议，相对于HTTP这种非持久的协议来说。 HTTP的生命周期通过Request来界定，也就是一个Request 一个Response，那么在HTTP1.0中，这次HTTP请求就结束了。在HTTP1.1中进行了改进，使得有一个keep-alive，也就是说，在一个HTTP连接中，可以发送多个Request，接收多个Response。 但是请记住 Request = Response ， 在HTTP中永远是这样，也就是说一个request只能有一个response。而且这个response也是被动的，不能主动发起。 Websocket是基于HTTP协议的，或者说借用了HTTP的协议来完成一部分握手。在握手阶段是一样的，首先我们来看个典型的Websocket握手。 GET /chat HTTP/1.1 Host: server.example.com Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw== Sec-WebSocket-Protocol: chat, superchat Sec-WebSocket-Version: 13 Origin: http://example.com 熟悉HTTP的童鞋可能发现了，这段类似HTTP协议的握手请求中，多了几个东西。 我会顺便讲解下作用。 Upgrade: websocket Connection: Upgrade 上面这个就是Websocket的核心了，告诉Nginx等服务器： “注意啦，我发起的是Websocket协议，快点帮我找到对应的处理器处理~不是那个老土的HTTP”。 Sec-WebSocket-Key: x3JJHMbDL1EzLkh9GBhXDw== Sec-WebSocket-Protocol: chat, superchat Sec-WebSocket-Version: 13 Sec-WebSocket-Key 是一个Base64 encode的值，这个是浏览器随机生成的，告诉服务器：“不要忽悠我，我要验证你是不是真的是Websocket助理。” Sec-WebSocket-Version 是告诉服务器所使用的Websocket Draft（协议版本），在最初的时候，Websocket协议还在 Draft 阶段，各种奇奇怪怪的协议都有，而且还有很多期奇奇怪怪不同的东西，什么Firefox和Chrome用的不是一个版本之类的，当初Websocket协议太多可是一个大难题。不过现在还好，已经定下来啦。 然后服务器会返回下列东西，表示已经接受到请求， 成功建立Websocket啦！ HTTP/1.1 101 Switching Protocols Upgrade: websocket Connection: Upgrade Sec-WebSocket-Accept: HSmrc0sMlYUkAGmm5OPpG2HaGWk= Sec-WebSocket-Protocol: chat Upgrade: websocket Connection: Upgrade Upgrade: websocket Connection: Upgrade 依然是固定的，告诉客户端即将升级的是Websocket协议，而不是mozillasocket，lurnarsocket或者shitsocket。 然后，Sec-WebSocket-Accept 这个则是经过服务器确认，并且加密过后的 Sec-WebSocket-Key。 至此，HTTP已经完成它所有工作了，接下来就是完全按照Websocket协议进行了。 他解决了HTTP的这几个难题。 首先，被动性，当服务器完成协议升级后（HTTP->Websocket），服务端就可以主动推送信息给客户端啦。 所以上面的情景可以做如下修改。 只需要经过一次HTTP请求，就可以做到源源不断的信息传送了。 （在程序设计中，这种设计叫做回调，即：你有信息了再来通知我，而不是我傻乎乎的每次跑来问你） 这样的协议解决了上面同步有延迟，而且还非常消耗资源的这种情况。 那么为什么他会解决服务器上消耗资源的问题呢？其实我们所用的程序是要经过两层代理的， 即HTTP协议在Nginx等服务器的解析下，然后再传送给相应的 web server来处理。 简单地说，我们有一个非常快速的接线员（Nginx），他负责把问题转交给相应的客服（web server）。 本身接线员基本上速度是足够的，但是每次都卡在客服（web server）了， 老有客服处理速度太慢。，导致客服不够。 Websocket就解决了这样一个难题，建立后， 可以直接跟接线员建立持久连接，有信息的时候客服想办法通知接线员， 然后接线员在统一转交给客户。这样就可以解决客服处理速度过慢的问题了。 同时，在传统的方式上，要不断的建立，关闭HTTP协议，由于HTTP是非状态性的， 每次都要重新传输identity info（鉴别信息），来告诉服务端你是谁。虽然接线员很快速，但是每次都要听这么一堆， 效率也会有所下降的，同时还得不断把这些信息转交给客服，不但浪费客服的处理时间， 而且还会在网路传输中消耗过多的流量/时间。但是Websocket只需要一次HTTP握手， 所以说整个通讯过程是建立在一次连接/状态中，也就避免了HTTP的非状态性，服务端会一直知道你的信息， 直到你关闭请求，这样就解决了接线员要反复解析HTTP协议，还要查看identity info的信息。 同时由客户主动询问，转换为服务器（推送）有信息的时候就发送（当然客户端还是等主动发送信息过来的。。）， 没有信息的时候就交给接线员（Nginx），不需要占用本身速度就慢的客服（web server）了 技术概览 在 WebSocket API，浏览器和服务器只需要要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送，改变了原有的B/S模式。 WebSocket技术应用的典型架构： WebSocket的技术原理： 浏览器端的websocket 发起的请求一般是： // javacsript var ws = new WebSocket(\"ws://127.0.0.1:4000\"); ws.onopen = function(){ console.log(\"succeed\"); }; ws.onerror = function(){ console.log(“error”); }; ws.onmessage = function(e){ console.log(e); } 当 new 一个 websocket 对象之后，就会向服务器发送一个 get 请求： 这个请求是对摸个服务器的端口发送的，一般的话，会预先在服务器将一个socket 绑定到一个端口上，客户端和服务器端在这个预定的端口上通信（我这里绑定的就是 4000 端口，默认情况下，websocke 使用 80 端口）。 然后，在服务器端的socket监听到这个packet 之后就生成一个新的 socket，将发送过来的数据中的 Sec-WebSocket-Key 解析出来，然后按照把“Sec-WebSocket-Ke”加上一个魔幻字符串“258EAFA5-E914-47DA-95CA-C5AB0DC85B11”。使用SHA-1加密，之后进行BASE-64编码，将结果做为“Sec-WebSocket-Accept”头的值，返回给客户端。 客户端收到这个之后，就会将 通信协议 upgrade 到 websocket 协议： 然后就会在这个持久的通道下进行通信，包括浏览器的询问，服务器的push，双方是在一个全双工的状态下相互通信。 WebSocket 通信协议 如上述的例子：切换后的websocket 协议中的 数据传输帧的格式(此时不再使用html协议) 官方文档给出的说明： 直接看这个，谁都会有点头大: 我画了一幅图来简单的解释这个 frame 的结构： 各字段的解释： FIN 1bit 表示信息的最后一帧，flag，也就是标记符 RSV 1-3 1bit each 以后备用的 默认都为 0 Opcode 4bit 帧类型， Mask 1bit 掩码，是否加密数据，默认必须置为1 Payload len 7bit 数据的长度，当这个7 bit的数据 == 126 时，后面的2 个字节也是表示数 据长度，当它 == 127 时，后面的 8 个字节表示数据长度 Masking-key 1 or 4 bit 掩码 Payload data playload len bytes 数据 所以我们这里的代码，通过判断 Playload len的值，来用 substr 截取 Masking-key 和 PlayloadData。 根据掩码解析数据的方法是： for( i = 0; i 001 指的是 opcode 官方的解释： 可以设置 opcode的值，来告诉浏览器这个frame的数据属性。 HTTP与WebSocket的关系 HTTPRFC2616协议和 WebSocket协议RFC6455 HTTP的消息 一个HTTP消息可能是request或者response消息，两种类型的消息都是由开始行（start-line），零个或多个header域，一个表示header域结束的空行（也就是，一个以CRLF为前缀的空行），一个可能为空的消息主体（message-body）。一个合格的HTTP客户端不应该在消息头或者尾添加多余的CRLF，服务端也会忽略这些字符。 header的值不包括任何前导或后续的LWS（线性空白），线性空白可能会出现在域值（filed-value）的第一个非空白字符之前或最后一个非空白字符之后。前导或后续的LWS可能会被移除而不会改变域值的语意。任何出现在filed-content之间的LWS可能会被一个SP（空格）代替。header域的顺序不重要，但建议把常用的header放在前边（协议里这么说的）。 HTTP的Request消息 RFC2616中这样定义HTTP Request 消息： Request = Request-Line *(( general-header | request-header（跟本次请求相关的一些header） | entity-header ) CRLF)（跟本次请求相关的一些header） CRLF [ message-body ] 一个HTTP的request消息以一个请求行开始，从第二行开始是header，接下来是一个空行，表示header结束，最后是消息体。 请求行的定义如下： //请求行的定义 Request-Line = Method SP Request-URL SP HTTP-Version CRLF //方法的定义 Method = \"OPTIONS\" | \"GET\" | \"HEAD\" |\"POST\" |\"PUT\" |\"DELETE\" |\"TRACE\" |\"CONNECT\" | extension-method //资源地址的定义 Request-URI =\"*\" | absoluteURI | abs_path | authotity（CONNECT） Request消息中使用的header可以是general-header或者request-header，request-header（后边会讲解）。其中有一个比较特殊的就是Host，Host会与reuqest Uri一起来作为Request消息的接收者判断请求资源的条件。 请求资源组织方法如下： 如果Request-URI是绝对地址（absoluteURI），这时请求里的主机存在于Request-URI里。任何出现在请求里Host头域值应当被忽略； 假如Request-URI不是绝对地址（absoluteURI），并且请求包括一个Host头域，则主机由该Host头域值决定； 假如由规则１或规则２定义的主机是一个无效的主机，则应当以一个400（错误请求）错误消息返回。 HTTP的Response消息 响应消息跟请求消息几乎一模一样，定义如下： Response = Status-Line *(( general-header | response-header | entity-header ) CRLF) CRLF [ message-body ] 可以看到，除了header不使用request-header之外，只有第一行不同，响应消息的第一行是状态行，其中就包含大名鼎鼎的返回码。 Status-Line的内容首先是协议的版本号，然后跟着返回码，最后是解释的内容，它们之间各有一个空格分隔，行的末尾以一个回车换行符作为结束。 定义如下： Status-Line = HTTP-Version SP Status-Code SP Reason-Phrase CRLF HTTP的返回码 返回码是一个3位数，第一位定义的返回码的类别，总共有5个类别，它们是： - 1xx: Informational - Request received, continuing process - 2xx: Success - The action was successfully received, understood, and accepted - 3xx: Redirection - Further action must be taken in order to complete the request - 4xx: Client Error - The request contains bad syntax or cannot be fulfilled - 5xx: Server Error - The server failed to fulfill an apparently valid request RFC2616中接着又给出了一系列返回码的扩展，这些都是我们平时会用到的，但是那些只是示例，HTTP1.1不强制通信各方遵守这些扩展的返回码，通信各方在返回码的实现上只需要遵守以上边定义的这5种类别的定义，意思就是，返回码的第一位要严格按照文档中所述的来，其他的随便定义。 任何人接收到一个不认识的返回码xyz，都可以把它当做x00来对待。对于不认识的返回码的响应消息，不可以缓存。 HTTP的Header RFC2616中定义了4种header类型，在通信各方都认可的情况下，请求头可以被扩展的（可信的扩展只能等到协议的版本更新），如果接收者收到了一个不认识的请求头，这个头将会被当做实体头。4种头类型如下。 1）通用头（General Header Fields）：可用于request，也可用于response的头，但不可作为实体头，只能作为消息的头。 general-header = Cache-Control ; Section 14.9 | Connection ; Section 14.10 | Date ; Section 14.18 | Pragma ; Section 14.32 | Trailer ; Section 14.40 | Transfer-Encoding ; Section 14.41 | Upgrade ; Section 14.42 | Via ; Section 14.45 | Warning ; Section 14.46 2）请求头（Request Header Fields）：被请求发起端用来改变请求行为的头。 request-header = Accept ; Section 14.1 | Accept-Charset ; Section 14.2 | Accept-Encoding ; Section 14.3 | Accept-Language ; Section 14.4 | Authorization ; Section 14.8 | Expect ; Section 14.20 | From ; Section 14.22 | Host ; Section 14.23 | If-Match ; Section 14.24 | If-Modified-Since ; Section 14.25 | If-None-Match ; Section 14.26 | If-Range ; Section 14.27 | If-Unmodified-Since ; Section 14.28 | Max-Forwards ; Section 14.31 | Proxy-Authorization ; Section 14.34 | Range ; Section 14.35 | Referer ; Section 14.36 | TE ; Section 14.39 | User-Agent ; Section 14.43 3）响应头（Response Header Fields）：被服务器用来对资源进行进一步的说明。 response-header = Accept-Ranges ; Section 14.5 | Age ; Section 14.6 | ETag ; Section 14.19 | Location ; Section 14.30 | Proxy-Authenticate ; Section 14.33 | Retry-After ; Section 14.37 | Server ; Section 14.38 | Vary ; Section 14.44 | WWW-Authenticate ; Section 14.47 4）实体头（Entity Header Fields）：如果消息带有消息体，实体头用来作为元信息；如果没有消息体，就是为了描述请求的资源的信息。 entity-header = Allow ; Section 14.7 | Content-Encoding ; Section 14.11 | Content-Language ; Section 14.12 | Content-Length ; Section 14.13 | Content-Location ; Section 14.14 | Content-MD5 ; Section 14.15 | Content-Range ; Section 14.16 | Content-Type ; Section 14.17 | Expires ; Section 14.21 | Last-Modified ; Section 14.29 | extension-header HTTP的消息体（Message Body）和实体主体（Entity Body） 如果有Transfer-Encoding头，那么消息体解码完了就是实体主体，如果没有Transfer-Encoding头，消息体就是实体主体。 RFC2616中是这样定义的： message-body = entity-body | 在request消息中，消息头中含有Content-Length或者Transfer-Encoding，标识会有一个消息体跟在后边。如果请求的方法不应该含有消息体（如OPTION），那么request消息一定不能含有消息体，即使客户端发送过去，服务器也不会读取消息体。 在response消息中，是否存在消息体由请求方法和返回码来共同决定。像1xx，204，304不会带有消息体。 HTTP的消息体的长度 消息体长度的确定有一下几个规则，它们顺序执行： 所有不应该返回内容的Response消息都不应该带有任何的消息体，消息会在第一个空行就被认为是终止了； 如果消息头含有Transfer-Encoding，且它的值不是identity，那么消息体的长度会使用chunked方式解码来确定，直到连接终止； 如果消息头中有Content-Length，那么它就代表了entity-length和transfer-length。如果同时含有Transfer-Encoding，则entity-length和transfer-length可能不会相等，那么Content-Length会被忽略； 如果消息的媒体类型是multipart/byteranges，并且transfer-length也没有指定，那么传输长度由这个媒体自己定义。通常是收发双发定义好了格式， HTTP1.1客户端请求里如果出现Range头域并且带有多个字节范围（byte-range）指示符，这就意味着客户端能解析multipart/byteranges响应； 如果是Response消息，也可以由服务器来断开连接，作为消息体结束。 从消息体中得到实体主体，它的类型由两个header来定义，Content-Type和Content-Encoding（通常用来做压缩）。如果有实体主体，则必须有Content-Type,如果没有，接收方就需要猜测，猜不出来就是用application/octet-stream。 HTTP连接 HTTP1.1的连接默认使用持续连接（persistent connection），持续连接指的是，有时是客户端会需要在短时间内向服务端请求大量的相关的资源，如果不是持续连接，那么每个资源都要建立一个新的连接，HTTP底层使用的是TCP，那么每次都要使用三次握手建立TCP连接，将造成极大的资源浪费。 持续连接可以带来很多的好处： 1）使用更少的TCP连接，对通信各方的压力更小； 2）可以使用管道（pipeline）来传输信息，这样请求方不需要等待结果就可以发送下一条信息，对于单个的TCP的使用更充分； 3）流量更小； 4）顺序请求的延时更小； 5）不需要重新建立TCP连接就可以传送error，关闭连接等信息。 WebSocket协议 只从RFC发布的时间看来，WebSocket要晚近很多，HTTP 1.1是1999年，WebSocket则是12年之后了。 WebSocket协议的开篇就说，本协议的目的是为了解决基于浏览器的程序需要拉取资源时必须发起多个HTTP请求和长时间的轮训的问题而创建的。 WebSocket协议还很年轻，RFC文档（RFC 6455：The WebSocket Protocol）相比HTTP的发布时间也很短，它的诞生是为了创建一种「双向通信」的协议，来作为HTTP协议的一个替代者。那么首先看一下它和HTTP（或者HTTP的长连接）的区别。 为什么要用WebSocket来替代HTTP WebSocket的目的就是解决传统Web网络传输中的双向通信的问题，HTTP1.1默认使用持久连接（persistent connection），在一个TCP连接上也可以传输多个Request/Response消息对，但是HTTP的基本模型还是一个Request对应一个Response。 以IM聊天系统为例，客户端要向服务器传送数据，同时服务器也需要实时的向客户端传送信息，一个聊天系统就是典型的双向通信。 要实现Web端双向通信，一般会使用这样几种解决方案： 轮询（polling）：轮询就会造成对网络和通信双方的资源的浪费，且非实时； 长轮询：客户端发送一个超时时间很长的Request，服务器hold住这个连接，在有新数据到达时返回Response，相比#1，占用的网络带宽少了，其他类似； 长连接：其实有些人对长连接的概念是模糊不清的，我这里讲的其实是HTTP的长连接（1）。如果你使用Socket来建立TCP的长连接（2），那么，这个长连接（2）跟我们这里要讨论的WebSocket是一样的，实际上TCP长连接就是WebSocket的基础，但是如果是HTTP的长连接，本质上还是Request/Response消息对，仍然会造成资源的浪费、实时性不强等问题。 WebSocket协议基础 WebSocket的目的是取代HTTP在双向通信场景下的使用，而且它的实现方式有些也是基于HTTP的（WS的默认端口是80和443）。现有的网络环境（客户端、服务器、网络中间人、代理等）对HTTP都有很好的支持，所以这样做可以充分利用现有的HTTP的基础设施，有点向下兼容的意味。 简单来讲，WS协议有两部分组成：握手和数据传输。 握手（handshake） 出于兼容性的考虑，WS的握手使用HTTP来实现（此文档中提到未来有可能会使用专用的端口和方法来实现握手），客户端的握手消息就是一个「普通的，带有Upgrade头的，HTTP Request消息」。所以这一个小节到内容大部分都来自于RFC2616，这里只是它的一种应用形式，下面是RFC6455文档中给出的一个客户端握手消息示例： GET /chat HTTP/1.1 //1 Host: server.example.com //2 Upgrade: websocket //3 Connection: Upgrade //4 Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ== //5 Origin: [url=http://example.com]http://example.com[/url] //6 Sec-WebSocket-Protocol: chat, superchat //7 Sec-WebSocket-Version: 13 //8 可以看到，前两行跟HTTP的Request的起始行一模一样，而真正在WS的握手过程中起到作用的是下面几个header域： Upgrade：upgrade是HTTP1.1中用于定义转换协议的header域。它表示，如果服务器支持的话，客户端希望使用现有的「网络层」已经建立好的这个「连接（此处是TCP连接）」，切换到另外一个「应用层」（此处是WebSocket）协议； Connection：HTTP1.1中规定Upgrade只能应用在「直接连接」中，所以带有Upgrade头的HTTP1.1消息必须含有Connection头，因为Connection头的意义就是，任何接收到此消息的人（往往是代理服务器）都要在转发此消息之前处理掉Connection中指定的域（不转发Upgrade域）。如果客户端和服务器之间是通过代理连接的，那么在发送这个握手消息之前首先要发送CONNECT消息来建立直接连接； Sec-WebSocket-＊：第7行标识了客户端支持的子协议的列表（关于子协议会在下面介绍），第8行标识了客户端支持的WS协议的版本列表，第5行用来发送给服务器使用（服务器会使用此字段组装成另一个key值放在握手返回信息里发送客户端）； Origin：作安全使用，防止跨站攻击，浏览器一般会使用这个来标识原始域。 如果服务器接受了这个请求，可能会发送如下这样的返回信息，这是一个标准的HTTP的Response消息。101表示服务器收到了客户端切换协议的请求，并且同意切换到此协议。 RFC2616规定只有切换到的协议「比HTTP1.1更好」的时候才能同意切换： HTTP/1.1 101 Switching Protocols //1 Upgrade: websocket. //2 Connection: Upgrade. //3 Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo= //4 Sec-WebSocket-Protocol: chat. //5 WebSocket协议Uri ws协议默认使用80端口，wss协议默认使用443端口： ws-URI = \"ws:\" \"//\" host [ \":\" port ] path [ \"?\" query ] wss-URI = \"wss:\" \"//\" host [ \":\" port ] path [ \"?\" query ] host = port = path = query = 注：wss协议是WebSocket使用SSL/TLS加密后的协议，类似天HTTP和HTTPS的关系。 在客户端发送握手之前要做的一些小事 在握手之前，客户端首先要先建立连接，一个客户端对于一个相同的目标地址（通常是域名或者IP地址，不是资源地址）同一时刻只能有一个处于CONNECTING状态（就是正在建立连接）的连接。 从建立连接到发送握手消息这个过程大致是这样的： 客户端检查输入的Uri是否合法。 客户端判断，如果当前已有指向此目标地址（IP地址）的连接（A）仍处于CONNECTING状态，需要等待这个连接（A）建立成功，或者建立失败之后才能继续建立新的连接： PS：如果当前连接是处于代理的网络环境中，无法判断IP地址是否相同，则认为每一个Host地址为一个单独的目标地址，同时客户端应当限制同时处于CONNECTING状态的连接数； PPS：这样可以防止一部分的DDOS攻击； PPPS：客户端并不限制同时处于「已成功」状态的连接数，但是如果一个客户端「持有大量已成功状态的连接的」，服务器或许会拒绝此客户端请求的新连接。 如果客户端处于一个代理环境中，它首先要请求它的代理来建立一个到达目标地址的TCP连接： 例如，如果客户端处于代理环境中，它想要连接某目标地址的80端口，它可能要收现发送以下消息： CONNECT example.com:80 HTTP/1.1 Host: example.com 如果客户端没有处于代理环境中，它就要首先建立一个到达目标地址的直接的TCP连接。 如果上一步中的TCP连接建立失败，则此WebSocket连接失败。如果协议是wss，则在上一步建立的TCP连接之上，使用TSL发送握手信息。如果失败，则此WebSocket连接失败；如果成功，则以后的所有数据都要通过此TSL通道进行发送。 对于客户端握手信息的一些小要求 客户端握手信息的要求： 握手必须是RFC2616中定义的Request消息 此Request消息的方法必须是GET，HTTP版本必须大于1.1 ： 以下是某WS的Uri对应的Request消息： ws://example.com/chat GET /chat HTTP/1.1 此Request消息中Request-URI部分（RFC2616中的概念）所定义的资型必须和WS协议的Uri中定义的资源相同。 此Request消息中必须含有Host头域，其内容必须和WS的Uri中定义的相同。 此Request消息必须包含Upgrade头域，其内容必须包含websocket关键字。 此Request消息必须包含Connection头域，其内容必须包含Upgrade指令。 此Request消息必须包含Sec-WebSocket-Key头域，其内容是一个Base64编码的16位随机字符。 如果客户端是浏览器，此Request消息必须包含Origin头域，其内容是参考RFC6454。 此Request消息必须包含Sec-WebSocket-Version头域，在此协议中定义的版本号是13。 此Request消息可能包含Sec-WebSocket-Protocol头域，其意义如上文中所述。 此Request消息可能包含Sec-WebSocket-Extensions头域，客户端和服务器可以使用此header来进行一些功能的扩展。 此Request消息可能包含任何合法的头域。如RFC2616中定义的那些。 在客户端接收到Response握手消息之后要做的一些事情 接收到Response握手消息之后： 如果返回的返回码不是101，则按照RFC2616进行处理。如果是101，进行下一步，开始解析header域，所有header域的值不区分大小写； 判断是否含有Upgrade头，且内容包含websocket； 判断是否含有Connection头，且内容包含Upgrade； 判断是否含有Sec-WebSocket-Accept头，其内容在下面介绍； 如果含有Sec-WebSocket-Extensions头，要判断是否之前的Request握手带有此内容，如果没有，则连接失败； 如果含有Sec-WebSocket-Protocol头，要判断是否之前的Request握手带有此协议，如果没有，则连接失败。 服务端的概念 服务端指的是所有参与处理WebSocket消息的基础设施，比如如果某服务器使用Nginx（A）来处理WebSocket，然后把处理后的消息传给响应的服务器（B），那么A和B都是这里要讨论的服务端的范畴。 接受了客户端的连接请求，服务端要做的一些事情 如果请求是HTTPS，则首先要使用TLS进行握手，如果失败，则关闭连接，如果成功，则之后的数据都通过此通道进行发送。 之后服务端可以进行一些客户端验证步骤（包括对客户端header域的验证），如果需要，则按照RFC2616来进行错误码的返回。 如果一切都成功，则返回成功的Response握手消息。 服务端发送的成功的Response握手 此握手消息是一个标准的HTTP Response消息，同时它包含了以下几个部分： 状态行（如上一篇RFC2616中所述）； Upgrade头域，内容为websocket； Connection头域，内容为Upgrade； Sec-WebSocket-Accept头域，其内容的生成步骤： a. 首先将Sec-WebSocket-Key的内容加上字符串258EAFA5-E914-47DA-95CA-C5AB0DC85B11（一个UUID）； b. 将#1中生成的字符串进行SHA1编码； c. 将#2中生成的字符串进行Base64编码。 Sec-WebSocket-Protocol头域（可选）； Sec-WebSocket-Extensions头域（可选）。 一旦这个握手发出去，服务端就认为此WebSocket连接已经建立成功，处于OPEN状态。它就可以开始发送数据了。 WebSocket的一些扩展 Sec-WebSocket-Version可以被通信双方用来支持更多的协议的扩展，RFC6455中定义的值为13，WebSocket的客户端和服务端可能回自定义更多的版本号来支持更多的功能。其使用方法如上文所述。 发送数据 WebSocket中所有发送的数据使用帧的形式发送。客户端发送的数据帧都要经过掩码处理，服务端发送的所有数据帧都不能经过掩码处理。否则对方需要发送关闭帧。 一个帧包含一个帧类型的标识码，一个负载长度，和负载。负载包括扩展内容和应用内容。 帧类型 帧类型是由一个4位长的叫Opcode的值表示，任何WebSocket的通信方收到一个位置的帧类型，都要以连接失败的方式断开此连接。 RFC6455中定义的帧类型如下所示： Opcode == 0 继续：表示此帧是一个继续帧，需要拼接在上一个收到的帧之后，来组成一个完整的消息。由于这种解析特性，非控制帧的发送和接收必须是相同的顺序。 Opcode == 1 文本帧。 Opcode == 2 二进制帧。 Opcode == 3 - 7 未来使用（非控制帧）。 Opcode == 8 关闭连接（控制帧）：此帧可能会包含内容，以表示关闭连接的原因。通信的某一方发送此帧来关闭WebSocket连接，收到此帧的一方如果之前没有发送此帧，则需要发送一个同样的关闭帧以确认关闭。如果双方同时发送此帧，则双方都需要发送回应的关闭帧。理想情况服务端在确认WebSocket连接关闭后，关闭相应的TCP连接，而客户端需要等待服务端关闭此TCP连接，但客户端在某些情况下也可以关闭TCP连接。 Opcode == 9 Ping：类似于心跳，一方收到Ping，应当立即发送Pong作为响应。 Opcode == 10 Pong：如果通信一方并没有发送Ping，但是收到了Pong，并不要求它返回任何信息。Pong帧的内容应当和收到的Ping相同。可能会出现一方收到很多的Ping，但是只需要响应最近的那一次就可以了。 Opcode == 11 - 15 未来使用（控制帧）。 帧的格式 具体的每一项代表什么意思在这里就不做详细的阐述了。 小结一下 以上说了这么多，其实Http和WebSocket的关系通过下图就能简单的理解了： 与HTTP比较 同样作为应用层的协议，WebSocket在现代的软件开发中被越来越多的实践，和HTTP有很多相似的地方，这里将它们简单的做一个纯个人、非权威的比较。 相同点 都是基于TCP的应用层协议； 都使用Request/Response模型进行连接的建立； 在连接的建立过程中对错误的处理方式相同，在这个阶段WS可能返回和HTTP相同的返回码； 都可以在网络中传输数据。 不同点 WS使用HTTP来建立连接，但是定义了一系列新的header域，这些域在HTTP中并不会使用； WS的连接不能通过中间人来转发，它必须是一个直接连接； WS连接建立之后，通信双方都可以在任何时刻向另一方发送数据； WS连接建立之后，数据的传输使用帧来传递，不再需要Request消息； WS的数据帧有序。 为什么引入WebSocket协议 Browser已经支持http协议，为什么还要开发一种新的WebSocket协议呢？我们知道http协议是一种单向的网络协议，在建立连接后，它只允许Browser/UA（UserAgent）向WebServer发出请求资源后，WebServer才能返回相应的数据。而WebServer不能主动的推送数据给Browser/UA，当初这么设计http协议也是有原因的，假设WebServer能主动的推送数据给Browser/UA，那Browser/UA就太容易受到攻击，一些广告商也会主动的把一些广告信息在不经意间强行的传输给客户端，这不能不说是一个灾难。那么单向的http协议给现在的网站或Web应用程序开发带来了哪些问题呢？ 让我们来看一个案例，现在假设我们想开发一个基于Web的应用程序去获取当前Web服务器的实时数据，例如股票的实时行情，火车票的剩余票数等等，这就需要Browser/UA与WebServer端之间反复的进行http通信，Browser不断的发送Get请求，去获取当前的实时数据。下面介绍几种常见的方式： Polling 这种方式就是通过Browser/UA定时的向Web服务器发送http的Get请求，服务器收到请求后，就把最新的数据发回给客户端（Browser/UA），Browser/UA得到数据后，就将其显示出来，然后再定期的重复这一过程。虽然这样可以满足需求，但是也仍然存在一些问题，例如在某段时间内Web服务器端没有更新的数据，但是Browser/UA仍然需要定时的发送Get请求过来询问，那么Web服务器就把以前的老数据再传送过来，Browser/UA把这些没有变化的数据再显示出来，这样显然既浪费了网络带宽，又浪费了CPU的利用率。如果说把Browser发送Get请求的周期调大一些，就可以缓解这一问题，但是如果在Web服务器端的数据更新很快时，这样又不能保证Web应用程序获取数据的实时性。 Long Polling 上面介绍了Polling遇到的问题，现在介绍一下LongPolling，它是对Polling的一种改进。 Browser/UA发送Get请求到Web服务器，这时Web服务器可以做两件事情，第一，如果服务器端有新的数据需要传送，就立即把数据发回给Browser/UA，Browser/UA收到数据后，立即再发送Get请求给Web Server；第二，如果服务器端没有新的数据需要发送，这里与Polling方法不同的是，服务器不是立即发送回应给Browser/UA，而是把这个请求保持住，等待有新的数据到来时，再来响应这个请求；当然了，如果服务器的数据长期没有更新，一段时间后，这个Get请求就会超时，Browser/UA收到超时消息后，再立即发送一个新的Get请求给服务器。然后依次循环这个过程。 这种方式虽然在某种程度上减小了网络带宽和CPU利用率等问题，但是仍然存在缺陷，例如假设服务器端的数据更新速率较快，服务器在传送一个数据包给Browser后必须等待Browser的下一个Get请求到来，才能传递第二个更新的数据包给Browser，那么这样的话，Browser显示实时数据最快的时间为2×RTT（往返时间），另外在网络拥塞的情况下，这个应该是不能让用户接受的。另外，由于http数据包的头部数据量往往很大（通常有400多个字节），但是真正被服务器需要的数据却很少（有时只有10个字节左右），这样的数据包在网络上周期性的传输，难免对网络带宽是一种浪费。 通过上面的分析可知，要是在Browser能有一种新的网络协议，能支持客户端和服务器端的双向通信，而且协议的头部又不那么庞大就好了。WebSocket就是肩负这样一个使命登上舞台的。 技术对比 HTTP协议是非持久化的，单向的网络协议，在建立连接后只允许浏览器向服务器发出请求后，服务器才能返回相应的数据。当需要即时通讯时，通过轮询在特定的时间间隔（如1秒），由浏览器向服务器发送Request请求，然后将最新的数据返回给浏览器。 这样的方法最明显的缺点就是需要不断的发送请求，而且通常HTTP request的Header是非常长的，为了传输一个很小的数据 需要付出巨大的代价，是很不合算的，占用了很多的宽带。这种方式即浪费带宽（HTTP HEAD 是比较大的），又消耗服务器 CPU 占用（没有信息也要接受请求）。 而是用 WebSocket 技术，则会大幅降低上面提到的消耗，如下图所示。 WebSocket与Socket的关系 Socket 和 WebSocket 有哪些区别和联系？ WebSocket 和 HTML5 是什么关系？ 必须在浏览器中才能使用 WebSocket 吗？ WebSocket 能和 Socket 一样传输 raw 数据么？ WebSocket 和 Socket 相比会多耗费流量么？ WebSocket 与 TCP 从 OSI 模型图中可以看出，HTTP、WebSocket 等应用层协议，都是基于 TCP 协议来传输数据的，我们可以把这些高级协议理解成对 TCP 的封装。 既然大家都使用 TCP 协议，那么大家的连接和断开，都要遵循 TCP 协议中的三次握手和四次握手 ，只是在连接之后发送的内容不同，或者是断开的时间不同。 对于 WebSocket 来说，它必须依赖 HTTP 协议进行一次握手 ，握手成功后，数据就直接从 TCP 通道传输，与 HTTP 无关了。 再来看看Socket Socket可以有很多意思，和IT较相关的本意大致是指在端到端的一个连接中，这两个端叫做Socket。对于IT从业者来说，它往往指的是TCP/IP网络环境中的两个连接端，大多数的API提供者（如操作系统，JDK）往往会提供基于这种概念的接口，所以对于开发者来说也往往是在说一种编程概念。同时，操作系统中进程间通信也有Socket的概念，但这个Socket就不是基于网络传输层的协议了。 Unix中的Socket 操作系统中也有使用到Socket这个概念用来进行进程间通信，它和通常说的基于TCP/IP的Socket概念十分相似，代表了在操作系统中传输数据的两方，只是它不再基于网络协议，而是操作系统本身的文件系统。 网络中的Socket 通常所说的Socket API，是指操作系统中（也可能不是操作系统）提供的对于传输层（TCP/UDP）抽象的接口。现行的Socket API大致都是遵循了BSD Socket规范（包括Windows）。这里称规范其实不太准确，规范其实是POSIX，但BSD Unix中对于Socket的实现被广为使用，所以成为了实际的规范。如果你要使用HTTP来构建服务，那么就不需要关心Socket，如果你想基于TCP/IP来构建服务，那么Socket可能就是你会接触到的API。 从上图中可以看到，HTTP是基于传输层的TCP协议的，而Socket API也是，所以只是从使用上说，可以认为Socket和HTTP类似（但一个是成文的互联网协议，一个是一直沿用的一种编程概念），是对于传输层协议的另一种直接使用，因为按照设计，网络对用户的接口都应该在应用层。 Socket名称的由来 和很多其他Internet上的事物一样，Socket这个名称来自于大名鼎鼎的ARPANET（Advanced Research Projects Agency），早期ARPANET中的Socket指的是一个源或者目的地址——大致就是今天我们所说的IP地址和端口号。最早的时候一个Socket指的是一个40位的数字（RFC33中说明了此用法，但在RFC36中并没有明确地说使用40位数字来标识一个地址），其中前32为指向的地址（socket number，大致相当于IP），后8位为发送数据的源（link，大致相当于端口号）。对他们的叫法有很多的版本，这里列举的并不严谨。 端口号的野史 随着ARPANET的发展，后来（RFC433，Socket Number List）socket number被明确地定义为一个40位的数字，其中后8位被用来制定某个特定的应用使用（比如1是Telnet）。这8位数有很多名字：link、socket name、AEN（another eight number，看到这个名字我也是醉了），工程师逗逼起来也是挺拼的。 后来在Internet的规范制定中，才真正的用起了port number这个词。至于为什么端口号是16位的，我想可能有两个原因，一是对于当时的工程师来说，如果每个端口号来标识一个程序，65535个端口号也差不多够用了。二可能是为了对齐吧，_!!。 Socket原本的意思 在上边提到的历史中使用到的Socket，包括TCP文档中使用到的Socket，其实指的是网络传输中的一端，是一个虚拟化的概念。 Socket 与 WebSocket 的关系 正如上节所述：Socket 其实并不是一个协议，它工作在 OSI 模型会话层（第5层），是为了方便大家直接使用更底层协议（一般是 TCP 或 UDP ）而存在的一个抽象层。 最早的一套 Socket API 是 Berkeley sockets ，采用 C 语言实现。它是 Socket 的事实标准，POSIX sockets 是基于它构建的，多种编程语言都遵循这套 API，在 JAVA、Python 中都能看到这套 API 的影子。 下面摘录一段更容易理解的文字（来自 http和socket之长连接和短连接区别）： Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 主机 A 的应用程序要能和主机 B 的应用程序通信，必须通过 Socket 建立连接，而建立 Socket 连接必须需要底层 TCP/IP 协议来建立 TCP 连接。建立 TCP 连接需要底层 IP 协议来寻址网络中的主机。我们知道网络层使用的 IP 协议可以帮助我们根据 IP 地址来找到目标主机，但是一台主机上可能运行着多个应用程序，如何才能与指定的应用程序通信就要通过 TCP 或 UPD 的地址也就是端口号来指定。这样就可以通过一个 Socket 实例唯一代表一个主机上的一个应用程序的通信链路了。 而 WebSocket 则不同，它是一个完整的 应用层协议，包含一套标准的 API。 所以，从使用上来说，WebSocket 更易用，而 Socket 更灵活。 "},"Chapter10/ziliao.html":{"url":"Chapter10/ziliao.html","title":"IM","keywords":"","body":"[1] 网络编程基础资料： 《TCP/IP详解 - 第11章·UDP：用户数据报协议》 《TCP/IP详解 - 第17章·TCP：传输控制协议》 《TCP/IP详解 - 第18章·TCP连接的建立与终止》 《TCP/IP详解 - 第21章·TCP的超时与重传》 《理论经典：TCP协议的3次握手与4次挥手过程详解》 《理论联系实际：Wireshark抓包分析TCP 3次握手、4次挥手过程》 《计算机网络通讯协议关系图（中文珍藏版）》 《NAT详解：基本原理、穿越技术(P2P打洞)、端口老化等》 《UDP中一个包的大小最大能多大？》 《Java新一代网络编程模型AIO原理及Linux系统AIO介绍》 《NIO框架入门(三)：iOS与MINA2、Netty4的跨平台UDP双向通信实战》 《NIO框架入门(四)：Android与MINA2、Netty4的跨平台UDP双向通信实战》 [2] 有关IM/推送的通信格式、协议的选择： 《为什么QQ用的是UDP协议而不是TCP协议？》 《移动端即时通讯协议选择：UDP还是TCP？》 《如何选择即时通讯应用的数据传输格式》 《强列建议将Protobuf作为你的即时通讯应用数据传输格式》 《移动端IM开发需要面对的技术问题（含通信协议选择）》 《简述移动端IM开发的那些坑：架构设计、通信协议和客户端》 《理论联系实际：一套典型的IM通信协议设计详解》 《58到家实时消息系统的协议设计等技术实践分享》 [3] 有关IM/推送的心跳保活处理： 《为何基于TCP协议的移动端IM仍然需要心跳保活机制？》 《微信团队原创分享：Android版微信后台保活实战分享(进程保活篇)》 《微信团队原创分享：Android版微信后台保活实战分享(网络保活篇)》 《移动端IM实践：实现Android版微信的智能心跳机制》 《移动端IM实践：WhatsApp、Line、微信的心跳策略分析》 [4] 有关WEB端即时通讯开发： 《新手入门贴：史上最全Web端即时通讯技术原理详解》 《Web端即时通讯技术盘点：短轮询、Comet、Websocket、SSE》 《SSE技术详解：一种全新的HTML5服务器推送事件技术》 《Comet技术详解：基于HTTP长连接的Web端实时通信技术》 《WebSocket详解（一）：初步认识WebSocket技术》 《socket.io实现消息推送的一点实践及思路》 [5] 有关IM架构设计： 《浅谈IM系统的架构设计》 《简述移动端IM开发的那些坑：架构设计、通信协议和客户端》 《一套原创分布式即时通讯(IM)系统理论架构方案》 《从零到卓越：京东客服即时通讯系统的技术架构演进历程》 《蘑菇街即时通讯/IM服务器开发之架构选择》 《腾讯QQ1.4亿在线用户的技术挑战和架构演进之路PPT》 《微信技术总监谈架构：微信之道——大道至简(演讲全文)》 《如何解读《微信技术总监谈架构：微信之道——大道至简》》 《快速裂变：见证微信强大后台架构从0到1的演进历程（一）》 《17年的实践：腾讯海量产品的技术方法论》 [6] 有关IM安全的文章： 《即时通讯安全篇（一）：正确地理解和使用Android端加密算法》 《即时通讯安全篇（二）：探讨组合加密算法在IM中的应用》 《即时通讯安全篇（三）：常用加解密算法与通讯安全讲解》 《即时通讯安全篇（四）：实例分析Android中密钥硬编码的风险》 《传输层安全协议SSL/TLS的Java平台实现简介和Demo演示》 《理论联系实际：一套典型的IM通信协议设计详解（含安全层设计）》 《微信新一代通信安全解决方案：基于TLS1.3的MMTLS详解》 《来自阿里OpenIM：打造安全可靠即时通讯服务的技术实践分享》 [7] 有关实时音视频开发： 《即时通讯音视频开发（一）：视频编解码之理论概述》 《即时通讯音视频开发（二）：视频编解码之数字视频介绍》 《即时通讯音视频开发（三）：视频编解码之编码基础》 《即时通讯音视频开发（四）：视频编解码之预测技术介绍》 《即时通讯音视频开发（五）：认识主流视频编码技术H.264》 《即时通讯音视频开发（六）：如何开始音频编解码技术的学习》 《即时通讯音视频开发（七）：音频基础及编码原理入门》 《即时通讯音视频开发（八）：常见的实时语音通讯编码标准》 《即时通讯音视频开发（九）：实时语音通讯的回音及回音消除\b概述》 《即时通讯音视频开发（十）：实时语音通讯的回音消除\b技术详解》 《即时通讯音视频开发（十一）：实时语音通讯丢包补偿技术详解》 《即时通讯音视频开发（十二）：多人实时音视频聊天架构探讨》 《即时通讯音视频开发（十三）：实时视频编码H.264的特点与优势》 《即时通讯音视频开发（十四）：实时音视频数据传输协议介绍》 《即时通讯音视频开发（十五）：聊聊P2P与实时音视频的应用情况》 《即时通讯音视频开发（十六）：移动端实时音视频开发的几个建议》 《即时通讯音视频开发（十七）：视频编码H.264、V8的前世今生》 《简述开源实时音视频技术WebRTC的优缺点》 《良心分享：WebRTC 零基础开发者教程（中文）》 [8] IM开发综合文章： 《移动端IM开发需要面对的技术问题》 《开发IM是自己设计协议用字节流好还是字符流好？》 《请问有人知道语音留言聊天的主流实现方式吗？》 《IM系统中如何保证消息的可靠投递（即QoS机制）》 《谈谈移动端 IM 开发中登录请求的优化》 《完全自已开发的IM该如何设计“失败重试”机制？》 《微信对网络影响的技术试验及分析（论文全文）》 《即时通讯系统的原理、技术和应用（技术论文）》 《开源IM工程“蘑菇街TeamTalk”的现状：一场有始无终的开源秀》 [9] 开源移动端IM技术框架资料： 《开源移动端IM技术框架MobileIMSDK：快速入门》 《开源移动端IM技术框架MobileIMSDK：常见问题解答》 《开源移动端IM技术框架MobileIMSDK：压力测试报告》 《开源移动端IM技术框架MobileIMSDK：Android版Demo使用帮助》 《开源移动端IM技术框架MobileIMSDK：Java版Demo使用帮助》 《开源移动端IM技术框架MobileIMSDK：iOS版Demo使用帮助》 《开源移动端IM技术框架MobileIMSDK：Android客户端开发指南》 《开源移动端IM技术框架MobileIMSDK：Java客户端开发指南》 《开源移动端IM技术框架MobileIMSDK：iOS客户端开发指南》 《开源移动端IM技术框架MobileIMSDK：Server端开发指南》 [10] 有关推送技术的文章： 《iOS的推送服务APNs详解：设计思路、技术原理及缺陷等》 《Android端消息推送总结：实现原理、心跳保活、遇到的问题等》 《扫盲贴：认识MQTT通信协议》 《一个基于MQTT通信协议的完整Android推送Demo》 《求教android消息推送：GCM、XMPP、MQTT三种方案的优劣》 《移动端实时消息推送技术浅析》 《扫盲贴：浅谈iOS和Android后台实时消息推送的原理和区别》 《绝对干货：基于Netty实现海量接入的推送服务技术要点》 《移动端IM实践：谷歌消息推送服务(GCM)研究（来自微信）》 《为何微信、QQ这样的IM工具不使用GCM服务推送消息？》 "},"Chapter10/CDN.html":{"url":"Chapter10/CDN.html","title":"CDN详解","keywords":"","body":"CDN详解 CDN 全称:Content Delivery Network或Content Ddistribute Network，即内容分发网络 基本思路： 尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和环节，使内容传输的更快、更稳定。通过在网络各处放置节点服务器所构成的在现有的互联网基础之上的一层智能虚拟网络，CDN系统能够实时地根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。 目的： 解决因分布、带宽、服务器性能带来的访问延迟问题，适用于站点加速、点播、直播等场景。使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度和成功率。 控制时延无疑是现代信息科技的重要指标，CDN的意图就是尽可能的减少资源在转发、传输、链路抖动等情况下顺利保障信息的连贯性。 优势： 如果你在经营一家网站，那你应该知道几点因素是你制胜的关键： 内容有吸引力 访问速度快 支持频繁的用户互动 可以在各处浏览无障碍 另外：你的网站必须能在复杂的网络环境下运行，考虑到全球的用户访问体验。你的网站也会随着使用越来越多的对象（如图片、帧、CSS及APIs）和形形色色的动作（分享、跟踪）而系统逐渐庞大。所以，系统变慢带来用户的流失。 Google及其它网站的研究表明，一个网站每慢一秒钟，就会丢失许多访客，甚至这些访客永远不会再次光顾这些网站。可以想象，如果网站是你的盈利渠道或是品牌窗口，那么网站速度慢将是一个致命的打击。 这就是你使用CDN的第一个也是最重要的原因：加速网站的访问 除此之外，CDN还有一些作用： 为了实现跨运营商、跨地域的全网覆盖 互联不互通、区域ISP地域局限、出口带宽受限制等种种因素都造成了网站的区域性无法访问。CDN加速可以覆盖全球的线路，通过和运营商合作，部署IDC资源，在全国骨干节点商，合理部署CDN边缘分发存储节点，充分利用带宽资源，平衡源站流量。 为了保障你的网站安全 CDN的负载均衡和分布式存储技术，可以加强网站的可靠性，相当无无形中给你的网站添加了一把保护伞，应对绝大部分的互联网攻击事件。防攻击系统也能避免网站遭到恶意攻击。 为了异地备援 当某个服务器发生意外故障时，系统将会调用其他临近的健康服务器节点进行服务，进而提供接近100%的可靠性，这就让你的网站可以做到永不宕机。 为了节约成本投入 使用CDN加速可以实现网站的全国铺设，你根据不用考虑购买服务器与后续的托管运维，服务器之间镜像同步，也不用为了管理维护技术人员而烦恼，节省了人力、精力和财力。 为了让你更专注业务本身 CDN加速厂商一般都会提供一站式服务，业务不仅限于CDN，还有配套的云存储、大数据服务、视频云服务等，而且一般会提供7x24运维监控支持，保证网络随时畅通，你可以放心使用。并且将更多的精力投入到发展自身的核心业务之上。 引入一个词：控制时延 无疑是现代信息科技的重要指标，CDN的意图就是尽可能的减少资源在转发、传输、链路抖动等情况下顺利保障信息的连贯性 根据论文《WAITING TIMES IN QUALITY OF EXPERIENCE FOR WEB BASED SERVICES》中提出的： 其指出基于人脑在等待不同时长的信息时，产生的不同意识行为，进而对信息获取产生的差别感官体验。 感知的持续时间 VS 客观持续时间 根据研究发现：当人们获得Voice这类声音讯息时，好感度随时间流逝下降的最为迅速，仅需16分钟“评估意见等级”下降了70%，可见音像资源受缓冲时间的影响可见一斑 0.1 s： 用户几乎感觉不到系统是否不连贯性。 1.0 s： 用户明显注意到时延的发生，但是在该时间内思维依然是连贯的。 10 s： 超过该时间的时延会使用户失去等待意愿。 控制时延无疑是现代信息科技的重要指标，CDN的意图就是尽可能的减少资源在转发、传输、链路抖动等情况下顺利保障信息的连贯性。 通俗点说就是在网速一定的前提下，CDN就像网络中快递员小哥 主要特点： 1、本地Cache加速，提高了企业站点（尤其含有大量图片和静态页面站点）的访问速度，并大大提高以上性质站点的稳定性 2、镜像服务消除了不同运营商之间互联的瓶颈造成的影响，实现了跨运营商的网络加速，保证不同网络中的用户都能得到良好的访问质量。 3、远程加速 远程访问用户根据DNS负载均衡技术 智能自动选择Cache服务器，选择最快的Cache服务器，加快远程访问的速度 4、带宽优化 自动生成服务器的远程Mirror（镜像）cache服务器，远程用户访问时从cache服务器上读取数据，减少远程访问的带宽、分担网络流量、减轻原站点WEB服务器负载等功能。 5、集群抗攻击 广泛分布的CDN节点加上节点之间的智能冗余机制，可以有效地预防黑客入侵以及降低各种D.D.o.S攻击对网站的影响，同时保证较好的服务质量 。 关键技术： 内容发布：它借助于建立索引、缓存、流分裂、组播（Multicast）等技术 内容路由：它是整体性的网络负载均衡技术，通过内容路由器中的重定向（DNS）机制，在多个远程POP上均衡用户的请求，以使用户请求得到最近内容源的响应； 内容交换：它根据内容的可用性、服务器的可用性以及用户的背景，在POP的缓存服务器上，利用应用层交换、流分裂、重定向（ICP、WCCP）等技术，智能地平衡负载流量； 性能管理：它通过内部和外部监控系统，获取网络部件的状况信息，测量内容发布的端到端性能（如包丢失、延时、平均带宽、启动时间、帧速率等），保证网络处于最佳的运行状态。 Q & A 1.CDN加速是对网站所在服务器加速，还是对其域名加速？ CDN是只对网站的某一个具体的域名加速。如果同一个网站有多个域名，则访客访问加入CDN的域名获得加速效果，访问未加入CDN的域名，或者直接访问IP地址，则无法获得CDN效果。 2.CDN和镜像站点比较有何优势？　　 CDN对网站的访客完全透明，不需要访客手动选择要访问的镜像站点，保证了网站对访客的友好性。　　 CDN对每个节点都有可用性检查，不合格的节点会第一时间剔出，从而保证了极高的可用率，而镜像站点无法实现这一点。　　 CDN部署简单，对原站基本不做任何改动即可生效。 3.CDN和双线机房相比有何优势？ 常见的双线机房只能解决网通和电信互相访问慢的问题，其它ISP（譬如教育网，移动网，铁通）互通的问题还是没得到解决。　　 而CDN是访问者就近取数据，而CDN的节点遍布各ISP，从而保证了网站到任意ISP的访问速度。另外CDN因为其流量分流到各节点的原理，天然获得抵抗网络攻击的能力。 4.CDN使用后，原来的网站是否需要做修改，做什么修改？ 一般而言，网站无需任何修改即可使用CDN获得加速效果。只是对需要判断访客IP程序，才需要做少量修改。 5.为什么我的网站更新后，通过CDN后看到网页还是旧网页，如何解决？　　 由于CDN采用各节点缓存的机制，网站的静态网页和图片修改后，如果CDN缓存没有做相应更新，则看到的还是旧的网页。 为了解决这个问题，CDN管理面板中提供了URL推送服务，来通知CDN各节点刷新自己的缓存。　　 在URL推送地址栏中，输入具体的网址或者图片地址，则各节点中的缓存内容即被统一删除，并且当即生效。　　 如果需要推送的网址和图片太多，可以选择目录推送，输入 http://www.kkk.com/newshttp://www.kkk.com/news 即可以对网站下news目录下所有网页和图片进行了刷新。 6.能不能让CDN不缓存某些即时性要求很高的网页和图片？ 只需要使用动态页面，asp，php，jsp等动态技术做成的页面不被CDN缓存，无需每次都要刷新。或者采用一个网站两个域名，一个启用CDN，另外一个域名不用CDN，对即时性要求高的页面和图片放在不用CDN的域名下。 7.网站新增了不少网页和图片，这些需要使用URL推送吗？　　 后来增加的网页和图片，不需要使用URL推送，因为它们本来就不存在缓存中。 8.网站用CDN后，有些地区反映无法访问了，怎么办？ CDN启用后，访客不能访问网站有很多种可能，可能是CDN的问题，也可能是源站点出现故障或者源站点被关闭，还可能是访客自己所在的网络出现问题，甚至我们实际故障排除中，还出现过客户自己计算机中毒，导致无法访问网站。　　 客户报告故障时，可随时联系我们24小时技术部进行处理。 "},"Chapter11/spring.html":{"url":"Chapter11/spring.html","title":"Part XI SPRING篇","keywords":"","body":"第十一章 SPRING篇 springIOC概述 IoC容器解析 ApplicationContext体系结构分析 BeanFactory体系结构分析 容器的始祖 DefaultListableBeanFactory IOC原理 BeanDefinition的Resource定位 BeanDefinition的载入和解析和注册 Bean对象的创建 Bean依赖注入 ApplicationContext容器refresh过程 BeanFactory和FactoryBean区别 BeanFactoryPostProcessor BeanPostProcessor AutowireCandidateResolver深度分析，解析@Lazy、@Qualifier注解的原理 Spring依赖注入@Autowired深层原理、源码级分析 Spring的循环依赖 AnnotationConfigApplicationContext(注解方式)初始化 ConfigurationClassPostProcessor SpringBoot中如何启动Tomcat流程 SpringBoot的启动流程 AutoConfigurationImportSelector FatJar的启动原理 CGLIB 动态代理 AOP的实现 AOP的原理 注解式AOP原理 spring事务 Spring事务传播 SpringMVC "},"Chapter11/sprimgIOC.html":{"url":"Chapter11/sprimgIOC.html","title":"springIOC概述","keywords":"","body":"春天来了----水 春天来了，冰雪消融，加工厂（BeanFactory）带着设计图来了， 加工厂专门生产各种水（Bean）。一脸懵逼？ 这里说的就是SpringIOC。IOC做了Spring的一大基石，这里比喻成春天的水也不为过吧。 你会不会想你把IOC比喻成水，那么AOP你比喻成啥东东啊？这里其实早就想好了(春天来了----风)[]，是不是很贴切？无孔不入、见缝插针。 概念 这里先确认几个概念 IOC：IInversion of Control，即“控制反转”，不是什么技术，而是一种设计思想。这里也不是Java或者Spring独有的，其它语言也有。 DI： Dependency Injection，即“依赖注入”，组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。 Spring中的DI其实是IOC具体实现方式，及通过依赖注入完成了控制反转。而且spring官方文档上也提出要把IOC的表述改了DI。 创建流程 先大致说着spring创建Bean的整体流程下面在细讲: 确认这个地方是不是春天真的来了(准备环境)-> 准备工厂图纸建设空工厂 -> 找水源（寻找class对象） -> 取水工具取水 -> 填充工厂 -> 调试工程（准备工厂环境） -> 专家提出修改意见（制定工厂后置处理器） -> 执行修改意见（执行工厂后置处理器）-> 定义产品包装方案 （定义Bean的后置处理器） -> 国际化扩张（国际化） -> 准备新闻发布会（定义事件） -> 通电运行（启动web容器） -> 邀请参会人员见证各个里程碑事件（定义监听器） -> 生产产品（实例化对象） -> 完成工厂建设完成并对外宣布可正式提供产品 确认这个地方是不是春天真的来了(准备环境)-> 准备工厂图纸建设空工厂 -> 找水源（寻找class对象） -> 取水工具取水 -> 填充工厂 -> 调试工程（准备工厂环境） -> 专家提出修改意见（制定工厂后置处理器） -> 执行修改意见（执行工厂后置处理器）-> 定义产品包装方案 （定义Bean的后置处理器） -> 国际化扩张（国际化） -> 准备新闻发布会（定义事件） -> 通电运行（启动web容器） -> 邀请参会人员见证各个里程碑事件（定义监听器） -> 生产产品（实例化对象） -> 代理商加持（AOP） -> 完成工厂建设完成并对外宣布可正式提供产品 为什么是两个线呢？其实你会发现第二条也就比第一条多了个代理商(AOP)。 因为AOP不是默认开启，需要手动开启。而且开启AOP后最终的对象也不是原来的的对象，而是代理对象。 这个很好理解，比如有些代理卖的不是原厂货而是莆田货。这里有个段子，警察问售价的人你怎么区分那个是真货那个是假货啊？用几天就坏的是真货。能用很长时间的是假货。 关于事件的定义和发布已经监听并不涉及到bean的流程创建所以上面可以精简为： 准备环境-> 建设空工厂 -> 寻找class对象 -> 获取对象并填充工厂 -> 准备工厂环境 -> 制定工厂后置处理器 -> 执行工厂后置处理器-> 定义Bean的后置处理器 -> 实例化对象 -> |AOP| -> 完成 这里不罗嗦 一切的起因来自于AbstractApplicationContext.refresh(), 我们就从这里说起。 准备环境 prepareRefresh(); 准备环境：获取环境变量、工程类型等 创建工厂并且完成找水和装水obtainFreshBeanFactory() 水源 用例 找水工具 别名 雨水(xml里定义的) 古老的业务Bean 现在基本很少使用 XmlBeanDefinitionReader 雪水 (@component注解修饰的) 包含@Service、@Controller、Repository 的业务Bean 海水 (@Bean修饰的) 被@Configuration修饰的配置类 海的盐 泉水 (ImportBeanDefinitionRegistrar注册) mybatis、aop、fegin等配置 通过AnnotationAttributes直接扫描包 冰泉 溪水 (实现FactoryBean方法) 河水 (properties 文件里) PropertiesBeanDefinitionReader 有点甜 湖水 (json 文件里) JsonBeanDefinitionReader 准备工厂图纸：创建一个空的工厂 new DefaultListableBeanFactory(getInternalParentBeanFactory()); 找水源：通BeanDefinitionReader（这里是抽象具体需要各自的实现类）（解析xml、扫描class文件等）获取class对象 取水： 或者是说装水是通过BeanDefinitionRegistry把class对象信息的包装对象BeanDefinition注册到DefaultListableBeanFactory里 找水和取水的代码在 AbstractBeanDefinitionReader的loadBeanDefinitions方法里 else { // Can only load single resources by absolute URL. // 获取资源 及 找水 Resource resource = resourceLoader.getResource(location); // 加载资源 及 装水 int count = loadBeanDefinitions(resource); if (actualResources != null) { actualResources.add(resource); } if (logger.isTraceEnabled()) { logger.trace(\"Loaded \" + count + \" bean definitions from location [\" + location + \"]\"); } return count; } 至此我们的水工厂()就已经彻底建设完成,而且里面的各个生产线也已经建设完成（能产生什么Bean都已经定义）。 当然如果你不满意水源的名字你个给这个生产线起个别名; 比如 “泉水”改为“冰泉”是不是就高大上了。 这里说一个小技巧：如果方法名是以do开头的，都是真正干活的关键方法。一定要看。比如 “doLoadBeanDefinitions”，“doRegisterBeanDefinitions”，“doGetBean” # BeanDefinition 都有什么信息 写几个主要信息 public interface BeanDefinition extends AttributeAccessor, BeanMetadataElement { // 我们可以看到，默认只提供 sington 和 prototype 两种， // 很多读者可能知道还有 request, session, globalSession, application, websocket 这几种， // 不过，它们属于基于 web 的扩展。 String SCOPE_SINGLETON = ConfigurableBeanFactory.SCOPE_SINGLETON; String SCOPE_PROTOTYPE = ConfigurableBeanFactory.SCOPE_PROTOTYPE; // 设置父 Bean，这里涉及到 bean 继承，不是 java 继承。请参见附录的详细介绍 // 一句话就是：继承父 Bean 的配置信息而已 void setParentName(String parentName); // 获取父 Bean String getParentName(); // 设置 Bean 的类名称，将来是要通过反射来生成实例的 void setBeanClassName(String beanClassName); // 获取 Bean 的类名称 String getBeanClassName(); // 设置 bean 的 scope void setScope(String scope); String getScope(); // 设置是否懒加载 void setLazyInit(boolean lazyInit); boolean isLazyInit(); // 设置该 Bean 依赖的所有的 Bean，注意，这里的依赖不是指属性依赖(如 @Autowire 标记的)， // 是 depends-on=\"\" 属性设置的值。 void setDependsOn(String... dependsOn); // 返回该 Bean 的所有依赖 String[] getDependsOn(); // 设置该 Bean 是否可以注入到其他 Bean 中，只对根据类型注入有效， // 如果根据名称注入，即使这边设置了 false，也是可以的 void setAutowireCandidate(boolean autowireCandidate); // 该 Bean 是否可以注入到其他 Bean 中 boolean isAutowireCandidate(); // 主要的。同一接口的多个实现，如果不指定名字的话，Spring 会优先选择设置 primary 为 true 的 bean void setPrimary(boolean primary); // 是否是 primary 的 boolean isPrimary(); // 如果该 Bean 采用工厂方法生成，指定工厂名称。对工厂不熟悉的读者，请参加附录 // 一句话就是：有些实例不是用反射生成的，而是用工厂模式生成的 void setFactoryBeanName(String factoryBeanName); // 获取工厂名称 String getFactoryBeanName(); // 指定工厂类中的 工厂方法名称 void setFactoryMethodName(String factoryMethodName); // 获取工厂类中的 工厂方法名称 String getFactoryMethodName(); // 构造器参数 ConstructorArgumentValues getConstructorArgumentValues(); // Bean 中的属性值，后面给 bean 注入属性值的时候会说到 MutablePropertyValues getPropertyValues(); // 是否 singleton boolean isSingleton(); // 是否 prototype boolean isPrototype(); // 如果这个 Bean 是被设置为 abstract，那么不能实例化， // 常用于作为 父bean 用于继承，其实也很少用...... boolean isAbstract(); 调试工程 prepareBeanFactory 准别工厂的基础环境可以理解为通电、通气、通交通（水工厂就不写通水了） 提出修改意见 postProcessBeanFactory 这个是个扩展方法，因为spring是面向扩展的。所以这个可以留给子类实现。 Spring之所以强大，为世人所推崇，除了它功能上为大家提供了便利外，还有一方面是它的完美架构，开放式的架构让使用它的程序员很容易根据业务需要扩展已经存在的功能。 这种开放式 的设计在Spring中随处可见，例如在本例中就提供了一个空的函数实现postProcessBeanFactory来 方便程序猿在业务上做进一步扩展 使用方式 比如我有俩个个类 @Component public class UserService { // 普通user类 } @Component public class UserExtService { //扩展User类 } 如果我创建一个类进行如下操作 @Component public class UserServiceBeanFactoryPostProcessor implements BeanFactoryPostProcessor { @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { BeanDefinition userServiceBeanDefinition = beanFactory.getBeanDefinition(\"userService\"); userServiceBeanDefinition.setBeanClassName(\"userExtService\"); } } 那么如下代码中context.getBean(\"userService\");或获取什么实例 @SpringBootApplication public class Application { public static void main(String[] args) { ConfigurableApplicationContext context= new SpringApplicationBuilder(Application.class) .run(args); context.getBean(\"userService\"); } } 答案是userExtService。 这就是 BeanFactoryPostProcessor的作用。我们可以从加载完成的beanFactory中获取并修改已有的BeanDefinition信息 那么就有人会提出。既然这里我们可以操作beanFactory中BeanDefinition信息，是不是就可以在这里进行新的BeanDefinition信息的注册（载入）呢？ 比如现在有个排污水是不是可以在这里偷偷放到beanFactory中 水源 用例 找水工具 别名 排污水 答案是不能 因为BeanFactoryPostProcessor作为beanFactory后置处理器。它的执行时机是在beanFactory已经组装完成。里面有什么都已经确定了。所以不能添加但是可以修改和删除。 顺便提一下BeanDefinitionRegistryPostProcessor这个类 前面已经总结出BeanFactoryPostProcessor接口是Spring初始化BeanFactory时对外暴露的扩展点，SpringIoC容器允许BeanFactoryPostProcessor在容器实例化任何bean之前读取bean的定义，并可以修改它。 BeanDefinitionRegistryPostProcessor继承自 BeanFactoryPostProcessor，比BeanFactoryPostProcessor具有更高的优先级，主要用来在常规的BeanFactoryPostProcessor检测开始之前注册其他bean定义。 特别是，你可以通过BeanDefinitionRegistryPostProcessor来注册一些常规的BeanFactoryPostProcessor，因为此时所有常规的BeanFactoryPostProcessor都还没开始被处理。 注：这边的 “常规 BeanFactoryPostProcessor” 主要用来跟BeanDefinitionRegistryPostProcessor区分。 执行修改意见 invokeBeanFactoryPostProcessors 有一个疑问 上面都已经注册了问什么不直接执行要单独拿出来执行呢？ 其实你想一下，现实中有很多专家，每个专家提的意见可能都不一样。 甚至可以不着专家评审提意见。这是不是就是上面说的面向扩展。 看看invokeBeanFactoryPostProcessors的注释和postProcessBeanFactory的注释 postProcessBeanFactory： 标准初始化后，修改应用程序上下文的内部bean工厂。所有bean定义都将被加载，但是尚未实例化任何bean。这允许在某些ApplicationContext实现中注册特殊的BeanPostProcessor等。 invokeBeanFactoryPostProcessors： 实例化并调用所有注册BeanFactoryPostProcessor的Bean，并遵循显式顺序（如果给定的话）。必须在单例实例化之前调用 我们在obtainFreshBeanFactory介绍中看到的的词是 “加载”、“装载”、“beanFactory”。这里出现了新词 “ApplicationContext” 和 “注册” 以及 “实例化”。 先说说beanFactory和ApplicationContext的区别吧 beanFactory：是Spring里面最低层、最核心的接口，提供了最简单的容器的功能，只提供了实例化对象和拿对象的功能；BeanFactory在启动的时候不会去实例化Bean，只有从容器中拿Bean的时候才会去实例化； ApplicationContext：应用上下文，继承BeanFactory接口、建立在BeanFactory基础之上，它是Spring更高级的容器，提供了更多的有用的功能； ApplicationContext在启动的时候就把所有的Bean全部实例化了。它还可以为Bean配置lazy-init=true来让Bean延迟实例化； 这里举一个例子就是spring整合mybatis的配置类 MapperScannerConfigurer 定时产品包装 registerBeanPostProcessors 注册产品后置处理器，这是只是注册并没有真正执行。 国际化 initMessageSource 就是之前的in18 准备新闻发布会 initApplicationEventMulticaster 事件发布不涉及到对象的管理 onRefresh 对外预留扩展。web容器就是在这里初始化的。 邀请参会人员见证 registerListeners 事件监听 生产产品 finishBeanFactoryInitialization 在这里真正完成对象的实例化. 有一个关键点 AbstractAutowireCapableBeanFactory.resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) 应用实例化之前的后处理器，以解决指定bean是否存在实例化之前的快捷方式。 @Nullable protected Object resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) { Object bean = null; if (!Boolean.FALSE.equals(mbd.beforeInstantiationResolved)) { // Make sure bean class is actually resolved at this point. if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) { Class targetType = determineTargetType(beanName, mbd); if (targetType != null) { bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName); if (bean != null) { bean = applyBeanPostProcessorsAfterInitialization(bean, beanName); } } } mbd.beforeInstantiationResolved = (bean != null); } return bean; } applyBeanPostProcessorsBeforeInstantiation: 将InstantiationAwareBeanPostProcessors应用于指定的bean定义（按类和名称），调用其postProcessBeforeInstantiation方法。任何返回的对象都将用作bean，而不是实际实例化目标bean。 后处理器返回的空值将导致目标Bean被实例化。 applyBeanPostProcessorsAfterInitialization: 从接口AutowireCapableBeanFactory复制的描述,将BeanPostProcessors应用于给定的现有bean实例，调用其postProcessAfterInitialization方法。 返回的Bean实例可能是原始实例的包装。 如果上面创建的对象是空的话就进入了AbstractAutowireCapableBeanFactory.doCreateBean,之前提过这是一个真正干活的方法。 继续深入进入initializeBean方法 protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) { if (System.getSecurityManager() != null) { AccessController.doPrivileged((PrivilegedAction) () -> { invokeAwareMethods(beanName, bean); return null; }, getAccessControlContext()); } else { invokeAwareMethods(beanName, bean); } Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { // 好熟悉啊 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { //关键的实现 invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, \"Invocation of init method failed\", ex); } if (mbd == null || !mbd.isSynthetic()) { // 又好熟悉啊 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } 深入到invokeCustomInitMethod方法 if (System.getSecurityManager() != null) { AccessController.doPrivileged((PrivilegedAction) () -> { //反射 ReflectionUtils.makeAccessible(methodToInvoke); return null; }); try { AccessController.doPrivileged((PrivilegedExceptionAction) () -> methodToInvoke.invoke(bean), getAccessControlContext()); } catch (PrivilegedActionException pae) { InvocationTargetException ex = (InvocationTargetException) pae.getException(); throw ex.getTargetException(); } } else { try { //反射 ReflectionUtils.makeAccessible(methodToInvoke); methodToInvoke.invoke(bean); } catch (InvocationTargetException ex) { throw ex.getTargetException(); } } 对象就是通过ReflectionUtils反射进行完成的 完成工厂建设完成并对外宣布可正式提供产品 finishRefresh 完成此上下文的刷新，调用LifecycleProcessor的onRefresh（）方法并发布。 总结 这里只是大概说了下bean的加载顺序，先理清思路。后面进行代码的逐行解读。 "},"Chapter11/IOC.html":{"url":"Chapter11/IOC.html","title":"IoC容器解析","keywords":"","body":"IoC容器解析 本节代码基于springboot 2.3.0 Spring IoC容器 org.springframework.context.ApplicationContext接口代表Spring IoC容器，主要负责bean的实例化、配置、装配，简而言之，Spring IoC容器是管理这些bean的（这里所说的bean指的是组成你的应用程序中的对象，并且这些对象被Spring所管理）。 容器如何知道哪些对象要进行实例化、配置和装配的呢？是通过读取配置文件元数据来达到这个效果的，配置文件元数据是用xml配置、Java注解和Java代码配置来表示的。 这使得作为程序员的我们，只需要向Spring容器提供配置元数据，Spring容器就能在我们的应用中实例化、配置和装配这些对象。org.springframework.beans和org.springframework.context包是Spring IoC容器的基础。 Spring提供了很多Application接口的实现。在单独的应用中，创建ClassPathXmlApplicationContext和FileSystemXmlApplicationContext的实例是非常常用的做法。示例如下： public static void main(String[] args) { SpringApplicationBuilder builder = new SpringApplicationBuilder() .listeners(new StartedListener()) .sources(Application.class) .bannerMode(Banner.Mode.OFF); builder.build().setWebApplicationType(WebApplicationType.NONE); ConfigurableApplicationContext context = builder.run(args); Beginning beginning = context.getBean(Beginning.class); beginning.start(); } 然而在大部分的应用场景中，不需要实例化一个或者多个Spring IoC容器的实例。 例如在web应用的场景下，只需要在web.xml中创建七行样板配置的代码如下： contextConfigLocation /WEB-INF/applicationContext.xml org.springframework.web.context.ContextLoaderListener 当然springBoot只需要进行如下设置 builder.build().setWebApplicationType(WebApplicationType.SERVLET); 下面这张图从更高的视角展示了Spring是怎样工作的。你的应用程序中的类是和配置元数据组合在一起，以便在ApplicationContext创建和初始化之后，你拥有了一个完全配置的、可执行的系统。 ApplicationContext设计解析 为了方便对ApplicationContext接口的层次结构有一个大概的认识，下面使用IDEA来生成ApplicationContext的继承关系图。 （选中ApplicationContext接口->右键->Diagrams->Show Diagrams...） 从上图就能很清楚的看出ApplicationContext继承的接口分为五类： BeanFactory：提供了能够管理任何对象的高级配置机制，这个接口是Spring框架中比较重要的一个接口。 ListableBeanFactory：从该接口的名字就能知道，该接口除了拥有BeanFactory的功能外，该接口还有能列出factory中所有bean的实例的能力。 HierarchicalBeanFactory：该接口除了拥有BeanFactory的功能外，还提供了BeanFactory分层的机制，查找bean的时候，除了在自身BeanFactory查找外，如果没有查找到，还会在父级BeanFactory进行查找。 MessageSource：消息资源的处理，用于国际化。 ApplicationEventPublisher：用于处理事件发布机制。 EnvironmentCapable：提供了Environment的访问能力。 ResourceLoader：用于加载资源的策略接口（例如类路径下的资源、系统文件下的资源等等）。 ResourcePatternResolver：用于将位置模式（例如Ant风格的路径模式）解析成资源对象的策略接口。classpath*:前缀能匹配所以类路径下的资源。 先看一下在ApplicationContext中定义的方法： @Nullable String getId(); // 获取ApplicationContext的唯一id String getApplicationName(); // 该上下文所属的已经部署了的应用的名字，默认为\"\" String getDisplayName(); // 友好的展示名字 long getStartupDate(); //该上下文第一次加载的时间 @Nullable ApplicationContext getParent(); //父级ApplicationContext /** * Expose AutowireCapableBeanFactory functionality for this context. * This is not typically used by application code, except for the purpose of * initializing bean instances that live outside of the application context, * applying the Spring bean lifecycle (fully or partly) to them. * Alternatively, the internal BeanFactory exposed by the * {@link ConfigurableApplicationContext} interface offers access to the * {@link AutowireCapableBeanFactory} interface too. The present method mainly * serves as a convenient, specific facility on the ApplicationContext interface. * NOTE: As of 4.2, this method will consistently throw IllegalStateException * after the application context has been closed. In current Spring Framework * versions, only refreshable application contexts behave that way; as of 4.2, * all application context implementations will be required to comply. * @return the AutowireCapableBeanFactory for this context * @throws IllegalStateException if the context does not support the * {@link AutowireCapableBeanFactory} interface, or does not hold an * autowire-capable bean factory yet (e.g. if {@code refresh()} has * never been called), or if the context has been closed already * @see ConfigurableApplicationContext#refresh() * @see ConfigurableApplicationContext#getBeanFactory() */ AutowireCapableBeanFactory getAutowireCapableBeanFactory() throws IllegalStateException; 前四个方法用于获取该ApplicationContext的一些基本信息，getAutowireCapableBeanFactory()用于暴露AutowireCapableBeanFactory的功能， 这通常不是提供给用于代码使用的，除非你想要在应用上下文的外面初始化bean的实例，关于AutowireCapableBeanFactory后面会有更加详细的解析。 BeanFactory BeanFactory是Spring框架中比较重要的一个接口，下面列出了这个接口中的方法的定义： String FACTORY_BEAN_PREFIX = \"&\"; // 获取bean Object getBean(String var1) throws BeansException; T getBean(String var1, Class var2) throws BeansException; Object getBean(String var1, Object... var2) throws BeansException; T getBean(Class var1) throws BeansException; T getBean(Class var1, Object... var2) throws BeansException; // 获取bean的提供者（对象工厂） ObjectProvider getBeanProvider(Class var1); ObjectProvider getBeanProvider(ResolvableType var1); boolean containsBean(String var1); // 是否包含指定名字的bean boolean isSingleton(String var1) throws NoSuchBeanDefinitionException; // 是否为单例 boolean isPrototype(String var1) throws NoSuchBeanDefinitionException; // 是否为原型 // 指定名字的bean是否和指定的类型匹配 boolean isTypeMatch(String var1, ResolvableType var2) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String var1, Class var2) throws NoSuchBeanDefinitionException; @Nullable Class getType(String var1) throws NoSuchBeanDefinitionException; // 获取指定名字的bean的类型 @Nullable Class getType(String var1, boolean var2) throws NoSuchBeanDefinitionException; String[] getAliases(String var1); // 获取指定名字的bean的所有别名 } 这些方法大致可以分为三类： getBean()方法用于获取匹配的bean的实例对象（有可能是Singleton或者Prototype的），如果没有找到匹配的bean则抛出BeansException子类的异常。 如果在当前的工厂实例中没有找到匹配的bean，会在父级的工厂中进行查找。带有args参数的getBean()方法，允许显式的去指定构造器或者工厂方法的参数，会覆盖了在bean的定义中定义的参数，这仅仅在创建一个新的实例的时候才起作用，而在获取一个已经存在的实例是不起作用的。 getBeanProvider()方法用于获取指定bean的提供者，可以看到它返回的是一个ObjectProvider，其父级接口是ObjectFactory。首先来看一下ObjectFactory，它是一个对象的实例工厂，只有一个方法： T getObject() throws BeansException; 调用这个方法返回的是一个对象的实例。此接口通常用于封装一个泛型工厂，在每次调用的时候返回一些目标对象新的实例。ObjectFactory和FactoryBean是类似的，只不过FactoryBean通常被定义为BeanFactory中的服务提供者（SPI）实例，而ObjectFactory通常是以API的形式提供给其他的bean。简单的来说，ObjectFactory一般是提供给开发者使用的，FactoryBean一般是提供给BeanFactory使用的。 ObjectProvider继承ObjectFactory，特为注入点而设计，允许可选择性的编程和宽泛的非唯一性的处理。在Spring 5.1的时候，该接口从Iterable扩展，提供了对Stream的支持。 该接口的方法如下： // 获取对象的实例，允许根据显式的指定构造器的参数去构造对象 T getObject(Object... var1) throws BeansException; // 获取对象的实例，如果不可用，则返回null @Nullable T getIfAvailable() throws BeansException; default T getIfAvailable(Supplier defaultSupplier) throws BeansException {} default void ifAvailable(Consumer dependencyConsumer) throws BeansException {} // 获取对象的实例，如果不是唯一的或者没有首先的bean，则返回null @Nullable T getIfUnique() throws BeansException; default T getIfUnique(Supplier defaultSupplier) throws BeansException {} default void ifUnique(Consumer dependencyConsumer) throws BeansException {} // 获取多个对象的实例 default Iterator iterator() {} default Stream stream() {} default Stream orderedStream() {} 这些接口是分为两类 一类是获取单个对象，getIfAvailable()方法用于获取可用的bean（没有则返回null），getIfUnique()方法用于获取唯一的bean（如果bean不是唯一的或者没有首选的bean返回null）。getIfAvailable(Supplier defaultSupplier)和getIfUnique(Supplier defaultSupplier)，如果没有获取到bean，则返回defaultSupplier提供的默认值，ifAvailable(Consumer dependencyConsumer)和ifUnique(Consumer dependencyConsumer)提供了以函数式编程的方式去消费获取到的bean。 另一类是获取多个对象，stream()方法返回连续的Stream，不保证bean的顺序（通常是bean的注册顺序）。orderedStream()方法返回连续的Stream，预先会根据工厂的公共排序比较器进行排序，一般是根据org.springframework.core.Ordered的约定进行排序。 其他的是一些工具性的方法： 通过名字判断是否包含指定bean的定义的containsBean(String name)方法 判断是单例和原型的isSingleton(String name)和isPrototype(String name)方法 判断给定bean的名字是否和类型匹配的isTypeMatch方法 根据bean的名字来获取其类型的getType(String name)方法 根据bean的名字来获取其别名的getAliases(String name)方法 或许你已经注意到了，有两个方法含有类型是ResolvableType的参数，那么ResolvableType是什么呢？假如说你要获取泛型类型的bean：MyBean，根据Class来获取，肯定是满足不了要求的，泛型在编译时会被擦除。使用ResolvableType就能满足此需求，代码如下： ResolvableType type = ResolvableType.forClassWithGenerics(MyType.class, TheType.class); ObjectProvider> op = applicationContext.getBeanProvider(type); MyType bean = op.getIfAvailable() 简单的来说，ResolvableType是对Java java.lang.reflect.Type的封装，并且提供了一些访问该类型的其他信息的方法（例如父类， 泛型参数，该类）。从成员变量、方法参数、方法返回类型、类来构建ResolvableType的实例。 ListableBeanFactory ListableBeanFactory接口有能列出工厂中所有的bean的能力，下面给出该接口中的所有方法： boolean containsBeanDefinition(String beanName); // 是否包含给定名字的bean的定义 int getBeanDefinitionCount(); // 工厂中bean的定义的数量 String[] getBeanDefinitionNames(); // 工厂中所有定义了的bean的名字 // 获取指定类型的bean的名字 String[] getBeanNamesForType(ResolvableType type); String[] getBeanNamesForType(ResolvableType type, boolean includeNonSingletons, boolean allowEagerInit); String[] getBeanNamesForType(@Nullable Class type); String[] getBeanNamesForType(@Nullable Class type, boolean includeNonSingletons, boolean allowEagerInit); // 获取所有使用提供的注解进行标注的bean的名字 String[] getBeanNamesForAnnotation(Class annotationType); // 查找指定bean中的所有指定的注解（会考虑接口和父类中的注解） @Nullable A findAnnotationOnBean(String beanName, Class annotationType) throws NoSuchBeanDefinitionException; // 根据指定的类型来获取所有的bean Map getBeansOfType(@Nullable Class type) throws BeansException; Map getBeansOfType(@Nullable Class type, boolean includeNonSingletons, boolean allowEagerInit) throws BeansException; // 获取所有使用提供的注解进行标注了的bean Map getBeansWithAnnotation(Class annotationType) throws BeansException; 上面的这些方法都不考虑祖先工厂中的bean，只会考虑在当前工厂中定义的bean。 前九个方法用于获取bean的一些信息 最后的三个方法用于获取所有满足条件的bean，返回结果Map中的键为bean的名字，值为bean的实例。这些方法都会考虑通过FactoryBean创建的bean，这也意味着FactoryBean会被初始化。为什么这里的三个方法不返回List？Map不光包含这些bean的实例，而且还包含bean的名字，而List只包含bean的实例。也就是说Map比List更加的通用。 HierarchicalBeanFactory HierarchicalBeanFactory接口定义了BeanFactory之间的分层结构，ConfigurableBeanFactory中的setParentBeanFactory方法能设置父级的BeanFactory，下面列出了HierarchicalBeanFactory中定义的方法： // 获取父级的BeanFactory @Nullable BeanFactory getParentBeanFactory(); // 本地的工厂是否包含指定名字的bean boolean containsLocalBean(String name); 这两个方法都比较直接明了，getParentBeanFactory方法用于获取父级BeanFactory。containsLocalBean 用于判断本地的工厂是否包含指定的bean，忽略在祖先工厂中定义的bean。 MessageSource MessageSource主要用于消息的国际化，下面是该接口中的方法定义： // 获取消息 @Nullable String getMessage(String code, @Nullable Object[] args, @Nullable String defaultMessage, Locale locale); String getMessage(String code, @Nullable Object[] args, Locale locale) throws NoSuchMessageException; String getMessage(MessageSourceResolvable resolvable, Locale locale) throws NoSuchMessageException; 以上的三个方法都是用于获取消息的，第一个方法提供了默认消息，第二个接口如果没有获取到指定的消息会抛出异常。第三个接口中的MessageSourceResolvable参数是对代码、参数值、默认值的一个封装。 ApplicationEventPublisher ApplicationEventPublisher接口封装了事件发布功能，提供Spring中事件的机制。接口中的方法定义如下： // 发布事件 default void publishEvent(ApplicationEvent event) { publishEvent((Object) event); } void publishEvent(Object event); 第一个方法用于发布特定于应用程序事件。第二个方法能发布任意的事件，如果事件不是ApplicationEvent，那么会被包裹成PayloadApplicationEvent事件。 EnvironmentCapable EnvironmentCapable提供了访问Environment的能力，该接口只有一个方法： Environment getEnvironment(); Environment表示当前正在运行的应用的环境变量，它分为两个部分：profiles和properties。它的父级接口PropertyResolver提供了property的访问能力。 ResourceLoader和ResourcePatternResolver 先来看一下ResourceLoader，该接口是用来加载资源（例如类路径或者文件系统中的资源）的策略接口。该接口中的方法如下： /** Pseudo URL prefix for loading from the class path: \"classpath:\". */ String CLASSPATH_URL_PREFIX = ResourceUtils.CLASSPATH_URL_PREFIX; Resource getResource(String location); // 根据指定的位置获取资源 @Nullable ClassLoader getClassLoader(); // 获取该资源加载器所使用的类加载器 该接口只有简单明了的两个方法，一个是用来获取指定位置的资源，一个用于获取资源加载器所使用的类加载器。Resource是从实际类型的底层资源（例如文件、类路径资源）进行抽象的资源描述符。 先看下Resource中的方法： boolean exists(); // 资源实际上是否存在 boolean isReadable(); // 资源是否可读 boolean isOpen(); // 检查资源是否为打开的流 boolean isFile(); // 资源是否为文件系统上的一个文件 URL getURL() throws IOException; // 获取url URI getURI() throws IOException; // 获取URI File getFile() throws IOException; // 获取文件 // 获取ReadableByteChannel default ReadableByteChannel readableChannel() throws IOException { return Channels.newChannel(getInputStream()); } long contentLength() throws IOException; // 资源的内容的长度 long lastModified() throws IOException; // 资源的最后修改时间 // 相对于当前的资源创建一个新的资源 Resource createRelative(String relativePath) throws IOException; @Nullable String getFilename(); // 获取资源的文件名 String getDescription(); // 获取资源的描述信息 接下来在来看一下ResourcePatternResolver，该接口用于解析一个位置模式（例如Ant风格的路径模式），该接口只有一个方法，如下： String CLASSPATH_ALL_URL_PREFIX = \"classpath*:\"; // 将给定的位置模式解析成资源对象 Resource[] getResources(String locationPattern) throws IOException; Spring IoC容器设计复盘 假如让你设计IoC容器，你该如何去做呢？首先你应该要明确你设计的容器的功能和特性，然后根据这些功能和特性设计出合理的接口。下面只是粗略的分析一下： IoC容器对bean的配置和管理，那么是不是需要设计一个接口来完成这些功能呢？（BeanFactory） 既然需要这些元数据的配置，那么是不是需要设计一个接口来完成对一些配置文件的读取。（ResourceLoader和Resource） 在IoC容器初始化、摧毁的时候，是不是可能要执行一些操作呢？那么是不是需要使用事件机制来完成呢？（ApplicationEventPublisher） "},"Chapter11/ApplicationContext.html":{"url":"Chapter11/ApplicationContext.html","title":"ApplicationContext体系结构分析","keywords":"","body":"ApplicationContext体系结构分析 上篇已经对IoC容器的设计进行了分析，本篇将对ApplicationContext经典的继承层次图进行详细的分析 本节代码基于springboot 2.3.0 继承层次图概览 使用IDEA的继承层次工具生成如下的图（选中ApplicationContext --> Ctrl+H）： 从上图能很清楚的看出，ApplicationContext的子接口分为四个部分： ConfigurableApplicationContext：大部分的应用上下文都实现了该接口 ReactiveWebApplicationContext: 在reactive方案容器的运行web的应用程序中使用 WebServerApplicationContext：在servlet方案容器运行web的应用程序中使用 ApplicationContextAssertProvider: 它另外支持AssertJ样式声明。可用于装饰现有的应用程序上下文或启动失败的应用程序上下文 ConfigurableApplicationContext分析 从上面的类的继承层次图能看到，ConfigurableApplicationContext是比较上层的一个接口，该接口也是比较重要的一个接口，几乎所有的应用上下文都实现了该接口。 该接口在ApplicationContext的基础上提供了配置应用上下文的能力，此外提供了生命周期的控制能力。先看一下该接口的继承关系图（为了更加简洁，去掉了ApplicationContext继承的接口）： Closeable接口用于关闭应用上下文，释放所有的资源和锁，这也包括摧毁所有缓存的单例的bean，常见的try-with-resources用法如下，执行完try体中的代码后会自动的调用close方法： try (ConfigurableApplicationContext cac = ...) { // 编写代码 ... } Lifecycle定义了启动/停止生命周期的控制的一些方法，其中的方法如下： void start(); // 启动组件 void stop(); // 停止组件 boolean isRunning(); // 组件是否正在运行 接下来看一下ConfigurableApplicationContext中的方法： String CONFIG_LOCATION_DELIMITERS = \",; \\t\\n\"; String CONVERSION_SERVICE_BEAN_NAME = \"conversionService\"; String LOAD_TIME_WEAVER_BEAN_NAME = \"loadTimeWeaver\"; String ENVIRONMENT_BEAN_NAME = \"environment\"; String SYSTEM_PROPERTIES_BEAN_NAME = \"systemProperties\"; String SYSTEM_ENVIRONMENT_BEAN_NAME = \"systemEnvironment\"; String SHUTDOWN_HOOK_THREAD_NAME = \"SpringContextShutdownHook\"; // 设置应用上下文唯一的id void setId(String id); // 设置应用程序上下文的父级 void setParent(@Nullable ApplicationContext parent); // 设置应用上下文的环境 void setEnvironment(ConfigurableEnvironment environment); @Override ConfigurableEnvironment getEnvironment(); // 添加一个新的BeanFactoryPostProcessor void addBeanFactoryPostProcessor(BeanFactoryPostProcessor postProcessor); // 添加应用程序监听器 void addApplicationListener(ApplicationListener listener); // 添加协议解析器，可能会覆盖默认的规则 void addProtocolResolver(ProtocolResolver resolver); // 加载或者刷新配置 void refresh() throws BeansException, IllegalStateException; // 向JVM runtime注册一个关闭钩子，JVM关闭时关闭这个上下文 void registerShutdownHook(); @Override void close(); // 应用程序上下文是否是激活状态 boolean isActive(); // 获取应用上下文内部的bean factory ConfigurableListableBeanFactory getBeanFactory() throws IllegalStateException; 上面的这些方法基本上是提供了对某些特性的实现进行支撑的方法。 看了这么多方法，下面看一下ApplicationContext的抽象的实现。 AbstractApplicationContext AbstractApplicationContext是ApplicationContext接口的抽象实现，这个抽象类仅仅是实现了公共的上下文特性。 这个抽象类使用了模板方法设计模式，需要具体的实现类去实现这些抽象的方法。对相关接口的实现如下： ApplicationContext接口的实现 ConfigurableApplicationContext接口的实现 BeanFactory接口的实现 ListableBeanFactory接口的实现 HierarchicalBeanFactory接口的实现 MessageSource接口的实现 ResourcePatternResolver的实现 Lifecycle接口的实现 本文不会详细的讲解这个类中的具体的实现细节，后面会有更加的详细的介绍。下面看下里面的抽象方法： // 刷新BeanFactory，用于执行实际的配置加载，该方法在其他的初始化工作之前被refresh()方法调用 protected abstract void refreshBeanFactory() throws BeansException, IllegalStateException; // 关闭BeanFactory，用于释放内部使用的BeanFactory· protected abstract void closeBeanFactory(); // 获取内部使用的BeanFactory public abstract ConfigurableListableBeanFactory getBeanFactory() throws IllegalStateException; 那么对需要实现的方法经过抽象后，只剩下少量的需要子类去实现的方法。 GenericApplicationContext GenericApplicationContext继承自AbstractApplicationContext，是为通用目的设计的，它能加载各种配置文件，例如xml，properties等等。 它的内部持有一个DefaultListableBeanFactory的实例，实现了BeanDefinitionRegistry接口，以便允许向其应用任何bean的定义的读取器。 为了能够注册bean的定义，refresh()只允许调用一次。常见的使用如下： GenericApplicationContext ctx = new GenericApplicationContext(); XmlBeanDefinitionReader xmlReader = new XmlBeanDefinitionReader(ctx); xmlReader.loadBeanDefinitions(new ClassPathResource(\"applicationContext.xml\")); PropertiesBeanDefinitionReader propReader = new PropertiesBeanDefinitionReader(ctx); propReader.loadBeanDefinitions(new ClassPathResource(\"otherBeans.properties\")); ctx.refresh(); MyBean myBean = (MyBean) ctx.getBean(\"myBean\"); .. 这个类的实现没有太多需要注意的地方，需要注意的有两点： 内部使用的DefaultListableBeanFactory的实例，提供了一些方法来配置该实例，例如是否允许bean定义的覆盖、是否允许bean之间的循环应用等等。 该类实现了BeanDefinitionRegistry，bean的定义注册。以便能通过BeanDefinitionReader读取bean的配置，并注册。BeanDefinitionRegistry接口的实现是直接使用内部的DefaultListableBeanFactory的实例。 GenericApplicationContext有六个子类 GenericXmlApplicationContext：内置了对XML的支持。它非常的方便和灵活，是ClassPathXmlApplicationContext和FileSystemXmlApplicationContext的一种替代品。可以发现，它的内部有一个XmlBeanDefinitionReader的实例，专门用于处理XML的配置。 StaticApplicationContext：主要用于编程式的注入bean和消息，而不是从外部的配置源读取bean的定义。主要是在测试时非常有用。通过阅读源代码可以看到，它的内部有一个StaticMessageSource的实例，使用addMessage方法添加消息。每次在编程式的注入bean时，都会创建一个GenericBeanDefinition的实例。 ResourceAdapterApplicationContext：是为JCA（J2EE Connector Architecture）的ResourceAdapter设计的，主要用于传递BootstrapContext的实例给实现了BootstrapContextAware接口且由spring管理的bean。覆盖了postProcessBeanFactory方法来实现此功能。 GenericGroovyApplicationContext：实现了GroovyObject接口以便能够使用点的语法（.xx）取代getBean方法来获取bean。它主要用于Groovy bean的定义，与GenericXmlApplicationContext一样，它也能解析XML格式定义的bean。内部使用GroovyBeanDefinitionReader来完成groovy脚本和XML的解析。 AnnotationConfigApplicationContext：提供了注解配置（例如：Configuration、Component、inject等）和类路径扫描（scan方法）的支持，可以使用register(Class... annotatedClasses)来注册一个一个的进行注册。实现了AnnotationConfigRegistry接口，来完成对注册配置的支持，只有两个方法：register和scan。内部使用AnnotatedBeanDefinitionReader来完成注解配置的解析，使用ClassPathBeanDefinitionScanner来完成类路径下的bean定义的扫描。 GenericReactiveWebApplicationContext： WebServerApplicationContext 该接口提供了在web应用中的配置，并提供如下接口 //返回由上下文创建的WebServer；如果尚未创建服务器，则返回null WebServer getWebServer(); //返回Web服务器应用程序上下文的名称空间；如果未设置名称空间，则返回null。 当多个Web服务器在同一应用程序中运行时（例如，在不同端口上运行的管理上下文），用于消除歧义 String getServerNamespace(); //如果指定的上下文是具有匹配服务器名称空间的WebServerApplicationContext，则返回true。 static boolean hasServerNamespace(ApplicationContext context, String serverNamespace) { return (context instanceof WebServerApplicationContext) && ObjectUtils .nullSafeEquals(((WebServerApplicationContext) context).getServerNamespace(), serverNamespace); } ConfigurableWebServerApplicationContext ConfigurableWebApplicationContext继承自WebApplicationContext和ConfigurableApplicationContext,并额外添加一个接口 void setServerNamespace(String serverNamespace); ServletWebServerApplicationContext ServletWebServerApplicationContext继承了GenericWebApplicationContext并实现了ConfigurableWebServerApplicationContext ApplicationContext容器的设计原理 以ClassPathXmlApplicationContext为例应用： // 根据配置文件创建spring容器 ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); // 从容器中获取Bean ConferenceServiceImpl conferenceService = (ConferenceServiceImpl)context.getBean(\"conferenceService\"); // 调用Bean方法 conferenceService.conference(); ClassPathXmlApplicationContext源码： package org.springframework.context.support; import org.springframework.beans.BeansException; import org.springframework.context.ApplicationContext; import org.springframework.core.io.ClassPathResource; import org.springframework.core.io.Resource; import org.springframework.util.Assert; public class ClassPathXmlApplicationContext extends AbstractXmlApplicationContext { private Resource[] configResources; public ClassPathXmlApplicationContext() { } public ClassPathXmlApplicationContext(ApplicationContext parent) { super(parent); } public ClassPathXmlApplicationContext(String configLocation) throws BeansException { this(new String[] {configLocation}, true, null); } public ClassPathXmlApplicationContext(String... configLocations) throws BeansException { this(configLocations, true, null); } public ClassPathXmlApplicationContext(String[] configLocations, ApplicationContext parent) throws BeansException { this(configLocations, true, parent); } public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh) throws BeansException { this(configLocations, refresh, null); } public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException { super(parent); setConfigLocations(configLocations); if (refresh) { refresh(); } } public ClassPathXmlApplicationContext(String path, Class clazz) throws BeansException { this(new String[] {path}, clazz); } public ClassPathXmlApplicationContext(String[] paths, Class clazz) throws BeansException { this(paths, clazz, null); } public ClassPathXmlApplicationContext(String[] paths, Class clazz, ApplicationContext parent) throws BeansException { super(parent); Assert.notNull(paths, \"Path array must not be null\"); Assert.notNull(clazz, \"Class argument must not be null\"); this.configResources = new Resource[paths.length]; for (int i = 0; i 构造器传入XML最后会调用到如下构造器： public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException { super(parent); setConfigLocations(configLocations); if (refresh) { refresh(); } } 最后会调用refresh()方法，这个方法就是IOC容器启动的入口，IOC容器里面进行了一序列复杂的操作， 这也是通往IOC容器核心实现原理的入口。 "},"Chapter11/BeanFactory.html":{"url":"Chapter11/BeanFactory.html","title":"BeanFactory体系结构分析","keywords":"","body":"BeanFactory体系结构分析] BeanFactory是Spring中非常重要的一个类，搞懂了它，你就知道了bean的初始化和摧毁过程，对于深入理解IOC有很大的帮助。 本节代码基于springboot 2.3.0 BeanFactory体系结构 首先看一下使用IDEA生成的继承层次图（图中去掉了ApplicationContext的继承图） 可以看到BeanFactory下的接口主要分为四个个： HierarchicalBeanFactory: 该接口除了拥有BeanFactory的功能外，还提供了BeanFactory分层的机制，查找bean的时候，除了在自身BeanFactory查找外，如果没有查找到，还会在父级BeanFactory进行查找。 SimpleJndiBeanFactory: AutowireCapableBeanFactory: 能够自动装配的bean的工厂需要实现此接口 ListableBeanFactory: 从该接口的名字就能知道，该接口除了拥有BeanFactory的功能外，该接口还有能列出factory中所有bean的实例的能力。 关于 BeanFactory、HierarchicalBeanFactory、ListableBeanFactory详解请查IoC容器解析 AutowireCapableBeanFactory 该接口提供了对现有bean进行自动装配的能力，设计目的不是为了用于一般的应用代码中，对于一般的应用代码应该使用BeanFactory和ListableBeanFactory。 其他框架的代码集成可以利用这个接口去装配和填充现有的bean的实例，但是Spring不会控制这些现有bean的生命周期。 你也许注意到了ApplicationContext中的getAutowireCapableBeanFactory()能获取到AutowireCapableBeanFactory的实例。 同样，也能实现BeanFactoryAware接口来接收BeanFactory（应用程序上下暴露的内部使用的BeanFactory）的实例，然后将其转换成AutowireCapableBeanFactory。 下面看一下这个接口中的静态成员变量和方法： // 定义了bean的装配策略 int AUTOWIRE_NO = 0; // 不进行装配 int AUTOWIRE_BY_NAME = 1; // 根据名字进行装配 int AUTOWIRE_BY_TYPE = 2; // 根据类型进行装配 int AUTOWIRE_CONSTRUCTOR = 3; // 根据构造函数进行装配 @Deprecated int AUTOWIRE_AUTODETECT = 4; // Spring3.0已经过时的方法，通过省视bean来决定适当的装载策略 //Spring5.1后增加，原始实例的后缀，例如\"com.mypackage.MyClass.ORIGINAL\"，强制返回给定的实例（没有代理） String ORIGINAL_INSTANCE_SUFFIX = \".ORIGINAL\"; // 完全创建给定类的一个新的实例，包括所有适用的BeanPostProcessor // 填充注解的field和方法，并且会应用所有的初始化回调函数 T createBean(Class beanClass) throws BeansException; // 装配bean，通过应用初始化之后的回调函数和bean属性的后置处理来填充给定的bean的实例 // 本质上是为了在创建新的实例或者反序列化实例时，填充（重新填充）实例中注解的field和方法 void autowireBean(Object existingBean) throws BeansException; // 配置给定的原始bean：自动装配bean的属性，应用bean的属性值、工厂回调函数（例如setBeanName,values） // 同样也会应用所有bean的后置处理器 Object configureBean(Object existingBean, String beanName) throws BeansException; // 使用指定的装配策略来完全创建一个新的bean的实例 Object createBean(Class beanClass, int autowireMode, boolean dependencyCheck) throws BeansException; // 使用指定的装配策略来实例化一个给定类新的bean的实例 // 不会应用标准的BeanPostProcessor回调函数或者在未来执行任何bean的初始化 Object autowire(Class beanClass, int autowireMode, boolean dependencyCheck) throws BeansException; // 根据名字和类型来自动装配给定bean的实例的属性 // 不会应用标准的BeanPostProcessor回调函数或者在未来执行任何bean的初始化 void autowireBeanProperties(Object existingBean, int autowireMode, boolean dependencyCheck) throws BeansException; // 应用给定名字的bean的定义的属性值到给定的bean的实例 // 该方法不会自动装配bean属性，仅仅应用明确定义的属性值 void applyBeanPropertyValues(Object existingBean, String beanName) throws BeansException; // 初始化给定的原始的bean应用bean的属性值、工厂回调函数（例如setBeanName,values） // 同样也会应用所有bean的后置处理器 Object initializeBean(Object existingBean, String beanName) throws BeansException; // 应用BeanPostProcessor到给定的现存的bean的实例，调用postProcessBeforeInitialization方法 Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException; // 应用BeanPostProcessor到给定的现存的bean的实例，postProcessAfterInitialization Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException; // 摧毁给定的bean的实例，应用DisposableBean规约和注册的DestructionAwareBeanPostProcessor void destroyBean(Object existingBean); // 解析唯一匹配给定对象类型的bean的实例，该方法是getBean(Class)的变种，只不过它还提供了匹配实例的bean的名字 NamedBeanHolder resolveNamedBean(Class requiredType) throws BeansException; // 解析给定bean的名字的实例，提供了用于暴露目标的工厂方法的依赖描述符 Object resolveBeanByName(String name, DependencyDescriptor descriptor) throws BeansException; // 针对在工厂中定义的bean来解析指定的依赖 @Nullable Object resolveDependency(DependencyDescriptor descriptor, @Nullable String requestingBeanName) throws BeansException; // 针对在工厂中定义的bean来解析指定的依赖 @Nullable Object resolveDependency(DependencyDescriptor descriptor, @Nullable String requestingBeanName, @Nullable Set autowiredBeanNames, @Nullable TypeConverter typeConverter) throws BeansException; ConfigurableBeanFactory ConfigurableBeanFactory提供了bean工厂的配置机制（除了BeanFactory接口中的bean的工厂的客户端方法）。该BeanFactory接口不适应一般的应用代码中，应该使用BeanFactory和ListableBeanFactory。 该扩展接口仅仅用于内部框架的使用，并且是对bean工厂配置方法的特殊访问。 ConfigurableBeanFactory继承自HierarchicalBeanFactory和SingletonBeanRegistry，下面先看下SingletonBeanRegistry： SingletonBeanRegistry是为了共享的bean的实例而定义的注册器，以统一的方式暴露单例管理机制。下面是在此接口中定义的方法： // 在bean的注册器中以给定的bean的名字将给定的现存对象注册为单例 void registerSingleton(String beanName, Object singletonObject); // 根据给定的bean的名字来获取单例bean，可能为null Object getSingleton(String beanName); // 是否包含给定名字的单例bean boolean containsSingleton(String beanName); // 获取所有在注册器中注册的单例bean的名字 String[] getSingletonNames(); // 获取所有在注册器中注册的单例bean的数量 int getSingletonCount(); // 获取在这个注册器中使用的单例的mutex（用于外部协同） Object getSingletonMutex(); 需要注意的是使用registerSingleton方法注册的单例bean，不会执行任何的初始化回调函数（尤其不会调用InitializingBean的afterPropertiesSet方法），同样也不会接收任何的摧毁回调函数。如果需要接收初始化和摧毁回调函数，请注册bean的定义而不是现存的实例对象。 接下来看下ConfigurableBeanFactory中定义的方法： // 作用域 String SCOPE_SINGLETON = \"singleton\"; // 单例作用域 String SCOPE_PROTOTYPE = \"prototype\"; // 原型作用域 // 设置父级bean工厂 void setParentBeanFactory(BeanFactory parentBeanFactory) throws IllegalStateException; // 设置bean的类加载器，默认为线程上下文类加载器 void setBeanClassLoader(@Nullable ClassLoader beanClassLoader); // 获取bean的类加载器 @Nullable ClassLoader getBeanClassLoader(); // 设置临时的类加载器 void setTempClassLoader(@Nullable ClassLoader tempClassLoader); // 获取临时的类加载器 @Nullable ClassLoader getTempClassLoader(); // 设置是否缓存bean的元数据 void setCacheBeanMetadata(boolean cacheBeanMetadata); // 是否缓存bean的元数据 boolean isCacheBeanMetadata(); // 设置bean的表达式解析器，以统一的EL兼容样式支持#{...}这样的表达式 void setBeanExpressionResolver(@Nullable BeanExpressionResolver resolver); // 获取bean的表达式解析器 @Nullable BeanExpressionResolver getBeanExpressionResolver(); // 设置转换服务，用于转换属性值 void setConversionService(@Nullable ConversionService conversionService); // 获取转换服务 @Nullable ConversionService getConversionService(); // 添加属性编辑器注册者 void addPropertyEditorRegistrar(PropertyEditorRegistrar registrar); // 为所有给定的属性注册自定义属性编辑器 void registerCustomEditor(Class requiredType, Class propertyEditorClass); // 使用在BeanFactory中注册的自定义编辑器来初始哈给定的属性编辑器注册者 void copyRegisteredEditorsTo(PropertyEditorRegistry registry); // 设置类型转换器 void setTypeConverter(TypeConverter typeConverter); // 获取类型转换器 TypeConverter getTypeConverter(); // 添加嵌入值解析器，例如注册属性 void addEmbeddedValueResolver(StringValueResolver valueResolver); // 在BeanFactory是否有注册嵌入值解析器 boolean hasEmbeddedValueResolver(); // 解析给定的嵌入的值 @Nullable String resolveEmbeddedValue(String value); // 添加bean的后置处理器 void addBeanPostProcessor(BeanPostProcessor beanPostProcessor); // 获取bean的后置处理器个数 int getBeanPostProcessorCount(); // 注册作用域 void registerScope(String scopeName, Scope scope); // 获取注册的作用域的名字 String[] getRegisteredScopeNames(); // 获取作用域 @Nullable Scope getRegisteredScope(String scopeName); // 提供一个与这个工厂有关的安全访问控制上下文 AccessControlContext getAccessControlContext(); // 从给定的其他的工厂拷贝所有相关的配置。不应该包含任何bean的定义元数据 void copyConfigurationFrom(ConfigurableBeanFactory otherFactory); // 注册别名 void registerAlias(String beanName, String alias) throws BeanDefinitionStoreException; // 解析所有别名的目标名称和在工厂中注册的别名，将给定的StringValueResolver应用于它们 void resolveAliases(StringValueResolver valueResolver); // 获取合并的bean的定义 BeanDefinition getMergedBeanDefinition(String beanName) throws NoSuchBeanDefinitionException; // 给定名字的bean是否为FactoryBean boolean isFactoryBean(String name) throws NoSuchBeanDefinitionException; // 显式的设置指定bean的目前在创建状态 void setCurrentlyInCreation(String beanName, boolean inCreation); // 指定的bean目前是否为在建状态 boolean isCurrentlyInCreation(String beanName); // 注册给定bean所依赖的bean void registerDependentBean(String beanName, String dependentBeanName); // 获取所有依赖于指定bean的bean的名字 String[] getDependentBeans(String beanName); // 获取所有指定bean所依赖的bean的名字 String[] getDependenciesForBean(String beanName); // 根据bean的定义来摧毁给定的bean的实例（通常是从工厂中获取到的原型实例） void destroyBean(String beanName, Object beanInstance); // 在当前目标作用域中摧毁指定的作用域中的bean void destroyScopedBean(String beanName); // 摧毁在工厂中的所有单例bean void destroySingletons(); 上面的大部分方法都是获取或者设置一些配置的信息，以便协同来完成BeanFactory的配置。 ConfigurableListableBeanFactory ConfigurableListableBeanFactory接口继承自ListableBeanFactory, AutowireCapableBeanFactory, ConfigurableBeanFactory。大多数具有列出能力的bean工厂都应该实现此接口。 此了这些接口的能力之外，该接口还提供了分析、修改bean的定义和单例的预先实例化的机制。这个接口不应该用于一般的客户端代码中，应该仅仅提供给内部框架使用。下面是这个接口的方法： AbstractBeanFactory AbstractBeanFactory继承自FactoryBeanRegistrySupport，实现了ConfigurableBeanFactory接口。AbstractBeanFactory是BeanFactory的抽象基础类实现，提供了完整的ConfigurableBeanFactory的能力。在这里不讨论该抽象类的实现细节，只要知道这个类是干什么的就行了，会面会有更加详细的章节来讨论。 单例缓存 别名的管理 FactoryBean的处理 用于子bean定义的bean的合并 bean的摧毁接口 自定义的摧毁方法 BeanFactory的继承管理 子类需要实现的模板方法如下： AbstractAutowireCapableBeanFactory AbstractAutowireCapableBeanFactory继承自AbstractBeanFactory，实现了AutowireCapableBeanFactory接口。该抽象了实现了默认的bean的创建。 提供了bean的创建、属性填充、装配和初始化 处理运行时bean的引用，解析管理的集合、调用初始化方法等 支持构造器自动装配，根据类型来对属性进行装配，根据名字来对属性进行装配 SimpleJndiBeanFactory 主线流程1 从接口BeanFactory到HierarchicalBeanFactory，再到ConfigurableBeanFactory，是一条主要的BeanFactory设计路径。 在这条接口设计路径中，BeanFactory定义了基本的IOC容器规范。 BeanFactory接口 该接口中包含了getBean()这样的IOC容器的基本方法； HierarchicalBeanFactory接口 该接口继承了BeanFactory接口，增加了getParentBeanFactory()方法，使BeanFactory具备了双亲IOC的管理功能。 ConfigurableBeanFactory接口 该接口继承了HierarchicalBeanFactory，主要定义了一些对BeanFactory的配置功能，比如通过setParentBeanFactory() 设置双亲IOC容器，通过addBeanPostProcessor()配置Bean的后置处理器等等。通过这些接口设计的叠加，定义了 BeanFactory就是简单IOC容器的基本功能。 主线流程2 以ApplicationContext为核心的接口设计，为第二条接口设计路线，主要涉及接口从BeanFactory到ListableBeanFactory， 再到ApplicationContext，再到WebApplicationContext或者ConfigurableApplicationContext接口。 在这个接口体系中，ListableBeanFactory和HierarchicalBeanFactory两个接口，连接BeanFactory接口和ApplicationContext接口定义。 ListableBeanFactory细化了BeanFactory接口功能，比如getBeanDefinitionNames()获取bean数组。同时，ApplicationContext接口继承了MessageSource等，增加了很多高级特性。 主线流程3 这里涉及的是主要的接口关系，而具体的IOC容器都是在这些接口下实现的，比如DefaultListableBeanFactory， 这个接口间接实现了ConfigurableBeanFactory接口，从而成为一个简单IOC容器实现。可以理解为主要接口定义了 IOC的骨架，相应IOC容器去做实现。 主线流程4 这个接口体系以BeanFactory和ApplicationContext为核心。 BeanFactory为IOC容器最基本的接口，ApplicationContext为IOC容器的高级形态接口。 以下分别去分析BeanFactory和ApplicationContext。 BeanFactory容器设计原理 BeanFactory接口提供了使用IOC容器的规范。在这个基础上提供了一系列的容器实现供使用。 这里以XmlBeanFactory为例，设计关系图如下： XmlBeanFactory源码： package org.springframework.beans.factory.xml; import org.springframework.beans.BeansException; import org.springframework.beans.factory.BeanFactory; import org.springframework.beans.factory.support.DefaultListableBeanFactory; import org.springframework.core.io.Resource; @Deprecated @SuppressWarnings({\"serial\", \"all\"}) public class XmlBeanFactory extends DefaultListableBeanFactory { private final XmlBeanDefinitionReader reader = new XmlBeanDefinitionReader(this); public XmlBeanFactory(Resource resource) throws BeansException { this(resource, null); } public XmlBeanFactory(Resource resource, BeanFactory parentBeanFactory) throws BeansException { super(parentBeanFactory); this.reader.loadBeanDefinitions(resource); } } XmlBeanFactory创建过程： 基于将要读取的xml文件，构建一个Resource对象作为XmlBeanFactory的构造器参数创建XmlBeanFactory。 ClassPathResource res = new ClassPathResource(\"beans.xml\"); XmlBeanFactory xmlBeanFactory = new XmlBeanFactory(res); 通过XmlBeanDefinitionReader的loadBeanDefinitions()调用处理从Resource中载入BeanDefinitions的过程， loadBeanDefinitions是IOC容器初始化的重要组成部分，最后IOC容器会基于BeanDefinitions完成容器初始化和 依赖注入的过程，这里只是简单介绍下，以下会深入源码分析IOC容器的初始化和依赖注入细节。 IOC容器建立的基本步骤： 先看下通过编程式方式使用IOC容器过程: ClassPathResource res = new ClassPathResource(\"beans.xml\"); DefaultListableBeanFactory factory = new DefaultListableBeanFactory(); XmlBeanDefinitionReader reader = new XmlBeanDefinitionReader(factory); reader.loadBeanDefinitions(res); 参考代码可以看到使用IOC容器主要经历了以下几个步骤： 1）创建IOC配置文件的抽象资源，资源包含了BeanDefinition的定义信息。 2）创建一个BeanFactory，这里使用DefaultListableBeanFactory。 3）创建一个载入BeanDefinition的读取器，这里使用XmlBeanDefinitionReader来载入XML文件形式的BeanDefinition。 4）XmlBeanDefinitionReader解析资源，完成Bean的载入和注册，建立起IOC容器，这个时候就可用了。 "},"Chapter11/DefaultListableBeanFactory.html":{"url":"Chapter11/DefaultListableBeanFactory.html","title":"容器的始祖 DefaultListableBeanFactory","keywords":"","body":"容器的始祖 DefaultListableBeanFactory 要说 XmlBeanFactory 就不得不先说它的父类 DefaultListableBeanFactory，因为 XmlBeanFactory 中的大部分功能实际上在 DefaultListableBeanFactory 中就已经提供好了，XmlBeanFactory 只是对 IO 流的读取做了一些定制而已。 DefaultListableBeanFactory 是一个完整的、功能成熟的 IoC 容器，如果你的需求很简单，甚至可以直接使用 DefaultListableBeanFactory，如果你的需求比较复杂，那么通过扩展 DefaultListableBeanFactory 的功能也可以达到，可以说 DefaultListableBeanFactory 是整个 Spring IoC 容器的始祖。 我们先来看一下 DefaultListableBeanFactory 的继承关系： 从这张类的关系图中可以看出，DefaultListableBeanFactory 实际上也是一个集大成者。在 Spring 中，针对 Bean 的不同操作都有不同的接口进行规范，每个接口都有自己对应的实现，最终在 DefaultListableBeanFactory 中将所有的实现汇聚到一起。从这张类的继承关系图中我们大概就能感受到 Spring 中关于类的设计是多么厉害，代码耦合度非常低。 这些类，在本系列后面的介绍中，大部分都会涉及到，现在我先大概介绍一下每个类的作用，大家先混个脸熟： BeanFactory：这个接口看名字就知道是一个 Bean 的工厂，BeanFactory 接口定义了各种获取 Bean 的方法、判断 Bean 是否存在、判断 Bean 是否单例等针对 Bean 的基础方法。 ListableBeanFactory：这个接口继承自 BeanFactory，在 BeanFactory 的基础上，扩展了 Bean 的查询方法，例如根据类型获取 BeanNames、根据注解获取 BeanNames、根据 Bean 获取注解等。 AutowireCapableBeanFactory：该接口继承自 BeanFactory，在 BeanFactory 的基础上，提供了 Bean 的创建、配置、注入、销毁等操作。有时候我们需要自己手动注入 Bean 的时候，可以考虑通过实现该接口来完成。AutowireCapableBeanFactory 在 Spring Security 中有一个重要的应用就是 ObjectPostProcessor，这个松哥将在 Spring Security 系列中和大家详细介绍。 HierarchicalBeanFactory：该接口继承自 BeanFactory，并在 BeanFactory 基础上添加了获取 parent beanfactory 的方法。 SingletonBeanRegistry：这个接口定义了对单例 Bean 的定义以及获取方法。 ConfigurableBeanFactory：这个接口主要定了针对 BeanFactory 的各种配置以及销毁的方法。 ConfigurableListableBeanFactory：这是 BeanFactory 的配置清单，这里定义了忽略的类型、接口，通过 Bean 的名称获取 BeanDefinition 、冻结 BeanDefinition 等。 AliasRegistry：这个接口定义了对 alias 的注册、移除、判断以及查询操作。 SimpleAliasRegistry：这个类实现了 AliasRegistry 接口并实现了它里边的方法，SimpleAliasRegistry 使用 ConcurrentHashMap 做载体，实现了对 alias 的注册、移除判断以及查询操作。 DefaultSingletonBeanRegistry：这个类基于 Java 中的集合，对 SingletonBeanRegistry 接口进行了实现。 FactoryBeanRegistrySupport：该类继承自 DefaultSingletonBeanRegistry，并在 DefaultSingletonBeanRegistry 的基础上，增加了获取 FactoryBean 类型、移除 FactoryBean 缓存的方法等等操作。 AbstractBeanFactory：实现了 ConfigurableBeanFactory 接口并继承自 FactoryBeanRegistrySupport，在 AbstractBeanFactory 中对 ConfigurableBeanFactory 中定义的方法进行了实现。 AbstractAutowireCapableBeanFactory：该类继承自 AbstractBeanFactory 并对 AutowireCapableBeanFactory 接口中定义的方法进行了落地实现。 BeanDefinitionRegistry：这个接口继承自 AliasRegistry 接口，并增加了一系列针对 BeanDefinition 的注册、移除、查询、判断等方法。 最后的 DefaultListableBeanFactory 自然就具备了上面所有的功能。 上面的内容可能看的大家眼花缭乱，这里通过几个简单实际的例子，来带大家使用一下 DefaultListableBeanFactory 的功能，可能大家的理解就比较清晰了。 DefaultListableBeanFactory 作为一个集大成者，提供了非常多的功能，我们一个一个来看。 "},"Chapter11/IOCPrinciple.html":{"url":"Chapter11/IOCPrinciple.html","title":"IOC原理","keywords":"","body":"IOC原理 IOC的从广义范围来说，意思是控制反转，什么是控制反转呢，大家肯定都知道，以前我们要使用一个类里的方法或者属性的时候，我们先要new出这个类的对象，然后用这个对象调用里面的方法的。 这是传统方式上的使用类里的方法和属性，但是这种方式存在很大的耦合性。为了降低耦合性，Spring出了IOC控制反转，它的含义是指，Spring帮助我们来创建对象就是我们所说的bean，并且管理bean对象，当我们需要用的时候，需要Spring提供给我们创建好的bean对象。 这样就从之前我们的主动创建对象变为了，由Spring来控制创建对象，来给我们使用，也就是控制反转的意思。 对于IOC它主要设计了两个接口用来表示容器 一个是BeanFactory(低级容器) 一个是ApplicationContext(高级容器) BeanFactory它是IOC容器的顶层的接口，对于BeanFactory这个容器来说，它相当一个HashMap，BeanName作为key，value就是这个bean的实例化对象，通常提供注册（put）和获取（get）两个功能。 这个实现主要是使用的工厂模式+反射技术，来进行对Bean的实例化，然后把这个实例化对象存到HashMap中，下面通过一个简单的例子来展现下BeanFactory是如何使用工厂模式+反射技术创建对象 创建一个接口类 public interface Shape { public void draw(); } 实现这个接口类 public class Circle implements Shape { public void draw(){ System.out.println(\"画圆形\"); } } public class Square implements Shape { public void draw(){ System.out.println(\"画方形\"); } } 创建一个工厂类 public class ReflectFactory { public Shape getInstance(String className) throws ClassNotFoundException, IllegalAccessException, InstantiationException { Shape shape =null; shape =(Shape)Class.forName(className).newInstance(); return shape; } } 创建一个Client类来进行测试 public class Client { public static void main(String[] args){ ShapeFactory shapeFactory = new ShapeFactory(); Shape shape= shapeFactory.getInstance(\"Circle\"); shape.draw(); } } 输出的是 画圆形 这个简单的小例子就是BeanFactory中通过工厂模式和反射创建对象的一个简单原理，当然BeanFactory的源码要比这个复杂的多。这里就是为了方便大家理解。 而对于ApplicationContext来说，它继承了BeanFactory这个接口，并且丰富了更多了功能在里面，这其中包括回调一些方法。这其中主要的就是有个refresh()方法，这个方法主要是刷新整个容器，即重新加载或者刷新所有的bean。 接下来给大家看下refresh()方法的源码。 1 public void refresh() throws BeansException, IllegalStateException { 2 Object var1 = this.startupShutdownMonitor; 3 //首先是synchronized加锁，加这个锁的原因是，避免如果你先调一次refresh()然后这次还没处理完又调一次，就会乱套了 4 synchronized(this.startupShutdownMonitor) { 5 //这个方法是做准备工作的，记录容器的启动时间、标记“已启动”状态、处理配置文件中的占位符 6 this.prepareRefresh(); 7 //这一步是把配置文件解析成一个个Bean，并且注册到BeanFactory中，注意这里只是注册进去，并没有初始化 8 ConfigurableListableBeanFactory beanFactory = this.obtainFreshBeanFactory(); 9 //设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean，这里都是spring里面的特殊处理 10 this.prepareBeanFactory(beanFactory); 11 12 try { 13 //具体的子类可以在这步的时候添加一些特殊的 BeanFactoryPostProcessor 的实现类，来完成一些其他的操作 14 this.postProcessBeanFactory(beanFactory); 15 //这个方法是调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 16 this.invokeBeanFactoryPostProcessors(beanFactory); 17 //这个方法注册 BeanPostProcessor 的实现类 18 this.registerBeanPostProcessors(beanFactory); 19 //这方法是初始化当前 ApplicationContext 的 MessageSource，国际化处理 20 this.initMessageSource(); 21 //这个方法初始化当前 ApplicationContext 的事件广播器 22 this.initApplicationEventMulticaster(); 23 //这个方法初始化一些特殊的 Bean（在初始化 singleton beans 之前） 24 this.onRefresh(); 25 //这个方法注册事件监听器，监听器需要实现 ApplicationListener 接口 26 this.registerListeners(); 27 28 //初始化所有的 singleton beans（单例bean），懒加载（non-lazy-init）的除外 29 this.finishBeanFactoryInitialization(beanFactory); 30 //方法是最后一步，广播事件 31 this.finishRefresh(); 32 } catch (BeansException var9) { 33 if(this.logger.isWarnEnabled()) { 34 this.logger.warn(\"Exception encountered during context initialization - cancelling refresh attempt: \" + var9); 35 } 36 //调用销毁bean的方法 37 this.destroyBeans(); 38 this.cancelRefresh(var9); 39 throw var9; 40 } finally { 41 this.resetCommonCaches(); 42 } 43 44 } 45 } 以上就是ApplicationContext中的refresh()这个方法所做的事，里面需要看的重点的方法是ConfigurableListableBeanFactory beanFactory = this.obtainFreshBeanFactory();和this.finishBeanFactoryInitialization(beanFactory);这两个方法，大家有兴趣可以看下里面的源码，这里面包含了bean的一个生命周期。 对上面的两大块分析完成后，那么对于IOC容器的启动过程是什么样呢，说的更直白就是ClassPathXmlApplicationContext这个类启动的时候做了啥？ 简单的理解，首先访问的是“高级容器”refresh方法，这个方法是使用低级容器加载所有的BeanDefinition和properties到IOC的容器中，低级容器加载成功后，高级容器开始开始处理一些回调功能。例如Bean后置处理器，回调setBeanFactory方法，和注册监听、发布事件、实例化单例Bean。 IOC做的事其实就是，低级容器（BeanFactory）加载配置文件，解析成BeanDefinition，然后放到一个map里，BeanName作为key ，这个Bean实例化的对象作为value， 使用的时候调用getBean方法，完成依赖注入。 高级容器 （ApplicationContext），它包含了低级容器的功能，当他执行 refresh 模板方法的时候，将刷新整个容器的 Bean。同时其作为高级容器，包含了太多的功能。他支持不同信息源头，支持 BeanFactory 工具类，支持层级容器，支持访问文件资源，支持事件发布通知，支持接口回调等等。 "},"Chapter11/ResourcePositioningOfBeanDefinition.html":{"url":"Chapter11/ResourcePositioningOfBeanDefinition.html","title":"BeanDefinition的Resource定位","keywords":"","body":"BeanDefinition的Resource定位 IOC容器初始化过程概述 IOC容器初始化是由上文提到的refresh()方法来启动的，这个方法标志着IOC容器正式启动。 IOC容器初始化过程分为三个过程： BeanDefinition的Resource定位过程。 这个Resource定位是指BeanDefinition资源定位，它由ResourceLoader通过统一的Resource接口完成。这个定位过程就是容器寻找数据的过程，就像水桶要装水首先需要找到水一样。 BeanDefinition的载入和解析过程。 这个过程就是根据上一步定位的Resource资源文件，把用户定义好的Bean表示成IOC容器内部的BeanDefinition数据结构。BeanDefinition实际就是POJO对象在IOC容器中的抽象，通过BeanDefinition定义数据结构，使IOC容器能够方便地对POJO对象也就是Bean进行管理。 向IOC容器注册BeanDefinition的过程。 这个过程是通过调用BeanDefinitionRegistry接口的实现来完成的，把载入过程生成的BeanDefinition向IOC容器注册。最终在IOC容器内部将BeanDefinition注入到一个HashMap中，IOC容器就是通过这个HashMap来持有这些BeanDefinition数据的。 注意： 这里需要注意的是，上面谈到的只是IOC容器的初始化过程，这个过程一般不包含Bean依赖注入的实现。Bean的定义和依赖注入是两个独立的过程。依赖注入一般发生在第一次getBean()向容器索要Bean的时候，但是如果配置了lazyinit，则初始化的时候这样的Bean已经触发了依赖注入。 这里先分析IOC容器的第一个过程，BeanDefinition的Resource定位过程。 BeanDefinition的Resource定位过程 一般我们通常使用的IOC容器有FileSystemXmlApplicationContext、ClassPathXmlApplicationContext、XmlWebApplicationContext、WebApplicationContext、AnnotationConfigApplicationContext等。 注解方式AnnotationConfigApplicationContext前置读取资源跟Xml方式容器有区别，注解方式AnnotationConfigApplicationContext是读取指定扫描包下的.class，组装成元数据，然后解析每个类，判断是否有@Component注解（@Service含@Component注解）进行BeanDefinition数据结构读取， Xml方式则是解析Spring配置的applicationContext.xml的文件，读取BeanDefinition数据结构，但是最后都走AbstractApplicationContext#refresh()模板方法，理解了Xml实现方式容器的核心，注解方式就比较好理解。 因为Xml实现方式理解IOC容器会更直观些，以Xml方式实现进行IOC容器源码解读。下面以FileSystemXmlApplicationContext为例，分析Resource的定位过程。 FileSystemXmlApplicationContext应用： // 根据配置文件创建IOC容器 ApplicationContext context = new FileSystemXmlApplicationContext(\"classpath:applicationContext.xml\"); // 从容器中获取Bean ConferenceServiceImpl conferenceService = (ConferenceServiceImpl)context.getBean(\"conferenceService\"); // 调用Bean方法 conferenceService.conference(); \"实线\"代表extends，\"虚线\"代表implements。从类图可以看到继承了ApplicationContext，而ApplicationContext 又继承了BeanFactory，所以FileSystemXmlApplicationContext具备了IOC的基本规范和一些高级特性。 在类图的最右上方可以看到继承了ResourceLoader，用以读入以Resource定义的BeanDefinition的能力。 FileSystemXmlApplicationContext源码： package org.springframework.context.support; import org.springframework.beans.BeansException; import org.springframework.context.ApplicationContext; import org.springframework.core.io.FileSystemResource; import org.springframework.core.io.Resource; public class FileSystemXmlApplicationContext extends AbstractXmlApplicationContext { public FileSystemXmlApplicationContext() { } public FileSystemXmlApplicationContext(ApplicationContext parent) { super(parent); } /** * 根据用户定义的Bean XML文件路径，载入BeanDefinition，自动创建IOC容器 */ public FileSystemXmlApplicationContext(String configLocation) throws BeansException { this(new String[] {configLocation}, true, null); } /** * 根据用户定义的多个Bean XML文件路径，载入BeanDefinition，自动创建IOC容器 */ public FileSystemXmlApplicationContext(String... configLocations) throws BeansException { this(configLocations, true, null); } /** * 根据用户定义的多个Bean XML文件路径，载入BeanDefinition，自动创建IOC容器，允许指定自己的双亲IOC容器 */ public FileSystemXmlApplicationContext(String[] configLocations, ApplicationContext parent) throws BeansException { this(configLocations, true, parent); } /** * 是否允许自动刷新上下文 */ public FileSystemXmlApplicationContext(String[] configLocations, boolean refresh) throws BeansException { this(configLocations, refresh, null); } /** * 在对象初始化的过程中，调用refresh()方法启动载入BeanDefinition过程 */ public FileSystemXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException { super(parent); setConfigLocations(configLocations); if (refresh) { refresh(); } } /** * 根据用户的xml构建Resource对象，这是一个模板方法， * 在BeanDefinitionReader的loadBeanDefinition()方法中被调用。 */ @Override protected Resource getResourceByPath(String path) { if (path != null && path.startsWith(\"/\")) { path = path.substring(1); } return new FileSystemResource(path); } } FileSystemXmlApplicationContext中有很多构造函数，实现了对参数configLocation进行处理，以XML文件方式 存在的BeanDefinition能够得到有效处理。比如，实现了getResourceByPath()方法，这个是一个模板方法，是为 读取Resource服务的。在初始化FileSystemXmlApplicationContext的过程中，通过refresh()来启动整个调用， 进入AbstractApplicationContext的refresh()方法。 @Override public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // Prepare this context for refreshing. // 调用容器准备刷新的方法，设置容器的启动时间为当前时间，容器关闭状态为false，同时给容器设置同步标识 prepareRefresh(); // Tell the subclass to refresh the internal bean factory. // 告诉子类启动refreshBeanFactory()方法， // Bean定义资源文件的载入从子类的refreshBeanFactory()方法启动[***重点***] ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. // 为BeanFactory配置容器特性，例如类加载器、事件处理器等 prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. // 为容器的某些子类指定特殊的BeanPost事件处理器，进行后置处理 postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. // 调用BeanFactory的后置处理器，这些后置处理器是在Bean定义中向容器注册的 invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. // 为BeanFactory注册BeanPost事件处理器，BeanPostProcessor是Bean后置处理器，用于监听容器触发的事件 registerBeanPostProcessors(beanFactory); // Initialize message source for this context. // 初始化信息源，和国际化相关 initMessageSource(); // Initialize event multicaster for this context. // 初始化容器事件传播器 initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. // 调用子类的某些特殊Bean初始化方法 onRefresh(); // Check for listener beans and register them. // 检查监听Bean并且将这些Bean向容器注册 registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. // 初始化所有剩余的(non-lazy-init)单态Bean finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. // 初始化容器的生命周期事件处理器，并发布容器的生命周期事件，结束refresh过程 finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } 上面refresh()方法的源码，是整个IOC容器初始化的过程。咱们这里讨论的是BeanDefinition的Resource资源的定位， 重点关注refresh()方法中的obtainFreshBeanFactory()方法，该方法也位于AbstractApplicationContext类中。 protected ConfigurableListableBeanFactory obtainFreshBeanFactory() { refreshBeanFactory(); //getBeanFactory获取的bean工厂将在refreshBeanFactory初始话 return getBeanFactory(); } 在上面的obtainFreshBeanFactory()方法中可以看到refreshBeanFactory()方法，是AbstractApplicationContext中的一个抽象方法，委托给子类具体实现 方法的具体实现在AbstractRefreshableApplicationContext中。 @Override protected final void refreshBeanFactory() throws BeansException { // 判断如果已经建立了BeanFactory，则销毁并关闭BeanFactory if (hasBeanFactory()) { destroyBeans(); closeBeanFactory(); } try { // 创建IoC容器，这里使用的是DefaultListableBeanFactory DefaultListableBeanFactory beanFactory = createBeanFactory(); // 对IoC容器进行定制化，如设置启动参数，开启注解的自动装配等 beanFactory.setSerializationId(getId()); customizeBeanFactory(beanFactory); /** * 启动对BeanDefinition的载入，这里使用了一个委派模式， * 在当前类中只定义了抽象的loadBeanDefinitions方法，具体的实现调用子类容器 */ loadBeanDefinitions(beanFactory); //这里就是getBeanFactory赋值的地方 this.beanFactory = beanFactory; } catch (IOException ex) { throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex); } } 在refreshBeanFactory()方法中，可以看到loadBeanDefinitions()是AbstractRefreshableApplicationContext中的 一个抽象方法，委托给子类具体实现，方法签名如下： protected abstract void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException; oadBeanDefinitions()抽象方法有多个实现： 从FileSystemXmlApplicationContext类图，可以知道选择AbstractXmlApplicationContext中的实现， 因为另外三个与FileSystemXmlApplicationContext没有关系。 @Override protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException { // Create a new XmlBeanDefinitionReader for the given BeanFactory. // 根据BeanFactory容器，创建XmlBeanDefinitionReader读取器 XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context's // resource loading environment. // 使用此上下文的资源加载环境配置Bean定义读取器 beanDefinitionReader.setEnvironment(this.getEnvironment()); //注意这里后面会用到 beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // Allow a subclass to provide custom initialization of the reader, // then proceed with actually loading the bean definitions. // 允许子类提供读取器的自定义初始化，然后继续加载bean定义信息 initBeanDefinitionReader(beanDefinitionReader); loadBeanDefinitions(beanDefinitionReader); } beanDefinitionReader.setResourceLoader(this); 设置一个ResourceLoader为本对象，根据继承关系可知 AbstractXmlApplicationContext extends AbstractRefreshableConfigApplicationContext AbstractRefreshableConfigApplicationContext extends AbstractRefreshableApplicationContext implements BeanNameAware, InitializingBean public abstract class AbstractRefreshableApplicationContext extends AbstractApplicationContext { public abstract class AbstractApplicationContext extends DefaultResourceLoader implements ConfigurableApplicationContext { 这里的ResourceLoader是一个DefaultResourceLoader 继续调用loadBeanDefinitions()，咱们现在目标就是定位BeanDEfinition的Resource看下方法源码： protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws BeansException, IOException { // 以Resource的方式获得配置文件的资源位置 Resource[] configResources = getConfigResources(); if (configResources != null) { reader.loadBeanDefinitions(configResources); } // 以String的形式获得配置文件位置 String[] configLocations = getConfigLocations(); if (configLocations != null) { reader.loadBeanDefinitions(configLocations); } } 对于ClassPathXmlApplicationContext追走第一个判断。 对于FileSystemXmlApplicationContext会走第二个if判断， 接着看源码。 @Override public int loadBeanDefinitions(String... locations) throws BeanDefinitionStoreException { // 如果locations为空，则停止Resource资源定位 Assert.notNull(locations, \"Location array must not be null\"); int counter = 0; for (String location : locations) { // 根据路径载入信息 counter += loadBeanDefinitions(location); } return counter; } loadBeanDefinitions()方法继续深入： public int loadBeanDefinitions(String location, Set actualResources) throws BeanDefinitionStoreException { // 这里得到当前定义的ResourceLoader,默认的使用DefaultResourceLoader（前面有注释） ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) { throw new BeanDefinitionStoreException( \"Cannot import bean definitions from location [\" + location + \"]: no ResourceLoader available\"); } /** * 这里对Resource的路径模式进行解析，得到需要的Resource集合， * 这些Resource集合指向了我们定义好的BeanDefinition的信息，可以是多个文件。 */ if (resourceLoader instanceof ResourcePatternResolver) { // Resource pattern matching available. try { // 调用DefaultResourceLoader的getResources完成具体的Resource定位 Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); int loadCount = loadBeanDefinitions(resources); if (actualResources != null) { for (Resource resource : resources) { actualResources.add(resource); } } if (logger.isDebugEnabled()) { logger.debug(\"Loaded \" + loadCount + \" bean definitions from location pattern [\" + location + \"]\"); } return loadCount; } catch (IOException ex) { throw new BeanDefinitionStoreException( \"Could not resolve bean definition resource pattern [\" + location + \"]\", ex); } } else { // Can only load single resources by absolute URL. // 通过ResourceLoader来完成位置定位（找水） Resource resource = resourceLoader.getResource(location); //后面要说的载入、解析和注册（装水） int loadCount = loadBeanDefinitions(resource); if (actualResources != null) { actualResources.add(resource); } if (logger.isDebugEnabled()) { logger.debug(\"Loaded \" + loadCount + \" bean definitions from location [\" + location + \"]\"); } return loadCount; } } ResourceLoader是一个接口类，其getResource()方法具体实现在DefaultResourceLoader，对于取得Resource的具体过程， 我们可以看下DefaultResourceLoader中getResource()方法的实现。 @Override public Resource getResource(String location) { Assert.notNull(location, \"Location must not be null\"); for (ProtocolResolver protocolResolver : getProtocolResolvers()) { Resource resource = protocolResolver.resolve(location, this); if (resource != null) { return resource; } } //处理所有/标识的Resource if (location.startsWith(\"/\")) { return getResourceByPath(location); } // 处理所有带有classpath标识的Resource else if (location.startsWith(CLASSPATH_URL_PREFIX)) { return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); } else { try { // 处理URL标识的Resource定位 URL url = new URL(location); return (ResourceUtils.isFileURL(url) ? new FileUrlResource(url) : new UrlResource(url)); } catch (MalformedURLException ex) { // No URL -> resolve as resource path. return getResourceByPath(location); } } } 在getResources()方法中，最显眼的是getResourceByPath()方法，在一开始的时候提到过，它是一个模板方法， protected Resource getResourceByPath(String path) { return new ClassPathContextResource(path, getClassLoader()); } 其中一个实现类就是FileSystemXmlApplicationContext，其方法签名如下： @Override protected Resource getResourceByPath(String path) { if (path != null && path.startsWith(\"/\")) { path = path.substring(1); } return new FileSystemResource(path); } 通过该方法，返回一个FileSystemResource对象，该对象扩展自Resource，而Resource扩展自InputStreamSource。 public FileSystemResource(String path) { Assert.notNull(path, \"Path must not be null\"); this.file = new File(path); this.path = StringUtils.cleanPath(path); } FileSystemResource类图： getResourceByPath方法调用过程： 通过FileSystemResource对象，Spring可以进行相关的I/O操作，完成BeanDefinition的Resource定位过程。 这里只是分析了FileSystemXmlApplicationContext的容器下Resource定位过程， 如果是其他的ApplicationContext，那么对应生成其他的Resource，比如ClassPathResource、 ServletContextResource等。关于Spring中Resource的种类，继承关系如下： 这些接口对应不同的Resource实现代表着不同的一样。 以FileSystemXmlApplicationContext容器实现原理为例，上面只是分析了BeanDefinition的Resource定位过程， 这个时候可以通过Resource对象来进行BeanDefinition的载入了。这里完成了水桶装水找水的过程，下一篇分析一下 水桶装水的过程，也即BeanDefinition的载入和解析过程。 "},"Chapter11/RegistrationOfBeanDefinition.html":{"url":"Chapter11/RegistrationOfBeanDefinition.html","title":"BeanDefinition的载入和解析和注册","keywords":"","body":"BeanDefinition的载入和解析和注册 上文分析了BeanDefiniton的Resource定位过程： 这篇文章分析下BeanDefinition信息的载入过程。 载入过程就是把Resource转化为BeanDefinition作为一个Spring IOC容器内部表示的数据结构的过程。 IOC容器对Bean的管理和依赖注入功能的实现，就是通过对其持有的BeanDefinition进行各种相关操作来完成的。 这些BeanDefinition数据在IOC容器中通过一个HashMap来保护和维护。在上文中，我们从refresh()入口开始， 一直追溯到AbstractBeanDefinitionReader.loadBeanDefinitions()，找到BeanDefinition需要的Resource资源文件。 public int loadBeanDefinitions(String location, Set actualResources) throws BeanDefinitionStoreException { // 这里得到当前定义的ResourceLoader,默认的使用DefaultResourceLoader ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) { throw new BeanDefinitionStoreException( \"Cannot import bean definitions from location [\" + location + \"]: no ResourceLoader available\"); } /** * 这里对Resource的路径模式进行解析，得到需要的Resource集合， * 这些Resource集合指向了我们定义好的BeanDefinition的信息，可以是多个文件。 */ if (resourceLoader instanceof ResourcePatternResolver) { // Resource pattern matching available. try { // 调用DefaultResourceLoader的getResources完成具体的Resource定位 Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); int loadCount = loadBeanDefinitions(resources); if (actualResources != null) { for (Resource resource : resources) { actualResources.add(resource); } } if (logger.isDebugEnabled()) { logger.debug(\"Loaded \" + loadCount + \" bean definitions from location pattern [\" + location + \"]\"); } return loadCount; } catch (IOException ex) { throw new BeanDefinitionStoreException( \"Could not resolve bean definition resource pattern [\" + location + \"]\", ex); } } else { // Can only load single resources by absolute URL. // 通过ResourceLoader来完成位置定位（找水） Resource resource = resourceLoader.getResource(location); // 载入、解析（装水） int loadCount = loadBeanDefinitions(resource); if (actualResources != null) { actualResources.add(resource); } if (logger.isDebugEnabled()) { logger.debug(\"Loaded \" + loadCount + \" bean definitions from location [\" + location + \"]\"); } return loadCount; } } 资源文件Resource有了，看下是如何将Resource文件载入到BeanDefinition中的？ 将xml文件转换成Document对象 Resource载入的具体实现在AbstractBeanDefinitionReader.loadBeanDefinitions()，源码： @Override public int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException { /** * 如果Resource为空，则停止BeanDefinition载入，然后启动载入BeanDefinition的过程， * 这个过程会遍历整个Resource集合所包含的BeanDefinition信息。 */ Assert.notNull(resources, \"Resource array must not be null\"); int counter = 0; for (Resource resource : resources) { counter += loadBeanDefinitions(resource); } return counter; } 这里调用loadBeanDefinitions(Resource... resources)方法，但是这个方法AbstractBeanDefinitionReader中没有具体实现。 具体实现在BeanDefinitionReader的子类XmlBeanDefinitionReader中实现。 @Override public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException { return loadBeanDefinitions(new EncodedResource(resource)); } public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException { Assert.notNull(encodedResource, \"EncodedResource must not be null\"); if (logger.isInfoEnabled()) { logger.info(\"Loading XML bean definitions from \" + encodedResource.getResource()); } // 这里是获取线程局部变量 Set currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) { currentResources = new HashSet(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); } if (!currentResources.add(encodedResource)) { throw new BeanDefinitionStoreException( \"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\"); } try { // 这里得到XML文件，新建IO文件输入流，准备从文件中读取内容 InputStream inputStream = encodedResource.getResource().getInputStream(); try { InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) { inputSource.setEncoding(encodedResource.getEncoding()); } // 具体读取过程的方法 return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); } finally { inputStream.close(); } } catch (IOException ex) { throw new BeanDefinitionStoreException( \"IOException parsing XML document from \" + encodedResource.getResource(), ex); } finally { currentResources.remove(encodedResource); if (currentResources.isEmpty()) { this.resourcesCurrentlyBeingLoaded.remove(); } } } protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException { try { // 将XML文件转换为DOM对象，解析过程由documentLoader实现 Document doc = doLoadDocument(inputSource, resource); // 这里是启动对Bean定义解析的详细过程，该解析过程会用到Spring的Bean配置规则 return registerBeanDefinitions(doc, resource); } catch (BeanDefinitionStoreException ex) { throw ex; } catch (SAXParseException ex) { throw new XmlBeanDefinitionStoreException(resource.getDescription(), \"Line \" + ex.getLineNumber() + \" in XML document from \" + resource + \" is invalid\", ex); } catch (SAXException ex) { throw new XmlBeanDefinitionStoreException(resource.getDescription(), \"XML document from \" + resource + \" is invalid\", ex); } catch (ParserConfigurationException ex) { throw new BeanDefinitionStoreException(resource.getDescription(), \"Parser configuration exception parsing XML from \" + resource, ex); } catch (IOException ex) { throw new BeanDefinitionStoreException(resource.getDescription(), \"IOException parsing XML document from \" + resource, ex); } catch (Throwable ex) { throw new BeanDefinitionStoreException(resource.getDescription(), \"Unexpected exception parsing XML document from \" + resource, ex); } } 到这里完成了XML转化为Document对象，主要经历两个过程：资源文件转化为IO和XML转换为Document对象。 按照Spring的Bean规则对Document对象进行解析 在上面分析中，通过调用XML解析器将Bean定义资源文件转换得到Document对象，但是这些Document对象并没有按照Spring的Bean规则进行解析。 需要按照Spring的Bean规则对Document对象进行解析。 如何对Document文件进行解析？ public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException { // 这里得到BeanDefinitionDocumentReader来对XML的BeanDefinition进行解析 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); // 获得容器中注册的Bean数量 int countBefore = getRegistry().getBeanDefinitionCount(); // 具体的解析过程在registerBeanDefinitions中完成 documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); // 统计解析的Bean数量 return getRegistry().getBeanDefinitionCount() - countBefore; } 按照Spring的Bean规则对Document对象解析的过程是在接口BeanDefinitionDocumentReader的实现类DefaultBeanDefinitionDocumentReader中实现的。 @Override public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) { this.readerContext = readerContext; logger.debug(\"Loading bean definitions\"); // 获取根元素 Element root = doc.getDocumentElement(); // 具体载入过程 doRegisterBeanDefinitions(root); } protected void doRegisterBeanDefinitions(Element root) { // Any nested elements will cause recursion in this method. In // order to propagate and preserve default-* attributes correctly, // keep track of the current (parent) delegate, which may be null. Create // the new (child) delegate with a reference to the parent for fallback purposes, // then ultimately reset this.delegate back to its original (parent) reference. // this behavior emulates a stack of delegates without actually necessitating one. BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createDelegate(getReaderContext(), root, parent); if (this.delegate.isDefaultNamespace(root)) { String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) { String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) { if (logger.isInfoEnabled()) { logger.info(\"Skipped XML bean definition file due to specified profiles [\" + profileSpec + \"] not matching: \" + getReaderContext().getResource()); } return; } } } preProcessXml(root); //从Document的根元素开始进行Bean定义的Document对象 parseBeanDefinitions(root, this.delegate); postProcessXml(root); this.delegate = parent; } 根据Element载入BeanDefinition。 protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) { // 如果使用了Spring默认的XML命名空间 if (delegate.isDefaultNamespace(root)) { // 遍历根元素的所有子节点 NodeList nl = root.getChildNodes(); for (int i = 0; i 该方法作用，使用Spring的Bean规则从Document的根元素开始进行Bean定义的Document对象。 这里主要是看节点元素是否是Spring的规范，因为它允许我们自定义解析规范，那么正常我们是利用Spring的规则， 所以我们来看parseDefaultElement(ele, delegate)方法。 private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) { // 如果元素节点是导入元素，进行导入解析 if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) { importBeanDefinitionResource(ele); } // 如果元素节点是别名元素，进行别名解析 else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) { processAliasRegistration(ele); } // 如果普通的元素，按照Spring的Bean规则解析元素 else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) { processBeanDefinition(ele, delegate); } // 如果普通的元素，调用doRegisterBeanDefinitions()递归处理 else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) { // recurse doRegisterBeanDefinitions(ele); } } 根据Bean的属性，调用不同的规则解析元素，这里讨论普通Bean元素的解析规则， 调用processBeanDefinition规则进行Bean处理。 protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) { /** * BeanDefinitionHolder是对BeanDefinition对象的封装，封装了BeanDefinition、Bean的名字和别名。 * 用来完成想IOC容器注册。得到BeanDefinitionHolder就意味着是通过BeanDefinitionParserDelegate * 对XML元素的信息按照Spring的Bean规则进行解析得到的。 * */ BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) { bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try { // Register the final decorated instance. // 这里是向IOC容器注册解析得到的BeanDefinition的地方 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); } catch (BeanDefinitionStoreException ex) { getReaderContext().error(\"Failed to register bean definition with name '\" + bdHolder.getBeanName() + \"'\", ele, ex); } // Send registration event. // 在BeanDefinition向IOC容器注册以后，发送消息 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); } } Bean资源的解析 具体解析过程委托给BeanDefinitionParserDelegate的parseBeanDefinitionElement来完成。 public BeanDefinitionHolder parseBeanDefinitionElement(Element ele) { return parseBeanDefinitionElement(ele, null); } public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, BeanDefinition containingBean) { // 在这里取得元素中定义的id、name、aliase属性值 String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List aliases = new ArrayList(); if (StringUtils.hasLength(nameAttr)) { String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); } String beanName = id; if (!StringUtils.hasText(beanName) && !aliases.isEmpty()) { beanName = aliases.remove(0); if (logger.isDebugEnabled()) { logger.debug(\"No XML 'id' specified - using '\" + beanName + \"' as bean name and \" + aliases + \" as aliases\"); } } if (containingBean == null) { checkNameUniqueness(beanName, aliases, ele); } // 启动对bean元素的详细解析 AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) { if (!StringUtils.hasText(beanName)) { try { if (containingBean != null) { beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); } else { beanName = this.readerContext.generateBeanName(beanDefinition); // Register an alias for the plain bean class name, if still possible, // if the generator returned the class name plus a suffix. // This is expected for Spring 1.2/2.0 backwards compatibility. String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null && beanName.startsWith(beanClassName) && beanName.length() > beanClassName.length() && !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) { aliases.add(beanClassName); } } if (logger.isDebugEnabled()) { logger.debug(\"Neither XML 'id' nor 'name' specified - \" + \"using generated bean name [\" + beanName + \"]\"); } } catch (Exception ex) { error(ex.getMessage(), ele); return null; } } String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); } return null; } 上面介绍了对Bean元素进行解析的过程，也即是BeanDefinition依据XML的定义被创建的过程。 这个BeanDefinition可以看成是对定义的抽象。这个数据对象中封装的数据大多都是与定义相关的， 也有很多就是我们在定义Bean时看到的那些Spring标记，比如常见的init-method、destory-method、factory-method等等， 这个BeanDefinition数据类型非常重要，它封装了很多基本数据，这些基本数据都是IOC容器需要的。 有了这些基本数据，IOC容器才能对Bean配置进行处理，才能实现相应的容器特性。 public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) { this.parseState.push(new BeanEntry(beanName)); /** * 这里只读取定义的中设置的class名字，然后载入到BeanDefinition中去， * 只是做个记录，并不涉及对象的实例化过程，对象的实例化实际上是在依赖注入时完成的。 */ String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) { className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); } try { String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) { parent = ele.getAttribute(PARENT_ATTRIBUTE); } // 生成需要的BeanDefinition对象，为Bean定义信息的载入做准备 AbstractBeanDefinition bd = createBeanDefinition(className, parent); // 对当前的Bean元素进行属性解析，并设置description的信息 parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); // 对Bean元素信息进行解析 parseMetaElements(ele, bd); parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); // 解析的构造函数设置 parseConstructorArgElements(ele, bd); // 解析的property设置 parsePropertyElements(ele, bd); parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; } catch (ClassNotFoundException ex) { error(\"Bean class [\" + className + \"] not found\", ele, ex); } catch (NoClassDefFoundError err) { error(\"Class that bean class [\" + className + \"] depends on not found\", ele, err); } catch (Throwable ex) { error(\"Unexpected failure during bean definition parsing\", ele, ex); } finally { this.parseState.pop(); } return null; } 上面是具体生成BeanDefinition的地方。在这里举一个对property进行解析的例子来完成对整个BeanDefinition载入过程的分析，还是在类BeanDefinitionParserDelegate的代码中， 一层一层的对BeanDefinition中的定义进行解析，比如从属性元素结合到具体的每一个属性元素，然后才到具体值的处理。 根据解析结果，对这些属性值的处理会被封装成PropertyValue对象并设置到BeanDefinition对象中去。 public void parsePropertyElements(Element beanEle, BeanDefinition bd) { // 获取bean元素下定义的所有节点 NodeList nl = beanEle.getChildNodes(); for (int i = 0; i public void parsePropertyElement(Element ele, BeanDefinition bd) { // 这里取得property的名字 String propertyName = ele.getAttribute(NAME_ATTRIBUTE); if (!StringUtils.hasLength(propertyName)) { error(\"Tag 'property' must have a 'name' attribute\", ele); return; } this.parseState.push(new PropertyEntry(propertyName)); try { /** * 如果同一个Bean中已经有同名的property存在，则不进行解析，直接返回。 * 如果再同一个Bean中有同名的property设置，那么起作用的只是第一个。 */ if (bd.getPropertyValues().contains(propertyName)) { error(\"Multiple 'property' definitions for property '\" + propertyName + \"'\", ele); return; } /** * 这里是解析property值的地方，返回的对象对应Bean定义的property属性设置的解析结果， * 这个解析结果会封装到PropertyValue对象中，然后设置。 */ Object val = parsePropertyValue(ele, bd, propertyName); PropertyValue pv = new PropertyValue(propertyName, val); parseMetaElements(ele, pv); pv.setSource(extractSource(ele)); bd.getPropertyValues().addPropertyValue(pv); } finally { this.parseState.pop(); } } public Object parsePropertyValue(Element ele, BeanDefinition bd, String propertyName) { String elementName = (propertyName != null) ? \" element for property '\" + propertyName + \"'\" : \" element\"; // Should only have one child element: ref, value, list, etc. NodeList nl = ele.getChildNodes(); Element subElement = null; for (int i = 0; i 这里对property子元素的解析过程，Array、List、Set、Map、Prop等各种元素都会在这里解析，生成对应的数据对象，比如ManagedList、ManagedArray、ManagedSet等等。 这些ManagedXX类是Spring对具体的BeanDefinition的数据封装。具体的解析可以从parsePropertySubElement()关于property子元素的解析深入追踪， 可以看到parseArrayElement、parseListElement、parseSetElement、parseMapElement、parsePropsElement等方法处理。 public Object parsePropertySubElement(Element ele, BeanDefinition bd) { return parsePropertySubElement(ele, bd, null); } public Object parsePropertySubElement(Element ele, BeanDefinition bd, String defaultValueType) { if (!isDefaultNamespace(ele)) { return parseNestedCustomElement(ele, bd); } else if (nodeNameEquals(ele, BEAN_ELEMENT)) { BeanDefinitionHolder nestedBd = parseBeanDefinitionElement(ele, bd); if (nestedBd != null) { nestedBd = decorateBeanDefinitionIfRequired(ele, nestedBd, bd); } return nestedBd; } else if (nodeNameEquals(ele, REF_ELEMENT)) { // A generic reference to any name of any bean. String refName = ele.getAttribute(BEAN_REF_ATTRIBUTE); boolean toParent = false; if (!StringUtils.hasLength(refName)) { // A reference to the id of another bean in the same XML file. refName = ele.getAttribute(LOCAL_REF_ATTRIBUTE); if (!StringUtils.hasLength(refName)) { // A reference to the id of another bean in a parent context. refName = ele.getAttribute(PARENT_REF_ATTRIBUTE); toParent = true; if (!StringUtils.hasLength(refName)) { error(\"'bean', 'local' or 'parent' is required for element\", ele); return null; } } } if (!StringUtils.hasText(refName)) { error(\" element contains empty target attribute\", ele); return null; } RuntimeBeanReference ref = new RuntimeBeanReference(refName, toParent); ref.setSource(extractSource(ele)); return ref; } else if (nodeNameEquals(ele, IDREF_ELEMENT)) { return parseIdRefElement(ele); } else if (nodeNameEquals(ele, VALUE_ELEMENT)) { return parseValueElement(ele, defaultValueType); } else if (nodeNameEquals(ele, NULL_ELEMENT)) { // It's a distinguished null value. Let's wrap it in a TypedStringValue // object in order to preserve the source location. TypedStringValue nullHolder = new TypedStringValue(null); nullHolder.setSource(extractSource(ele)); return nullHolder; } else if (nodeNameEquals(ele, ARRAY_ELEMENT)) { return parseArrayElement(ele, bd); } else if (nodeNameEquals(ele, LIST_ELEMENT)) { return parseListElement(ele, bd); } else if (nodeNameEquals(ele, SET_ELEMENT)) { return parseSetElement(ele, bd); } else if (nodeNameEquals(ele, MAP_ELEMENT)) { return parseMapElement(ele, bd); } else if (nodeNameEquals(ele, PROPS_ELEMENT)) { return parsePropsElement(ele); } else { error(\"Unknown property sub-element: [\" + ele.getNodeName() + \"]\", ele); return null; } } 看看一个List这样的睡醒配置是怎样被解析的？ public List parseListElement(Element collectionEle, BeanDefinition bd) { String defaultElementType = collectionEle.getAttribute(VALUE_TYPE_ATTRIBUTE); NodeList nl = collectionEle.getChildNodes(); ManagedList target = new ManagedList(nl.getLength()); target.setSource(extractSource(collectionEle)); target.setElementTypeName(defaultElementType); target.setMergeEnabled(parseMergeAttribute(collectionEle)); // 具体的List元素解析过程 parseCollectionElements(nl, target, bd, defaultElementType); return target; } protected void parseCollectionElements( NodeList elementNodes, Collection target, BeanDefinition bd, String defaultElementType) { // 遍历所有的元素节点，并判断其类型是否为Element for (int i = 0; i 经过逐层地解析，我们在XML文件中定义的BeanDefinition就被整个载入到了IOC容器中，并在容器中建立了数据映射。 在IOC容器中建立了对应的数据结构，或者可以看成是POJO对象在IOC容器中的抽象，这些数据结构可以以AbstracBeanDefinition为入口，让IOC容器执行索引、查询和操作。 每一个简单的POJO操作的背后其实都含着一个复杂的抽象过程，经过以上的载入过程，IOC容器大致完成了管理Bean对象的数据准备工作也即是数据初始化过程。 严格来说，这个时候容器还没有起作用，要完全发挥容器的作用，还需要完成数据向容器的注册，也即IOC容器注册BeanDefinition。 BeanDefinition的注册 在BeanDefinition载入和解析这些动作完成之后，用户定义的BeanDefinition信息已经在IOC容器内建立了自己的数据结构以及 相应的数据表示，但此时这些数据不能供IOC容器直接使用，需要在IOC容器中对这些BeanDefinition 数据进行注册。这个注册为IOC容器提供了更友好的使用方式，在DefaultListableBeanFactory中， 是通过一个HashMap来持有载入的BeanDefinition的，这个HashMap的定义如下： @Override protected final void refreshBeanFactory() throws BeansException { if (hasBeanFactory()) { destroyBeans(); closeBeanFactory(); } try { //创建IoC容器，这里使用的是DefaultListableBeanFactory DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); customizeBeanFactory(beanFactory); /** * 启动对BeanDefinition的载入，这里使用了一个委派模式， * 在当前类中只定义了抽象的loadBeanDefinitions方法，具体的实现调用子类容器 */ loadBeanDefinitions(beanFactory); this.beanFactory = beanFactory; } catch (IOException ex) { throw new ApplicationContextException(\"I/O error parsing bean definition source for \" + getDisplayName(), ex); } } private final Map beanDefinitionMap = new ConcurrentHashMap(256); 下面分析BeanDefinition如何注册到HashMap的。 从上一章节的BeanDefinition()处理开始看看IOC容器如何注册BeanDefinition。 protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) { /** * BeanDefinitionHolder是对BeanDefinition对象的封装，封装了BeanDefinition、Bean的名字和别名。 * 用来完成想IOC容器注册。得到BeanDefinitionHolder就意味着是通过BeanDefinitionParserDelegate * 对XML元素的信息按照Spring的Bean规则进行解析得到的。 * */ BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) { bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try { // Register the final decorated instance. // 这里是向IOC容器注册解析得到的BeanDefinition的地方 BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); } catch (BeanDefinitionStoreException ex) { getReaderContext().error(\"Failed to register bean definition with name '\" + bdHolder.getBeanName() + \"'\", ele, ex); } // Send registration event. // 在BeanDefinition向IOC容器注册以后，发送消息 getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); } } BeanDefinition注册在BeanDefinitionReaderUtils.registerBeanDefinition()方法中完成。 public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException { // Register bean definition under primary name. // 根据定义的唯一的beanName注册Beandefinition String beanName = definitionHolder.getBeanName(); registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // Register aliases for bean name, if any. // 如果有别名，注册别名 String[] aliases = definitionHolder.getAliases(); if (aliases != null) { for (String alias : aliases) { registry.registerAlias(beanName, alias); } } } 注册registerBeanDefinition()方法的具体实现在BeanDefinitionRegistry接口的实现类DefaultListableBeanFactory中来完成。 @Override public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException { // BeanName和BeanDefinition不能为空，否则停止注册 Assert.hasText(beanName, \"Bean name must not be empty\"); Assert.notNull(beanDefinition, \"BeanDefinition must not be null\"); if (beanDefinition instanceof AbstractBeanDefinition) { try { ((AbstractBeanDefinition) beanDefinition).validate(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \"Validation of bean definition failed\", ex); } } BeanDefinition oldBeanDefinition; // 检查是否有相同名字的BeanDefinition已经在IOC容器中注册了，如果有同名的BeanDefinition， // 但又不允许覆盖，就会抛出异常，否则覆盖BeanDefinition。 oldBeanDefinition = this.beanDefinitionMap.get(beanName); if (oldBeanDefinition != null) { if (!isAllowBeanDefinitionOverriding()) { throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \"Cannot register bean definition [\" + beanDefinition + \"] for bean '\" + beanName + \"': There is already [\" + oldBeanDefinition + \"] bound.\"); } else if (oldBeanDefinition.getRole() updatedDefinitions = new ArrayList(this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; if (this.manualSingletonNames.contains(beanName)) { Set updatedSingletons = new LinkedHashSet(this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; } } } else {// 正在启动注册阶段，容器这个时候还是空的。 // Still in startup registration phase this.beanDefinitionMap.put(beanName, beanDefinition); this.beanDefinitionNames.add(beanName); this.manualSingletonNames.remove(beanName); } this.frozenBeanDefinitionNames = null; } // 重置所有已经注册过的BeanDefinition或单例模式的BeanDefinition的缓存 if (oldBeanDefinition != null || containsSingleton(beanName)) { resetBeanDefinition(beanName); } } 到这里完成了BeanDefinition的注册，就算完了IOC容器的初始化过程。 此时，在使用的IOC容器DefaultListableBeanFactory中已经建立了整个Bean的配置信息，而且这些BeanDefinition已经可以被容器使用了， 它们都在beanDefinitionMap里被检索和使用。容器的作用就是对这些信息进行处理和维护。 这些信息就是容器建立依赖反转的基础，有了这些基础数据，就可以进一步完成依赖注入， 下一篇讨论依赖注入的实现原理Bean的创建，Bean依赖注入。 "},"Chapter11/BeanObjectCreation.html":{"url":"Chapter11/BeanObjectCreation.html","title":"Bean对象的创建","keywords":"","body":"Bean对象的创建 Bean对象创建概要 容器初始化的工作主要是在IOC容器中建立了BeanDefinition数据映射，并通过HashMap持有数据，BeanDefinition都在beanDefinitionMap里被检索和使用。 在IOC容器BeanFactory中，有一个getBean的接口定义，通过这个接口实现可以获取到Bean对象。但是，这个Bean对象并不是一个普通的Bean对象，它是一个处理完依赖关系后的Bean对象。 所以一个getBean()实现里面，分为两个大步骤来处理返回用户需要的Bean对象： 根据BeanDefinition创建Bean对象，也即Bean对象的创建。 创建出来的Bean是一个还没有建立依赖关系的Bean，所有需要完成依赖关系建立，叫做Bean依赖注入。 本文先分析如何根据BeanDefinition数据结构创建用户需要的Bean，并且搞清楚Bean的创建时机，因为有些人说Bean在第一次使用时进行创建的，有些人又说在IOC容器初始化的时候就给创建好了，然而并不都对。 Bean的创建时机分为两大类： 非抽象，并且单例，并且非懒加载（Spring Bean默认单例Singleton，非懒加载）的对象是在IOC容器初始化时通过refresh()#finishBeanFactoryInitialization()完成创建的，创建完后放在本地缓存里面，用的时候直接取即可，这么做是因为在初始化的时候，可能就需要使用Bean，同时，可以提高使用时获取的效率； 初始化时创建的Bean放在Map里面，private final Map singletonObjects = new ConcurrentHashMap(256);使用时直接从Map缓存获取。 而非单例或懒加载对象都是在第一次使用时，getBean()的时候创建的；如果想在使用时才进行初始化，可以设置@Scope(\"prototype\")为原型模式或者加上@Lazy默认是true懒加载。 创建Bean 在IOC容器初始化的refresh()方法第11大步，有个独立的方法，就是处理Bean创建的，即bean实例化处理。 refresh()#finishBeanFactoryInitialization()源码： protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) { // Initialize conversion service for this context. // 为Bean工厂设置类型转化器 if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) && beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) { beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); } // Register a default embedded value resolver if no bean post-processor // (such as a PropertyPlaceholderConfigurer bean) registered any before: // at this point, primarily for resolution in annotation attribute values. if (!beanFactory.hasEmbeddedValueResolver()) { beanFactory.addEmbeddedValueResolver(new StringValueResolver() { @Override public String resolveStringValue(String strVal) { return getEnvironment().resolvePlaceholders(strVal); } }); } // Initialize LoadTimeWeaverAware beans early to allow for registering their transformers early. String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) { getBean(weaverAwareName); } // Stop using the temporary ClassLoader for type matching. beanFactory.setTempClassLoader(null); // Allow for caching all bean definition metadata, not expecting further changes. //冻结所有的Bean定义 ， 至此注册的Bean定义将不被修改或任何进一步的处理 beanFactory.freezeConfiguration(); // Instantiate all remaining (non-lazy-init) singletons. 初始化时创建Bean的入口 beanFactory.preInstantiateSingletons(); } DefaultListableBeanFactory#preInstantiateSingletons()源码： @Override public void preInstantiateSingletons() throws BeansException { if (this.logger.isDebugEnabled()) { this.logger.debug(\"Pre-instantiating singletons in \" + this); } // Iterate over a copy to allow for init methods which in turn register new bean definitions. // While this may not be part of the regular factory bootstrap, it does otherwise work fine. //获取我们容器中所有Bean定义的名称 List beanNames = new ArrayList(this.beanDefinitionNames); // Trigger initialization of all non-lazy singleton beans... for (String beanName : beanNames) { //合并我们的bean定义 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); //非抽象，单例、非懒加载才会进入if逻辑 if (!bd.isAbstract() && bd.isSingleton() && !bd.isLazyInit()) { if (isFactoryBean(beanName)) { //是 给beanName+前缀 & 符号 final FactoryBean factory = (FactoryBean) getBean(FACTORY_BEAN_PREFIX + beanName); boolean isEagerInit; if (System.getSecurityManager() != null && factory instanceof SmartFactoryBean) { isEagerInit = AccessController.doPrivileged(new PrivilegedAction() { @Override public Boolean run() { return ((SmartFactoryBean) factory).isEagerInit(); } }, getAccessControlContext()); } else { isEagerInit = (factory instanceof SmartFactoryBean && ((SmartFactoryBean) factory).isEagerInit()); } if (isEagerInit) { //调用真正的getBean getBean(beanName); } } else { // 在这里，调用getBean()进行bean实例创建 //非工厂Bean就是普通的bean getBean(beanName); } } } // Trigger post-initialization callback for all applicable beans... 获取所有的bean的名称 至此所有的单实例的bean已经加入到单实例Bean的缓存池中，所谓的单实例缓存池实际上就是一个ConcurrentHashMap for (String beanName : beanNames) { //从单例缓存池中获取所有的对象 Object singletonInstance = getSingleton(beanName); //判断当前的bean是否实现了SmartInitializingSingleton接口 if (singletonInstance instanceof SmartInitializingSingleton) { final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) { AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { smartSingleton.afterSingletonsInstantiated(); return null; } }, getAccessControlContext()); } else { //触发实例化之后的方法afterSingletonsInstantiated smartSingleton.afterSingletonsInstantiated(); } } } } 接下来真正创建Bean的入口就是getBean()，跟你第一次使用Bean的getBean()是一个入口，只是一个在初始化时调用，一个在第一次使用时调用，所以，是一套代码，原理一样。 下面从就从BeanFactory入手去看getBean()的实现。 package org.springframework.beans.factory; import org.springframework.beans.BeansException; import org.springframework.core.ResolvableType; public interface BeanFactory { String FACTORY_BEAN_PREFIX = \"&\"; Object getBean(String name) throws BeansException; T getBean(String name, Class requiredType) throws BeansException; T getBean(Class requiredType) throws BeansException; Object getBean(String name, Object... args) throws BeansException; T getBean(Class requiredType, Object... args) throws BeansException; boolean containsBean(String name); boolean isSingleton(String name) throws NoSuchBeanDefinitionException; boolean isPrototype(String name) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException; boolean isTypeMatch(String name, Class typeToMatch) throws NoSuchBeanDefinitionException; Class getType(String name) throws NoSuchBeanDefinitionException; String[] getAliases(String name); } 从getBean(String name)最简单明了的方法入手看实现，该方法在很多类中有实现，重点研究AbstractBeanFactory中的实现方法。 @Override public Object getBean(String name) throws BeansException { //真正的获取Bean的逻辑 return doGetBean(name, null, null, false); } protected T doGetBean( final String name, final Class requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException { //在这里传入进来的name可能是别名、也有可能是工厂beanName,所以在这里需要转换 final String beanName = transformedBeanName(name); Object bean; // Eagerly check singleton cache for manually registered singletons. // 先从缓存中获得Bean，处理那些已经被创建过的单例模式的Bean,对这种Bean的请求不需要重复地创建 Object sharedInstance = getSingleton(beanName); if (sharedInstance != null && args == null) { if (logger.isDebugEnabled()) { if (isSingletonCurrentlyInCreation(beanName)) { logger.debug(\"Returning eagerly cached instance of singleton bean '\" + beanName + \"' that is not fully initialized yet - a consequence of a circular reference\"); } else { logger.debug(\"Returning cached instance of singleton bean '\" + beanName + \"'\"); } } // 这里的getObjectForBeanInstance完成的是FactoryBean的相关处理，以取得FactoryBean的生产结果 /** * 如果sharedInstance是普通的单例bean，下面的方法会直接返回。但如果 * sharedInstance是FactoryBean类型的，则需调用getObject工厂方法获取真正的 * bean实例。如果用户想获取 FactoryBean 本身，这里也不会做特别的处理，直接返回 * 即可。毕竟 FactoryBean 的实现类本身也是一种 bean，只不过具有一点特殊的功能而已。 */ bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { // Fail if we're already creating this bean instance: // We're assumably within a circular reference. //Spring只能解决单例对象的setter注入的循环依赖,不能解决构造器注入，也不能解决多实例的循环依赖 if (isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } // Check if bean definition exists in this factory. /** * 对IOC容器中的BeanDefinition是否存在进行检查，检查是否能在当前的BeanFactory中取得需要的Bean。 * 如果在当前的工厂中取不到，则到双亲BeanFactory中去取； * 如果当前的双亲工厂取不到，就顺着双亲BeanFactory链一直向上查找 */ //判断是否有父工厂 BeanFactory parentBeanFactory = getParentBeanFactory(); //若存在父工厂,切当前的bean工厂不存在当前的bean定义,那么bean定义是存在于父beanFactory中 if (parentBeanFactory != null && !containsBeanDefinition(beanName)) { // Not found -> check parent. //获取bean的原始名称 String nameToLookup = originalBeanName(name); if (args != null) { // Delegation to parent with explicit args. // 委托给构造函数getBean()处理 return (T) parentBeanFactory.getBean(nameToLookup, args); } else { // 没有args，委托给标准的getBean()处理 // No args -> delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); } } /** * 方法参数typeCheckOnly ，是用来判断调用getBean(...) 方法时，表示是否为仅仅进行类型检查获取Bean对象 * 如果不是仅仅做类型检查，而是创建Bean对象，则需要调用markBeanAsCreated(String beanName) 方法，进行记录 */ if (!typeCheckOnly) { markBeanAsCreated(beanName); } try { // 根据Bean的名字获取BeanDefinition //从容器中获取beanName相应的GenericBeanDefinition对象，并将其转换为RootBeanDefinition对象 final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); //检查当前创建的bean定义是不是抽象的bean定义 checkMergedBeanDefinition(mbd, beanName, args); // Guarantee initialization of beans that the current bean depends on. // 获取当前Bean的所有依赖Bean，这样会触发getBean的递归调用，直到渠道一个没有任何依赖的Bean为止 //处理dependsOn的依赖(这个不是我们所谓的循环依赖 而是bean创建前后的依赖) //依赖bean的名称 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { for (String dep : dependsOn) { //beanName是当前正在创建的bean,dep是正在创建的bean的依赖的bean的名称 if (isDependent(beanName, dep)) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Circular depends-on relationship between '\" + beanName + \"' and '\" + dep + \"'\"); } //保存的是依赖beanName之间的映射关系：依赖beanName -> beanName的集合 registerDependentBean(dep, beanName); //获取dependsOn的bean getBean(dep); } } // Create bean instance. // 以下是创建Bean实例 // 创建sigleton bean if (mbd.isSingleton()) { //把beanName和一个singletonFactory匿名内部类传入用于回调 sharedInstance = getSingleton(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { try { //创建bean的逻辑 return createBean(beanName, mbd, args); } catch (BeansException ex) { // Explicitly remove instance from singleton cache: It might have been put there // eagerly by the creation process, to allow for circular reference resolution. // Also remove any beans that received a temporary reference to the bean. //创建bean的过程中发生异常,需要销毁关于当前bean的所有信息 destroySingleton(beanName); throw ex; } } }); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } // 创建prototype bean else if (mbd.isPrototype()) { // It's a prototype -> create a new instance. Object prototypeInstance = null; try { beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); } else { String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) { throw new IllegalStateException(\"No Scope registered for scope name '\" + scopeName + \"'\"); } try { Object scopedInstance = scope.get(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { beforePrototypeCreation(beanName); try { return createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } } }); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); } catch (IllegalStateException ex) { throw new BeanCreationException(beanName, \"Scope '\" + scopeName + \"' is not active for the current thread; consider \" + \"defining a scoped proxy for this bean if you intend to refer to it from a singleton\", ex); } } } catch (BeansException ex) { cleanupAfterBeanCreationFailure(beanName); throw ex; } } // Check if required type matches the type of the actual bean instance. // 对创建的Bean进行类型检查，如果没有问题，就返回这个新创建的Bean，这个Bean已经是包含依赖关系的Bean if (requiredType != null && bean != null && !requiredType.isAssignableFrom(bean.getClass())) { try { return getTypeConverter().convertIfNecessary(bean, requiredType); } catch (TypeMismatchException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Failed to convert bean '\" + name + \"' to required type '\" + ClassUtils.getQualifiedName(requiredType) + \"'\", ex); } throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); } } return (T) bean; } getBean()方法只是创建对象的起点，doGetBean()只是getBean()的具体执行，在doGetBean()中会调用createBean()方法，在这个过程中，Bean对象会依据BeanDefinition定义的要求生成。 AbstractBeanFactory中的createBean是一个抽象方法，具体的实现在AbstractAutowireCapableBeanFactory中。 @Override protected Object createBean(String beanName, RootBeanDefinition mbd, Object[] args) throws BeanCreationException { if (logger.isDebugEnabled()) { logger.debug(\"Creating instance of bean '\" + beanName + \"'\"); } RootBeanDefinition mbdToUse = mbd; // Make sure bean class is actually resolved at this point, and // clone the bean definition in case of a dynamically resolved Class // which cannot be stored in the shared merged bean definition. // 判断需要创建的Bean是否可以实例化， Class resolvedClass = resolveBeanClass(mbd, beanName); if (resolvedClass != null && !mbd.hasBeanClass() && mbd.getBeanClassName() != null) { mbdToUse = new RootBeanDefinition(mbd); mbdToUse.setBeanClass(resolvedClass); } // Prepare method overrides. try { mbdToUse.prepareMethodOverrides(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(mbdToUse.getResourceDescription(), beanName, \"Validation of method overrides failed\", ex); } try { // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. // 如果Bean配置了PostProcessor，则返回一个proxy代理对象 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) { return bean; } } catch (Throwable ex) { throw new BeanCreationException(mbdToUse.getResourceDescription(), beanName, \"BeanPostProcessor before instantiation of bean failed\", ex); } // 创建Bean的调用 Object beanInstance = doCreateBean(beanName, mbdToUse, args); if (logger.isDebugEnabled()) { logger.debug(\"Finished creating instance of bean '\" + beanName + \"'\"); } return beanInstance; } protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException { // Instantiate the bean. // 这个BeanWrapper是用来持有创建出来的Bean对象 //BeanWrapper是对Bean的包装，其接口中所定义的功能很简单包括设置获取被包装的对象，获取被包装bean的属性描述器 BeanWrapper instanceWrapper = null; // 如果是Singleton，先把缓存中的同名Bean清除 if (mbd.isSingleton()) { //从没有完成的FactoryBean中移除 instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } // 通过createBeanInstance()创建Bean if (instanceWrapper == null) { //使用合适的实例化策略来创建新的实例：工厂方法、构造函数自动注入、简单初始化 比较复杂也很重要 instanceWrapper = createBeanInstance(beanName, mbd, args); } //从beanWrapper中获取我们的早期对象 final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); Class beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); mbd.resolvedTargetType = beanType; // Allow post-processors to modify the merged bean definition. synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { //进行后置处理@AutoWired的注解的预解析 applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Post-processing of merged bean definition failed\", ex); } mbd.postProcessed = true; } } // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. /** * 该对象进行判断是否能够暴露早期对象的条件 * 单实例 this.allowCircularReferences 默认为true * isSingletonCurrentlyInCreation(表示当前的bean对象正在创建singletonsCurrentlyInCreation包含当前正在创建的bean) */ boolean earlySingletonExposure = (mbd.isSingleton() && this.allowCircularReferences && isSingletonCurrentlyInCreation(beanName)); //上述条件满足，允许中期暴露对象 if (earlySingletonExposure) { if (logger.isDebugEnabled()) { logger.debug(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); } //把我们的早期对象包装成一个singletonFactory对象 该对象提供了一个getObject方法,该方法内部调用getEarlyBeanReference方法 addSingletonFactory(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { return getEarlyBeanReference(beanName, mbd, bean); } }); } // Initialize the bean instance. // 对Bean进行初始化，这个exposedObject在初始化以后会返回作为依赖注入完成后的Bean Object exposedObject = bean; try { //给我们的属性进行赋值(调用set方法进行赋值) populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) { //进行对象初始化操作(在这里可能生成代理对象) exposedObject = initializeBean(beanName, exposedObject, mbd); } } catch (Throwable ex) { if (ex instanceof BeanCreationException && beanName.equals(((BeanCreationException) ex).getBeanName())) { throw (BeanCreationException) ex; } else { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Initialization of bean failed\", ex); } } //允许早期对象的引用 if (earlySingletonExposure) { /** * 去缓存中获取到我们的对象 由于传递的allowEarlyReference 是false 要求只能在一级二级缓存中去获取 * 正常普通的bean(不存在循环依赖的bean) 创建的过程中，压根不会把三级缓存提升到二级缓存中 */ Object earlySingletonReference = getSingleton(beanName, false); //能够获取到 if (earlySingletonReference != null) { //经过后置处理的bean和早期的bean引用还相等的话(表示当前的bean没有被代理过) if (exposedObject == bean) { exposedObject = earlySingletonReference; } //处理依赖的bean else if (!this.allowRawInjectionDespiteWrapping && hasDependentBean(beanName)) { String[] dependentBeans = getDependentBeans(beanName); Set actualDependentBeans = new LinkedHashSet(dependentBeans.length); for (String dependentBean : dependentBeans) { if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) { actualDependentBeans.add(dependentBean); } } if (!actualDependentBeans.isEmpty()) { throw new BeanCurrentlyInCreationException(beanName, \"Bean with name '\" + beanName + \"' has been injected into other beans [\" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + \"] in its raw version as part of a circular reference, but has eventually been \" + \"wrapped. This means that said other beans do not use the final version of the \" + \"bean. This is often the result of over-eager type matching - consider using \" + \"'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\"); } } } } // Register bean as disposable. try { //注册销毁的bean的销毁接口 registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Invalid destruction signature\", ex); } return exposedObject; } 看下真正创建Bean对象的方法createBeanInstance()，该方法会生成包含Java对象的Bean，这个Bean生成有很多中方式，可以通过工厂方法生成，也可以通过容器的Autowire特性生成， 这些生成方式都是由相关的BeanDefinition来指定的。看下以下正在创建对象的源码。 protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) { // Make sure bean class is actually resolved at this point. // 确认需要创建的Bean实例的类可以实例化 //从bean定义中解析出当前bean的class对象 Class beanClass = resolveBeanClass(mbd, beanName); // 以下通过工厂方法对Bean进行实例化 //检测类的访问权限。默认情况下，对于非 public 的类，是允许访问的。若禁止访问，这里会抛出异常 if (beanClass != null && !Modifier.isPublic(beanClass.getModifiers()) && !mbd.isNonPublicAccessAllowed()) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Bean class isn't public, and non-public access not allowed: \" + beanClass.getName()); } //工厂方法,我们通过配置类来进行配置的话 采用的就是工厂方法 if (mbd.getFactoryMethodName() != null) { return instantiateUsingFactoryMethod(beanName, mbd, args); } // Shortcut when re-creating the same bean... // 重新创建Bean的快捷方式 //判断当前构造函数是否被解析过 boolean resolved = false; //有没有必须进行依赖注入 boolean autowireNecessary = false; /** * 通过getBean传入进来的构造函数是否来指定需要推断构造函数 * 若传递进来的args不为空，那么就可以直接选出对应的构造函数 */ if (args == null) { //判断我们的bean定义信息中的resolvedConstructorOrFactoryMethod(用来缓存我们的已经解析的构造函数或者工厂方法) synchronized (mbd.constructorArgumentLock) { if (mbd.resolvedConstructorOrFactoryMethod != null) { //修改已经解析过的构造函数的标志 resolved = true; //修改标记为ture 标识构造函数或者工厂方法已经解析过 autowireNecessary = mbd.constructorArgumentsResolved; } } } //若被解析过 if (resolved) { if (autowireNecessary) { //通过有参的构造函数进行反射调用 return autowireConstructor(beanName, mbd, null, null); } else { //调用无参数的构造函数进行创建对象 return instantiateBean(beanName, mbd); } } // Need to determine the constructor... // 以下使用构造函数对Bean进行实例化 //通过bean的后置处理器进行选举出合适的构造函数对象 Constructor[] ctors = determineConstructorsFromBeanPostProcessors(beanClass, beanName); //通过后置处理器解析出构造器对象不为null或获取bean定义中的注入模式是构造器注入或bean定义信息ConstructorArgumentValues或获取通过getBean的方式传入的构造器函数参数类型不为null if (ctors != null || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_CONSTRUCTOR || mbd.hasConstructorArgumentValues() || !ObjectUtils.isEmpty(args)) { //通过构造函数创建对象 return autowireConstructor(beanName, mbd, ctors, args); } // No special handling: simply use no-arg constructor. // 使用默认的构造器函数对Bean进行实例化 //使用无参数的构造函数调用创建对象 return instantiateBean(beanName, mbd); } instantiateBean()方法采用默认构造器实例化bean的过程。 protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) { try { Object beanInstance; final BeanFactory parent = this; if (System.getSecurityManager() != null) { beanInstance = AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { return getInstantiationStrategy().instantiate(mbd, beanName, parent); } }, getAccessControlContext()); } else { beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); } BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw; } catch (Throwable ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Instantiation of bean failed\", ex); } } 类中使用默认的实例化策略进行实例化，默认采用CGLIB对Bean进行实例化。CGLIB是一个常用的字节码生成器的类库，它提供了一些列的API来提供生成和转换Java的字节码的功能。 @Override public Object instantiate(RootBeanDefinition bd, String beanName, BeanFactory owner) { // Don't override the class with CGLIB if no overrides. if (bd.getMethodOverrides().isEmpty()) { // 获取指定的构造器或者生产对象工厂方法来对Bean进行实例化 Constructor constructorToUse; synchronized (bd.constructorArgumentLock) { constructorToUse = (Constructor) bd.resolvedConstructorOrFactoryMethod; if (constructorToUse == null) { final Class clazz = bd.getBeanClass(); if (clazz.isInterface()) { throw new BeanInstantiationException(clazz, \"Specified class is an interface\"); } try { if (System.getSecurityManager() != null) { constructorToUse = AccessController.doPrivileged(new PrivilegedExceptionAction>() { @Override public Constructor run() throws Exception { return clazz.getDeclaredConstructor((Class[]) null); } }); } else { constructorToUse = clazz.getDeclaredConstructor((Class[]) null); } bd.resolvedConstructorOrFactoryMethod = constructorToUse; } catch (Throwable ex) { throw new BeanInstantiationException(clazz, \"No default constructor found\", ex); } } } // 通过BeanUtils进行实例化，这个BeanUtils实例化通过Constructor来实例化Bean， // 在BeanUtils中可以看到具体的调用ctor.newInstance(args) return BeanUtils.instantiateClass(constructorToUse); } else { // Must generate CGLIB subclass. // 使用CGLIB进行实例化 return instantiateWithMethodInjection(bd, beanName, owner); } } public static T instantiateClass(Constructor ctor, Object... args) throws BeanInstantiationException { Assert.notNull(ctor, \"Constructor must not be null\"); try { ReflectionUtils.makeAccessible(ctor); return ctor.newInstance(args); } catch (InstantiationException ex) { throw new BeanInstantiationException(ctor, \"Is it an abstract class?\", ex); } catch (IllegalAccessException ex) { throw new BeanInstantiationException(ctor, \"Is the constructor accessible?\", ex); } catch (IllegalArgumentException ex) { throw new BeanInstantiationException(ctor, \"Illegal arguments for constructor\", ex); } catch (InvocationTargetException ex) { throw new BeanInstantiationException(ctor, \"Constructor threw exception\", ex.getTargetException()); } } 如果感兴趣，可以一直追溯newInstance()方法，最后调用一个Native方法创建对象。 到此，Bean对象创建完成。 "},"Chapter11/BeanDependencyInjection.html":{"url":"Chapter11/BeanDependencyInjection.html","title":"Bean依赖注入","keywords":"","body":"Bean依赖注入 从上文“Bean对象的创建”中提到过，BeanFactory中getBean方法不仅是bean创建分析的入口，也是依赖注入分析的入口，因为只有完成这两个步，才能返回给用户一个预期的Bean对象。 回顾一下上文中的AbstractAutowireCapableBeanFactory.doCreateBean()方法源码： protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException { // Instantiate the bean. // 这个BeanWrapper是用来持有创建出来的Bean对象 BeanWrapper instanceWrapper = null; // 如果是Singleton，先把缓存中的同名Bean清除 if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } // 通过createBeanInstance()创建Bean if (instanceWrapper == null) { instanceWrapper = createBeanInstance(beanName, mbd, args); } final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); Class beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); mbd.resolvedTargetType = beanType; // Allow post-processors to modify the merged bean definition. synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Post-processing of merged bean definition failed\", ex); } mbd.postProcessed = true; } } // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. boolean earlySingletonExposure = (mbd.isSingleton() && this.allowCircularReferences && isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { if (logger.isDebugEnabled()) { logger.debug(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); } addSingletonFactory(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { return getEarlyBeanReference(beanName, mbd, bean); } }); } // Initialize the bean instance. // 对Bean进行初始化，这个exposedObject在初始化以后会返回座位依赖注入完成后的Bean Object exposedObject = bean; try { populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) { exposedObject = initializeBean(beanName, exposedObject, mbd); } } catch (Throwable ex) { if (ex instanceof BeanCreationException && beanName.equals(((BeanCreationException) ex).getBeanName())) { throw (BeanCreationException) ex; } else { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Initialization of bean failed\", ex); } } if (earlySingletonExposure) { Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) { if (exposedObject == bean) { exposedObject = earlySingletonReference; } else if (!this.allowRawInjectionDespiteWrapping && hasDependentBean(beanName)) { String[] dependentBeans = getDependentBeans(beanName); Set actualDependentBeans = new LinkedHashSet(dependentBeans.length); for (String dependentBean : dependentBeans) { if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) { actualDependentBeans.add(dependentBean); } } if (!actualDependentBeans.isEmpty()) { throw new BeanCurrentlyInCreationException(beanName, \"Bean with name '\" + beanName + \"' has been injected into other beans [\" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + \"] in its raw version as part of a circular reference, but has eventually been \" + \"wrapped. This means that said other beans do not use the final version of the \" + \"bean. This is often the result of over-eager type matching - consider using \" + \"'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\"); } } } } // Register bean as disposable. try { registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Invalid destruction signature\", ex); } return exposedObject; } 这个doCreateBean()中有两个方法，在上文已经从createBeanInstance()方法分析了Bean的创建过程，还有另外一个populateBean()方法，这个是依赖注入的分析入口。 protected void populateBean(String beanName, RootBeanDefinition mbd, BeanWrapper bw) { // 获取BeanDefinition中设置的property值 PropertyValues pvs = mbd.getPropertyValues(); if (bw == null) { if (!pvs.isEmpty()) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Cannot apply property values to null instance\"); } else { // Skip property population phase for null instance. return; } } // Give any InstantiationAwareBeanPostProcessors the opportunity to modify the // state of the bean before properties are set. This can be used, for example, // to support styles of field injection. boolean continueWithPropertyPopulation = true; if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof InstantiationAwareBeanPostProcessor) { InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) { continueWithPropertyPopulation = false; break; } } } } if (!continueWithPropertyPopulation) { return; } // 开始进行依赖注入过程，先处理autowire的注入 if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) { MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // Add property values based on autowire by name if applicable. // 根据Bean的名字，完成Bean的autowire if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) { autowireByName(beanName, mbd, bw, newPvs); } // Add property values based on autowire by type if applicable. // 根据Bean的类型，完成Bean的autowire if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) { autowireByType(beanName, mbd, bw, newPvs); } pvs = newPvs; } boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) { PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof InstantiationAwareBeanPostProcessor) { InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) { return; } } } } if (needsDepCheck) { checkDependencies(beanName, mbd, filteredPds, pvs); } } // 对属性进行注入 applyPropertyValues(beanName, mbd, bw, pvs); } 看这块代码有几点我们要明确： 后面所讲的内容全部是bean在xml中的定义的内容，我们平时用的@Resource @Autowired并不是在这里解析的，那些属于Spring注解的内容。 2。 这里的autowire跟@Autowired不一样，autowire是Spring配置文件中的一个配置，@Autowired是一个注解。 一般Spring不建议autowire的配置。 下面分析applyPropertyValues()方法实现了属性注入的解析过程。 protected void applyPropertyValues(String beanName, BeanDefinition mbd, BeanWrapper bw, PropertyValues pvs) { if (pvs == null || pvs.isEmpty()) { return; } MutablePropertyValues mpvs = null; List original; if (System.getSecurityManager() != null) { if (bw instanceof BeanWrapperImpl) { ((BeanWrapperImpl) bw).setSecurityContext(getAccessControlContext()); } } if (pvs instanceof MutablePropertyValues) { mpvs = (MutablePropertyValues) pvs; if (mpvs.isConverted()) { // Shortcut: use the pre-converted values as-is. try { bw.setPropertyValues(mpvs); return; } catch (BeansException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Error setting property values\", ex); } } original = mpvs.getPropertyValueList(); } else { original = Arrays.asList(pvs.getPropertyValues()); } TypeConverter converter = getCustomTypeConverter(); if (converter == null) { converter = bw; } // BeanDefinition在BeanDefinitionValueResolver中完成 BeanDefinitionValueResolver valueResolver = new BeanDefinitionValueResolver(this, beanName, mbd, converter); // Create a deep copy, resolving any references for values. // 为解析值创建一个副本，副本的数据将会被注入到Bean中 List deepCopy = new ArrayList(original.size()); boolean resolveNecessary = false; for (PropertyValue pv : original) { if (pv.isConverted()) { deepCopy.add(pv); } else { String propertyName = pv.getName(); Object originalValue = pv.getValue(); Object resolvedValue = valueResolver.resolveValueIfNecessary(pv, originalValue); Object convertedValue = resolvedValue; boolean convertible = bw.isWritableProperty(propertyName) && !PropertyAccessorUtils.isNestedOrIndexedProperty(propertyName); if (convertible) { convertedValue = convertForProperty(resolvedValue, propertyName, bw, converter); } // Possibly store converted value in merged bean definition, // in order to avoid re-conversion for every created bean instance. if (resolvedValue == originalValue) { if (convertible) { pv.setConvertedValue(convertedValue); } deepCopy.add(pv); } else if (convertible && originalValue instanceof TypedStringValue && !((TypedStringValue) originalValue).isDynamic() && !(convertedValue instanceof Collection || ObjectUtils.isArray(convertedValue))) { pv.setConvertedValue(convertedValue); deepCopy.add(pv); } else { resolveNecessary = true; deepCopy.add(new PropertyValue(pv, convertedValue)); } } } if (mpvs != null && !resolveNecessary) { mpvs.setConverted(); } // Set our (possibly massaged) deep copy. try { // 这里是依赖注入放生的地方，将副本依赖注入，具体实现在BeanWrapperImpl中完成 bw.setPropertyValues(new MutablePropertyValues(deepCopy)); } catch (BeansException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Error setting property values\", ex); } } 通过使用BeanDefinitionValueResolver来对BeanDefinition进行解析，然后注入到property中。 下面分析下BeanDefinitionValueResolver中如何解析，以对Bean Reference进行解析为例。 BeanDefinitionValueResolver.resolveValueIfNecessary()方法源码： public Object resolveValueIfNecessary(Object argName, Object value) { // We must check each value to see whether it requires a runtime reference // to another bean to be resolved. // 检查每个值，看看它是否需要一个运行时引用来解析另一个bean if (value instanceof RuntimeBeanReference) { RuntimeBeanReference ref = (RuntimeBeanReference) value; return resolveReference(argName, ref); } else if (value instanceof RuntimeBeanNameReference) { String refName = ((RuntimeBeanNameReference) value).getBeanName(); refName = String.valueOf(doEvaluate(refName)); if (!this.beanFactory.containsBean(refName)) { throw new BeanDefinitionStoreException( \"Invalid bean name '\" + refName + \"' in bean reference for \" + argName); } return refName; } else if (value instanceof BeanDefinitionHolder) { // Resolve BeanDefinitionHolder: contains BeanDefinition with name and aliases. BeanDefinitionHolder bdHolder = (BeanDefinitionHolder) value; return resolveInnerBean(argName, bdHolder.getBeanName(), bdHolder.getBeanDefinition()); } else if (value instanceof BeanDefinition) { // Resolve plain BeanDefinition, without contained name: use dummy name. BeanDefinition bd = (BeanDefinition) value; String innerBeanName = \"(inner bean)\" + BeanFactoryUtils.GENERATED_BEAN_NAME_SEPARATOR + ObjectUtils.getIdentityHexString(bd); return resolveInnerBean(argName, innerBeanName, bd); } // 对ManagedArray进行解析 else if (value instanceof ManagedArray) { // May need to resolve contained runtime references. ManagedArray array = (ManagedArray) value; Class elementType = array.resolvedElementType; if (elementType == null) { String elementTypeName = array.getElementTypeName(); if (StringUtils.hasText(elementTypeName)) { try { elementType = ClassUtils.forName(elementTypeName, this.beanFactory.getBeanClassLoader()); array.resolvedElementType = elementType; } catch (Throwable ex) { // Improve the message by showing the context. throw new BeanCreationException( this.beanDefinition.getResourceDescription(), this.beanName, \"Error resolving array type for \" + argName, ex); } } else { elementType = Object.class; } } return resolveManagedArray(argName, (List) value, elementType); } // 对ManagedList进行解析 else if (value instanceof ManagedList) { // May need to resolve contained runtime references. return resolveManagedList(argName, (List) value); } // 对ManagedSet进行解析 else if (value instanceof ManagedSet) { // May need to resolve contained runtime references. return resolveManagedSet(argName, (Set) value); } // 对ManagedMap进行解析 else if (value instanceof ManagedMap) { // May need to resolve contained runtime references. return resolveManagedMap(argName, (Map) value); } // 对ManagedProperties进行解析 else if (value instanceof ManagedProperties) { Properties original = (Properties) value; Properties copy = new Properties(); for (Map.Entry propEntry : original.entrySet()) { Object propKey = propEntry.getKey(); Object propValue = propEntry.getValue(); if (propKey instanceof TypedStringValue) { propKey = evaluate((TypedStringValue) propKey); } if (propValue instanceof TypedStringValue) { propValue = evaluate((TypedStringValue) propValue); } copy.put(propKey, propValue); } return copy; } // 对TypedStringValue进行解析 else if (value instanceof TypedStringValue) { // Convert value to target type here. TypedStringValue typedStringValue = (TypedStringValue) value; Object valueObject = evaluate(typedStringValue); try { Class resolvedTargetType = resolveTargetType(typedStringValue); if (resolvedTargetType != null) { return this.typeConverter.convertIfNecessary(valueObject, resolvedTargetType); } else { return valueObject; } } catch (Throwable ex) { // Improve the message by showing the context. throw new BeanCreationException( this.beanDefinition.getResourceDescription(), this.beanName, \"Error converting typed String value for \" + argName, ex); } } else { return evaluate(value); } } 第一个if条件中的代码，表示对于RuntimeBeanReference类型的注入在resolveReference中。 private Object resolveReference(Object argName, RuntimeBeanReference ref) { try { // 从reference名字，这个refName实在载入BeanDefinition时根据配置生成的 String refName = ref.getBeanName(); refName = String.valueOf(doEvaluate(refName)); // 如果ref是在双亲IOC容器中，那就到双亲IOC容器中获取 if (ref.isToParent()) { if (this.beanFactory.getParentBeanFactory() == null) { throw new BeanCreationException( this.beanDefinition.getResourceDescription(), this.beanName, \"Can't resolve reference to bean '\" + refName + \"' in parent factory: no parent factory available\"); } return this.beanFactory.getParentBeanFactory().getBean(refName); } // 在当前IOC容器中获取Bean，这里会触发一个getBean的过程，如果依赖注入没有发生， // 这里会触发相应的依赖注入的发生 else { Object bean = this.beanFactory.getBean(refName); this.beanFactory.registerDependentBean(refName, this.beanName); return bean; } } catch (BeansException ex) { throw new BeanCreationException( this.beanDefinition.getResourceDescription(), this.beanName, \"Cannot resolve reference to bean '\" + ref.getBeanName() + \"' while setting \" + argName, ex); } } 对managedList的处理过程。 private List resolveManagedList(Object argName, List ml) { List resolved = new ArrayList(ml.size()); for (int i = 0; i 完成这个解析过程后，也即是BeanDefinitionValueResolver的resolveValueIfNecessary()方法中的相关内容执行完成后， 这个时候未依赖注入准备好了条件，这是真正把Bean对象设置到所有依赖的因一个Bean属性中的地方，其中处理的属性是各种各样的。 依赖注入的发生在BeanWrapperImpl中实现的。 bean属性注入通过setPropertyValues()方法完成，这是PropertyAccessor接口方法，实现类是AbstractPropertyAccessor，看一下调用的源码。 AbstractPropertyAccessor.setPropertyValues()方法源码： 调用入口 @Override public void setPropertyValues(PropertyValues pvs) throws BeansException { setPropertyValues(pvs, false, false); } @Override public void setPropertyValues(PropertyValues pvs, boolean ignoreUnknown, boolean ignoreInvalid) throws BeansException { List propertyAccessExceptions = null; // 得到属性列表 List propertyValues = (pvs instanceof MutablePropertyValues ? ((MutablePropertyValues) pvs).getPropertyValueList() : Arrays.asList(pvs.getPropertyValues())); for (PropertyValue pv : propertyValues) { try { // This method may throw any BeansException, which won't be caught // here, if there is a critical failure such as no matching field. // We can attempt to deal only with less serious exceptions. // 属性注入 setPropertyValue(pv); } catch (NotWritablePropertyException ex) { if (!ignoreUnknown) { throw ex; } // Otherwise, just ignore it and continue... } catch (NullValueInNestedPathException ex) { if (!ignoreInvalid) { throw ex; } // Otherwise, just ignore it and continue... } catch (PropertyAccessException ex) { if (propertyAccessExceptions == null) { propertyAccessExceptions = new LinkedList(); } propertyAccessExceptions.add(ex); } } // If we encountered individual exceptions, throw the composite exception. if (propertyAccessExceptions != null) { PropertyAccessException[] paeArray = propertyAccessExceptions.toArray(new PropertyAccessException[propertyAccessExceptions.size()]); throw new PropertyBatchUpdateException(paeArray); } } 代码追溯到： protected void setPropertyValue(PropertyTokenHolder tokens, PropertyValue pv) throws BeansException { if (tokens.keys != null) { processKeyedProperty(tokens, pv); } else { processLocalProperty(tokens, pv); } } 最后到在processKeyedProperty中完成依赖注入的处理： private void processKeyedProperty(PropertyTokenHolder tokens, PropertyValue pv) { Object propValue = getPropertyHoldingValue(tokens); String lastKey = tokens.keys[tokens.keys.length - 1]; if (propValue.getClass().isArray()) { PropertyHandler ph = getLocalPropertyHandler(tokens.actualName); Class requiredType = propValue.getClass().getComponentType(); int arrayIndex = Integer.parseInt(lastKey); Object oldValue = null; try { if (isExtractOldValueForEditor() && arrayIndex = length && arrayIndex componentType = propValue.getClass().getComponentType(); Object newArray = Array.newInstance(componentType, arrayIndex + 1); System.arraycopy(propValue, 0, newArray, 0, length); setPropertyValue(tokens.actualName, newArray); propValue = getPropertyValue(tokens.actualName); } Array.set(propValue, arrayIndex, convertedValue); } catch (IndexOutOfBoundsException ex) { throw new InvalidPropertyException(getRootClass(), this.nestedPath + tokens.canonicalName, \"Invalid array index in property path '\" + tokens.canonicalName + \"'\", ex); } } else if (propValue instanceof List) { PropertyHandler ph = getPropertyHandler(tokens.actualName); Class requiredType = ph.getCollectionType(tokens.keys.length); List list = (List) propValue; int index = Integer.parseInt(lastKey); Object oldValue = null; if (isExtractOldValueForEditor() && index = size && index mapKeyType = ph.getMapKeyType(tokens.keys.length); Class mapValueType = ph.getMapValueType(tokens.keys.length); Map map = (Map) propValue; // IMPORTANT: Do not pass full property name in here - property editors // must not kick in for map keys but rather only for map values. TypeDescriptor typeDescriptor = TypeDescriptor.valueOf(mapKeyType); Object convertedMapKey = convertIfNecessary(null, null, lastKey, mapKeyType, typeDescriptor); Object oldValue = null; if (isExtractOldValueForEditor()) { oldValue = map.get(convertedMapKey); } // Pass full property name and old value in here, since we want full // conversion ability for map values. Object convertedMapValue = convertIfNecessary(tokens.canonicalName, oldValue, pv.getValue(), mapValueType, ph.nested(tokens.keys.length)); map.put(convertedMapKey, convertedMapValue); } else { throw new InvalidPropertyException(getRootClass(), this.nestedPath + tokens.canonicalName, \"Property referenced in indexed property path '\" + tokens.canonicalName + \"' is neither an array nor a List nor a Map; returned value was [\" + propValue + \"]\"); } } "},"Chapter11/refresh.html":{"url":"Chapter11/refresh.html","title":"ApplicationContext容器refresh过程","keywords":"","body":"ApplicationContext容器refresh过程 又回到这里 public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { //准备刷新的上下文 环境 prepareRefresh(); //初始化BeanFactory，并进行XML文件读取 /* * ClassPathXMLApplicationContext包含着BeanFactory所提供的一切特征，在这一步骤中将会复用 * BeanFactory中的配置文件读取解析及其他功能，这一步之后，ClassPathXmlApplicationContext * 实际上就已经包含了BeanFactory所提供的功能，也就是可以进行Bean的提取等基础操作了。 */ ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); //对beanFactory进行各种功能填充 prepareBeanFactory(beanFactory); try { //子类覆盖方法做额外处理 /* * Spring之所以强大，为世人所推崇，除了它功能上为大家提供了便利外，还有一方面是它的 * 完美架构，开放式的架构让使用它的程序员很容易根据业务需要扩展已经存在的功能。这种开放式 * 的设计在Spring中随处可见，例如在本例中就提供了一个空的函数实现postProcessBeanFactory来 * 方便程序猿在业务上做进一步扩展 */ postProcessBeanFactory(beanFactory); //激活各种beanFactory处理器 invokeBeanFactoryPostProcessors(beanFactory); //注册拦截Bean创建的Bean处理器，这里只是注册，真正的调用实在getBean时候 registerBeanPostProcessors(beanFactory); //为上下文初始化Message源，即不同语言的消息体，国际化处理 initMessageSource(); //初始化应用消息广播器，并放入“applicationEventMulticaster”bean中 initApplicationEventMulticaster(); //留给子类来初始化其它的Bean onRefresh(); //在所有注册的bean中查找Listener bean，注册到消息广播器中 registerListeners(); //初始化剩下的单实例（非惰性的） finishBeanFactoryInitialization(beanFactory); //完成刷新过程，通知生命周期处理器lifecycleProcessor刷新过程，同时发出ContextRefreshEvent通知别人 finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); } destroyBeans(); cancelRefresh(ex); throw ex; } finally { resetCommonCaches(); } } } 我们简单的分析下代码的步骤： 初始化前的准备工作，例如对系统属性或者环境变量进行准备及验证。在某种情况下项目的使用需要读取某些系统变量，而这个变量的设置很可能会影响着系统的正确性，那么ClassPathXmlApplicationContext为我们提供的这个准备函数就显得非常必要，他可以在spring启动的时候提前对必须的环境变量进行存在性验证。 初始化BeanFactory，并进行XML文件读取。之前提到ClassPathXmlApplicationContext包含着对BeanFactory所提供的一切特征，那么这一步中将会复用BeanFactory中的配置文件读取解析其他功能，这一步之后ClassPathXmlApplicationContext实际上就已经包含了BeanFactory所提供的功能，也就是可以进行Bean的提取等基本操作了。 对BeanFactory进行各种功能填充@Qualifier和@Autowired应该是大家非常熟悉的注解了，那么这两个注解正是在这一步骤中增加支持的。 子类覆盖方法做额外处理。spring之所以强大，为世人所推崇，除了它功能上为大家提供了遍历外，还有一方面是它完美的架构，开放式的架构让使用它的程序员很容易根据业务需要扩展已经存在的功能。这种开放式的设计在spring中随处可见，例如本利中就提供了一个空的函数实现postProcessBeanFactory来方便程序员在业务上做进一步的扩展。 激活各种BeanFactory处理器 注册拦截bean创建的bean处理器，这里只是注册，真正的调用是在getBean时候 为上下文初始化Message源，及对不同语言的小西天进行国际化处理 初始化应用消息广播器，并放入“applicationEventMulticaster”bean中 留给子类来初始化其他的bean 在所有注册的bean中查找listener bean，注册到消息广播器中 初始化剩下的单实例（非惰性的） 完成刷新过程，通知生命周期处理器lifecycleProcessor刷新过程，同时发出ContextRefreshEvent通知别人。 接下来我们就详细的讲解每一个过程 prepareRefresh刷新上下文的准备工作 /** * 准备刷新上下文环境，设置它的启动日期和活动标志，以及执行任何属性源的初始化。 * Prepare this context for refreshing, setting its startup date and * active flag as well as performing any initialization of property sources. */ protected void prepareRefresh() { this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); // 在上下文环境中初始化任何占位符属性源。(空的方法,留给子类覆盖) initPropertySources(); // 验证需要的属性文件是否都已放入环境中 getEnvironment().validateRequiredProperties(); // 允许收集早期的应用程序事件，一旦有了多播器，就可以发布…… this.earlyApplicationEvents = new LinkedHashSet<>(); } obtainFreshBeanFactory->读取xml并初始化BeanFactory 前面文章有详细讲解 功能扩展 如上图所示prepareBeanFactory(beanFactory)就是在功能上扩展的方法，而在进入这个方法前spring已经完成了对配置的解析，接下来我们详细分析下次函数，进入方法体： protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) { // Tell the internal bean factory to use the context's class loader etc. //设置beanFactory的classLoader为当前context的classloader beanFactory.setBeanClassLoader(getClassLoader()); //设置beanFactory的表达式语言处理器，Spring3增加了表达式语言的支持， //默认可以使用#{bean.xxx}的形式来调用相关属性值 beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); //为beanFactory增加了一个的propertyEditor，这个主要是对bean的属性等设置管理的一个工具 beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // Configure the bean factory with context callbacks. beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); //设置了几个忽略自动装配的接口 beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); // BeanFactory interface not registered as resolvable type in a plain factory. // MessageSource registered (and found for autowiring) as a bean. //设置了几个自动装配的特殊规则 beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); // Register early post-processor for detecting inner beans as ApplicationListeners. beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); // Detect a LoadTimeWeaver and prepare for weaving, if found. //增加对AspectJ的支持 if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); // Set a temporary ClassLoader for type matching. beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } // Register default environment beans. //添加默认的系统环境bean if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); } if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); } if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) { beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); } } 详细分析下代码发现上面函数主要是在以下方法进行了扩展： 对SPEL语言的支持 增加对属性编辑器的支持 增加对一些内置类的支持，如EnvironmentAware、MessageSourceAware的注入 设置了依赖功能可忽略的接口 注册一些固定依赖的属性 增加了AspectJ的支持 将相关环境变量及属性以单例模式注册 增加对SPEL语言的支持 Spring表达式语言全称为“Spring Expression Language”，缩写为“SpEL”，类似于Struts 2x中使用的OGNL语言，SpEL是单独模块，只依赖于core模块，不依赖于其他模块，可以单独使用。 SpEL使用#{…}作为定界符，所有在大框号中的字符都将被认为是SpEL，使用格式如下： 上面只是列举了其中最简单的使用方式，SpEL功能非常强大，使用好可以大大提高开发效率。在源码中通过代码beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver())，注册语言解析器，就可以对SpEL进行解析了，那么之后是在什么地方调用这个解析器的呢？ 之前说beanFactory中说过Spring在bean进行初始化的时候会有属性填充的一步，而在这一步中Spring会调用AbstractAutowireCapabelBeanFactory类的applyPropertyValues来进行属性值得解析。同时这个步骤中一般通过AbstractBeanFactory中的evaluateBeanDefinitionString方法进行SpEL解析，方法代码如下： protected Object evaluateBeanDefinitionString(String value, BeanDefinition beanDefinition) { if (this.beanExpressionResolver == null) { return value; } Scope scope = (beanDefinition != null ? getRegisteredScope(beanDefinition.getScope()) : null); return this.beanExpressionResolver.evaluate(value, new BeanExpressionContext(this, scope)); } BeanFactory的后处理 BeanFactory作为spring中容器功能的基础，用于存放所有已经加载的bean，为例保证程序上的高可扩展性，spring针对BeanFactory做了大量的扩展，比如我们熟悉的PostProcessor就是在这里实现的。接下来我们就深入分析下BeanFactory后处理 激活注册的BeanFactoryPostProcessor 在正式介绍BeanFactoryPostProcessor的后处理前我们先简单的了解下其用法，BeanFactoryPostProcessor接口跟BeanPostProcessor类似，都可以对bean的定义（配置元数据）进行处理，也就是说spring IoC容器允许BeanFactoryPostProcessor在容器实际实例化任何其他的bean之前读取配置元数据，并可能修改他。也可以配置多个BeanFactoryPostProcessor，可以通过order属性来控制BeanFactoryPostProcessor的执行顺序（此属性必须当BeanFactoryPostProcessor实现了Ordered的接口时才可以赊账，因此在实现BeanFactoryPostProcessor时应该考虑实现Ordered接口）。 如果想改变世纪的bean实例（例如从配置元数据创建的对象），那最好使用BeanPostProcessor。同样的BeanFactoryPostProcessor的作用域范围是容器级别的，它只是和你所使用的容器有关。如果你在容器中定义了一个BeanFactoryPostProcessor，它仅仅对此容器中的bean进行后置处理。BeanFactoryPostProcessor不会对定义在另一个容器中的bean进行后置处理，即使这两个容器都在同一层次上。在spring中存在对于BeanFactoryPostProcessor的典型应用，如PropertyPlaceholderConfigurer。 BeanFactoryPostProcessor的典型应用：PropertyPlaceholderConfigurer classpath:bean.properties 在这个bean中指定了配置文件的位置。其实还是有个问题，这个userHandler只不过是spring框架管理的一个bean，并没有被别的bean或者对象引用，spring的beanFactory是怎么知道这个需要从这个bean中获取配置信息呢？我们看下PropertyPlaceholderConfigurer这个类的层次结构，如下图： 从上图中我们可以看到PropertyPlaceholderConfigurer间接的继承了BeanFactoryPostProcessor接口，这是一个很特别的接口，当spring加载任何实现了这个接口的bean的配置时，都会在bean工厂载入所有bean的配置之后执行postProcessBeanFactory方法。在PropertyResourceConfigurer类中实现了postProcessBeanFactory方法，在方法中先后调用了mergeProperties、convertProperties、processProperties这三个方法，分别得到配置，将得到的配置转换为合适的类型，最后将配置内容告知BeanFactory。 正是通过实现BeanFactoryPostProcessor接口，BeanFactory会在实例化任何bean之前获得配置信息，从而能够正确的解析bean描述文件中的变量引用。 public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { try { Properties mergedProps = this.mergeProperties(); this.convertProperties(mergedProps); this.processProperties(beanFactory, mergedProps); } catch (IOException var3) { throw new BeanInitializationException(\"Could not load properties\", var3); } } 自定义BeanFactoryPostProcessor 编写实现了BeanFactoryPostProcessor接口的MyBeanFactoryPostProcessor的容器后处理器，如下代码： public class MyBeanFactoryPostProcessor implements BeanFactoryPostProcessor { public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { System.out.println(\"对容器进行后处理。。。。\"); } } 然后在配置文件中注册这个bean，如下： 最后编写测试代码： public class Test { public static void main(String[] args) { ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); User user = (User)context.getBean(\"user\"); System.out.println(user.getName()); } } 激活BeanFactoryPostProcessor(invokeBeanFactoryPostProcessors) 在了解BeanFactoryPostProcessor的用法后我们便可以深入的研究BeanFactoryPostProcessor的调用过程了，其是在方法invokeBeanFactoryPostProcessors(beanFactory)中实现的，进入到方法内部： /** * Instantiate and invoke all registered BeanFactoryPostProcessor beans, * respecting explicit order if given. * Must be called before singleton instantiation. */ protected void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory) { // 1.getBeanFactoryPostProcessors(): 拿到当前应用上下文beanFactoryPostProcessors变量中的值 // 2.invokeBeanFactoryPostProcessors: 实例化并调用所有已注册的BeanFactoryPostProcessor PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(beanFactory, getBeanFactoryPostProcessors()); // Detect a LoadTimeWeaver and prepare for weaving, if found in the meantime // (e.g. through an @Bean method registered by ConfigurationClassPostProcessor) if (beanFactory.getTempClassLoader() == null && beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) { beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); } } getBeanFactoryPostProcessors /** * Return the list of BeanFactoryPostProcessors that will get applied * to the internal BeanFactory. */ public List getBeanFactoryPostProcessors() { return this.beanFactoryPostProcessors; } 这边 getBeanFactoryPostProcessors() 会拿到当前应用上下文中已经注册的 BeanFactoryPostProcessor，在默认情况下，this.beanFactoryPostProcessors 是返回空的。 但是在SpringApplication.prepareContext()中会有一个添加,而且prepareContext比refresh()先执行 //1 applyInitializers(context); //2 if (this.lazyInitialization) { context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor()); } 如何添加自定义 BeanFactoryPostProcessor 到 this.beanFactoryPostProcessors 变量中了？ 深入进入上面方法applyInitializers(context)中，最后会进入ApplicationContextInitializer 接口中。接口说明如下 用于在刷新之前初始化Spring ConfigurableApplicationContext。 通常在需要对应用程序上下文进行一些编程初始化的Web应用程序中使用。 例如，注册属性源或针对上下文环境激活配置文件。 请参见ContextLoader和FrameworkServlet支持，分别声明“ contextInitializerClasses”上下文参数和init参数。 鼓励ApplicationContextInitializer处理器检测是否已实现Spring的Ordered接口或是否存在@Order注释，并在调用之前对实例进行相应的排序。 在这就可以通过实现ApplicationContextInitializer来添加BeanFactoryPostProcessor。 @Component public class UserService { } @Component public class UserExtService { } @Component public class UserServiceBeanFactoryPostProcessor implements BeanFactoryPostProcessor { @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { BeanDefinition userServiceBeanDefinition = beanFactory.getBeanDefinition(\"userService\"); userServiceBeanDefinition.setBeanClassName(\"userExtService\"); } } public class UserServiceInit implements ApplicationContextInitializer { @Override public void initialize(ConfigurableApplicationContext applicationContext) { UserServiceBeanFactoryPostProcessor firstBeanDefinitionRegistryPostProcessor = new UserServiceBeanFactoryPostProcessor(); // 将自定义的firstBeanDefinitionRegistryPostProcessor添加到应用上下文中 applicationContext.addBeanFactoryPostProcessor(firstBeanDefinitionRegistryPostProcessor); // ...自定义操作 System.out.println(\"SpringApplicationContextInitializer#initialize\"); } } @SpringBootApplication public class Application { public static void main(String[] args) { ConfigurableApplicationContext context = new SpringApplicationBuilder(Application.class) .initializers(new UserServiceInit()) .run(args); } } invokeBeanFactoryPostProcessors public static void invokeBeanFactoryPostProcessors(ConfigurableListableBeanFactory beanFactory, List beanFactoryPostProcessors) { // Invoke BeanDefinitionRegistryPostProcessors first, if any. // 1、首先调用BeanDefinitionRegistryPostProcessors Set processedBeans = new HashSet<>(); // 1.判断beanFactory是否为BeanDefinitionRegistry，beanFactory为DefaultListableBeanFactory,而DefaultListableBeanFactory实现了BeanDefinitionRegistry接口，因此这边为true if (beanFactory instanceof BeanDefinitionRegistry) { BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; // 定义BeanFactoryPostProcessor 用于存放普通的BeanFactoryPostProcessor List regularPostProcessors = new ArrayList<>(); // 定义BeanDefinitionRegistryPostProcessor集合 用于存放BeanDefinitionRegistryPostProcessor List registryProcessors = new ArrayList<>(); // 循环手动注册的beanFactoryPostProcessors，将BeanDefinitionRegistryPostProcessor和普通BeanFactoryPostProcessor区分开 for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) { // 如果是BeanDefinitionRegistryPostProcessor的实例话,则调用其postProcessBeanDefinitionRegistry方法,对bean进行注册操作 if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) { // 如果是BeanDefinitionRegistryPostProcessor类型,则直接调用其postProcessBeanDefinitionRegistry BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; registryProcessor.postProcessBeanDefinitionRegistry(registry); registryProcessors.add(registryProcessor); } else { // 否则则将其当做普通的BeanFactoryPostProcessor处理,直接加入regularPostProcessors集合,以备后续处理 regularPostProcessors.add(postProcessor); } } // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // Separate between BeanDefinitionRegistryPostProcessors that implement // PriorityOrdered, Ordered, and the rest. // 用于保存本次要执行的BeanDefinitionRegistryPostProcesso List currentRegistryProcessors = new ArrayList<>(); // First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered. // 首先调用实现了PriorityOrdered(有限排序接口)的BeanDefinitionRegistryPostProcessors，找出所有实现BeanDefinitionRegistryPostProcessor接口的Bean的beanName String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { // 获取ppName对应的bean实例, 添加到currentRegistryProcessors中 currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); // 将要被执行的加入processedBeans，避免后续重复执行 processedBeans.add(ppName); } } // 排序 sortPostProcessors(currentRegistryProcessors, beanFactory); // 加入registryProcessors集合 registryProcessors.addAll(currentRegistryProcessors); // 调用所有实现了PriorityOrdered的的BeanDefinitionRegistryPostProcessors的postProcessBeanDefinitionRegistry方法,注册bean invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); // 清空currentRegistryProcessors,以备下次使用 currentRegistryProcessors.clear(); // Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered. // 其次,调用实现了Ordered(普通排序接口)的BeanDefinitionRegistryPostProcessors postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (!processedBeans.contains(ppName) && beanFactory.isTypeMatch(ppName, Ordered.class)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); } } // 排序 sortPostProcessors(currentRegistryProcessors, beanFactory); // 加入registryProcessors集合 registryProcessors.addAll(currentRegistryProcessors); // 调用所有实现了PriorityOrdered的的BeanDefinitionRegistryPostProcessors的postProcessBeanDefinitionRegistry方法,注册bean invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); // 清空currentRegistryProcessors,以备下次使用 currentRegistryProcessors.clear(); // Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear. // 最后,调用其他的BeanDefinitionRegistryPostProcessors boolean reiterate = true; while (reiterate) { reiterate = false; postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (!processedBeans.contains(ppName)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); reiterate = true; } } // 排序 sortPostProcessors(currentRegistryProcessors, beanFactory); // 加入registryProcessors集合 registryProcessors.addAll(currentRegistryProcessors); // 调用其他的BeanDefinitionRegistryPostProcessors的postProcessBeanDefinitionRegistry方法,注册bean invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); // 清空currentRegistryProcessors,以备下次使用 currentRegistryProcessors.clear(); } // Now, invoke the postProcessBeanFactory callback of all processors handled so far. // 调用所有BeanDefinitionRegistryPostProcessor(包括手动注册和通过配置文件注册) // 和BeanFactoryPostProcessor(只有手动注册)的回调函数-->postProcessBeanFactory invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); } // 2、如果不是BeanDefinitionRegistry的实例,那么直接调用其回调函数即可-->postProcessBeanFactory else { // Invoke factory processors registered with the context instance. invokeBeanFactoryPostProcessors(beanFactoryPostProcessors, beanFactory); } // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // 3、上面的代码已经处理完了所有的BeanDefinitionRegistryPostProcessors和手动注册的BeanFactoryPostProcessor // 接下来要处理通过配置文件注册的BeanFactoryPostProcessor // 首先获取所有的BeanFactoryPostProcessor(注意:这里获取的集合会包含BeanDefinitionRegistryPostProcessors) String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // Separate between BeanFactoryPostProcessors that implement PriorityOrdered, Ordered, and the rest. // 这里,将实现了PriorityOrdered,Ordered的处理器和其他的处理器区分开来,分别进行处理 // PriorityOrdered有序处理器 List priorityOrderedPostProcessors = new ArrayList<>(); // Ordered有序处理器 List orderedPostProcessorNames = new ArrayList<>(); // 无序处理器 List nonOrderedPostProcessorNames = new ArrayList<>(); for (String ppName : postProcessorNames) { // 判断processedBeans是否包含当前处理器(processedBeans中的处理器已经被处理过);如果包含,则不做任何处理 if (processedBeans.contains(ppName)) { // skip - already processed in first phase above } else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { // 加入到PriorityOrdered有序处理器集合 priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { // 加入到Ordered有序处理器集合 orderedPostProcessorNames.add(ppName); } else { // 加入到无序处理器集合 nonOrderedPostProcessorNames.add(ppName); } } // First, invoke the BeanFactoryPostProcessors that implement PriorityOrdered. // 首先调用实现了PriorityOrdered接口的处理器 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); // Next, invoke the BeanFactoryPostProcessors that implement Ordered. // 其次,调用实现了Ordered接口的处理器 List orderedPostProcessors = new ArrayList<>(); for (String postProcessorName : orderedPostProcessorNames) { orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); } sortPostProcessors(orderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // Finally, invoke all other BeanFactoryPostProcessors. // 最后,调用无序处理器 List nonOrderedPostProcessors = new ArrayList<>(); for (String postProcessorName : nonOrderedPostProcessorNames) { nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); } invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // Clear cached merged bean definitions since the post-processors might have // modified the original metadata, e.g. replacing placeholders in values... // 清理元数据 beanFactory.clearMetadataCache(); } 循环遍历 BeanFactoryPostProcessor 中的 postProcessBeanFactory 方法 private static void invokeBeanFactoryPostProcessors( Collection postProcessors, ConfigurableListableBeanFactory beanFactory) { for (BeanFactoryPostProcessor postProcessor : postProcessors) { postProcessor.postProcessBeanFactory(beanFactory); } } 注册BeanPostProcessor(registerBeanPostProcessors) 在上文中提到了BeanFactoryPostProcessor的调用，接下来我们就探索下BeanPostProcessor。但这里并不是调用，而是注册，真正的调用其实是在bean的实例化阶段进行的，这是一个很重要的步骤，也是很多功能BeanFactory不知道的重要原因。spring中大部分功能都是通过后处理器的方式进行扩展的，这是spring框架的一个特写，但是在BeanFactory中其实并没有实现后处理器的自动注册，所以在调用的时候如果没有进行手动注册其实是不能使用的。但是ApplicationContext中却添加了自动注册功能，如自定义一个后处理器： public class MyInstantiationAwareBeanPostProcessor implements InstantiationAwareBeanPostProcessor { public Object postProcessBeforeInstantiation(Class beanClass, String beanName) throws BeansException { System.out.println(\"befor\"); return null; } } 然后在配置文件中添加bean的配置： 这样的话再使用BeanFactory的方式进行加载的bean在加载时不会有任何改变的，而在使用ApplicationContext方式获取的bean时就会打印出“before”，而这个特性就是咋registryBeanPostProcessor方法中完成的。 我们继续深入分析registryBeanPostProcessors的方法实现： protected void registerBeanPostProcessors(ConfigurableListableBeanFactory beanFactory) { PostProcessorRegistrationDelegate.registerBeanPostProcessors(beanFactory, this); } public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) { String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); /* * BeanPostProcessorChecker是一个普通的信息打印，可能会有些情况当spring的配置中的后 * 处理器还没有被注册就已经开了bean的初始化，这时就会打印出BeanPostProcessorChecker中 * 设定的信息 */ int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length; beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); //使用PriorityOrdered来保证顺序 List priorityOrderedPostProcessors = new ArrayList<>(); List internalPostProcessors = new ArrayList<>(); //使用Ordered来保证顺序 List orderedPostProcessorNames = new ArrayList<>(); //无序BeanPostProcessor List nonOrderedPostProcessorNames = new ArrayList<>(); for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { nonOrderedPostProcessorNames.add(ppName); } } //第一步，注册所有实现了PriorityOrdered的BeanPostProcessor sortPostProcessors(priorityOrderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); //注册实现了Ordered的BeanPostProcessor List orderedPostProcessors = new ArrayList<>(); for (String ppName : orderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } sortPostProcessors(orderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, orderedPostProcessors); //注册所有的无序的BeanPostProcessor List nonOrderedPostProcessors = new ArrayList<>(); for (String ppName : nonOrderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); //注册所有的内部BeanFactoryProcessor sortPostProcessors(internalPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, internalPostProcessors); // Re-register post-processor for detecting inner beans as ApplicationListeners, //添加ApplicationListener探测器 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext)); } 我们可以看到先从容器中获取所有类型为 BeanPostProcessor.class 的Bean的name数组，然后通过 BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); 获取Bean的实例，最后通过 registerBeanPostProcessors(beanFactory, orderedPostProcessors);将获取到的BeanPostProcessor实例添加到容器的属性中，如下 private static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, List postProcessors) { for (BeanPostProcessor postProcessor : postProcessors) { beanFactory.addBeanPostProcessor(postProcessor); } } @Override public void addBeanPostProcessor(BeanPostProcessor beanPostProcessor) { Assert.notNull(beanPostProcessor, \"BeanPostProcessor must not be null\"); // Remove from old position, if any this.beanPostProcessors.remove(beanPostProcessor); // Track whether it is instantiation/destruction aware if (beanPostProcessor instanceof InstantiationAwareBeanPostProcessor) { this.hasInstantiationAwareBeanPostProcessors = true; } if (beanPostProcessor instanceof DestructionAwareBeanPostProcessor) { this.hasDestructionAwareBeanPostProcessors = true; } // Add to end of list this.beanPostProcessors.add(beanPostProcessor); } 可以看到将 beanPostProcessor 实例添加到容器的 beanPostProcessors 属性中 例子 我们通常在使用 Mybatis + Spring 时，经常用到的 org.mybatis.spring.mapper.MapperScannerConfigurer 就是一个BeanDefinitionRegistryPostProcessor。MapperScannerConfigurer 在 postProcessBeanDefinitionRegistry 方法中进行了一些操作，主要是：扫描 basePackage 指定的目录，将该目录下的类（通常是 DAO/MAPPER 接口）封装成 BeanDefinition 并加载到 BeanFactory 中。 因此，我们可以看到我们项目中的 DAO（MAPPER）接口，通常都没有使用注解或 XML 的方式注册到 Spring 容器，但是我们还是可以在 Service 服务中，使用 @Autowire 注解来将其注入到 Service 中，就是因为这个原因。 初始化Message资源 不是很重要 初始事件广播器(initApplicationEventMulticaster) 初始化ApplicationEventMulticaster是在方法initApplicationEventMulticaster()中实现的，进入到方法体，如下： protected void initApplicationEventMulticaster() { ConfigurableListableBeanFactory beanFactory = getBeanFactory(); // 1、默认使用内置的事件广播器,如果有的话. // 我们可以在配置文件中配置Spring事件广播器或者自定义事件广播器 // 例如: if (beanFactory.containsLocalBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME)) { this.applicationEventMulticaster = beanFactory.getBean(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, ApplicationEventMulticaster.class); } // 2、否则,新建一个事件广播器,SimpleApplicationEventMulticaster是spring的默认事件广播器 else { this.applicationEventMulticaster = new SimpleApplicationEventMulticaster(beanFactory); beanFactory.registerSingleton(APPLICATION_EVENT_MULTICASTER_BEAN_NAME, this.applicationEventMulticaster); } } 通过源码可以看到其实现逻辑与initMessageSource基本相同，其步骤如下： 查找是否有name为applicationEventMulticaster的bean，如果有放到容器里，如果没有，初始化一个系统默认的SimpleApplicationEventMulticaster放入容器 查找手动设置的applicationListeners，添加到applicationEventMulticaster里 查找定义的类型为ApplicationListener的bean，设置到applicationEventMulticaster 初始化完成、对earlyApplicationEvents里的事件进行通知（此容器仅仅是广播器未建立的时候保存通知信息，一旦容器建立完成，以后均直接通知） 在系统操作时候，遇到的各种bean的通知事件进行通知 可以看到的是applicationEventMulticaster是一个标准的观察者模式，对于他内部的监听者applicationListeners，每次事件到来都会一一获取通知。 onRefresh 注册监听器(registerListeners) protected void registerListeners() { // Register statically specified listeners first. // 首先,注册指定的静态事件监听器,在spring boot中有应用 for (ApplicationListener listener : getApplicationListeners()) { getApplicationEventMulticaster().addApplicationListener(listener); } // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let post-processors apply to them! // 其次,注册普通的事件监听器 String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false); for (String listenerBeanName : listenerBeanNames) { getApplicationEventMulticaster().addApplicationListenerBean(listenerBeanName); } // Publish early application events now that we finally have a multicaster... // 如果有早期事件的话,在这里进行事件广播 // 因为前期SimpleApplicationEventMulticaster尚未注册，无法发布事件， // 因此早期的事件会先存放在earlyApplicationEvents集合中，这里把它们取出来进行发布 // 所以早期事件的发布时间节点是早于其他事件的 Set earlyEventsToProcess = this.earlyApplicationEvents; // 早期事件广播器是一个Set集合,保存了无法发布的早期事件,当SimpleApplicationEventMulticaster // 创建完之后随即进行发布,同事也要将其保存的事件释放 this.earlyApplicationEvents = null; if (earlyEventsToProcess != null) { for (ApplicationEvent earlyEvent : earlyEventsToProcess) { getApplicationEventMulticaster().multicastEvent(earlyEvent); } } } 我们来看一下Spring的事件监昕的简单用法 定义监听事件 public class TestEvent extends ApplicationonEvent { public String msg; public TestEvent (Object source ) { super (source ); } public TestEvent (Object source , String msg ) { super(source); this.msg = msg ; } public void print () { System.out.println(msg) ; } } 定义监昕器 public class TestListener implement ApplicationListener { public void onApplicationEvent (ApplicationEvent event ) { if (event instanceof TestEvent ) { TestEvent testEvent = (TestEvent) event ; testEvent print () ; } } } 添加配置文件 # @Test public void MyAopTest() { ApplicationContext ac = new ClassPathXmlApplicationContext(\"spring-aop.xml\"); TestEvent event = new TestEvent (“hello” ,”msg”) ; context.publishEvent(event); } 源码分析 protected void publishEvent(Object event, ResolvableType eventType) { Assert.notNull(event, \"Event must not be null\"); if (logger.isTraceEnabled()) { logger.trace(\"Publishing event in \" + getDisplayName() + \": \" + event); } // Decorate event as an ApplicationEvent if necessary ApplicationEvent applicationEvent; //支持两种事件1、直接继承ApplicationEvent，2、其他时间，会被包装为PayloadApplicationEvent，可以使用getPayload获取真实的通知内容 if (event instanceof ApplicationEvent) { applicationEvent = (ApplicationEvent) event; } else { applicationEvent = new PayloadApplicationEvent(this, event); if (eventType == null) { eventType = ((PayloadApplicationEvent)applicationEvent).getResolvableType(); } } // Multicast right now if possible - or lazily once the multicaster is initialized if (this.earlyApplicationEvents != null) { //如果有预制行添加到预制行，预制行在执行一次后被置为null，以后都是直接执行 this.earlyApplicationEvents.add(applicationEvent); } else { //广播event事件 getApplicationEventMulticaster().multicastEvent(applicationEvent, eventType); } // Publish event via parent context as well... //父bean同样广播 if (this.parent != null) { if (this.parent instanceof AbstractApplicationContext) { ((AbstractApplicationContext) this.parent).publishEvent(event, eventType); } else { this.parent.publishEvent(event); } } } 查找所有的监听者，依次遍历，如果有线程池，利用线程池进行发送，如果没有则直接发送，如果针对比较大的并发量，我们应该采用线程池模式，将发送通知和真正的业务逻辑进行分离 public void multicastEvent(final ApplicationEvent event, ResolvableType eventType) { ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); for (final ApplicationListener listener : getApplicationListeners(event, type)) { Executor executor = getTaskExecutor(); if (executor != null) { executor.execute(new Runnable() { @Override public void run() { invokeListener(listener, event); } }); } else { invokeListener(listener, event); } } } 调用invokeListener protected void invokeListener(ApplicationListener listener, ApplicationEvent event) { ErrorHandler errorHandler = getErrorHandler(); if (errorHandler != null) { try { listener.onApplicationEvent(event); } catch (Throwable err) { errorHandler.handleError(err); } } else { try { listener.onApplicationEvent(event); } catch (ClassCastException ex) { // Possibly a lambda-defined listener which we could not resolve the generic event type for LogFactory.getLog(getClass()).debug(\"Non-matching event type for listener: \" + listener, ex); } } } 初始化其他的单例Bean(非延迟加载的) 前面章节已有详细讲解 finishRefresh 完成刷新过程,通知生命周期处理器lifecycleProcessor刷新过程,同时发出ContextRefreshEvent通知 protected void finishRefresh() { // Clear context-level resource caches (such as ASM metadata from scanning). // 清空资源缓存 clearResourceCaches(); // Initialize lifecycle processor for this context. // 初始化生命周期处理器 initLifecycleProcessor(); // Propagate refresh to lifecycle processor first. // 调用生命周期处理器的onRefresh方法 getLifecycleProcessor().onRefresh(); // Publish the final event. // 推送容器刷新事件 publishEvent(new ContextRefreshedEvent(this)); // Participate in LiveBeansView MBean, if active. LiveBeansView.registerApplicationContext(this); } "},"Chapter11/FactoryBean.html":{"url":"Chapter11/FactoryBean.html","title":"BeanFactory和FactoryBean区别","keywords":"","body":"BeanFactory和FactoryBean区别 BeanFactory和FactoryBean概念 BeanFactory和FactoryBean在Spring中是两个使用频率很高的类，它们在拼写上非常相似， 需要注意的是，两者除了名字看上去像一点外，从实质上来说是一个没有多大关系的东西。 BeanFactory是一个IOC容器或Bean对象工厂； FactoryBean是一个Bean； 在Spring中有两种Bean，一种是普通Bean，另一种就是像FactoryBean这样的工厂Bean，无论是那种Bean，都是由IOC容器来管理的。 FactoryBean可以说为IOC容器中Bean的实现提供了更加灵活的方式，FactoryBean在IOC容器的基础上给Bean的实现 加上了一个简单工厂模式和装饰模式，我们可以在getObject()方法中灵活配置。 BeanFactory和FactoryBean深入源码 BeanFactory BeanFactory是IOC最基本的容器，负责生产和管理bean，它为其他具体的IOC容器提供了最基本的规范， 例如DefaultListableBeanFactory，XmlBeanFactory，ApplicationContext 等具体的容器都是实现了BeanFactory， FactoryBean FactoryBean是一个接口，当在IOC容器中的Bean实现了FactoryBean后，通过getBean(String BeanName)获取 到的Bean对象并不是FactoryBean的实现类对象，而是这个实现类中的getObject()方法返回的对象。 要想获取FactoryBean的实现类，就要getBean(&BeanName)，在BeanName之前加上&。 package org.springframework.beans.factory; public interface FactoryBean { // 返回由FactoryBean创建的Bean的实例 T getObject() throws Exception; // 返回FactoryBean创建的Bean的类型 Class getObjectType(); // 确定由FactoryBean创建的Bean的作用域是singleton还是prototype boolean isSingleton(); } 实例分析 AppleBean public class AppleBean { } @Component public class AppleFactoryBean implements FactoryBean{ @Override public Object getObject() throws Exception { return new AppleBean(); } @Override public Class getObjectType() { return AppleBean.class; } @Override public boolean isSingleton() { return false; } } @Configuration @ComponentScan public class AppConfiguration { } public class StartTest { public static void main(String[] args){ ApplicationContext context = new AnnotationConfigApplicationContext(AppConfiguration.class); // 得到的是apple System.out.println(context.getBean(\"appleFactoryBean\")); // 得到的是apple工厂 System.out.println(context.getBean(\"&appleFactoryBean\")); } com.jpeony.spring.bean.AppleBean@679b62af com.jpeony.spring.bean.AppleFactoryBean@5cdd8682 从结果可以看出第一个打印出来的是在getObject()中new的AppleBean对象，是一个普通的Bean， 第二个通过加上&获取的是实现了FactoryBean接口的AppleFactoryBean对象，是一个工厂Bean。 实现源码分析 前面基本使用方式已经知道了，再来看看Spring如何实现的。在使用getBean()的时候可以找到最终调用的是AbstractBeanFactory.doGetBean() protected T doGetBean( final String name, final Class requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException { //1.如果是FactoryBean这种Bean都是以&开头。所以需要去掉& //2.如果name的别名，则需要在SimpleAliasRegistry里面进行判断获取的真正的那么 final String beanName = transformedBeanName(name); Object bean; // Eagerly check singleton cache for manually registered singletons. //从缓存中获取对象，避免重复创建 Object sharedInstance = getSingleton(beanName); System.out.println(sharedInstance); if (sharedInstance != null && args == null) { if (logger.isDebugEnabled()) { if (isSingletonCurrentlyInCreation(beanName)) { logger.debug(\"Returning eagerly cached instance of singleton bean '\" + beanName + \"' that is not fully initialized yet - a consequence of a circular reference\"); } else { logger.debug(\"Returning cached instance of singleton bean '\" + beanName + \"'\"); } } /*这里 的 getObjectForBeanInstance 完成 的 是 FactoryBean 的 相关 处理， 以 取得 FactoryBean 的 生产 结果, BeanFactory 和 FactoryBean 的 区别 已经 在前面 讲过， 这个 过程 在后面 还会 详细 地 分析*/ bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { // Fail if we're already creating this bean instance: // We're assumably within a circular reference. if (isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } // Check if bean definition exists in this factory. /* 这里 对 IoC 容器 中的 BeanDefintion 是否 存在 进行检查， 检查 是否 能在 当前 的 BeanFactory 中 取得 需要 的 Bean。 如果 在 当前 的 工厂 中 取 不到， 则 到 双亲 BeanFactory 中 去取； 如果 当前 的 双亲 工厂 取 不到， 那就 顺着 双亲 BeanFactory 链 一直 向上 查找*/ BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null && !containsBeanDefinition(beanName)) { // Not found -> check parent. String nameToLookup = originalBeanName(name); if (args != null) { // Delegation to parent with explicit args. return (T) parentBeanFactory.getBean(nameToLookup, args); } else { // No args -> delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); } } if (!typeCheckOnly) { markBeanAsCreated(beanName); } try { //通过beanName获得BeanDefinition对象 //这里 根据 Bean 的 名字 取得 BeanDefinition final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // Guarantee initialization of beans that the current bean depends on. //检查bean配置是否设置了depend-on属性。即实例A需要先实例B //获取 当前 Bean 的 所有 依赖 Bean， 这样 会 触发 getBean 的 递归 调用， 直到 取 到 一个 没有 // 任何 依赖 的 Bean 为止 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { for (String dep : dependsOn) { if (isDependent(beanName, dep)) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Circular depends-on relationship between '\" + beanName + \"' and '\" + dep + \"'\"); } registerDependentBean(dep, beanName); try { //Bean依赖别的bean，直接递归getBean即可 getBean(dep); } catch (NoSuchBeanDefinitionException ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"'\" + beanName + \"' depends on missing bean '\" + dep + \"'\", ex); } } } /*这里 通过 调用 createBean 方法 创建 Singleton bean 的 实例， 这里 有一个 回 调 函数 getObject， 会在 getSingleton 中 调用 ObjectFactory 的 createBean*/ // Create bean instance. if (mbd.isSingleton()) { sharedInstance = getSingleton(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { try { return createBean(beanName, mbd, args); } catch (BeansException ex) { // Explicitly remove instance from singleton cache: It might have been put there // eagerly by the creation process, to allow for circular reference resolution. // Also remove any beans that received a temporary reference to the bean. destroySingleton(beanName); throw ex; } } }); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } else if (mbd.isPrototype()) { // It's a prototype -> create a new instance. Object prototypeInstance = null; try { beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); } else { String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) { throw new IllegalStateException(\"No Scope registered for scope name '\" + scopeName + \"'\"); } try { Object scopedInstance = scope.get(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { beforePrototypeCreation(beanName); try { return createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } } }); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); } catch (IllegalStateException ex) { throw new BeanCreationException(beanName, \"Scope '\" + scopeName + \"' is not active for the current thread; consider \" + \"defining a scoped proxy for this bean if you intend to refer to it from a singleton\", ex); } } } catch (BeansException ex) { cleanupAfterBeanCreationFailure(beanName); throw ex; } } // Check if required type matches the type of the actual bean instance. // 这里 对 创建 的 Bean 进行 类型 检查， 如果 没有 问题， 就 返回 这个 新 创建 的 Bean， 这个 Bean 已经 //是 包含 了 依赖 关系 的 Bean if (requiredType != null && bean != null && !requiredType.isInstance(bean)) { try { return getTypeConverter().convertIfNecessary(bean, requiredType); } catch (TypeMismatchException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Failed to convert bean '\" + name + \"' to required type '\" + ClassUtils.getQualifiedName(requiredType) + \"'\", ex); } throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); } } return (T) bean; } 通过上面的源码可以看到先通过IOC的 createBean(beanName, mbd, args);最后都会走bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd);这个方法就是FactoryBean调用getObject()方法的地方。看一下源码 protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, RootBeanDefinition mbd) { // Don't let calling code try to dereference the factory if the bean isn't a factory. if (BeanFactoryUtils.isFactoryDereference(name) && !(beanInstance instanceof FactoryBean)) { throw new BeanIsNotAFactoryException(transformedBeanName(name), beanInstance.getClass()); } // Now we have the bean instance, which may be a normal bean or a FactoryBean. // If it's a FactoryBean, we use it to create a bean instance, unless the // caller actually wants a reference to the factory. if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) { return beanInstance; } Object object = null; if (mbd == null) { object = getCachedObjectForFactoryBean(beanName); } if (object == null) { // Return bean instance from factory. FactoryBean factory = (FactoryBean) beanInstance; // Caches object obtained from FactoryBean if it is a singleton. if (mbd == null && containsBeanDefinition(beanName)) { mbd = getMergedLocalBeanDefinition(beanName); } boolean synthetic = (mbd != null && mbd.isSynthetic()); object = getObjectFromFactoryBean(factory, beanName, !synthetic); } return object; } 分析一下上面的源码。首先判断当前获得的Bean是否是FactoryBean如果不是直接返回，如果是FactoryBean那么从缓存中获得，如果缓存中没有则创建也就是最后object = getObjectFromFactoryBean(factory, beanName, !synthetic); protected Object getObjectFromFactoryBean(FactoryBean factory, String beanName, boolean shouldPostProcess) { if (factory.isSingleton() && containsSingleton(beanName)) { synchronized (getSingletonMutex()) { Object object = this.factoryBeanObjectCache.get(beanName); if (object == null) { object = doGetObjectFromFactoryBean(factory, beanName); // Only post-process and store if not put there already during getObject() call above // (e.g. because of circular reference processing triggered by custom getBean calls) Object alreadyThere = this.factoryBeanObjectCache.get(beanName); if (alreadyThere != null) { object = alreadyThere; } else { if (object != null && shouldPostProcess) { if (isSingletonCurrentlyInCreation(beanName)) { // Temporarily return non-post-processed object, not storing it yet.. return object; } beforeSingletonCreation(beanName); try { object = postProcessObjectFromFactoryBean(object, beanName); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"Post-processing of FactoryBean's singleton object failed\", ex); } finally { afterSingletonCreation(beanName); } } if (containsSingleton(beanName)) { this.factoryBeanObjectCache.put(beanName, (object != null ? object : NULL_OBJECT)); } } } return (object != NULL_OBJECT ? object : null); } } else { Object object = doGetObjectFromFactoryBean(factory, beanName); if (object != null && shouldPostProcess) { try { object = postProcessObjectFromFactoryBean(object, beanName); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"Post-processing of FactoryBean's object failed\", ex); } } return object; } } 上面的源码会做一些前置后置处理。主要创建Object的方法是object = doGetObjectFromFactoryBean(factory, beanName); private Object doGetObjectFromFactoryBean(final FactoryBean factory, final String beanName) throws BeanCreationException { Object object; try { if (System.getSecurityManager() != null) { AccessControlContext acc = getAccessControlContext(); try { object = AccessController.doPrivileged(new PrivilegedExceptionAction() { @Override public Object run() throws Exception { return factory.getObject(); } }, acc); } catch (PrivilegedActionException pae) { throw pae.getException(); } } else { object = factory.getObject(); } } catch (FactoryBeanNotInitializedException ex) { throw new BeanCurrentlyInCreationException(beanName, ex.toString()); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"FactoryBean threw exception on object creation\", ex); } // Do not accept a null value for a FactoryBean that's not fully // initialized yet: Many FactoryBeans just return null then. if (object == null && isSingletonCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException( beanName, \"FactoryBean which is currently in creation returned null from getObject\"); } return object; } 看上面的源码object = factory.getObject();最终都是通过你自己写的FactoryBean里面的getObject()来返回你需要的对象 java 总结 BeanFactory是一个IOC基础容器。 FactoryBean是一个Bean，不是一个普通Bean，是一个工厂Bean。 FactoryBean实现与工厂模式和装饰模式类似。 通过转义符&来区分获取FactoryBean产生的对象和FactoryBean对象本身（FactoryBean实现类） "},"Chapter11/BeanFactoryPostProcessor.html":{"url":"Chapter11/BeanFactoryPostProcessor.html","title":"BeanFactoryPostProcessor","keywords":"","body":"BeanFactoryPostProcessor 本文源码为spring5.2.9版本 回归到 AbstractApplicationContext的*refresh的方法,发现和BeanFactoryPostProcessor有关的方法有两个 // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); postProcessBeanFactory postProcessBeanFactory是一个无具体实现的方法，用户增强和扩展。普通spring没用相关的重写类。但是如果你使用web工程就会用到。 例如 GenericWebApplicationContext、 ServletWebServerApplicationContext、 AnnotationConfigServletWebApplicationContext、 AnnotationConfigServletWebServerApplicationContext、 AnnotationConfigReactiveWebServerApplicationContext 如果你对spring集成关系有所了解的化你会知道 AnnotationConfigServletWebServerApplicationContext extends ServletWebServerApplicationContext extends GenericWebApplicationContext AnnotationConfigServletWebApplicationContext extends GenericWebApplicationContext AnnotationConfigReactiveWebServerApplicationContext extends ReactiveWebServerApplicationContext extends GenericReactiveWebApplicationContext 这里不讲关于web的知识。所以略过。你只需要知道到 postProcessBeanFactory spring是预留的扩展方法。接下来就是重点 invokeBeanFactoryPostProcessors 方法。 invokeBeanFactoryPostProcessors BeanFactoryPostProcessor：beanFactory的后置处理器；在BeanFactory标准初始化之后调用，来定制和修改BeanFactory的内容；所有的bean定义已经保存加载到beanFactory，但是bean的实例还未创建。 BeanDefinitionRegistryPostProcessor 是 BeanFactoryPostProcessor 的子类。 这里需要提到一点， 如果是实现BeanFactoryPostProcessor接口，那么postProcessBeanFactory里只允许对BeanDefinition里已有的注册信息进行修改而不允许新增。这个接口更关注修改。 而BeanDefinitionRegistryPostProcessor这个接口支持自定义beanDefinition的注册，及在标准的注册完成后，实现这个接口可以向beanDefinitionMap中注册自定义的beanDefinition，这个接口从定义上看更多的是关注注册。 示例 package io.github.firehuo.spring.service; import org.springframework.stereotype.Component; @Component public class FireService { public FireService() { System.out.println(\"FireService...constructor\"); } } package io.github.firehuo.spring.service; public class OrdinaryObject { public OrdinaryObject() { System.out.println(\"OrdinaryObject...constructor\"); } } package io.github.firehuo.spring.beanfactorypostprocessor; import io.github.firehuo.spring.service.OrdinaryObject; import org.springframework.beans.BeansException; import org.springframework.beans.factory.config.ConfigurableListableBeanFactory; import org.springframework.beans.factory.support.BeanDefinitionRegistry; import org.springframework.beans.factory.support.BeanDefinitionRegistryPostProcessor; import org.springframework.beans.factory.support.GenericBeanDefinition; import org.springframework.stereotype.Component; @Component public class FireBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor { @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException { System.out.println(\"FireBeanDefinitionRegistryPostProcessor =======\"); // 向beanDefinitionMap中注册自定义的beanDefinition对象 GenericBeanDefinition beanDefinition = new GenericBeanDefinition(); beanDefinition.setBeanClass(OrdinaryObject.class); registry.registerBeanDefinition(\"ordinaryObject\", beanDefinition); } @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { } } package io.github.firehuo.spring.beanfactorypostprocessor; import org.springframework.beans.BeansException; import org.springframework.beans.factory.config.BeanFactoryPostProcessor; import org.springframework.beans.factory.config.ConfigurableListableBeanFactory; import org.springframework.stereotype.Component; import java.util.Arrays; @Component public class FireBeanFactoryPostProcessor implements BeanFactoryPostProcessor { @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { System.out.println(\"FireBeanFactoryPostProcessor...postProcessBeanFactory...\"); int count = beanFactory.getBeanDefinitionCount(); String[] names = beanFactory.getBeanDefinitionNames(); System.out.println(\"当前BeanFactory中有\"+count+\" 个Bean\"); System.out.println(Arrays.asList(names)); } } 2020-11-04 14:08:56.613 INFO 15252 --- [ main] io.github.firehuo.spring.Application : No active profile set, falling back to default profiles: default FireBeanDefinitionRegistryPostProcessor ======= FireBeanFactoryPostProcessor...postProcessBeanFactory... 当前BeanFactory中有42 个Bean [org.springframework.context.annotation.internalConfigurationAnnotationProcessor, org.springframework.context.annotation.internalAutowiredAnnotationProcessor, org.springframework.context.annotation.internalCommonAnnotationProcessor, org.springframework.context.event.internalEventListenerProcessor, org.springframework.context.event.internalEventListenerFactory, application, org.springframework.boot.autoconfigure.internalCachingMetadataReaderFactory, fireBeanDefinitionRegistryPostProcessor, fireBeanFactoryPostProcessor, fireService, org.springframework.boot.autoconfigure.AutoConfigurationPackages, org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration, propertySourcesPlaceholderConfigurer, org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration, mbeanExporter, objectNamingStrategy, mbeanServer, org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration, springApplicationAdminRegistrar, org.springframework.boot.autoconfigure.aop.AopAutoConfiguration$ClassProxyingConfiguration, org.springframework.boot.autoconfigure.aop.AopAutoConfiguration, org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration, applicationAvailability, org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration, org.springframework.boot.context.properties.ConfigurationPropertiesBindingPostProcessor, org.springframework.boot.context.internalConfigurationPropertiesBinderFactory, org.springframework.boot.context.internalConfigurationPropertiesBinder, org.springframework.boot.context.properties.BoundConfigurationProperties, org.springframework.boot.context.properties.ConfigurationBeanFactoryMetadata, org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration, lifecycleProcessor, spring.lifecycle-org.springframework.boot.autoconfigure.context.LifecycleProperties, org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration, spring.info-org.springframework.boot.autoconfigure.info.ProjectInfoProperties, org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration, taskExecutorBuilder, applicationTaskExecutor, spring.task.execution-org.springframework.boot.autoconfigure.task.TaskExecutionProperties, org.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration, taskSchedulerBuilder, spring.task.scheduling-org.springframework.boot.autoconfigure.task.TaskSchedulingProperties, ordinaryObject] FireService...constructor OrdinaryObject...constructor 2020-11-04 14:08:57.211 INFO 15252 --- [ main] io.github.firehuo.spring.Application : Started Application in 1.089 seconds (JVM running for 2.005) 根据启动日志可以发现BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry在BeanFactoryPostProcessor的postProcessBeanFactory方法前执行。 而BeanFactoryPostProcessor的postProcessBeanFactory方法是在FireService创建之前执行的。并且OrdinaryObject作为spring bean 进行了实例化。 详解invokeBeanFactoryPostProcessors流程 进入 invokeBeanFactoryPostProcessors，我们首先会迎来第一个 if 判断，判断当前使用的工厂对象是不是 BeanDefinitionRegistry，这个判断 99% 都会返回 true，为什么呢？ 除非自己继承整个工厂的顶级接口 AliasRegistry 去实现一个完全由自己设计的工厂，这个判断才会走向 else 分支。一般项目根本遇不到这种需求，所以不必深究，因此我们关注 if 分支中的逻辑就好。 如果你有印象可以知道这里的beanFactory是返回了一个**DefaultListableBeanFactory**，而 DefaultListableBeanFactory implements BeanDefinitionRegistry。 if (beanFactory instanceof BeanDefinitionRegistry) { // 通常情况都会进入这块代码段执行，因此这段代码的逻辑才是研究的重点 // .... } else { // 除非改写 beanFactory，且不让它实现 BeanDefinitionRegistry，否则都不会执行这段代码 // 一般来说，这样的改写情况少之又少，少到可以忽略不计 // .... } 然后，我们看到方法中初始化了 2 个集合： if (beanFactory instanceof BeanDefinitionRegistry) { BeanDefinitionRegistry registry = (BeanDefinitionRegistry) beanFactory; // 第一个集合存放程序员手动提供给 Spring 的 BeanFactoryPostProcessor，手动代表不是通过扫描注解得到的，而是我们自己添加进去的。后面画外题中将讲解。 List regularPostProcessors = new ArrayList<>(); // 存放执行过程中找到的 BeanDefinitionRegistryPostProcessor。 // 这些被存放的后置处理器有一个共同的特点就是都已经执行过父类接口的 postProcessBeanDefinitionRegistry，而它们被存放在 registryProcessors 集合中的原因是 // 方便之后会被 invokeBeanFactoryPostProcessors 方法调用，遍历执行子类接口 BeanFactoryPostProcessor 的 postProcessBeanFactory 方法 List registryProcessors = new ArrayList<>(); 接着我们遇到第一个循环。这个循环是为了循环程序员自己手动添加的后置处理器（添加的方法会后续章节阐述）。此时的FireBeanFactoryPostProcessor和FireBeanDefinitionRegistryPostProcessor还没有注册进来。所以此时不会处理。 // 遍历程序员手动添加的 BeanFactory 后置处理器 for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) { / 如果是 BeanDefinitionRegistryPostProcessor，先调用 postProcessBeanDefinitionRegistry 方法，再放入 registryProcessors 集合 if (postProcessor instanceof BeanDefinitionRegistryPostProcessor) { BeanDefinitionRegistryPostProcessor registryProcessor = (BeanDefinitionRegistryPostProcessor) postProcessor; registryProcessor.postProcessBeanDefinitionRegistry(registry); registryProcessors.add(registryProcessor); } else { // 如果只是 BeanFactoryPostProcessor，那么先放入 regularPostProcessors 集合中， // 等待 BeanDefinitionRegistryPostProcessor 都被找到，都执行完 postProcessBeanDefinitionRegistry 方法和 postProcessBeanFactory 方法 // 再执行 regularPostProcessors 集合中的 BeanFactoryPostProcessor.postProcessBeanFactory regularPostProcessors.add(postProcessor); } } // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // Separate between BeanDefinitionRegistryPostProcessors that implement // PriorityOrdered, Ordered, and the rest. List currentRegistryProcessors = new ArrayList<>(); // First, invoke the BeanDefinitionRegistryPostProcessors that implement PriorityOrdered. String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { // ConfigurationClassPostProcessor 实现了 PriorityOrdered 接口，isTypeMatch 返回 true，继续执行 if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { // 执行 getBean 后，会创建 ConfigurationClassPostProcessor 的 Bean 对象，并添加到 beanFactory 的单例池 singletonObjects 中去 // 然后把返回的 Bean 对象放入 currentRegistryProcessors 集合中 currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); } } 紧接着，源码中声明并创建 List currentRegistryProcessors = new ArrayList<>();。 这个集合的作用： 将实现 PriorityOrdered, Ordered 以及剩余的其他 BeanDefinitionRegistryPostProcessor 分离开来 存放通过 beanFactory.getBean 获取并创建的 BeanDefinitionRegistryPostProcessor 的 Bean对象 方便对当前正要执行的一组 BeanDefinitionRegistryPostProcessor 进行必要的排序（比如按照 Ordered 接口的 getOrder 的值，或者按照 beanName 排序） 然后，ListableBeanFactory.getBeanNamesForType 这个方法返回与给定类型（包括子类）匹配的bean的名称。那么能不能找到呢？答案是肯定的。getBeanNamesForType 方法会从 beanDefinitionMap 中寻找合适的类型，然后返回对应的 beanName。 上图是当前 beanDefinitionMap 中存放着所有 beaName 及其的对应 BeanDefinition 实例，从中找到了一个符合条件的后置处理器，他的 beanName 是 \"org.springframework.context.annotation.internalConfigurationAnnotationProcessor\"。 至于internalConfigurationAnnotationProcessor来自于哪里可以作为一个疑问？答案在AnnotationConfigApplicationContext(注解方式)初始化中。 @SpringBootApplication public class Application { public static void main(String[] args) { ConfigurableApplicationContext context = new SpringApplicationBuilder(Application.class).run(args); Object o = context.getBean(\"org.springframework.context.annotation.internalConfigurationAnnotationProcessor\"); System.out.println(o.getClass()); } } 我们通过getClass()方法得到这个Bean定义所代表的Java类是 org.springframework.context.annotation.ConfigurationClassPostProecssor。 它是唯一的 Spring 内置 BeanDefinitionRegistryPostProcessor 实例。 public class ConfigurationClassPostProcessor implements BeanDefinitionRegistryPostProcessor, PriorityOrdered, ResourceLoaderAware, BeanClassLoaderAware, EnvironmentAware { ConfigurationClassPostProecssor 实现 PriorityOrdered 接口，所以会进入 if 内执行代码。 执行 getBean 后，会创建 ConfigurationClassPostProcessor 的 Bean 对象，并添加到 beanFactory 的单例池 singletonObjects 中去。然后把返回的 Bean 对象放入 currentRegistryProcessors 集合中 如下图所示，执行 getBean 之后，DefaultListableBeanFactory 的单例池中新增了一个 ConfigurationClassPostProcessor Bean对象。 ConfigurationClassPostProcessor 这个类是 Spring 初始化的时候就放置到容器里面的，他做的事情很简单，就是解析 Spring 配置类，然后扫描指定的包路径，将符合条件的类，比如@Component，@Bean注解的类，转换成 BeanDefinition，然后存放到 beanDefinitionMap 中。真正执行这些操作是在被调用 postProcessBeanDefinitionRegistry 方法时。 我们接着就来说说这个 invokeBeanDefinitionRegistryPostProcessors 方法： //currentRegistryProcessors 对currentRegistryProcessors进行排序 sortPostProcessors(currentRegistryProcessors, beanFactory); // registryProcessors 中存放的是所有 BeanDefinitionRegistryPostProcessor registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); // 清空当前执行的 BeanDefinitionRegistryPostProcessor，至此currentRegistryProcessors中的BeanDefinitionRegistryPostProcessor全部执行了postProcessBeanDefinitionRegistry方法。 currentRegistryProcessors.clear(); private static void invokeBeanDefinitionRegistryPostProcessors( Collection postProcessors, BeanDefinitionRegistry registry) { for (BeanDefinitionRegistryPostProcessor postProcessor : postProcessors) { //触发 postProcessBeanDefinitionRegistry 方法 postProcessor.postProcessBeanDefinitionRegistry(registry); } } 这个方法会遍历 BeanDefinitionRegistryPostProcessor 列表并且执行 postProcessBeanDefinitionRegistry 方法。 在首次执行时，ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry 方法完成了对项目的扫描， 在此之后，Spring容器中有值了，有了我们配置的所有的应该被 Spring 管理的类！ 比如在本文实验中， FireBeanDefinitionRegistryPostProcessor 和 FireBeanFactoryPostProcessor 还有 FireService 被扫描到并放入了 beanDefinitionMap 中。 注意此时OrdinaryObject还没有注册到beanDefinitionMap中 继续往下看，发现和上面的代码一样，唯一不同的地方就是本次寻找的是实现了Ordered 接口的 BeanDefinitionRegistryPostProcessor // Next, invoke the BeanDefinitionRegistryPostProcessors that implement Ordered. postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { if (!processedBeans.contains(ppName) && beanFactory.isTypeMatch(ppName, Ordered.class)) { currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); } } sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); 这次执行时，postProcessorNames 可以获得 2 个候选,一个是 Spring 内置的 org.springframework.context.annotation.internalConfigurationAnnotationProcessor， 另一个是我们自定义的 FireBeanDefinitionRegistryPostProcessor。 processedBeans 集合存放所有已经处理过的 BeanFactoryPostProcessor 及其子类接口 BeanDefinitionRegistyPostProcessor。 ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry 已经执行过了，所以这一次不会再执行。FireBeanDefinitionRegistryPostProcessor 因为没有实现 Ordered 接口,所以本次也不会执行。 接着往下看，开始调用剩余的 BeanDefintionRegistryPostProcessor: // Finally, invoke all other BeanDefinitionRegistryPostProcessors until no further ones appear. boolean reiterate = true; while (reiterate) { reiterate = false; postProcessorNames = beanFactory.getBeanNamesForType(BeanDefinitionRegistryPostProcessor.class, true, false); for (String ppName : postProcessorNames) { // 与第二次不同，这次不需要实现 Ordered 接口 if (!processedBeans.contains(ppName)) { // 获取并创建 BeanDefinitionRegistryPostProcessor Bean对象 currentRegistryProcessors.add(beanFactory.getBean(ppName, BeanDefinitionRegistryPostProcessor.class)); processedBeans.add(ppName); reiterate = true; } } sortPostProcessors(currentRegistryProcessors, beanFactory); registryProcessors.addAll(currentRegistryProcessors); // 执行currentRegistryProcessors中存放的 BeanDefinitionRegistryPostProcessor Bean对象的 postProcessBeanDefinitonRegistry 方法, //在这里由于执行了FireBeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry方法，所以OrdinaryObject被注册到beanDefinitionMap中 invokeBeanDefinitionRegistryPostProcessors(currentRegistryProcessors, registry); currentRegistryProcessors.clear(); } 问题：为什么要自旋？ 这里之所以要“自旋”（引入while循环和reiterate变量），主要是为了解决嵌套 BeanDefinitionRegistryPostProcessor 的扫描和执行问题。 BeanDefinitionRegistryPostProcessor 是一个接口，在执行它的 postProcessBeanDefinitionRegistry 方法时，可能又注册了一些新的 BeanDefinition， 这些新的 BeanDefinition 又是 BeanDefinitionRegistryPostProcessor 类型的，这种类似于俄罗斯套娃一样，每一个里面都会进行一些注册，谁也不知道会进行多少层，故而需要一个死循环，只要有， 就一直遍历寻找，直到执行完为止！类似与下图这样： 问题：为什么有三段差不多的代码？ spring 为保证不同类型，不同提供者实现的 BeanDefinitionRegistryPostProcessor 的执行顺序，而写了三段看上去差不多的代码。 执行顺序为 首先执行 spring 内置的且实现了 PriorityOrdered 接口的后置处理器 然后执行第三方或者程序员提供的，实现了 Ordered 接口的后置处理器 最后执行第三方或者程序员提供的未处理过的后置处理器 再接着就要执行 invokeBeanFactoryPostProcessors 方法，遍历并且执行 BeanFactoryPostProcessor.postProcessBeanFactory方法。 接着首先是遍历存放在 registryProcessors 中 BeanDefinitionRegistryPostProcessor 实现类对象，并且执行还未被执行过的父类方法 postProcessBeanFactory ，且对象顺序和刚才执行 postProcessBeanDefinitonRegistry 是一样的。 // Now, invoke the postProcessBeanFactory callback of all processors handled so far. // 按照和 postProcessBeanDefinitonRegistry 执行顺序一样的顺序执行 BeanDefinitionRegistryPostProcessor 的 postProcessBeanFactory invokeBeanFactoryPostProcessors(registryProcessors, beanFactory); // 执行手动添加的 BeanFactoryPostProcessor 的 postProcessBeanFactory 方法 invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory); 至此，所有 BeanDefinitionRegistryPostProcessor 对象的 postProcessBeanDefinitonRegistry 和 postProcessBeanFactory 都按顺序执行完毕！ 下面的则是关于BeanFactoryPostProcessor的执行问题。 所以这里也解释了前文示例中提到的“根据启动日志可以发现BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry在BeanFactoryPostProcessor的postProcessBeanFactory方法前执行” 注意这里FireBeanFactoryPostProcessor的postProcessBeanFactory方法还没有执行 BeanFactoryPostProcessor 的执行 进入下半场 BeanFactoryPostProcessor 的执行，首先映入眼帘的是 // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let the bean factory post-processors apply to them! // 首先寻找所有 BeanFactoryPostProcesssor 类 String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanFactoryPostProcessor.class, true, false); // Separate between BeanFactoryPostProcessors that implement PriorityOrdered, // Ordered, and the rest. List priorityOrderedPostProcessors = new ArrayList<>(); List orderedPostProcessorNames = new ArrayList<>(); List nonOrderedPostProcessorNames = new ArrayList<>(); for (String ppName : postProcessorNames) { if (processedBeans.contains(ppName)) { // skip - already processed in first phase above } else if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { priorityOrderedPostProcessors.add(beanFactory.getBean(ppName, BeanFactoryPostProcessor.class)); } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { nonOrderedPostProcessorNames.add(ppName); } } // First, invoke the BeanFactoryPostProcessors that implement PriorityOrdered. sortPostProcessors(priorityOrderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(priorityOrderedPostProcessors, beanFactory); 这段代码不难理解，先寻找所有的 BeanFactoryPostProcessor 类，初始化三个集合，分别存放实现了 PriorityOrdered, Ordered 及其他剩余的 BeanFactoryPostProcessor。 如果是 processedBeans 中已经存在的，那么就会注解跳过，避免重复执行！ 值得注意的是 如果是实现 PriorityOrdered 接口的，会直接 getBean 获取并创建实例，并将实例加入到priorityOrderedPostProcessors集合中 但是实现了 Ordered 接口以及剩余的，会把 beanName 存放到 orderedPostProcessorNames 和 nonOrderedPostProcessorNames 集合中，注意此时他没有进行实例化！ 剩余的一点代码： // Next, invoke the BeanFactoryPostProcessors that implement Ordered. List orderedPostProcessors = new ArrayList<>(orderedPostProcessorNames.size()); // 遍历实现了 Ordered 的 beanName集合 for (String postProcessorName : orderedPostProcessorNames) { // 通过 getBean 实例化，再添加到集合中 orderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); } sortPostProcessors(orderedPostProcessors, beanFactory); invokeBeanFactoryPostProcessors(orderedPostProcessors, beanFactory); // Finally, invoke all other BeanFactoryPostProcessors. List nonOrderedPostProcessors = new ArrayList<>(nonOrderedPostProcessorNames.size()); for (String postProcessorName : nonOrderedPostProcessorNames) { nonOrderedPostProcessors.add(beanFactory.getBean(postProcessorName, BeanFactoryPostProcessor.class)); } invokeBeanFactoryPostProcessors(nonOrderedPostProcessors, beanFactory); // Clear cached merged bean definitions since the post-processors might have // modified the original metadata, e.g. replacing placeholders in values... beanFactory.clearMetadataCache(); 总结 总结了几条主要规律： BeanDefinitionRegistryPostProcessor 早于 BeanFactoryPostProcessor 执行 BeanDefinitionRegistryPostProcessor.postProcessBeanDefinitionRegistry -> BeanDefinitionRegistryPostProcessor.postProcessBeanFactory -> BeanFactoryPostProcessor.postProcessBeanFactory 程序员手动添加的 -> PriorityOrdered -> Ordered -> 剩余的 所有的 BeanFactoryPostProcessor 及其子类 BeanDefinitionRegistryPostProcessor 不会重复执行 画外题--手动添加 BeanFactoryPostProcessor 首先我们要知道，给 Spring 容器提供 BeanFactoryPostProcessor 或者 BeanDefinitionRegistryPostProcessor 的 Bean 对象的方式有两种： 通过加注解，让 Spring 扫描到类（把一个类交给 Spring 管理，整个类的创建过程是交给 Spring 来处理的） 手动添加对象到 Spring 中（自己控制对象的实例化过程，然后再交给 Spring 管理） 示例面试题，如何把一个对象提供给 Spring 管理？ FactoryBean 是一种方式 向 BeanFactory 添加一个单例对象 AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.getBeanFactory().registerSingleton(\"indexService\", new IndexService()); ctx.register(AppConfig.class); ctx.refresh(); ctx.getBean(\"indexService\"); 再回到如何手动添加 BeanFactoryPostProcessor 并提供给 Spring 管理？我们来看代码示例： public class Fire implements BeanFactoryPostProcessor { @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { System.out.println(\"Fire ------ BeanFactoryPostProcessor, 由程序员手动添加，再交给 Spring 管理的\"); } } 这个类都没有加上 @Component 注解,然后我们这样操作： public class MyApplication { public static void main(String[] args) throws IOException { AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(); ctx.addBeanFactoryPostProcessor(new Fire()); ctx.register(AppConfig.class); ctx.refresh(); } } 程序员手动添加的 ManSubY 的父类方法率先执行 程序员手动添加的 ManB 是直接实现 BeanFactoryPostProcessor 的类，在所有直接实现 BeanFactoryPostProcessor 的类中率先执行 "},"Chapter11/BeanPostProcessor.html":{"url":"Chapter11/BeanPostProcessor.html","title":"BeanPostProcessor","keywords":"","body":"BeanPostProcessor 本文源码为spring5.2.9版本 BeanPostProcessor就是Bean的后置处理器，主要作用就是Bean实例化之后，在初始化（initialization）之前和之后调用自定义的方法 改变一些属性。 这里的initialization包含： Bean里面定义的initMethod，InitializingBean的afterPropertiesSet 此外还有一个annotation@PostConstruct和@PreDestroy，也是可以对Bean进行扩展的，但是他们的逻辑和上面的InitializingBean和自定义的initMethod的底层有一点点区别。 他们类似BeanPostProcessor，他对应的类是CommonAnnotationBeanPostProcessor。 回归到 AbstractApplicationContext的*refresh的方法,发现和BeanPostProcessor有关的方法有一个。 // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); 但是不过你之前看过refresh讲解的话，你因该知道registerBeanPostProcessors只是注册，真正的使用是在finishBeanFactoryInitialization方法中。 示例 @Component public class FireService { public FireService() { System.out.println(\"FireService...constructor\"); } } public class OrdinaryObject { public OrdinaryObject() { System.out.println(\"OrdinaryObject...constructor\"); } } @Component public class FireBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor { @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException { System.out.println(\"FireBeanDefinitionRegistryPostProcessor =======\"); // 向beanDefinitionMap中注册自定义的beanDefinition对象 GenericBeanDefinition beanDefinition = new GenericBeanDefinition(); beanDefinition.setBeanClass(OrdinaryObject.class); registry.registerBeanDefinition(\"ordinaryObject\", beanDefinition); } @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException { } } @Component public class FireBeanPostProcessor implements BeanPostProcessor { //前置处理器 @Override public Object postProcessBeforeInitialization(Object bean, String beanName) { System.out.println(\"postProcessBeforeInitialization...\" + beanName + \"=>\" + bean); return bean; } //后置处理器 @Override public Object postProcessAfterInitialization(Object bean, String beanName) { System.out.println(\"postProcessAfterInitialization...\" + beanName + \"=>\" + bean); return bean; } } 执行结果 2020-11-04 17:15:19.238 INFO 4660 --- [ main] io.github.firehuo.spring.Application : No active profile set, falling back to default profiles: default FireBeanDefinitionRegistryPostProcessor ======= postProcessBeforeInitialization...application=>io.github.firehuo.spring.Application$$EnhancerBySpringCGLIB$$9693e87d@6e15fe2 postProcessAfterInitialization...application=>io.github.firehuo.spring.Application$$EnhancerBySpringCGLIB$$9693e87d@6e15fe2 FireService...constructor postProcessBeforeInitialization...fireService=>io.github.firehuo.spring.service.FireService@57ad2aa7 postProcessAfterInitialization...fireService=>io.github.firehuo.spring.service.FireService@57ad2aa7 postProcessBeforeInitialization...org.springframework.boot.autoconfigure.AutoConfigurationPackages=>org.springframework.boot.autoconfigure.AutoConfigurationPackages$BasePackages@934b6cb postProcessAfterInitialization...org.springframework.boot.autoconfigure.AutoConfigurationPackages=>org.springframework.boot.autoconfigure.AutoConfigurationPackages$BasePackages@934b6cb postProcessBeforeInitialization...org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration=>org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration@3b74ac8 postProcessAfterInitialization...org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration=>org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration@3b74ac8 postProcessBeforeInitialization...org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration=>org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration@6d026701 postProcessAfterInitialization...org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration=>org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration@6d026701 postProcessBeforeInitialization...objectNamingStrategy=>org.springframework.boot.autoconfigure.jmx.ParentAwareNamingStrategy@27d5a580 postProcessAfterInitialization...objectNamingStrategy=>org.springframework.boot.autoconfigure.jmx.ParentAwareNamingStrategy@27d5a580 postProcessBeforeInitialization...mbeanServer=>com.sun.jmx.mbeanserver.JmxMBeanServer@1be6f5c3 postProcessAfterInitialization...mbeanServer=>com.sun.jmx.mbeanserver.JmxMBeanServer@1be6f5c3 postProcessBeforeInitialization...mbeanExporter=>org.springframework.jmx.export.annotation.AnnotationMBeanExporter@18e36d14 postProcessAfterInitialization...mbeanExporter=>org.springframework.jmx.export.annotation.AnnotationMBeanExporter@18e36d14 postProcessBeforeInitialization...org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration=>org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration@13d4992d postProcessAfterInitialization...org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration=>org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration@13d4992d postProcessBeforeInitialization...springApplicationAdminRegistrar=>org.springframework.boot.admin.SpringApplicationAdminMXBeanRegistrar@176b75f7 postProcessAfterInitialization...springApplicationAdminRegistrar=>org.springframework.boot.admin.SpringApplicationAdminMXBeanRegistrar@176b75f7 postProcessBeforeInitialization...org.springframework.boot.autoconfigure.aop.AopAutoConfiguration$ClassProxyingConfiguration=>org.springframework.boot.autoconfigure.aop.AopAutoConfiguration$ClassProxyingConfiguration@479ceda0 postProcessAfterInitialization...org.springframework.boot.autoconfigure.aop.AopAutoConfiguration$ClassProxyingConfiguration=>org.springframework.boot.autoconfigure.aop.AopAutoConfiguration$ClassProxyingConfiguration@479ceda0 postProcessBeforeInitialization...org.springframework.boot.autoconfigure.aop.AopAutoConfiguration=>org.springframework.boot.autoconfigure.aop.AopAutoConfiguration@6d07a63d postProcessAfterInitialization...org.springframework.boot.autoconfigure.aop.AopAutoConfiguration=>org.springframework.boot.autoconfigure.aop.AopAutoConfiguration@6d07a63d postProcessBeforeInitialization...org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration=>org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration@571c5681 postProcessAfterInitialization...org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration=>org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration@571c5681 postProcessBeforeInitialization...applicationAvailability=>org.springframework.boot.availability.ApplicationAvailabilityBean@59e32960 postProcessAfterInitialization...applicationAvailability=>org.springframework.boot.availability.ApplicationAvailabilityBean@59e32960 postProcessBeforeInitialization...org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration=>org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration@5b67bb7e postProcessAfterInitialization...org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration=>org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration@5b67bb7e postProcessBeforeInitialization...org.springframework.boot.context.properties.BoundConfigurationProperties=>org.springframework.boot.context.properties.BoundConfigurationProperties@609db546 postProcessAfterInitialization...org.springframework.boot.context.properties.BoundConfigurationProperties=>org.springframework.boot.context.properties.BoundConfigurationProperties@609db546 postProcessBeforeInitialization...org.springframework.boot.context.properties.ConfigurationBeanFactoryMetadata=>org.springframework.boot.context.properties.ConfigurationBeanFactoryMetadata@20f5281c postProcessAfterInitialization...org.springframework.boot.context.properties.ConfigurationBeanFactoryMetadata=>org.springframework.boot.context.properties.ConfigurationBeanFactoryMetadata@20f5281c postProcessBeforeInitialization...org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration=>org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration@56c4278e postProcessAfterInitialization...org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration=>org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration@56c4278e postProcessBeforeInitialization...spring.lifecycle-org.springframework.boot.autoconfigure.context.LifecycleProperties=>org.springframework.boot.autoconfigure.context.LifecycleProperties@1e8b7643 postProcessAfterInitialization...spring.lifecycle-org.springframework.boot.autoconfigure.context.LifecycleProperties=>org.springframework.boot.autoconfigure.context.LifecycleProperties@1e8b7643 postProcessBeforeInitialization...lifecycleProcessor=>org.springframework.context.support.DefaultLifecycleProcessor@73d983ea postProcessAfterInitialization...lifecycleProcessor=>org.springframework.context.support.DefaultLifecycleProcessor@73d983ea postProcessBeforeInitialization...spring.info-org.springframework.boot.autoconfigure.info.ProjectInfoProperties=>org.springframework.boot.autoconfigure.info.ProjectInfoProperties@6f44a157 postProcessAfterInitialization...spring.info-org.springframework.boot.autoconfigure.info.ProjectInfoProperties=>org.springframework.boot.autoconfigure.info.ProjectInfoProperties@6f44a157 postProcessBeforeInitialization...org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration=>org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration@6bc407fd postProcessAfterInitialization...org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration=>org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration@6bc407fd postProcessBeforeInitialization...org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration=>org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration@291f18 postProcessAfterInitialization...org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration=>org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration@291f18 postProcessBeforeInitialization...spring.task.execution-org.springframework.boot.autoconfigure.task.TaskExecutionProperties=>org.springframework.boot.autoconfigure.task.TaskExecutionProperties@36b4091c postProcessAfterInitialization...spring.task.execution-org.springframework.boot.autoconfigure.task.TaskExecutionProperties=>org.springframework.boot.autoconfigure.task.TaskExecutionProperties@36b4091c postProcessBeforeInitialization...taskExecutorBuilder=>org.springframework.boot.task.TaskExecutorBuilder@6d167f58 postProcessAfterInitialization...taskExecutorBuilder=>org.springframework.boot.task.TaskExecutorBuilder@6d167f58 postProcessBeforeInitialization...org.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration=>org.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration@327bcebd postProcessAfterInitialization...org.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration=>org.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration@327bcebd postProcessBeforeInitialization...spring.task.scheduling-org.springframework.boot.autoconfigure.task.TaskSchedulingProperties=>org.springframework.boot.autoconfigure.task.TaskSchedulingProperties@38b27cdc postProcessAfterInitialization...spring.task.scheduling-org.springframework.boot.autoconfigure.task.TaskSchedulingProperties=>org.springframework.boot.autoconfigure.task.TaskSchedulingProperties@38b27cdc postProcessBeforeInitialization...taskSchedulerBuilder=>org.springframework.boot.task.TaskSchedulerBuilder@6239aba6 postProcessAfterInitialization...taskSchedulerBuilder=>org.springframework.boot.task.TaskSchedulerBuilder@6239aba6 OrdinaryObject...constructor postProcessBeforeInitialization...ordinaryObject=>io.github.firehuo.spring.service.OrdinaryObject@3e6104fc postProcessAfterInitialization...ordinaryObject=>io.github.firehuo.spring.service.OrdinaryObject@3e6104fc 2020-11-04 17:15:19.796 INFO 4660 --- [ main] io.github.firehuo.spring.Application : Started Application in 1.07 seconds (JVM running for 2.009) class org.springframework.context.annotation.ConfigurationClassPostProcessor 根据输出日志可以发现，无论是FireService还是OrdinaryObject都是在调用构造函数（及实例化）之后才执行的postProcessBeforeInitialization和postProcessAfterInitialization。 进行如下改造.并执行 @Component public class FireService implements InitializingBean { public FireService() { System.out.println(\"FireService...constructor\"); } @Override public void afterPropertiesSet() throws Exception { System.out.println(\"FireService...afterPropertiesSet\"); } @PostConstruct public void init(){ System.out.println(\"=======FireService init======\"); } } @Component public class FireBeanPostProcessor implements BeanPostProcessor { //前置处理器 @Override public Object postProcessBeforeInitialization(Object bean, String beanName) { if (\"fireService\".equals(beanName)){ System.out.println(\"postProcessBeforeInitialization...\" + beanName + \"=>\" + bean); } return bean; } //后置处理器 @Override public Object postProcessAfterInitialization(Object bean, String beanName) { if (\"fireService\".equals(beanName)){ System.out.println(\"postProcessAfterInitialization...\" + beanName + \"=>\" + bean); } return bean; } } 2020-11-04 17:25:25.583 INFO 11192 --- [ main] io.github.firehuo.spring.Application : No active profile set, falling back to default profiles: default FireService...constructor postProcessBeforeInitialization...fireService=>io.github.firehuo.spring.service.FireService@1722011b =======FireService init====== FireService...afterPropertiesSet postProcessAfterInitialization...fireService=>io.github.firehuo.spring.service.FireService@1722011b 2020-11-04 17:25:26.150 INFO 11192 --- [ main] io.github.firehuo.spring.Application : Started Application in 1.054 seconds (JVM running for 2.244) 源码详解 registerBeanPostProcessors public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) { //找出所有实现BeanPostProcessor接口的类 String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); // Register BeanPostProcessorChecker that logs an info message when // a bean is created during BeanPostProcessor instantiation, i.e. when // a bean is not eligible for getting processed by all BeanPostProcessors. // BeanPostProcessor的目标计数 int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length; //添加BeanPostProcessorChecker(主要用于记录信息)到beanFactory中 beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); 此时的postProcessorNames包含四个,其中我们自定义的FireBeanPostProcessor赫然在列。 继续向下。如果你看过关于BeanFactoryPostProcessor的讲解的话。你会发现里面也有相似的逻辑。 定义不同的变量用于区分: 实现PriorityOrdered接口的BeanPostProcessor、实现Ordered接口的BeanPostProcessor、普通BeanPostProcessor。 // Separate between BeanPostProcessors that implement PriorityOrdered, // Ordered, and the rest. // 用于存放实现PriorityOrdered接口的BeanPostProcessor List priorityOrderedPostProcessors = new ArrayList<>(); // 用于存放Spring内部的BeanPostProcessor List internalPostProcessors = new ArrayList<>(); //用于存放实现Ordered接口的BeanPostProcessor的beanName List orderedPostProcessorNames = new ArrayList<>(); //用于存放普通BeanPostProcessor的beanName List nonOrderedPostProcessorNames = new ArrayList<>(); for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { nonOrderedPostProcessorNames.add(ppName); } } 继续 // First, register the BeanPostProcessors that implement PriorityOrdered. //首先, 注册实现PriorityOrdered接口的BeanPostProcessors //对priorityOrderedPostProcessors进行排序 sortPostProcessors(priorityOrderedPostProcessors, beanFactory); //注册priorityOrderedPostProcessors registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); // Next, register the BeanPostProcessors that implement Ordered. //接下来, 注册实现Ordered接口的BeanPostProcessors List orderedPostProcessors = new ArrayList<>(orderedPostProcessorNames.size()); for (String ppName : orderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } // 对实现Ordered的 进行排序 sortPostProcessors(orderedPostProcessors, beanFactory); //全部添加到beanFactory中 beanFactory.addBeanPostProcessor registerBeanPostProcessors(beanFactory, orderedPostProcessors); 注意这里的registerBeanPostProcessors方法最终是添加到AbstractBeanFactory里的 private final List beanPostProcessors = new CopyOnWriteArrayList<>();中 继续 // Now, register all regular BeanPostProcessors. //注册所有常规的BeanPostProcessors List nonOrderedPostProcessors = new ArrayList<>(nonOrderedPostProcessorNames.size()); for (String ppName : nonOrderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } //全部添加到beanFactory中 beanFactory.addBeanPostProcessor registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); // Finally, re-register all internal BeanPostProcessors. // 最后, 重新注册所有内部BeanPostProcessors（相当于内部的BeanPostProcessor会被移到处理器链的末尾） sortPostProcessors(internalPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, internalPostProcessors); // Re-register post-processor for detecting inner beans as ApplicationListeners, // moving it to the end of the processor chain (for picking up proxies etc). //注册ApplicationListenerDetector 放到最后 beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext)); 排序与注册方法 private static void sortPostProcessors(List postProcessors, ConfigurableListableBeanFactory beanFactory) { Comparator comparatorToUse = null; if (beanFactory instanceof DefaultListableBeanFactory) { // 1.获取设置的比较器 comparatorToUse = ((DefaultListableBeanFactory) beanFactory).getDependencyComparator(); } if (comparatorToUse == null) { // 2.如果没有设置比较器, 则使用默认的OrderComparator comparatorToUse = OrderComparator.INSTANCE; } // 3.使用比较器对postProcessors进行排序 postProcessors.sort(comparatorToUse); } /** * 遍历添加BeanPostProcessor */ private static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, List postProcessors) { for (BeanPostProcessor postProcessor : postProcessors) { beanFactory.addBeanPostProcessor(postProcessor); } } /** * 添加BeanPostProcessor * 先移除之后再添加，顺序会放到最后 */ @Override public void addBeanPostProcessor(BeanPostProcessor beanPostProcessor) { Assert.notNull(beanPostProcessor, \"BeanPostProcessor must not be null\"); this.beanPostProcessors.remove(beanPostProcessor); //先移除 if (beanPostProcessor instanceof InstantiationAwareBeanPostProcessor) { this.hasInstantiationAwareBeanPostProcessors = true; } if (beanPostProcessor instanceof DestructionAwareBeanPostProcessor) { this.hasDestructionAwareBeanPostProcessors = true; } this.beanPostProcessors.add(beanPostProcessor); //再添加 } finishBeanFactoryInitialization 这里对于finishBeanFactoryInitialization 并不是讲解全部，只是讲解关于BeanPostProcessors的部分。所以我们直接定位到AbstractAutowireCapableBeanFactory的initializeBean方法。初始化bean。 前面已经说过BeanPostProcessors作用于bean的实例化后。 初始化之前和之后都有处理。 /** * Initialize the given bean instance, applying factory callbacks * as well as init methods and bean post processors. * Called from {@link #createBean} for traditionally defined beans, * and from {@link #initializeBean} for existing bean instances. * @param beanName the bean name in the factory (for debugging purposes) * @param bean the new bean instance we may need to initialize * @param mbd the bean definition that the bean was created with * (can also be {@code null}, if given an existing bean instance) * @return the initialized bean instance (potentially wrapped) * @see BeanNameAware * @see BeanClassLoaderAware * @see BeanFactoryAware * @see #applyBeanPostProcessorsBeforeInitialization * @see #invokeInitMethods * @see #applyBeanPostProcessorsAfterInitialization */ protected Object initializeBean(String beanName, Object bean, @Nullable RootBeanDefinition mbd) { //首先判断 是否设置了SecurityManagers ，如果设置了，就进行相关的权限配置 和 Aware 的扩展 if (System.getSecurityManager() != null) { AccessController.doPrivileged((PrivilegedAction) () -> { invokeAwareMethods(beanName, bean); return null; }, getAccessControlContext()); } else { //如果没有 直接进行 Aware 的扩展 ,Aware 这块就是 对相关的Bean 额外的配置一些响应的属性 invokeAwareMethods(beanName, bean); } Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { //. 判断bean 是不是应用程序自己定义的，如果不是 ，那就 遍历 运行 BeanPostProcessors 的postProcessBeforeInitialization 方法 这里有一个 getBeanPostProcessors() 方法，里面是获取所有的 实现了BeanPostProcessors 接口的类，这里是如果获取到的呢，在何时放进去的呢，下面会提到. // 上面说到 的 注解 @PostConstruct 是在这一步运行的 ，相当于 BeanPostProcessors 的Beforxxxx 方法 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { //运行 invokeInitMethods ，这里有两种 ，一种是 继承了 InitializingBean ，那就实现 对应的afterPropertiesSet() 方法，或者是自定义的 InitMethod ，通过反射 去调用 上面的InitializingBean 和 自定义的initMethod 是在这一步运行 invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, \"Invocation of init method failed\", ex); } if (mbd == null || !mbd.isSynthetic()) { //判断bean 是不是应用程序自己定义的，如果不是 ，那就 遍历 运行 BeanPostProcessors 的postProcessAfterInitialization @PreDestory是在这一步 完成 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } applyBeanPostProcessorsBeforeInitialization @Override public Object applyBeanPostProcessorsBeforeInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor processor : getBeanPostProcessors()) { Object current = processor.postProcessBeforeInitialization(result, beanName); if (current == null) { return result; } result = current; } return result; } 这里的getBeanPostProcessors()就是在registerBeanPostProcessors中注册的。 invokeInitMethods protected void invokeInitMethods(String beanName, Object bean, @Nullable RootBeanDefinition mbd) throws Throwable { boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean && (mbd == null || !mbd.isExternallyManagedInitMethod(\"afterPropertiesSet\"))) { if (logger.isTraceEnabled()) { logger.trace(\"Invoking afterPropertiesSet() on bean with name '\" + beanName + \"'\"); } if (System.getSecurityManager() != null) { try { AccessController.doPrivileged((PrivilegedExceptionAction) () -> { ((InitializingBean) bean).afterPropertiesSet(); return null; }, getAccessControlContext()); } catch (PrivilegedActionException pae) { throw pae.getException(); } } else { ((InitializingBean) bean).afterPropertiesSet(); } } if (mbd != null && bean.getClass() != NullBean.class) { String initMethodName = mbd.getInitMethodName(); if (StringUtils.hasLength(initMethodName) && !(isInitializingBean && \"afterPropertiesSet\".equals(initMethodName)) && !mbd.isExternallyManagedInitMethod(initMethodName)) { // invoke 反射执行init方法 invokeCustomInitMethod(beanName, bean, mbd); } } } BeanPostProcessor使用 Spring底层对BeanPostProcessor的使用：bean赋值，注入其他组件，@Autowired，生命周期注解功能，@Async等等都是利用 BeanPostProcessor来完成的。 ApplicationContextAwareProcessor可以帮我们给组件中注入IOC容器 BeanValidationPostProcessor用来做数据校验 InitDestroyAnnotationBeanPostProcessor用来处理PostConstruct注解和PreDestroy注解 AutowiredAnnotationBeanPostProcessor用来处理Autowired注解 参照Spring依赖注入@Autowired深层原理、源码级分析 "},"Chapter11/AutowireCandidateResolver.html":{"url":"Chapter11/AutowireCandidateResolver.html","title":"AutowireCandidateResolver深度分析，解析@Lazy、@Qualifier注解的原理","keywords":"","body":"AutowireCandidateResolver深度分析，解析@Lazy、@Qualifier注解的原理 关于AutowireCandidateResolver接口，可能绝大多数小伙伴都会觉得陌生。但若谈起@Autowired、@Primary、@Qualifier、@Value、@Lazy等注解，相信没有小伙伴是不知道的吧。 AutowireCandidateResolver用于确定特定的Bean定义是否符合特定的依赖项的候选者的策略接口。 // @since 2.5 伴随着@Autowired体系出现 public interface AutowireCandidateResolver { // 判断给定的bean定义是否允许被依赖注入（bean定义的默认值都是true） default boolean isAutowireCandidate(BeanDefinitionHolder bdHolder, DependencyDescriptor descriptor) { return bdHolder.getBeanDefinition().isAutowireCandidate(); } // 给定的descriptor是否是必须的~~~ // @since 5.0 default boolean isRequired(DependencyDescriptor descriptor) { return descriptor.isRequired(); } // QualifierAnnotationAutowireCandidateResolver对它有实现 // @since 5.1 此方法出现得非常的晚 default boolean hasQualifier(DependencyDescriptor descriptor) { return false; } // 是否给一个建议值 注入的时候~~~QualifierAnnotationAutowireCandidateResolvert有实现 // @since 3.0 @Nullable default Object getSuggestedValue(DependencyDescriptor descriptor) { return null; } // 如果注入点injection point需要的话，就创建一个proxy来作为最终的解决方案ContextAnnotationAutowireCandidateResolver // @since 4.0 @Nullable default Object getLazyResolutionProxyIfNecessary(DependencyDescriptor descriptor, @Nullable String beanName) { return null; } } 查看它的继承树： SimpleAutowireCandidateResolver (org.springframework.beans.factory.support) GenericTypeAwareAutowireCandidateResolver (org.springframework.beans.factory.support) QualifierAnnotationAutowireCandidateResolver (org.springframework.beans.factory.annotation) ContextAnnotationAutowireCandidateResolver (org.springframework.context.annotation) LazyRepositoryInjectionPointResolver in RepositoryConfigurationDelegate (org.springframework.data.repository.config) 层次特点非常明显：每一层都只有一个类，所以毫无疑问，最后一个实现类肯定是功能最全的了。 GenericTypeAwareAutowireCandidateResolver 从名字可以看出和泛型有关。Spring4.0后的泛型依赖注入主要是它来实现的，所以这个类也是Spring4.0后出现的 //@since 4.0 它能够根据泛型类型进行匹配~~~~ 【泛型依赖注入】 public class GenericTypeAwareAutowireCandidateResolver extends SimpleAutowireCandidateResolver implements BeanFactoryAware { // 它能处理类型 毕竟@Autowired都是按照类型匹配的 @Nullable private BeanFactory beanFactory; // 是否允许被依赖~~~ // 因为bean定义里默认是true，绝大多数情况下我们不会修改它~~~ // 所以继续执行：checkGenericTypeMatch 看看泛型类型是否能够匹配上 // 若能够匹配上 这个就会被当作候选的Bean了~~~ @Override public boolean isAutowireCandidate(BeanDefinitionHolder bdHolder, DependencyDescriptor descriptor) { // 如果bean定义里面已经不允许了 那就不往下走了 显然我们不会这么做 if (!super.isAutowireCandidate(bdHolder, descriptor)) { // If explicitly false, do not proceed with any other checks... return false; } // 处理泛型依赖的核心方法~~~ 也是本实现类的灵魂 // 注意：这里还兼容到了工厂方法模式FactoryMethod // 所以即使你返回BaseDao它是能够很好的处理好类型的~~~ return checkGenericTypeMatch(bdHolder, descriptor); } ... } 本实现类的主要任务就是解决了泛型依赖，此类虽然为实现类，但也不建议直接使用，因为功能还不完整~ QualifierAnnotationAutowireCandidateResolver 这个实现类非常非常的重要，它继承自GenericTypeAwareAutowireCandidateResolver，所以它不仅仅能处理org.springframework.beans.factory.annotation.Qualifier、@Value，还能够处理泛型依赖注入，因此功能已经很完善了~~~ 在Spring2.5之后都使用它来处理依赖关系~ Spring4.0之前它继承自SimpleAutowireCandidateResolver，Spring4.0之后才继承自GenericTypeAwareAutowireCandidateResolver 它不仅仅能够处理@Qualifier注解，也能够处理通过@Value注解解析表达式得到的suggested value，也就是说它还实现了接口方法getSuggestedValue()； getSuggestedValue()方法是Spring3.0后提供的，因为@Value注解是Spring3.0后提供的强大注解。 // @since 2.5 public class QualifierAnnotationAutowireCandidateResolver extends GenericTypeAwareAutowireCandidateResolver { // 支持的注解类型，默认支持@Qualifier和JSR-330的javax.inject.Qualifier注解 private final Set> qualifierTypes = new LinkedHashSet<>(2); private Class valueAnnotationType = Value.class; // 你可可以通过构造函数，增加你自定义的注解的支持~~~ // 注意都是add 不是set public QualifierAnnotationAutowireCandidateResolver(Class qualifierType) { Assert.notNull(qualifierType, \"'qualifierType' must not be null\"); this.qualifierTypes.add(qualifierType); } public QualifierAnnotationAutowireCandidateResolver(Set> qualifierTypes) { Assert.notNull(qualifierTypes, \"'qualifierTypes' must not be null\"); this.qualifierTypes.addAll(qualifierTypes); } // 后面讲的CustomAutowireConfigurer 它会调用这个方法来自定义注解 public void addQualifierType(Class qualifierType) { this.qualifierTypes.add(qualifierType); } //@Value注解类型Spring也是允许我们改成自己的类型的 public void setValueAnnotationType(Class valueAnnotationType) { this.valueAnnotationType = valueAnnotationType; } // 这个实现，比父类的实现就更加的严格了，区分度也就越高了~~~ // checkQualifiers方法是本类的核心，灵魂 // 它有两个方法getQualifiedElementAnnotation和getFactoryMethodAnnotation表名了它支持filed和方法 @Override public boolean isAutowireCandidate(BeanDefinitionHolder bdHolder, DependencyDescriptor descriptor) { boolean match = super.isAutowireCandidate(bdHolder, descriptor); // 这里发现，及时父类都匹配上了，我本来还得再次校验一把~~~ if (match) { // @Qualifier注解在此处生效 最终可能匹配出一个或者0个出来 match = checkQualifiers(bdHolder, descriptor.getAnnotations()); // 若字段上匹配上了还不行，还得看方法上的这个注解 if (match) { // 这里处理的是方法入参们~~~~ 只有方法有入参才需要继续解析 MethodParameter methodParam = descriptor.getMethodParameter(); if (methodParam != null) { Method method = methodParam.getMethod(); // 这个处理非常有意思：methodParam.getMethod()表示这个入参它所属于的方法 // 如果它不属于任何方法或者属于方法的返回值是void 才去看它头上标注的@Qualifier注解 if (method == null || void.class == method.getReturnType()) { match = checkQualifiers(bdHolder, methodParam.getMethodAnnotations()); } } } } return match; } ... protected boolean isQualifier(Class annotationType) { for (Class qualifierType : this.qualifierTypes) { if (annotationType.equals(qualifierType) || annotationType.isAnnotationPresent(qualifierType)) { return true; } } return false; } // 这里显示的使用了Autowired 注解，我个人感觉这里是不应该的~~~~ 毕竟已经到这一步了 应该脱离@Autowired注解本身 // 当然，这里相当于是做了个fallback~~~还算可以接受吧 @Override public boolean isRequired(DependencyDescriptor descriptor) { if (!super.isRequired(descriptor)) { return false; } Autowired autowired = descriptor.getAnnotation(Autowired.class); return (autowired == null || autowired.required()); } // 标注的所有注解里 是否有@Qualifier这个注解~ @Override public boolean hasQualifier(DependencyDescriptor descriptor) { for (Annotation ann : descriptor.getAnnotations()) { if (isQualifier(ann.annotationType())) { return true; } } return false; } // @since 3.0 这是本类的另外一个核心 解析@Value注解 // 需要注意的是此类它不负责解析占位符啥的 只复杂把字符串返回 // 最终是交给value = evaluateBeanDefinitionString(strVal, bd);它处理~~~ @Override @Nullable public Object getSuggestedValue(DependencyDescriptor descriptor) { // 拿到value注解（当然不一定是@Value注解 可以自定义嘛） 并且拿到它的注解属性value值~~~ 比如#{person} Object value = findValue(descriptor.getAnnotations()); if (value == null) { // 相当于@Value注解标注在方法入参上 也是阔仪的~~~~~ MethodParameter methodParam = descriptor.getMethodParameter(); if (methodParam != null) { value = findValue(methodParam.getMethodAnnotations()); } } return value; } ... } 这个注解的功能已经非常强大了，Spring4.0之前都是使用的它去解决候选、依赖问题，但也不建议直接使用，因为下面这个，也就是它的子类更为强大~ ContextAnnotationAutowireCandidateResolver 官方把这个类描述为：策略接口的完整实现。它不仅仅支持上面所有描述的功能，还支持@Lazy懒处理~~~(注意此处懒处理(延迟处理)，不是懒加载~) @Lazy一般含义是懒加载，它只会作用于BeanDefinition.setLazyInit()。而此处给它增加了一个能力：延迟处理（代理处理） // @since 4.0 出现得挺晚，它支持到了@Lazy 是功能最全的AutowireCandidateResolver public class ContextAnnotationAutowireCandidateResolver extends QualifierAnnotationAutowireCandidateResolver { // 这是此类本身唯一做的事，此处精析 // 返回该 lazy proxy 表示延迟初始化，实现过程是查看在 @Autowired 注解处是否使用了 @Lazy = true 注解 @Override @Nullable public Object getLazyResolutionProxyIfNecessary(DependencyDescriptor descriptor, @Nullable String beanName) { // 如果isLazy=true 那就返回一个代理，否则返回null // 相当于若标注了@Lazy注解，就会返回一个代理（当然@Lazy注解的value值不能是false） return (isLazy(descriptor) ? buildLazyResolutionProxy(descriptor, beanName) : null); } // 这个比较简单，@Lazy注解标注了就行（value属性默认值是true） // @Lazy支持标注在属性上和方法入参上~~~ 这里都会解析 protected boolean isLazy(DependencyDescriptor descriptor) { for (Annotation ann : descriptor.getAnnotations()) { Lazy lazy = AnnotationUtils.getAnnotation(ann, Lazy.class); if (lazy != null && lazy.value()) { return true; } } MethodParameter methodParam = descriptor.getMethodParameter(); if (methodParam != null) { Method method = methodParam.getMethod(); if (method == null || void.class == method.getReturnType()) { Lazy lazy = AnnotationUtils.getAnnotation(methodParam.getAnnotatedElement(), Lazy.class); if (lazy != null && lazy.value()) { return true; } } } return false; } // 核心内容，是本类的灵魂~~~ protected Object buildLazyResolutionProxy(final DependencyDescriptor descriptor, final @Nullable String beanName) { Assert.state(getBeanFactory() instanceof DefaultListableBeanFactory, \"BeanFactory needs to be a DefaultListableBeanFactory\"); // 这里毫不客气的使用了面向实现类编程，使用了DefaultListableBeanFactory.doResolveDependency()方法~~~ final DefaultListableBeanFactory beanFactory = (DefaultListableBeanFactory) getBeanFactory(); //TargetSource 是它实现懒加载的核心原因，在AOP那一章节了重点提到过这个接口，此处不再叙述 // 它有很多的著名实现如HotSwappableTargetSource、SingletonTargetSource、LazyInitTargetSource、 //SimpleBeanTargetSource、ThreadLocalTargetSource、PrototypeTargetSource等等非常多 // 此处因为只需要自己用，所以采用匿名内部类的方式实现~~~ 此处最重要是看getTarget方法，它在被使用的时候（也就是代理对象真正使用的时候执行~~~） TargetSource ts = new TargetSource() { @Override public Class getTargetClass() { return descriptor.getDependencyType(); } @Override public boolean isStatic() { return false; } // getTarget是调用代理方法的时候会调用的，所以执行每个代理方法都会执行此方法，这也是为何doResolveDependency // 我个人认为它在效率上，是存在一定的问题的~~~所以此处建议尽量少用@Lazy~~~ //不过效率上应该还好，对比http、序列化反序列化处理，简直不值一提 所以还是无所谓 用吧 @Override public Object getTarget() { Object target = beanFactory.doResolveDependency(descriptor, beanName, null, null); if (target == null) { Class type = getTargetClass(); // 对多值注入的空值的友好处理（不要用null） if (Map.class == type) { return Collections.emptyMap(); } else if (List.class == type) { return Collections.emptyList(); } else if (Set.class == type || Collection.class == type) { return Collections.emptySet(); } throw new NoSuchBeanDefinitionException(descriptor.getResolvableType(), \"Optional dependency not present for lazy injection point\"); } return target; } @Override public void releaseTarget(Object target) { } }; // 使用ProxyFactory 给ts生成一个代理 // 由此可见最终生成的代理对象的目标对象其实是TargetSource,而TargetSource的目标才是我们业务的对象 ProxyFactory pf = new ProxyFactory(); pf.setTargetSource(ts); Class dependencyType = descriptor.getDependencyType(); // 如果注入的语句是这么写的private AInterface a; 那这类就是借口 值是true // 把这个接口类型也得放进去（不然这个代理都不属于这个类型，反射set的时候岂不直接报错了吗？？？？） if (dependencyType.isInterface()) { pf.addInterface(dependencyType); } return pf.getProxy(beanFactory.getBeanClassLoader()); } } 它很好的用到了TargetSource这个接口，结合动态代理来支持到了@Lazy注解。 标注有@Lazy注解完成注入的时候，最终注入只是一个此处临时生成的代理对象，只有在真正执行目标方法的时候才会去容器内拿到真是的bean实例来执行目标方法。 特别注意：此代理对象非彼代理对象，这个一定一定一定要区分开来~ 通过@Lazy注解能够解决很多情况下的循环依赖问题，它的基本思想是先'随便'给你创建一个代理对象先放着，等你真正执行方法的时候再实际去容器内找出目标实例执行~ 我们要明白这种解决问题的思路带来的好处是能够解决很多场景下的循环依赖问题，但是要知道它每次执行目标方法的时候都会去执行TargetSource.getTarget()方法，所以需要做好缓存，避免对执行效率的影响（实测执行效率上的影响可以忽略不计） ContextAnnotationAutowireCandidateResolver这个处理器才是被Bean工厂最终最终使用的，因为它的功能是最全的~ 回顾一下，注册进Bean工厂的参考代码处： public abstract class AnnotationConfigUtils { ... public static Set registerAnnotationConfigProcessors(BeanDefinitionRegistry registry, @Nullable Object source) { DefaultListableBeanFactory beanFactory = unwrapDefaultListableBeanFactory(registry); if (beanFactory != null) { // 设置默认的排序器 支持@Order等 if (!(beanFactory.getDependencyComparator() instanceof AnnotationAwareOrderComparator)) { beanFactory.setDependencyComparator(AnnotationAwareOrderComparator.INSTANCE); } // 设置依赖注入的候选处理器 // 可以看到只要不是ContextAnnotationAutowireCandidateResolver类型 直接升级为最强类型 if (!(beanFactory.getAutowireCandidateResolver() instanceof ContextAnnotationAutowireCandidateResolver)) { beanFactory.setAutowireCandidateResolver(new ContextAnnotationAutowireCandidateResolver()); } } ... // ====下面是大家熟悉的注册默认6大后置处理器：==== // 1.ConfigurationClassPostProcessor // 2.AutowiredAnnotationBeanPostProcessor // 3.CommonAnnotationBeanPostProcessor // 4.Jpa的PersistenceAnnotationProcessor（没导包就不会注册） // 5.EventListenerMethodProcessor // 6.DefaultEventListenerFactory } ... } 此段代码执行还是非常早的，在容器的刷新时的ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();这一步就完成了~ 最后，把这个四个哥们 从上至下 简单总结如下： SimpleAutowireCandidateResolver 相当于一个简单的适配器 GenericTypeAwareAutowireCandidateResolver 判断泛型是否匹配，支持泛型依赖注入（From Spring4.0） QualifierAnnotationAutowireCandidateResolver 处理 @Qualifier 和 @Value 注解 ContextAnnotationAutowireCandidateResolver 处理 @Lazy 注解，重写了 getLazyResolutionProxyIfNecessary 方法。 "},"Chapter11/Autowired.html":{"url":"Chapter11/Autowired.html","title":"Spring依赖注入@Autowired深层原理、源码级分析","keywords":"","body":"Spring依赖注入@Autowired深层原理、源码级分析 前言 关于Spring IOC的依赖注入（DI机制），之前虽有过分析，但总感觉一直落了一块：对@Autowired注解元数据的解析部分。 解析@Resource注解的不是这个类，而是CommonAnnotationBeanPostProcessor，但本文只会以AutowiredAnnotationBeanPostProcessor为例做深入分析 getBean()时，进行bean对象的创建和bean依赖注入，跟xml方式类似，只是对于依赖注入@Autowired处理依赖有区别。 解析注解元信息阶段 public class AutowiredAnnotationBeanPostProcessor extends InstantiationAwareBeanPostProcessorAdapter implements MergedBeanDefinitionPostProcessor, PriorityOrdered, BeanFactoryAware { protected final Log logger = LogFactory.getLog(getClass()); private final Set> autowiredAnnotationTypes = new LinkedHashSet<>(4); private String requiredParameterName = \"required\"; private boolean requiredParameterValue = true; private int order = Ordered.LOWEST_PRECEDENCE - 2; @Nullable private ConfigurableListableBeanFactory beanFactory; private final Set lookupMethodsChecked = Collections.newSetFromMap(new ConcurrentHashMap<>(256)); private final Map, Constructor[]> candidateConstructorsCache = new ConcurrentHashMap<>(256); // 方法注入、字段filed注入 本文的重中之重 // 此处InjectionMetadata这个类非常重要，到了此处@Autowired注解含义已经没有了，完全被准备成这个元数据了 所以方便我们自定义注解的支持~~~优秀 // InjectionMetadata持有targetClass、Collection injectedElements等两个重要属性 // 其中InjectedElement这个抽象类最重要的两个实现为：AutowiredFieldElement和AutowiredMethodElement private final Map injectionMetadataCache = new ConcurrentHashMap<>(256); /** * Create a new {@code AutowiredAnnotationBeanPostProcessor} for Spring's * standard {@link Autowired @Autowired} and {@link Value @Value} annotations. * Also supports JSR-330's {@link javax.inject.Inject @Inject} annotation, * if available. */ // 这是它唯一构造函数 默认支持下面三种租借（当然@Inject需要额外导包） // 请注意：此处@Value注解也是被依赖注入解析的~~~~~~~~ // 当然如果你需要支持到你的自定义注解，你还可以调用下面的set方法添加。。。 @SuppressWarnings(\"unchecked\") public AutowiredAnnotationBeanPostProcessor() { this.autowiredAnnotationTypes.add(Autowired.class); this.autowiredAnnotationTypes.add(Value.class); try { this.autowiredAnnotationTypes.add((Class) ClassUtils.forName(\"javax.inject.Inject\", AutowiredAnnotationBeanPostProcessor.class.getClassLoader())); logger.trace(\"JSR-330 'javax.inject.Inject' annotation found and supported for autowiring\"); } catch (ClassNotFoundException ex) { // JSR-330 API not available - simply skip. } } /** * Set the 'autowired' annotation type, to be used on constructors, fields, * setter methods, and arbitrary config methods. * The default autowired annotation types are the Spring-provided * {@link Autowired @Autowired} and {@link Value @Value} annotations as well * as JSR-330's {@link javax.inject.Inject @Inject} annotation, if available. * This setter property exists so that developers can provide their own * (non-Spring-specific) annotation type to indicate that a member is supposed * to be autowired. */ // 下面两个方法可以自定义支持的依赖注入注解类型 public void setAutowiredAnnotationType(Class autowiredAnnotationType) { Assert.notNull(autowiredAnnotationType, \"'autowiredAnnotationType' must not be null\"); this.autowiredAnnotationTypes.clear(); this.autowiredAnnotationTypes.add(autowiredAnnotationType); } /** * Set the 'autowired' annotation types, to be used on constructors, fields, * setter methods, and arbitrary config methods. * The default autowired annotation types are the Spring-provided * {@link Autowired @Autowired} and {@link Value @Value} annotations as well * as JSR-330's {@link javax.inject.Inject @Inject} annotation, if available. * This setter property exists so that developers can provide their own * (non-Spring-specific) annotation types to indicate that a member is supposed * to be autowired. */ public void setAutowiredAnnotationTypes(Set> autowiredAnnotationTypes) { Assert.notEmpty(autowiredAnnotationTypes, \"'autowiredAnnotationTypes' must not be empty\"); this.autowiredAnnotationTypes.clear(); this.autowiredAnnotationTypes.addAll(autowiredAnnotationTypes); } /** * Set the name of an attribute of the annotation that specifies whether it is required. * @see #setRequiredParameterValue(boolean) */ public void setRequiredParameterName(String requiredParameterName) { this.requiredParameterName = requiredParameterName; } /** * Set the boolean value that marks a dependency as required. * For example if using 'required=true' (the default), this value should be * {@code true}; but if using 'optional=false', this value should be {@code false}. * @see #setRequiredParameterName(String) */ public void setRequiredParameterValue(boolean requiredParameterValue) { this.requiredParameterValue = requiredParameterValue; } public void setOrder(int order) { this.order = order; } @Override public int getOrder() { return this.order; } // bean工厂必须是ConfigurableListableBeanFactory的（此处放心使用，唯独只有SimpleJndiBeanFactory不是它的子类而已~） @Override public void setBeanFactory(BeanFactory beanFactory) { if (!(beanFactory instanceof ConfigurableListableBeanFactory)) { throw new IllegalArgumentException( \"AutowiredAnnotationBeanPostProcessor requires a ConfigurableListableBeanFactory: \" + beanFactory); } this.beanFactory = (ConfigurableListableBeanFactory) beanFactory; } // 第一个非常重要的核心方法~~~ //它负责1、解析@Autowired等注解然后转换 // 2、把注解信息转换为InjectionMetadata然后缓存到上面的injectionMetadataCache里面 // postProcessMergedBeanDefinition的执行时机非常早，在doCreateBean()前部分完成bean定义信息的合并 @Override public void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class beanType, String beanName) { // findAutowiringMetadata方法重要，完成了解析注解、缓存下来的操作 InjectionMetadata metadata = findAutowiringMetadata(beanName, beanType, null); metadata.checkConfigMembers(beanDefinition); } @Override public void resetBeanDefinition(String beanName) { this.lookupMethodsChecked.remove(beanName); this.injectionMetadataCache.remove(beanName); } @Override @Nullable public Constructor[] determineCandidateConstructors(Class beanClass, final String beanName) throws BeanCreationException { // Let's check for lookup methods here... if (!this.lookupMethodsChecked.contains(beanName)) { if (AnnotationUtils.isCandidateClass(beanClass, Lookup.class)) { try { Class targetClass = beanClass; do { ReflectionUtils.doWithLocalMethods(targetClass, method -> { Lookup lookup = method.getAnnotation(Lookup.class); if (lookup != null) { Assert.state(this.beanFactory != null, \"No BeanFactory available\"); LookupOverride override = new LookupOverride(method, lookup.value()); try { RootBeanDefinition mbd = (RootBeanDefinition) this.beanFactory.getMergedBeanDefinition(beanName); mbd.getMethodOverrides().addOverride(override); } catch (NoSuchBeanDefinitionException ex) { throw new BeanCreationException(beanName, \"Cannot apply @Lookup to beans without corresponding bean definition\"); } } }); targetClass = targetClass.getSuperclass(); } while (targetClass != null && targetClass != Object.class); } catch (IllegalStateException ex) { throw new BeanCreationException(beanName, \"Lookup method resolution failed\", ex); } } this.lookupMethodsChecked.add(beanName); } // Quick check on the concurrent map first, with minimal locking. Constructor[] candidateConstructors = this.candidateConstructorsCache.get(beanClass); if (candidateConstructors == null) { // Fully synchronized resolution now... synchronized (this.candidateConstructorsCache) { candidateConstructors = this.candidateConstructorsCache.get(beanClass); if (candidateConstructors == null) { Constructor[] rawCandidates; try { rawCandidates = beanClass.getDeclaredConstructors(); } catch (Throwable ex) { throw new BeanCreationException(beanName, \"Resolution of declared constructors on bean Class [\" + beanClass.getName() + \"] from ClassLoader [\" + beanClass.getClassLoader() + \"] failed\", ex); } List> candidates = new ArrayList<>(rawCandidates.length); Constructor requiredConstructor = null; Constructor defaultConstructor = null; Constructor primaryConstructor = BeanUtils.findPrimaryConstructor(beanClass); int nonSyntheticConstructors = 0; for (Constructor candidate : rawCandidates) { if (!candidate.isSynthetic()) { nonSyntheticConstructors++; } else if (primaryConstructor != null) { continue; } MergedAnnotation ann = findAutowiredAnnotation(candidate); if (ann == null) { Class userClass = ClassUtils.getUserClass(beanClass); if (userClass != beanClass) { try { Constructor superCtor = userClass.getDeclaredConstructor(candidate.getParameterTypes()); ann = findAutowiredAnnotation(superCtor); } catch (NoSuchMethodException ex) { // Simply proceed, no equivalent superclass constructor found... } } } if (ann != null) { if (requiredConstructor != null) { throw new BeanCreationException(beanName, \"Invalid autowire-marked constructor: \" + candidate + \". Found constructor with 'required' Autowired annotation already: \" + requiredConstructor); } boolean required = determineRequiredStatus(ann); if (required) { if (!candidates.isEmpty()) { throw new BeanCreationException(beanName, \"Invalid autowire-marked constructors: \" + candidates + \". Found constructor with 'required' Autowired annotation: \" + candidate); } requiredConstructor = candidate; } candidates.add(candidate); } else if (candidate.getParameterCount() == 0) { defaultConstructor = candidate; } } if (!candidates.isEmpty()) { // Add default constructor to list of optional constructors, as fallback. if (requiredConstructor == null) { if (defaultConstructor != null) { candidates.add(defaultConstructor); } else if (candidates.size() == 1 && logger.isInfoEnabled()) { logger.info(\"Inconsistent constructor declaration on bean with name '\" + beanName + \"': single autowire-marked constructor flagged as optional - \" + \"this constructor is effectively required since there is no \" + \"default constructor to fall back to: \" + candidates.get(0)); } } candidateConstructors = candidates.toArray(new Constructor[0]); } else if (rawCandidates.length == 1 && rawCandidates[0].getParameterCount() > 0) { candidateConstructors = new Constructor[] {rawCandidates[0]}; } else if (nonSyntheticConstructors == 2 && primaryConstructor != null && defaultConstructor != null && !primaryConstructor.equals(defaultConstructor)) { candidateConstructors = new Constructor[] {primaryConstructor, defaultConstructor}; } else if (nonSyntheticConstructors == 1 && primaryConstructor != null) { candidateConstructors = new Constructor[] {primaryConstructor}; } else { candidateConstructors = new Constructor[0]; } this.candidateConstructorsCache.put(beanClass, candidateConstructors); } } } return (candidateConstructors.length > 0 ? candidateConstructors : null); } @Override public PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) { InjectionMetadata metadata = findAutowiringMetadata(beanName, bean.getClass(), pvs); try { metadata.inject(bean, beanName, pvs); } catch (BeanCreationException ex) { throw ex; } catch (Throwable ex) { throw new BeanCreationException(beanName, \"Injection of autowired dependencies failed\", ex); } return pvs; } @Deprecated @Override public PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) { return postProcessProperties(pvs, bean, beanName); } /** * 'Native' processing method for direct calls with an arbitrary target instance, * resolving all of its fields and methods which are annotated with one of the * configured 'autowired' annotation types. * @param bean the target instance to process * @throws BeanCreationException if autowiring failed * @see #setAutowiredAnnotationTypes(Set) */ public void processInjection(Object bean) throws BeanCreationException { Class clazz = bean.getClass(); InjectionMetadata metadata = findAutowiringMetadata(clazz.getName(), clazz, null); try { metadata.inject(bean, null, null); } catch (BeanCreationException ex) { throw ex; } catch (Throwable ex) { throw new BeanCreationException( \"Injection of autowired dependencies failed for class [\" + clazz + \"]\", ex); } } // 方法名为查找到该bean的依赖注入元信息，内部只要查找到了就会加入到缓存内，下次没必要再重复查找了~ // 它是一个模版方法，真正做事的方法是：buildAutowiringMetadata 它复杂把标注有@Autowired注解的属性转换为Metadata元数据信息 从而消除注解的定义 // 此处查找包括了字段依赖注入和方法依赖注入~~~ private InjectionMetadata findAutowiringMetadata(String beanName, Class clazz, @Nullable PropertyValues pvs) { // Fall back to class name as cache key, for backwards compatibility with custom callers. String cacheKey = (StringUtils.hasLength(beanName) ? beanName : clazz.getName()); // Quick check on the concurrent map first, with minimal locking. InjectionMetadata metadata = this.injectionMetadataCache.get(cacheKey); if (InjectionMetadata.needsRefresh(metadata, clazz)) { synchronized (this.injectionMetadataCache) { metadata = this.injectionMetadataCache.get(cacheKey); if (InjectionMetadata.needsRefresh(metadata, clazz)) { if (metadata != null) { metadata.clear(pvs); } metadata = buildAutowiringMetadata(clazz); this.injectionMetadataCache.put(cacheKey, metadata); } } } return metadata; } // 这里我认为是整个依赖注入前期工作的精髓所在，简单粗暴的可以理解为：它把以依赖注入都转换为InjectionMetadata元信息，待后续使用 // 这里会处理字段注入、方法注入~~~ // 注意：Autowired使用在static字段/方法上、0个入参的方法上（不会报错 只是无效） // 显然方法的访问级别、是否final都是可以正常被注入进来的~~~ private InjectionMetadata buildAutowiringMetadata(final Class clazz) { if (!AnnotationUtils.isCandidateClass(clazz, this.autowiredAnnotationTypes)) { return InjectionMetadata.EMPTY; } List elements = new ArrayList<>(); Class targetClass = clazz; do { final List currElements = new ArrayList<>(); ReflectionUtils.doWithLocalFields(targetClass, field -> { MergedAnnotation ann = findAutowiredAnnotation(field); if (ann != null) { if (Modifier.isStatic(field.getModifiers())) { if (logger.isInfoEnabled()) { logger.info(\"Autowired annotation is not supported on static fields: \" + field); } return; } boolean required = determineRequiredStatus(ann); currElements.add(new AutowiredFieldElement(field, required)); } }); ReflectionUtils.doWithLocalMethods(targetClass, method -> { Method bridgedMethod = BridgeMethodResolver.findBridgedMethod(method); if (!BridgeMethodResolver.isVisibilityBridgeMethodPair(method, bridgedMethod)) { return; } MergedAnnotation ann = findAutowiredAnnotation(bridgedMethod); if (ann != null && method.equals(ClassUtils.getMostSpecificMethod(method, clazz))) { if (Modifier.isStatic(method.getModifiers())) { if (logger.isInfoEnabled()) { logger.info(\"Autowired annotation is not supported on static methods: \" + method); } return; } if (method.getParameterCount() == 0) { if (logger.isInfoEnabled()) { logger.info(\"Autowired annotation should only be used on methods with parameters: \" + method); } } boolean required = determineRequiredStatus(ann); PropertyDescriptor pd = BeanUtils.findPropertyForMethod(bridgedMethod, clazz); currElements.add(new AutowiredMethodElement(method, required, pd)); } }); elements.addAll(0, currElements); targetClass = targetClass.getSuperclass(); } while (targetClass != null && targetClass != Object.class); return InjectionMetadata.forElements(elements, clazz); } @Nullable private MergedAnnotation findAutowiredAnnotation(AccessibleObject ao) { MergedAnnotations annotations = MergedAnnotations.from(ao); for (Class type : this.autowiredAnnotationTypes) { MergedAnnotation annotation = annotations.get(type); if (annotation.isPresent()) { return annotation; } } return null; } /** * Determine if the annotated field or method requires its dependency. * A 'required' dependency means that autowiring should fail when no beans * are found. Otherwise, the autowiring process will simply bypass the field * or method when no beans are found. * @param ann the Autowired annotation * @return whether the annotation indicates that a dependency is required */ @SuppressWarnings({\"deprecation\", \"cast\"}) protected boolean determineRequiredStatus(MergedAnnotation ann) { // The following (AnnotationAttributes) cast is required on JDK 9+. return determineRequiredStatus((AnnotationAttributes) ann.asMap(mergedAnnotation -> new AnnotationAttributes(mergedAnnotation.getType()))); } /** * Determine if the annotated field or method requires its dependency. * A 'required' dependency means that autowiring should fail when no beans * are found. Otherwise, the autowiring process will simply bypass the field * or method when no beans are found. * @param ann the Autowired annotation * @return whether the annotation indicates that a dependency is required * @deprecated since 5.2, in favor of {@link #determineRequiredStatus(MergedAnnotation)} */ @Deprecated protected boolean determineRequiredStatus(AnnotationAttributes ann) { return (!ann.containsKey(this.requiredParameterName) || this.requiredParameterValue == ann.getBoolean(this.requiredParameterName)); } /** * Obtain all beans of the given type as autowire candidates. * @param type the type of the bean * @return the target beans, or an empty Collection if no bean of this type is found * @throws BeansException if bean retrieval failed */ protected Map findAutowireCandidates(Class type) throws BeansException { if (this.beanFactory == null) { throw new IllegalStateException(\"No BeanFactory configured - \" + \"override the getBeanOfType method or specify the 'beanFactory' property\"); } return BeanFactoryUtils.beansOfTypeIncludingAncestors(this.beanFactory, type); } /** * Register the specified bean as dependent on the autowired beans. */ private void registerDependentBeans(@Nullable String beanName, Set autowiredBeanNames) { if (beanName != null) { for (String autowiredBeanName : autowiredBeanNames) { if (this.beanFactory != null && this.beanFactory.containsBean(autowiredBeanName)) { this.beanFactory.registerDependentBean(autowiredBeanName, beanName); } if (logger.isTraceEnabled()) { logger.trace(\"Autowiring by type from bean name '\" + beanName + \"' to bean named '\" + autowiredBeanName + \"'\"); } } } } /** * Resolve the specified cached method argument or field value. */ @Nullable private Object resolvedCachedArgument(@Nullable String beanName, @Nullable Object cachedArgument) { if (cachedArgument instanceof DependencyDescriptor) { DependencyDescriptor descriptor = (DependencyDescriptor) cachedArgument; Assert.state(this.beanFactory != null, \"No BeanFactory available\"); return this.beanFactory.resolveDependency(descriptor, beanName, null, null); } else { return cachedArgument; } } /** * Class representing injection information about an annotated field. */ private class AutowiredFieldElement extends InjectionMetadata.InjectedElement { private final boolean required; private volatile boolean cached = false; @Nullable private volatile Object cachedFieldValue; public AutowiredFieldElement(Field field, boolean required) { super(field, null); this.required = required; } @Override protected void inject(Object bean, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable { Field field = (Field) this.member; Object value; if (this.cached) { value = resolvedCachedArgument(beanName, this.cachedFieldValue); } else { DependencyDescriptor desc = new DependencyDescriptor(field, this.required); desc.setContainingClass(bean.getClass()); Set autowiredBeanNames = new LinkedHashSet<>(1); Assert.state(beanFactory != null, \"No BeanFactory available\"); TypeConverter typeConverter = beanFactory.getTypeConverter(); try { value = beanFactory.resolveDependency(desc, beanName, autowiredBeanNames, typeConverter); } catch (BeansException ex) { throw new UnsatisfiedDependencyException(null, beanName, new InjectionPoint(field), ex); } synchronized (this) { if (!this.cached) { if (value != null || this.required) { this.cachedFieldValue = desc; registerDependentBeans(beanName, autowiredBeanNames); if (autowiredBeanNames.size() == 1) { String autowiredBeanName = autowiredBeanNames.iterator().next(); if (beanFactory.containsBean(autowiredBeanName) && beanFactory.isTypeMatch(autowiredBeanName, field.getType())) { this.cachedFieldValue = new ShortcutDependencyDescriptor( desc, autowiredBeanName, field.getType()); } } } else { this.cachedFieldValue = null; } this.cached = true; } } } if (value != null) { ReflectionUtils.makeAccessible(field); field.set(bean, value); } } } /** * Class representing injection information about an annotated method. */ private class AutowiredMethodElement extends InjectionMetadata.InjectedElement { private final boolean required; private volatile boolean cached = false; @Nullable private volatile Object[] cachedMethodArguments; public AutowiredMethodElement(Method method, boolean required, @Nullable PropertyDescriptor pd) { super(method, pd); this.required = required; } @Override protected void inject(Object bean, @Nullable String beanName, @Nullable PropertyValues pvs) throws Throwable { if (checkPropertySkipping(pvs)) { return; } Method method = (Method) this.member; Object[] arguments; if (this.cached) { // Shortcut for avoiding synchronization... arguments = resolveCachedArguments(beanName); } else { int argumentCount = method.getParameterCount(); arguments = new Object[argumentCount]; DependencyDescriptor[] descriptors = new DependencyDescriptor[argumentCount]; Set autowiredBeans = new LinkedHashSet<>(argumentCount); Assert.state(beanFactory != null, \"No BeanFactory available\"); TypeConverter typeConverter = beanFactory.getTypeConverter(); for (int i = 0; i it = autowiredBeans.iterator(); Class[] paramTypes = method.getParameterTypes(); for (int i = 0; i requiredType; public ShortcutDependencyDescriptor(DependencyDescriptor original, String shortcut, Class requiredType) { super(original); this.shortcut = shortcut; this.requiredType = requiredType; } @Override public Object resolveShortcut(BeanFactory beanFactory) { return beanFactory.getBean(this.shortcut, this.requiredType); } } } 为什么@Autowired没有生效？ 这个问题是有一次小伙伴私聊问我的，我把它放在此处，希望能帮助到遇到类似情况的小伙伴们。 我们知道我们使用@Autowired注入Bean，要么就报错，要么就正常work，那么何来不生效的情况呢（不生效指的是值没注入进入，仍为null）？（注意：不考虑requied=false的情况） 我个人总结如下两点，若遇到同样问题，可从下面着手去定位： 你@Autowired依赖的Bean以及@Autowired所在Bean是否交给Spring容器管理了？ Tips：虽然依赖注入中@Autowired所在的Bean并不必须得交给Spring容器管理，但此处不考虑这种case（因为我预估很少人知道这个知识点并且还能会用这个知识点，对于此知识点有兴趣的参考博文：【小家Spring】为脱离Spring IOC容器管理的Bean赋能【依赖注入】的能力，并分析原理（借助AutowireCapableBeanFactory赋能）） 你@Autowired所在的Bean的初始化时机是否比AutowiredAnnotationBeanPostProcessor还早？ @Autowired注解必须靠AutowiredAnnotationBeanPostProcessor才能解析。而它虽然实现了PriorityOrdered接口，但是它毕竟还是个BeanPostProcessor，所以生效时机还是会晚于BeanFactoryPostProcessor的，所以若你在BeanFactoryPostProcessor里getBean()，那依赖注入铁定不生效呀~ "},"Chapter11/CircularDependency.html":{"url":"Chapter11/CircularDependency.html","title":"Spring的循环依赖","keywords":"","body":"spring 循环依赖 在Spring中，对象的实例化是通过反射实现的，而对象的属性则是在对象实例化之后通过一定的方式设置的。 这个过程可以按照如下方式进行理解： 如何检测是否有循环依赖 可以 Bean在创建的时候给其打个标记，如果递归调用回来发现正在创建中的话--->即可说明循环依赖。 怎么解决的 Spring的循环依赖的理论依据其实是基于Java的引用传递，当我们获取到对象的引用时，对象的field或zh属性是可以延后设置的(但是构造器必须是在获取引用之前)。 createBeanInstance：实例化，其实也就是 调用对象的构造方法实例化对象 populateBean：填充属性，这一步主要是多bean的依赖属性进行填充 initializeBean：调用spring xml中的init() 方法。 从上面讲述的单例bean初始化步骤我们可以知道，循环依赖主要发生在第一、第二步。也就是构造器循环依赖和field循环依赖。 那么我们要解决循环引用也应该从初始化过程着手，对于单例来说，在Spring容器整个生命周期内，有且只有一个对象，所以很容易想到这个对象应该存在Cache中，Spring为了解决单例的循环依赖问题，使用了三级缓存。 源码怎么实现的 public class DefaultSingletonBeanRegistry extends SimpleAliasRegistry implements SingletonBeanRegistry { /** Maximum number of suppressed exceptions to preserve. */ private static final int SUPPRESSED_EXCEPTIONS_LIMIT = 100; /** Cache of singleton objects: bean name to bean instance. */ private final Map singletonObjects = new ConcurrentHashMap<>(256); /** Cache of singleton factories: bean name to ObjectFactory. */ private final Map> singletonFactories = new HashMap<>(16); /** Cache of early singleton objects: bean name to bean instance. */ private final Map earlySingletonObjects = new ConcurrentHashMap<>(16); 这三级缓存分别指： 三级缓存singletonFactories ： 单例对象工厂的cache 二级缓存earlySingletonObjects ：提前暴光的单例对象的Cache 。【用于检测循环引用，与singletonFactories互斥】 一级缓存singletonObjects：单例对象的cache protected T doGetBean(final String name, @Nullable final Class requiredType, @Nullable final Object[] args, boolean typeCheckOnly) throws BeansException { // 尝试通过bean名称获取目标bean对象，比如这里的A对象 Object sharedInstance = getSingleton(beanName); // 我们这里的目标对象都是单例的 if (mbd.isSingleton()) { // 这里就尝试创建目标对象，第二个参数传的就是一个ObjectFactory类型的对象，这里是使用Java8的lamada // 表达式书写的，只要上面的getSingleton()方法返回值为空，则会调用这里的getSingleton()方法来创建 // 目标对象 sharedInstance = getSingleton(beanName, () -> { try { // 尝试创建目标对象 return createBean(beanName, mbd, args); } catch (BeansException ex) { throw ex; } }); } return (T) bean; } 这里的doGetBean()方法是非常关键的一个方法（中间省略了其他代码），上面也主要有两个步骤 第一个步骤的getSingleton()方法的作用是尝试从缓存中获取目标对象，如果没有获取到，则尝试获取半成品的目标对象；如果第一个步骤没有获取到目标对象的实例，那么就进入第二个步骤 第二个步骤的getSingleton()方法的作用是尝试创建目标对象，并且为该对象注入其所依赖的属性。 这里其实就是主干逻辑，我们前面图中已经标明，在整个过程中会调用三次doGetBean()方法 第一次调用的时候会尝试获取A对象实例，此时走的是第一个getSingleton()方法，由于没有已经创建的A对象的成品或半成品，因而这里得到的是null 然后就会调用第二个getSingleton()方法，创建A对象的实例，然后递归的调用doGetBean()方法，尝试获取B对象的实例以注入到A对象中 此时由于Spring容器中也没有B对象的成品或半成品，因而还是会走到第二个getSingleton()方法，在该方法中创建B对象的实例 创建完成之后，尝试获取其所依赖的A的实例作为其属性，因而还是会递归的调用doGetBean()方法 此时需要注意的是，在前面由于已经有了一个半成品的A对象的实例，因而这个时候，再尝试获取A对象的实例的时候，会走第一个getSingleton()方法 在该方法中会得到一个半成品的A对象的实例，然后将该实例返回，并且将其注入到B对象的属性a中，此时B对象实例化完成。 然后，将实例化完成的B对象递归的返回，此时就会将该实例注入到A对象中，这样就得到了一个成品的A对象。 我们这里可以阅读上面的第一个getSingleton()方法： @Nullable protected Object getSingleton(String beanName, boolean allowEarlyReference) { // 尝试从缓存中获取成品的目标对象，如果存在，则直接返回 Object singletonObject = this.singletonObjects.get(beanName); // 如果缓存中不存在目标对象，则判断当前对象是否已经处于创建过程中，在前面的讲解中，第一次尝试获取A对象 // 的实例之后，就会将A对象标记为正在创建中，因而最后再尝试获取A对象的时候，这里的if判断就会为true if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) { synchronized (this.singletonObjects) { singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null && allowEarlyReference) { // 这里的singletonFactories是一个Map，其key是bean的名称，而值是一个ObjectFactory类型的 // 对象，这里对于A和B而言，调用图其getObject()方法返回的就是A和B对象的实例，无论是否是半成品 ObjectFactory singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) { // 获取目标对象的实例 singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); } } } } return singletonObject; } 这里我们会存在一个问题就是A的半成品实例是如何实例化的，然后是如何将其封装为一个ObjectFactory类型的对象，并且将其放到上面的singletonFactories属性中的。 这主要是在前面的第二个getSingleton()方法中，其最终会通过其传入的第二个参数，从而调用createBean()方法，该方法的最终调用是委托给了另一个doCreateBean()方法进行的 protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException { // 实例化当前尝试获取的bean对象，比如A对象和B对象都是在这里实例化的 BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { instanceWrapper = createBeanInstance(beanName, mbd, args); } // 判断Spring是否配置了支持提前暴露目标bean，也就是是否支持提前暴露半成品的bean boolean earlySingletonExposure = (mbd.isSingleton() && this.allowCircularReferences && isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { // 如果支持，这里就会将当前生成的半成品的bean放到singletonFactories中，这个singletonFactories // 就是前面第一个getSingleton()方法中所使用到的singletonFactories属性，也就是说，这里就是 // 封装半成品的bean的地方。而这里的getEarlyBeanReference()本质上是直接将放入的第三个参数，也就是 // 目标bean直接返回 addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, mbd, bean)); } try { // 在初始化实例之后，这里就是判断当前bean是否依赖了其他的bean，如果依赖了， // 就会递归的调用getBean()方法尝试获取目标bean populateBean(beanName, mbd, instanceWrapper); } catch (Throwable ex) { // 省略... } return exposedObject; } 到这里，Spring整个解决循环依赖问题的实现思路已经比较清楚了。对于整体过程，读者朋友只要理解两点： Spring是通过递归的方式获取目标bean及其所依赖的bean的； Spring实例化一个bean的时候，是分两步进行的，首先实例化目标bean，然后为其注入属性。 结合这两点，也就是说，Spring在实例化一个bean的时候，是首先递归的实例化其所依赖的所有bean，直到某个bean没有依赖其他bean，此时就会将该实例返回，然后反递归的将获取到的bean设置为各个上层bean的属性的。 这里就是解决循环依赖的关键，这段代码发生在createBeanInstance之后，也就是说单例对象此时已经被创建出来(调用了构造器)。这个对象已经被生产出来了，虽然还不完美（还没有进行初始化的第二步和第三步），但是已经能被人认出来了（根据对象引用能定位到堆中的对象），所以Spring此时将这个对象提前曝光出来让大家认识，让大家使用。 这样做有什么好处呢？让我们来分析一下“A的某个field或者setter依赖了B的实例对象，同时B的某个field或者setter依赖了A的实例对象”这种循环依赖的情况。A首先完成了初始化的第一步，并且将自己提前曝光到singletonFactories中，此时进行初始化的第二步，发现自己依赖对象B，此时就尝试去get(B)，发现B还没有被create，所以走create流程，B在初始化第一步的时候发现自己依赖了对象A，于是尝试get(A)，尝试一级缓存singletonObjects(肯定没有，因为A还没初始化完全)，尝试二级缓存earlySingletonObjects（也没有），尝试三级缓存singletonFactories，由于A通过ObjectFactory将自己提前曝光了，所以B能够通过ObjectFactory.getObject拿到A对象(虽然A还没有初始化完全，但是总比没有好呀)，B拿到A对象后顺利完成了初始化阶段1、2、3，完全初始化之后将自己放入到一级缓存singletonObjects中。此时返回A中，A此时能拿到B的对象顺利完成自己的初始化阶段2、3，最终A也完成了初始化，进去了一级缓存singletonObjects中，而且更加幸运的是，由于B拿到了A的对象引用，所以B现在hold住的A对象完成了初始化。 知道了这个原理时候，肯定就知道为啥Spring不能解决“A的构造方法中依赖了B的实例对象，同时B的构造方法中依赖了A的实例对象”这类问题了！因为加入singletonFactories三级缓存的前提是执行了构造器，所以构造器的循环依赖没法解决 其实用一级缓存加三级缓存就可以解决循环依赖的问题了 但是为什么要采用二级缓存了 ？ 试想一下：假如对象A循环依赖B、C，也就是A与B A与C都存在循环依赖 在实例化A的时候会提前实例化B和C 在示例化B的时候， 已经提前利用三级缓存生成了A的代理类 但是由于A的属性赋值这个阶段还没结束 一级缓存还没有A 所以在实例化C的时候又要从三级缓存中拿代码块生成A的代理类 这这样 对我们的程序是没问题的 因为代理类依托的原始类一直都是A 所以程序功能是没影响的 但是这样存在性能的浪费 如果我们把代码快生成的A的代理类 存放到二级缓存中那么C需要的时候就可以直接获取到这个类了 从而不会又执行代码块生成一个了 那么为什么Spring中还需要singletonFactories呢？ 这是难点，基于上面的场景想一个问题：如果A的原始对象注入给B的属性之后，A的原始对象进行了AOP产生了一个代理对象，此时就会出现，对于A而言，它的Bean对象其实应该是AOP之后的代理对象，而B的a属性对应的并不是AOP之后的代理对象，这就产生了冲突。 B依赖的A和最终的A不是同一个对象。 那么如何解决这个问题？这个问题可以说没有办法解决。 因为在一个Bean的生命周期最后，Spring提供了BeanPostProcessor可以去对Bean进行加工，这个加工不仅仅只是能修改Bean的属性值，也可以替换掉当前Bean。 @Component public class FireService { public FireService() { System.out.println(\"FireService...constructor\"); } } @Component public class FireBeanPostProcessor implements BeanPostProcessor { //前置处理器 @Override public Object postProcessBeforeInitialization(Object bean, String beanName) { if (\"fireService\".equals(beanName)){ System.out.println(\"postProcessBeforeInitialization...\" + beanName + \"=>\" + bean); } return bean; } //后置处理器 @Override public Object postProcessAfterInitialization(Object bean, String beanName) { if (\"fireService\".equals(beanName)){ System.out.println(bean); FireService fireService = new FireService(); return fireService; } return bean; } } 所以在BeanPostProcessor中可以完全替换掉某个beanName对应的bean对象。 而BeanPostProcessor的执行在Bean的生命周期中是处于属性注入之后的，循环依赖是发生在属性注入过程中的，所以很有可能导致，注入给B对象的A对象和经历过完整生命周期之后的A对象，不是一个对象。这就是有问题的。 所以在这种情况下的循环依赖，Spring是解决不了的，因为在属性注入时，Spring也不知道A对象后续会经过哪些BeanPostProcessor以及会对A对象做什么处理。 Spring到底解决了哪种情况下的循环依赖 虽然上面的情况可能发生，但是肯定发生得很少，我们通常在开发过程中，不会这样去做，但是，某个beanName对应的最终对象和原始对象不是一个对象却会经常出现，这就是AOP。 AOP就是通过一个BeanPostProcessor来实现的，这个BeanPostProcessor就是AnnotationAwareAspectJAutoProxyCreator，它的父类是AbstractAutoProxyCreator，而在Spring中AOP利用的要么是JDK动态代理，要么CGLib的动态代理，所以如果给一个类中的某个方法设置了切面，那么这个类最终就需要生成一个代理对象。 一般过程就是：A类--->生成一个普通对象-->属性注入-->基于切面生成一个代理对象-->把代理对象放入singletonObjects单例池中。 而AOP可以说是Spring中除开IOC的另外一大功能，而循环依赖又是属于IOC范畴的，所以这两大功能想要并存，Spring需要特殊处理。 如何处理的，就是利用了第三级缓存singletonFactories。 首先，singletonFactories中存的是某个beanName对应的ObjectFactory，在bean的生命周期中，生成完原始对象之后，就会构造一个ObjectFactory存入singletonFactories中。 这个ObjectFactory是一个函数式接口，所以支持Lambda表达式：() -> getEarlyBeanReference(beanName, mbd, bean) AbstractAutowireCapableBeanFactory#doCreateBean // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. boolean earlySingletonExposure = (mbd.isSingleton() && this.allowCircularReferences && isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { if (logger.isTraceEnabled()) { logger.trace(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); } addSingletonFactory(beanName, () -> getEarlyBeanReference(beanName, mbd, bean)); } 上面的Lambda表达式就是一个ObjectFactory，执行该Lambda表达式就会去执行getEarlyBeanReference方法，而该方法如下： protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) { Object exposedObject = bean; if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) { for (BeanPostProcessor bp : getBeanPostProcessors()) { if (bp instanceof SmartInstantiationAwareBeanPostProcessor) { SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); } } } return exposedObject; } 该方法会去执行SmartInstantiationAwareBeanPostProcessor中的getEarlyBeanReference方法，而这个接口下的实现类中只有两个类实现了这个方法，一个是AbstractAutoProxyCreator， 一个是InstantiationAwareBeanPostProcessorAdapter，它的实现如下： AbstractAutoProxyCreator @Override public Object getEarlyBeanReference(Object bean, String beanName) { Object cacheKey = getCacheKey(bean.getClass(), beanName); this.earlyProxyReferences.put(cacheKey, bean); return wrapIfNecessary(bean, beanName, cacheKey); } InstantiationAwareBeanPostProcessorAdapter @Override public Object getEarlyBeanReference(Object bean, String beanName) throws BeansException { return bean; } 所以很明显，在整个Spring中，默认就只有AbstractAutoProxyCreator真正意义上实现了getEarlyBeanReference方法，而该类就是用来进行AOP的。上文提到的AnnotationAwareAspectJAutoProxyCreator的父类就是AbstractAutoProxyCreator。 那么getEarlyBeanReference方法到底在干什么？ 首先得到一个cachekey，cachekey就是beanName。 然后把beanName和bean（这是原始对象）存入earlyProxyReferences中 调用wrapIfNecessary进行AOP，得到一个代理对象。 那么，什么时候会调用getEarlyBeanReference方法呢？回到循环依赖的场景中 左边文字： 这个ObjectFactory就是上文说的labmda表达式，中间有getEarlyBeanReference方法，注意存入singletonFactories时并不会执行lambda表达式，也就是不会执行getEarlyBeanReference方法 右边文字： 从singletonFactories根据beanName得到一个ObjectFactory，然后执行ObjectFactory，也就是执行getEarlyBeanReference方法，此时会得到一个A原始对象经过AOP之后的代理对象，然后把该代理对象放入earlySingletonObjects中，注意此时并没有把代理对象放入singletonObjects中，那什么时候放入到singletonObjects中呢？ 我们这个时候得来理解一下earlySingletonObjects的作用，此时，我们只得到了A原始对象的代理对象，这个对象还不完整，因为A原始对象还没有进行属性填充，所以此时不能直接把A的代理对象放入singletonObjects中，所以只能把代理对象放入earlySingletonObjects，假设现在有其他对象依赖了A，那么则可以从earlySingletonObjects中得到A原始对象的代理对象了，并且是A的同一个代理对象。 当B创建完了之后，A继续进行生命周期，而A在完成属性注入后，会按照它本身的逻辑去进行AOP，而此时我们知道A原始对象已经经历过了AOP，所以对于A本身而言，不会再去进行AOP了，那么怎么判断一个对象是否经历过了AOP呢？会利用上文提到的earlyProxyReferences，在AbstractAutoProxyCreator的postProcessAfterInitialization方法中，会去判断当前beanName是否在earlyProxyReferences，如果在则表示已经提前进行过AOP了，无需再次进行AOP。 对于A而言，进行了AOP的判断后，以及BeanPostProcessor的执行之后，就需要把A对应的对象放入singletonObjects中了，但是我们知道，应该是要A的代理对象放入singletonObjects中，所以此时需要从earlySingletonObjects中得到代理对象，然后入singletonObjects中。右边文字： 总结 至此，总结一下三级缓存： singletonObjects：缓存某个beanName对应的经过了完整生命周期的bean earlySingletonObjects：缓存提前拿原始对象进行了AOP之后得到的代理对象，原始对象还没有进行属性注入和后续的BeanPostProcessor等生命周期 singletonFactories：缓存的是一个ObjectFactory，主要用来去生成原始对象进行了AOP之后得到的代理对象，在每个Bean的生成过程中，都会提前暴露一个工厂，这个工厂可能用到，也可能用不到，如果没有出现循环依赖依赖本bean，那么这个工厂无用，本bean按照自己的生命周期执行，执行完后直接把本bean放入singletonObjects中即可，如果出现了循环依赖依赖了本bean，则另外那个bean执行ObjectFactory提交得到一个AOP之后的代理对象(如果有AOP的话，如果无需AOP，则直接得到一个原始对象)。 其实还要一个缓存，就是earlyProxyReferences，它用来记录某个原始对象是否进行过AOP了。 "},"Chapter11/AnnotationConfigApplicationContext.html":{"url":"Chapter11/AnnotationConfigApplicationContext.html","title":"AnnotationConfigApplicationContext(注解方式)初始化","keywords":"","body":"AnnotationConfigApplicationContext(注解方式)初始化 IOC容器注解方式实现概述 Spring注解方式减少了大量XML配置工作和代码复杂性，降低开发和维护成本。 常用的Spring注解@ComponentScan，@Service，@Autowired等， Spring对这些注解都是怎么运行工作的，对应注解一放，功能轻飘飘实现了，有没有感觉很神奇？ 使用用例 //@SpringBootApplication @ComponentScan(excludeFilters = {@ComponentScan.Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @ComponentScan.Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class)}) public class Application { public static void main(String[] args) { // ConfigurableApplicationContext context = new SpringApplicationBuilder(Application.class).run(args); // new AnnotationConfigApplicationContext(\"io.github.firehuo.spring\"); new AnnotationConfigApplicationContext(Application.class); } } 主要就两种方式 new AnnotationConfigApplicationContext(\"io.github.firehuo.spring\"); new AnnotationConfigApplicationContext(Application.class); 第一种不用在Application上加@ComponentScan注解，因为已经传入了扫描的路径；第二种需要加@ComponentScan注解，这个注解是从@SpringBootApplication复制出来的（白嫖的代码-_-）。 注解使用实例 我们在这里提出几个凝问，带着这几个问题去分析源码。 @ComponentScan去扫描东西，扫描的是啥，把什么东西变成IOC容器需要的东西，然后注册到IOC容器？ @Service注解的类，如何注册到IOC容器的？ @Autowired是怎么像XML方式一样处理依赖关系的？ 这里分析AnnotationConfigApplicationContext实现原理，理解了这个注解容器关于IOC的实现原理，以及@ComponentScan注解实现原理， 等你去看SpringBoot启动源码的时候，感觉相对比较容易些。 AnnotationConfigApplicationContext类图。 IOC后置处理器注入 AnnotationConfigApplicationContext有两种用法。 一种是传入@ComponentScan注解的类，另外一种是直接传入要扫描的包。 直接传入扫描包相对比较简单，上来就开始扫描。 public AnnotationConfigApplicationContext(String... basePackages) { this(); scan(basePackages); refresh(); } 传入@ComponentScan注解类相对比较麻烦些，需要绕一圈，才能调到scan()方法。 这里挑传入@ComponentScan注解类分析，因为我们基本都是使用@ComponentScan， 直接传入扫描包只是@ComponentScan注解类实现方式一个子集，理解了@ComponentScan， 自然就理解了直接传入扫描包的方式。 从构造器看起！ public AnnotationConfigApplicationContext(Class... annotatedClasses) { this(); register(annotatedClasses); refresh(); } 上来看到了一个醒目的refresh()方法，莫慌，等会再看他，先去找水。 这里跟XML方式有区别，XML方式的BeanFactory是在refresh()#obtainFreshBeanFactory()方法里面去获取的， 而注解方式则是通过父类构造器直接构建的，一上来就创建容器，默认用的DefaultListableBeanFactory。 public class AnnotationConfigApplicationContext extends GenericApplicationContext implements AnnotationConfigRegistry { private final AnnotatedBeanDefinitionReader reader; private final ClassPathBeanDefinitionScanner scanner; public class GenericApplicationContext extends AbstractApplicationContext implements BeanDefinitionRegistry { private final DefaultListableBeanFactory beanFactory; @Nullable private ResourceLoader resourceLoader; private boolean customClassLoader = false; private final AtomicBoolean refreshed = new AtomicBoolean(); /** * Create a new GenericApplicationContext. * @see #registerBeanDefinition * @see #refresh */ public GenericApplicationContext() { this.beanFactory = new DefaultListableBeanFactory(); } 到这里，IOC容器有了，接着往下看 ，this()： public AnnotationConfigApplicationContext() { // 注解BeanDefinition读取器 this.reader = new AnnotatedBeanDefinitionReader(this); // ClassPath扫描器 this.scanner = new ClassPathBeanDefinitionScanner(this); } AnnotatedBeanDefinitionReader 追踪到AnnotationConfigUtils#registerAnnotationConfigProcessors()： /** * Register all relevant annotation post processors in the given registry. * @param registry the registry to operate on * @param source the configuration source element (already extracted) * that this registration was triggered from. May be {@code null}. * @return a Set of BeanDefinitionHolders, containing all bean definitions * that have actually been registered by this call */ public static Set registerAnnotationConfigProcessors( BeanDefinitionRegistry registry, @Nullable Object source) { // 获取BeanFactory DefaultListableBeanFactory beanFactory = unwrapDefaultListableBeanFactory(registry); if (beanFactory != null) { if (!(beanFactory.getDependencyComparator() instanceof AnnotationAwareOrderComparator)) { beanFactory.setDependencyComparator(AnnotationAwareOrderComparator.INSTANCE); } if (!(beanFactory.getAutowireCandidateResolver() instanceof ContextAnnotationAutowireCandidateResolver)) { beanFactory.setAutowireCandidateResolver(new ContextAnnotationAutowireCandidateResolver()); } } // BeanDefinitionHolder包装类集合，在后续注入应用bean的时候，需要返回给调用方，调用方有逻辑处理 Set beanDefs = new LinkedHashSet<>(8); // 注册BeanFactory的后置处理器 //最最重要的后置处理器ConfigurationClassPostProcessor CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME = org.springframework.context.annotation.internalConfigurationAnnotationProcessor if (!registry.containsBeanDefinition(CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(ConfigurationClassPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)); } //@Autowired的后置处理器 if (!registry.containsBeanDefinition(AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(AutowiredAnnotationBeanPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)); } // Check for JSR-250 support, and if present add the CommonAnnotationBeanPostProcessor. if (jsr250Present && !registry.containsBeanDefinition(COMMON_ANNOTATION_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(CommonAnnotationBeanPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, COMMON_ANNOTATION_PROCESSOR_BEAN_NAME)); } // Check for JPA support, and if present add the PersistenceAnnotationBeanPostProcessor. if (jpaPresent && !registry.containsBeanDefinition(PERSISTENCE_ANNOTATION_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(); try { def.setBeanClass(ClassUtils.forName(PERSISTENCE_ANNOTATION_PROCESSOR_CLASS_NAME, AnnotationConfigUtils.class.getClassLoader())); } catch (ClassNotFoundException ex) { throw new IllegalStateException( \"Cannot load optional framework class: \" + PERSISTENCE_ANNOTATION_PROCESSOR_CLASS_NAME, ex); } def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, PERSISTENCE_ANNOTATION_PROCESSOR_BEAN_NAME)); } if (!registry.containsBeanDefinition(EVENT_LISTENER_PROCESSOR_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(EventListenerMethodProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, EVENT_LISTENER_PROCESSOR_BEAN_NAME)); } if (!registry.containsBeanDefinition(EVENT_LISTENER_FACTORY_BEAN_NAME)) { RootBeanDefinition def = new RootBeanDefinition(DefaultEventListenerFactory.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, EVENT_LISTENER_FACTORY_BEAN_NAME)); } return beanDefs; } 如果你看过BeanFactoryPostProcessor这边文章，那么一定对internalConfigurationAnnotationProcessor和ConfigurationClassPostProcessor有所印象。 同样是BeanFactoryPostProcessor这边文章中的一个疑问这里给出答案。 前文提到过这是这个类完成了对@Component的扫描。这个后面细说。 同样 AutowiredAnnotationBeanPostProcessor后置处理器完成了对@Autowired的处理，详情请看Spring依赖注入@Autowired深层原理、源码级分析 继续进行，进入AnnotationConfigUtils#registerPostProcessor方法。 private static BeanDefinitionHolder registerPostProcessor( BeanDefinitionRegistry registry, RootBeanDefinition definition, String beanName) { definition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); // 向容器注入IOC后置处理器Bean registry.registerBeanDefinition(beanName, definition); return new BeanDefinitionHolder(definition, beanName); } RootBeanDefinition是个啥？ 是BeanDefinition的其中一个实现，是Bean元数据存储的数据结构，存入Bean的相关信息。 public class RootBeanDefinition extends AbstractBeanDefinition { @Nullable private BeanDefinitionHolder decoratedDefinition; @Nullable private AnnotatedElement qualifiedElement; /** Determines if the definition needs to be re-merged. */ volatile boolean stale; boolean allowCaching = true; boolean isFactoryMethodUnique = false; @Nullable volatile ResolvableType targetType; /** Package-visible field for caching the determined Class of a given bean definition. */ @Nullable volatile Class resolvedTargetType; /** Package-visible field for caching if the bean is a factory bean. */ @Nullable volatile Boolean isFactoryBean; /** Package-visible field for caching the return type of a generically typed factory method. */ @Nullable volatile ResolvableType factoryMethodReturnType; /** Package-visible field for caching a unique factory method candidate for introspection. */ @Nullable volatile Method factoryMethodToIntrospect; /** Common lock for the four constructor fields below. */ final Object constructorArgumentLock = new Object(); /** Package-visible field for caching the resolved constructor or factory method. */ @Nullable Executable resolvedConstructorOrFactoryMethod; /** Package-visible field that marks the constructor arguments as resolved. */ boolean constructorArgumentsResolved = false; /** Package-visible field for caching fully resolved constructor arguments. */ @Nullable Object[] resolvedConstructorArguments; /** Package-visible field for caching partly prepared constructor arguments. */ @Nullable Object[] preparedConstructorArguments; /** Common lock for the two post-processing fields below. */ final Object postProcessingLock = new Object(); boolean postProcessed = false; @Nullable volatile Boolean beforeInstantiationResolved; @Nullable private Set externallyManagedConfigMembers; @Nullable private Set externallyManagedInitMethods; @Nullable private Set externallyManagedDestroyMethods; public RootBeanDefinition() { super(); } BeanDefinitionHolder是个啥？ public class BeanDefinitionHolder implements BeanMetadataElement { private final BeanDefinition beanDefinition; private final String beanName; @Nullable private final String[] aliases; 对Bean进行包装，主要存bean名字，别名，还有BeanDefinition数据结构信息。 DefaultListableBeanFactory#registerBeanDefinition()： 往IOC容器注入BeanDefinition数据结构。 DefaultListableBeanFactory是IOC容器的具体实现，关于XML实现方式的源码里面有详细分析。 BeanDefinition数据结构信息注入到容器，本质上是存储到HashMap中，key是beanName，value是Bean的数据结构， 存入这些东西，就相当于往桶里面装水，之后getBean的时候，才能根据相应beanName等来获取对应的对象。 /** Map of bean definition objects, keyed by bean name */ private final Map beanDefinitionMap = new ConcurrentHashMap(256); @Override public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException { // BeanName和BeanDefinition不能为空，否则停止注册 Assert.hasText(beanName, \"Bean name must not be empty\"); Assert.notNull(beanDefinition, \"BeanDefinition must not be null\"); if (beanDefinition instanceof AbstractBeanDefinition) { try { ((AbstractBeanDefinition) beanDefinition).validate(); } catch (BeanDefinitionValidationException ex) { throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \"Validation of bean definition failed\", ex); } } BeanDefinition oldBeanDefinition; // 检查是否有相同名字的BeanDefinition已经在IOC容器中注册了，如果有同名的BeanDefinition， // 但又不允许覆盖，就会抛出异常，否则覆盖BeanDefinition。 oldBeanDefinition = this.beanDefinitionMap.get(beanName); if (oldBeanDefinition != null) { if (!isAllowBeanDefinitionOverriding()) { throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, \"Cannot register bean definition [\" + beanDefinition + \"] for bean '\" + beanName + \"': There is already [\" + oldBeanDefinition + \"] bound.\"); } else if (oldBeanDefinition.getRole() updatedDefinitions = new ArrayList(this.beanDefinitionNames.size() + 1); updatedDefinitions.addAll(this.beanDefinitionNames); updatedDefinitions.add(beanName); this.beanDefinitionNames = updatedDefinitions; if (this.manualSingletonNames.contains(beanName)) { Set updatedSingletons = new LinkedHashSet(this.manualSingletonNames); updatedSingletons.remove(beanName); this.manualSingletonNames = updatedSingletons; } } } else {// 正在启动注册阶段，容器这个时候还是空的。 // Still in startup registration phase this.beanDefinitionMap.put(beanName, beanDefinition); this.beanDefinitionNames.add(beanName); this.manualSingletonNames.remove(beanName); } this.frozenBeanDefinitionNames = null; } // 重置所有已经注册过的BeanDefinition或单例模式的BeanDefinition的缓存 if (oldBeanDefinition != null || containsSingleton(beanName)) { resetBeanDefinition(beanName); } } this.beanDefinitionMap.put(beanName, beanDefinition);完成bean的注册。 后置处理器注册完成后效果，可以看到有6个后置处理器，具体数据结构是RootBeanDefinition。 ClassPathBeanDefinitionScanner public ClassPathBeanDefinitionScanner(BeanDefinitionRegistry registry, boolean useDefaultFilters) { //getOrCreateEnvironment()创建Spring运行环境StandardEnvironment。 this(registry, useDefaultFilters, getOrCreateEnvironment(registry)); } private static Environment getOrCreateEnvironment(BeanDefinitionRegistry registry) { Assert.notNull(registry, \"BeanDefinitionRegistry must not be null\"); if (registry instanceof EnvironmentCapable) { return ((EnvironmentCapable) registry).getEnvironment(); } return new StandardEnvironment(); } 返回ClassPathBeanDefinitionScanner继续深入 public ClassPathBeanDefinitionScanner(BeanDefinitionRegistry registry, boolean useDefaultFilters, Environment environment, @Nullable ResourceLoader resourceLoader) { Assert.notNull(registry, \"BeanDefinitionRegistry must not be null\"); this.registry = registry; if (useDefaultFilters) { registerDefaultFilters(); } setEnvironment(environment); setResourceLoader(resourceLoader); } registerDefaultFilters()需要拦截的注解进行过滤。 protected void registerDefaultFilters() { //注意这里Component.class this.includeFilters.add(new AnnotationTypeFilter(Component.class)); ClassLoader cl = ClassPathScanningCandidateComponentProvider.class.getClassLoader(); try { this.includeFilters.add(new AnnotationTypeFilter( ((Class) ClassUtils.forName(\"javax.annotation.ManagedBean\", cl)), false)); logger.trace(\"JSR-250 'javax.annotation.ManagedBean' found and supported for component scanning\"); } catch (ClassNotFoundException ex) { // JSR-250 1.1 API (as included in Java EE 6) not available - simply skip. } try { this.includeFilters.add(new AnnotationTypeFilter( ((Class) ClassUtils.forName(\"javax.inject.Named\", cl)), false)); logger.trace(\"JSR-330 'javax.inject.Named' annotation found and supported for component scanning\"); } catch (ClassNotFoundException ex) { // JSR-330 API not available - simply skip. } } 有没有发现，搞了半天，好像跟咱们刚才说的@Service，@Auworied没有半毛钱关系。 上面的这些分析，只是Spring本身对Bean后置处理器的注册，应用程序的注册还得继续。 AppConfig启动类的注册 在上面AnnotationConfigApplicationContext分析了this()构造器，下面分析register(componentClasses)注册启动类。 public AnnotationConfigApplicationContext(Class... componentClasses) { this(); register(componentClasses); refresh(); } private void doRegisterBean(Class beanClass, @Nullable String name, @Nullable Class[] qualifiers, @Nullable Supplier supplier, @Nullable BeanDefinitionCustomizer[] customizers) { // 获取注解类路径上的所有注解基本信息 AnnotatedGenericBeanDefinition abd = new AnnotatedGenericBeanDefinition(beanClass); if (this.conditionEvaluator.shouldSkip(abd.getMetadata())) { return; } abd.setInstanceSupplier(supplier); // AppConfig启动类注册 ScopeMetadata scopeMetadata = this.scopeMetadataResolver.resolveScopeMetadata(abd); abd.setScope(scopeMetadata.getScopeName()); String beanName = (name != null ? name : this.beanNameGenerator.generateBeanName(abd, this.registry)); AnnotationConfigUtils.processCommonDefinitionAnnotations(abd); if (qualifiers != null) { for (Class qualifier : qualifiers) { if (Primary.class == qualifier) { abd.setPrimary(true); } else if (Lazy.class == qualifier) { abd.setLazyInit(true); } else { abd.addQualifier(new AutowireCandidateQualifier(qualifier)); } } } if (customizers != null) { for (BeanDefinitionCustomizer customizer : customizers) { customizer.customize(abd); } } // 注册启动类的Bean BeanDefinitionHolder definitionHolder = new BeanDefinitionHolder(abd, beanName); definitionHolder = AnnotationConfigUtils.applyScopedProxyMode(scopeMetadata, definitionHolder, this.registry); BeanDefinitionReaderUtils.registerBeanDefinition(definitionHolder, this.registry); } AppConfig启动类也注册到IOC容器后，这个时候容器有7个Bean，6个后置处理器和1个启动Bean， 具体数据结构用AnnotatedGenericBeanDefinition存储。 应用程序BeanDefinition的Resource定位、载入、解析、注册 ConfigurationClassPostProcessor是Spring中最最最重要的后置处理器！没有之一！ ConfigurationClassPostProcessor registerBeanPostProcessors() 参照 BeanPostProcessor 和 Spring依赖注入@Autowired深层原理、源码级分析 总结 注解方式@ComponentScan定义要扫描的文件包，IOC解析@ComponentScan注解，对classpath下改包的.class(字节码)文件进行解析，通过IO读取，解析为元数据对象。 根据元数据对象是否有注解，进行判断是否创建Bean注册到IOC容器中。 @Autowired注解依赖在Autowired后置处理器进行元数据解析，在bean创建是也还会处理一次依赖注入。 "},"Chapter11/ConfigurationClassPostProcessor.html":{"url":"Chapter11/ConfigurationClassPostProcessor.html","title":"ConfigurationClassPostProcessor","keywords":"","body":"Spring中最最最重要的后置处理器ConfigurationClassPostProcessor 没有之一！ 本文源码为spring5.2.9版本 疑惑 在阅读本文之前，可以先思考一下以下几个问题。 @Configuration注解的作用是什么，Spring是如何解析加了@Configuration注解的类？ Spring在什么时候对@ComponentScan、@ComponentScans注解进行了解析？ Spring什么时候解析了@Import注解，如何解析的？ Spring什么时候解析了@Bean注解？ 整体感官 代码追踪 这里是接上篇文章AnnotationConfigApplicationContext(注解方式)初始化 里面的启动类为，后面会用到 new AnnotationConfigApplicationContext(Application.class); ConfigurationClassPostProcessor 实现了 BeanDefinitionRegistryPostProcessor 接口，而 BeanDefinitionRegistryPostProcessor 接口继承了 BeanFactoryPostProcessor 接口，所以 ConfigurationClassPostProcessor 中需要重写 postProcessBeanDefinitionRegistry() 方法和 postProcessBeanFactory() 方法。 而ConfigurationClassPostProcessor类的作用就是通过这两个方法去实现的。 参考BeanFactoryPostProcessor /** * Derive further bean definitions from the configuration classes in the registry. */ @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) { int registryId = System.identityHashCode(registry); if (this.registriesPostProcessed.contains(registryId)) { throw new IllegalStateException( \"postProcessBeanDefinitionRegistry already called on this post-processor against \" + registry); } if (this.factoriesPostProcessed.contains(registryId)) { throw new IllegalStateException( \"postProcessBeanFactory already called on this post-processor against \" + registry); } this.registriesPostProcessed.add(registryId); processConfigBeanDefinitions(registry); } 进入 processConfigBeanDefinitions public void processConfigBeanDefinitions(BeanDefinitionRegistry registry) { // configCandidates 配置类候选人 List configCandidates = new ArrayList<>(); String[] candidateNames = registry.getBeanDefinitionNames(); for (String beanName : candidateNames) { BeanDefinition beanDef = registry.getBeanDefinition(beanName); if (beanDef.getAttribute(ConfigurationClassUtils.CONFIGURATION_CLASS_ATTRIBUTE) != null) { //已经处理过了及 以及设置configurationClass属性 不再处理 if (logger.isDebugEnabled()) { // log 日志 logger.debug(\"Bean definition has already been processed as a configuration class: \" + beanDef); } } // checkConfigurationClassCandidate()会判断一个是否是一个配置类,并为BeanDefinition设置属性为lite或者full。 // 在这儿为BeanDefinition设置lite和full属性值是为了后面在使用；lite和full均表示这个BeanDefinition对应的类是一个配置类 else if (ConfigurationClassUtils.checkConfigurationClassCandidate(beanDef, this.metadataReaderFactory)) { //只有有@Configuration注解且不是代理类的才会加入 configCandidates.add(new BeanDefinitionHolder(beanDef, beanName)); } } // Return immediately if no @Configuration classes were found if (configCandidates.isEmpty()) { return; } // Sort by previously determined @Order value, if applicable configCandidates.sort((bd1, bd2) -> { int i1 = ConfigurationClassUtils.getOrder(bd1.getBeanDefinition()); int i2 = ConfigurationClassUtils.getOrder(bd2.getBeanDefinition()); return Integer.compare(i1, i2); }); // Detect any custom bean name generation strategy supplied through the enclosing application context SingletonBeanRegistry sbr = null; if (registry instanceof SingletonBeanRegistry) { sbr = (SingletonBeanRegistry) registry; if (!this.localBeanNameGeneratorSet) { // beanName的生成器，因为后面会扫描出所有加入到spring容器中calss类，然后把这些class解析成BeanDefinition类，此时需要利用BeanNameGenerator为这些BeanDefinition生成beanName BeanNameGenerator generator = (BeanNameGenerator) sbr.getSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR); if (generator != null) { this.componentScanBeanNameGenerator = generator; this.importBeanNameGenerator = generator; } } } if (this.environment == null) { this.environment = new StandardEnvironment(); } // Parse each @Configuration class // 定义一个解析器，解析所有加了@Configuration注解的类 ConfigurationClassParser parser = new ConfigurationClassParser( this.metadataReaderFactory, this.problemReporter, this.environment, this.resourceLoader, this.componentScanBeanNameGenerator, registry); Set candidates = new LinkedHashSet<>(configCandidates); Set alreadyParsed = new HashSet<>(configCandidates.size()); do { / /解析配置类，在此处会解析配置类上的注解(ComponentScan扫描出的类，@Import注册的类，以及@Bean方法定义的类) // 注意：这一步只会将加了@Configuration注解以及通过@ComponentScan注解扫描的类才会加入到BeanDefinitionMap中 // 通过其他注解(例如@Import、@Bean)的方式，在parse()方法这一步并不会将其解析为BeanDefinition放入到BeanDefinitionMap中，而是先解析成ConfigurationClass类 // 真正放入到map中是在下面的this.reader.loadBeanDefinitions()方法中实现的 parser.parse(candidates); parser.validate(); Set configClasses = new LinkedHashSet<>(parser.getConfigurationClasses()); configClasses.removeAll(alreadyParsed); // Read the model and create bean definitions based on its content if (this.reader == null) { this.reader = new ConfigurationClassBeanDefinitionReader( registry, this.sourceExtractor, this.resourceLoader, this.environment, this.importBeanNameGenerator, parser.getImportRegistry()); } // 将上一步parser解析出的ConfigurationClass类加载成BeanDefinition // 实际上经过上一步的parse()后，解析出来的bean已经放入到BeanDefinition中了，但是由于这些bean可能会引入新的bean，例如实现了ImportBeanDefinitionRegistrar或者ImportSelector接口的bean，或者bean中存在被@Bean注解的方法 // 因此需要执行一次loadBeanDefinition()，这样就会执行ImportBeanDefinitionRegistrar或者ImportSelector接口的方法或者@Bean注释的方法 this.reader.loadBeanDefinitions(configClasses); alreadyParsed.addAll(configClasses); candidates.clear(); // 这里判断registry.getBeanDefinitionCount() > candidateNames.length的目的是为了知道reader.loadBeanDefinitions(configClasses)这一步有没有向BeanDefinitionMap中添加新的BeanDefinition // 实际上就是看配置类(例如AppConfig类会向BeanDefinitionMap中添加bean) // 如果有，registry.getBeanDefinitionCount()就会大于candidateNames.length // 这样就需要再次遍历新加入的BeanDefinition，并判断这些bean是否已经被解析过了，如果未解析，需要重新进行解析 // 这里的AppConfig类向容器中添加的bean，实际上在parser.parse()这一步已经全部被解析了 // 所以为什么还需要做这个判断，目前没看懂，似乎没有任何意义。 if (registry.getBeanDefinitionCount() > candidateNames.length) { String[] newCandidateNames = registry.getBeanDefinitionNames(); Set oldCandidateNames = new HashSet<>(Arrays.asList(candidateNames)); Set alreadyParsedClasses = new HashSet<>(); for (ConfigurationClass configurationClass : alreadyParsed) { alreadyParsedClasses.add(configurationClass.getMetadata().getClassName()); } // 如果有未解析的类，则将其添加到candidates中，这样candidates不为空，就会进入到下一次的while的循环中 for (String candidateName : newCandidateNames) { if (!oldCandidateNames.contains(candidateName)) { BeanDefinition bd = registry.getBeanDefinition(candidateName); if (ConfigurationClassUtils.checkConfigurationClassCandidate(bd, this.metadataReaderFactory) && !alreadyParsedClasses.contains(bd.getBeanClassName())) { candidates.add(new BeanDefinitionHolder(bd, candidateName)); } } } candidateNames = newCandidateNames; } } while (!candidates.isEmpty()); // Register the ImportRegistry as a bean in order to support ImportAware @Configuration classes if (sbr != null && !sbr.containsSingleton(IMPORT_REGISTRY_BEAN_NAME)) { sbr.registerSingleton(IMPORT_REGISTRY_BEAN_NAME, parser.getImportRegistry()); } if (this.metadataReaderFactory instanceof CachingMetadataReaderFactory) { // Clear cache in externally provided MetadataReaderFactory; this is a no-op // for a shared cache since it'll be cleared by the ApplicationContext. ((CachingMetadataReaderFactory) this.metadataReaderFactory).clearCache(); } } ConfigurationClassUtils#checkConfigurationClassCandidate 该方法是用来判断一个是否是一个配置类，并为BeanDefinition设置属性为lite或者full。 如果加了@Configuration，且proxyBeanMethods=false，那么对应的BeanDefinition为full。 如果加了@Configuration，且配置了@Bean，@Component，@ComponentScan，@Import，@ImportResource这些注解，则为lite 这两种情况都会被加入到configCandidates中；其它的不会加入到configCandidates中。 @Configuration的proxyBeanMethods为flase 不会被代理，如果为true会被CGLIB代理，如果只是普通类的话建议设置为 flase ,这样能提升性能 /** * Check whether the given bean definition is a candidate for a configuration class * (or a nested component class declared within a configuration/component class, * to be auto-registered as well), and mark it accordingly. * @param beanDef the bean definition to check * @param metadataReaderFactory the current factory in use by the caller * @return whether the candidate qualifies as (any kind of) configuration class */ public static boolean checkConfigurationClassCandidate( BeanDefinition beanDef, MetadataReaderFactory metadataReaderFactory) { String className = beanDef.getBeanClassName(); if (className == null || beanDef.getFactoryMethodName() != null) { return false; } AnnotationMetadata metadata; if (beanDef instanceof AnnotatedBeanDefinition && className.equals(((AnnotatedBeanDefinition) beanDef).getMetadata().getClassName())) { // Can reuse the pre-parsed metadata from the given BeanDefinition... metadata = ((AnnotatedBeanDefinition) beanDef).getMetadata(); } else if (beanDef instanceof AbstractBeanDefinition && ((AbstractBeanDefinition) beanDef).hasBeanClass()) { // Check already loaded Class if present... // since we possibly can't even load the class file for this Class. Class beanClass = ((AbstractBeanDefinition) beanDef).getBeanClass(); if (BeanFactoryPostProcessor.class.isAssignableFrom(beanClass) || BeanPostProcessor.class.isAssignableFrom(beanClass) || AopInfrastructureBean.class.isAssignableFrom(beanClass) || EventListenerFactory.class.isAssignableFrom(beanClass)) { return false; } metadata = AnnotationMetadata.introspect(beanClass); } else { try { MetadataReader metadataReader = metadataReaderFactory.getMetadataReader(className); metadata = metadataReader.getAnnotationMetadata(); } catch (IOException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Could not find class file for introspecting configuration annotations: \" + className, ex); } return false; } } Map config = metadata.getAnnotationAttributes(Configuration.class.getName()); if (config != null && !Boolean.FALSE.equals(config.get(\"proxyBeanMethods\"))) { // 含有@Configuration注解，且不为代理类 那么对应的BeanDefinition的configurationClass属性值设置为full beanDef.setAttribute(CONFIGURATION_CLASS_ATTRIBUTE, CONFIGURATION_CLASS_FULL); } else if (config != null || isConfigurationCandidate(metadata)) { // 含有@Bean,@Component,@ComponentScan,@Import,@ImportResource注解，configurationClass属性值设置为lite beanDef.setAttribute(CONFIGURATION_CLASS_ATTRIBUTE, CONFIGURATION_CLASS_LITE); } else { return false; } // It's a full or lite configuration candidate... Let's determine the order value, if any. Integer order = getOrder(metadata); if (order != null) { beanDef.setAttribute(ORDER_ATTRIBUTE, order); } return true; } ConfigurationClassParser#parse方法 parse()方法会解析配置类上的注解(ComponentScan扫描出的类，@Import注册的类，以及@Bean方法定义的类)，解析完以后(解析成ConfigurationClass类)， 会将解析出的结果放入到parser的configurationClasses这个属性中(这个属性是个Map)。parse会将@Import注解要注册的类解析为BeanDefinition， 但是不会把解析出来的BeanDefinition放入到BeanDefinitionMap中，真正放入到map中是在这一行代码实现的: this.reader.loadBeanDefinitions(configClasses) 还记得前面提到的 new AnnotationConfigApplicationContext(Application.class); 在AnnotationConfigApplicationContext的构造方法中，我们传入了一个AppConfig类，那么candidates的大小为1，里面的元素为AppConfig类所对应的BeanDefinitionHolder(或者说是BeanDefinition,BeanDefinitionHolder只是将BeanDefinition封装了一下，可以简单的认为两者等价)。 AnnotationConfigApplicationContext构造方法可以传入多个类，对应的candidates的大小等于这里传入类的个数(这种说法其实不太严谨，因为AnnotationConfigApplicationContext.register()方法也能像容器中注册配置类) public void parse(Set configCandidates) { for (BeanDefinitionHolder holder : configCandidates) { BeanDefinition bd = holder.getBeanDefinition(); //根据BeanDefinition类型的不同，调用parse()不同的重载方法， 实际上最终都是调用processConfigurationClass()方法 try { if (bd instanceof AnnotatedBeanDefinition) { parse(((AnnotatedBeanDefinition) bd).getMetadata(), holder.getBeanName()); } else if (bd instanceof AbstractBeanDefinition && ((AbstractBeanDefinition) bd).hasBeanClass()) { parse(((AbstractBeanDefinition) bd).getBeanClass(), holder.getBeanName()); } else { parse(bd.getBeanClassName(), holder.getBeanName()); } } catch (BeanDefinitionStoreException ex) { throw ex; } catch (Throwable ex) { throw new BeanDefinitionStoreException( \"Failed to parse configuration class [\" + bd.getBeanClassName() + \"]\", ex); } } // 处理延迟importSelector this.deferredImportSelectorHandler.process(); } 继续进入 parse方法 至于走那个分值后面再补充，这里可任意进入一个，我选择了最后一个。 protected final void parse(@Nullable String className, String beanName) throws IOException { Assert.notNull(className, \"No bean class name for configuration class bean definition\"); MetadataReader reader = this.metadataReaderFactory.getMetadataReader(className); processConfigurationClass(new ConfigurationClass(reader, beanName), DEFAULT_EXCLUSION_FILTER); } processConfigurationClass方法 protected void processConfigurationClass(ConfigurationClass configClass, Predicate filter) throws IOException { if (this.conditionEvaluator.shouldSkip(configClass.getMetadata(), ConfigurationPhase.PARSE_CONFIGURATION)) { return; } ConfigurationClass existingClass = this.configurationClasses.get(configClass); if (existingClass != null) { if (configClass.isImported()) { if (existingClass.isImported()) { existingClass.mergeImportedBy(configClass); } // Otherwise ignore new imported config class; existing non-imported class overrides it. return; } else { // Explicit bean definition found, probably replacing an import. // Let's remove the old one and go with the new one. this.configurationClasses.remove(configClass); this.knownSuperclasses.values().removeIf(configClass::equals); } } // Recursively process the configuration class and its superclass hierarchy. // 处理配置类，由于配置类可能存在父类(若父类的全类名是以java开头的，则除外)，所有需要将configClass变成sourceClass去解析，然后返回sourceClass的父类。 // 如果此时父类为空，则不会进行while循环去解析，如果父类不为空，则会循环的去解析父类 // SourceClass的意义：简单的包装类，目的是为了以统一的方式去处理带有注解的类，不管这些类是如何加载的 // 如果无法理解，可以把它当做一个黑盒，不会影响看spring源码的主流程 SourceClass sourceClass = asSourceClass(configClass, filter); do { // 核心处理逻辑 sourceClass = doProcessConfigurationClass(configClass, sourceClass, filter); } while (sourceClass != null); // 将解析的配置类存储起来，这样回到parse()方法时，能取到值 this.configurationClasses.put(configClass, configClass); } 代码追踪到ConfigurationClassParser#doProcessConfigurationClass() doProcessConfigurationClass()方法中，执行流程如下: 处理内部类，如果内部类也是一个配置类(判断一个类是否是一个配置类，通过ConfigurationClassUtils.checkConfigurationClassCandidate()可以判断)，则递归处理。 处理属性资源文件，加了@PropertySource注解。 首先解析出类上的@ComponentScan和@ComponentScans注解，然后根据配置的扫描包路径，利用ASM技术(ASM技术是一种操作字节码的技术，有兴趣的朋友可以去网上了解下)扫描出所有需要交给Spring管理的类，由于扫描出的类中可能也被加了@ComponentScan和@ComponentScans注解，因此需要进行递归解析，直到所有加了这两个注解的类被解析完成。 处理@Import注解。通过@Import注解，有三种方式可以将一个Bean注册到Spring容器中。 处理@ImportResource注解，解析配置文件。 处理加了@Bean注解的方法。 通过processInterfaces()处理接口的默认方法，从JDK8开始，接口中的方法可以有自己的默认实现，因此，如果这个接口中的方法也加了@Bean注解，也需要被解析。(很少用) 解析父类，如果被解析的配置类继承了某个类，那么配置类的父类也会被进行解析doProcessConfigurationClass()(父类是JDK内置的类例外，即全类名以java开头的)。 @Nullable protected final SourceClass doProcessConfigurationClass( ConfigurationClass configClass, SourceClass sourceClass, Predicate filter) throws IOException { if (configClass.getMetadata().isAnnotated(Component.class.getName())) { // Recursively process any member (nested) classes first //首先递归处理任何成员（嵌套）类,如果也是配置类加入到candidates并执行processConfigurationClass方法 processMemberClasses(configClass, sourceClass, filter); } // Process any @PropertySource annotations //处理属性资源文件，加了@PropertySource注解 for (AnnotationAttributes propertySource : AnnotationConfigUtils.attributesForRepeatable( sourceClass.getMetadata(), PropertySources.class, org.springframework.context.annotation.PropertySource.class)) { if (this.environment instanceof ConfigurableEnvironment) { processPropertySource(propertySource); } else { logger.info(\"Ignoring @PropertySource annotation on [\" + sourceClass.getMetadata().getClassName() + \"]. Reason: Environment must implement ConfigurableEnvironment\"); } } // Process any @ComponentScan annotations Set componentScans = AnnotationConfigUtils.attributesForRepeatable( sourceClass.getMetadata(), ComponentScans.class, ComponentScan.class); if (!componentScans.isEmpty() && !this.conditionEvaluator.shouldSkip(sourceClass.getMetadata(), ConfigurationPhase.REGISTER_BEAN)) { for (AnnotationAttributes componentScan : componentScans) { // The config class is annotated with @ComponentScan -> perform the scan immediately //解析出类上的@ComponentScan和@ComponentScans注解 Set scannedBeanDefinitions = this.componentScanParser.parse(componentScan, sourceClass.getMetadata().getClassName()); // Check the set of scanned definitions for any further config classes and parse recursively if needed for (BeanDefinitionHolder holder : scannedBeanDefinitions) { BeanDefinition bdCand = holder.getBeanDefinition().getOriginatingBeanDefinition(); if (bdCand == null) { bdCand = holder.getBeanDefinition(); } if (ConfigurationClassUtils.checkConfigurationClassCandidate(bdCand, this.metadataReaderFactory)) { //如果是配置类继续递归解析 parse(bdCand.getBeanClassName(), holder.getBeanName()); } } } } // Process any @Import annotations //处理Import注解注册的bean，这一步只会将import注册的bean变为ConfigurationClass,不会变成BeanDefinition， 而是在loadBeanDefinitions()方法中变成BeanDefinition，再放入到BeanDefinitionMap中 processImports(configClass, sourceClass, getImports(sourceClass), filter, true); // Process any @ImportResource annotations //处理@ImportResource注解引入的配置文件 AnnotationAttributes importResource = AnnotationConfigUtils.attributesFor(sourceClass.getMetadata(), ImportResource.class); if (importResource != null) { String[] resources = importResource.getStringArray(\"locations\"); Class readerClass = importResource.getClass(\"reader\"); for (String resource : resources) { String resolvedResource = this.environment.resolveRequiredPlaceholders(resource); configClass.addImportedResource(resolvedResource, readerClass); } } // Process individual @Bean methods //// 处理加了@Bean注解的方法 Set beanMethods = retrieveBeanMethodMetadata(sourceClass); for (MethodMetadata methodMetadata : beanMethods) { configClass.addBeanMethod(new BeanMethod(methodMetadata, configClass)); } // Process default methods on interfaces processInterfaces(configClass, sourceClass); // Process superclass, if any if (sourceClass.getMetadata().hasSuperClass()) { String superclass = sourceClass.getMetadata().getSuperClassName(); if (superclass != null && !superclass.startsWith(\"java\") && !this.knownSuperclasses.containsKey(superclass)) { this.knownSuperclasses.put(superclass, configClass); // Superclass found, return its annotation metadata and recurse return sourceClass.getSuperClass(); } } // No superclass -> processing is complete return null; } this.reader.loadBeanDefinitions() 该方法实际上是将通过@Import、@Bean等注解方式注册的类解析成BeanDefinition，然后注册到BeanDefinitionMap中。 public void loadBeanDefinitions(Set configurationModel) { TrackedConditionEvaluator trackedConditionEvaluator = new TrackedConditionEvaluator(); for (ConfigurationClass configClass : configurationModel) { //循环调用loadBeanDefinitionsForConfigurationClass() loadBeanDefinitionsForConfigurationClass(configClass, trackedConditionEvaluator); } } private void loadBeanDefinitionsForConfigurationClass( ConfigurationClass configClass, TrackedConditionEvaluator trackedConditionEvaluator) { if (trackedConditionEvaluator.shouldSkip(configClass)) { String beanName = configClass.getBeanName(); if (StringUtils.hasLength(beanName) && this.registry.containsBeanDefinition(beanName)) { this.registry.removeBeanDefinition(beanName); } this.importRegistry.removeImportingClass(configClass.getMetadata().getClassName()); return; } if (configClass.isImported()) { // 如果一个bean是通过@Import(ImportSelector)的方式添加到容器中的，那么此时configClass.isImported()返回的是true // 而且configClass的importedBy属性里面存储的是ConfigurationClass就是将bean导入的类 registerBeanDefinitionForImportedConfigurationClass(configClass); } for (BeanMethod beanMethod : configClass.getBeanMethods()) { // 判断当前的bean中是否含有@Bean注解的方法，如果有，需要把这些方法产生的bean放入到BeanDefinitionMap当中 loadBeanDefinitionsForBeanMethod(beanMethod); } loadBeanDefinitionsFromImportedResources(configClass.getImportedResources()); // 如果bean上存在@Import注解，且import的是一个实现了ImportBeanDefinitionRegistrar接口,则执行ImportBeanDefinitionRegistrar的registerBeanDefinitions()方法 loadBeanDefinitionsFromRegistrars(configClass.getImportBeanDefinitionRegistrars()); } postProcessBeanFactory() 该方法是对BeanFactory进行处理，用来干预BeanFactory的创建过程。主要干了两件事，(1)对加了@Configuration注解的类进行CGLIB代理。(2)向Spring中添加一个后置处理器ImportAwareBeanPostProcessor。 @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) { int factoryId = System.identityHashCode(beanFactory); if (this.factoriesPostProcessed.contains(factoryId)) { throw new IllegalStateException( \"postProcessBeanFactory already called on this post-processor against \" + beanFactory); } this.factoriesPostProcessed.add(factoryId); //下面的if语句不会进入，因为在执行BeanFactoryPostProcessor时，会先执行BeanDefinitionRegistryPostProcessor的postProcessorBeanDefinitionRegistry()方法 // 而在执行postProcessorBeanDefinitionRegistry方法时，都会调用processConfigBeanDefinitions方法，这与postProcessorBeanFactory()方法的执行逻辑是一样的 // postProcessorBeanFactory()方法也会调用processConfigBeanDefinitions方法，为了避免重复执行，所以在执行方法之前会先生成一个id，将id放入到一个set当中，每次执行之前 // 先判断id是否存在，所以在此处，永远不会进入到if语句中 if (!this.registriesPostProcessed.contains(factoryId)) { // BeanDefinitionRegistryPostProcessor hook apparently not supported... // Simply call processConfigurationClasses lazily at this point then. // BeanDefinitionRegistryPostProcessor hook apparently not supported... // Simply call processConfigurationClasses lazily at this point then. // 该方法在这里不会被执行到 processConfigBeanDefinitions((BeanDefinitionRegistry) beanFactory); } // 对加了@Configuration注解的配置类进行Cglib代理 enhanceConfigurationClasses(beanFactory); // 添加一个BeanPostProcessor后置处理器 beanFactory.addBeanPostProcessor(new ImportAwareBeanPostProcessor(beanFactory)); } "},"Chapter11/springTomcat.html":{"url":"Chapter11/springTomcat.html","title":"SpringBoot中如何启动Tomcat流程","keywords":"","body":"SpringBoot中如何启动Tomcat流程 本章代码基于springboot 2.2.2 SpringBoot项目之所以部署简单，其很大一部分原因就是因为不用自己折腾Tomcat相关配置，因为其本身内置了各种Servlet容器。 一直好奇： SpringBoot是怎么通过简单运行一个main函数，就能将容器启动起来，并将自身部署到其上 。此文想梳理清楚这个问题。 我们从SpringBoot的启动入口中分析： Context 创建 // Create, load, refresh and run the ApplicationContext context = createApplicationContext(); 在SpringBoot 的 run 方法中，我们发现其中很重要的一步就是上面的一行代码。注释也写的很清楚： 创建、加载、刷新、运行 ApplicationContext。 继续往里面走。 /** * Strategy method used to create the {@link ApplicationContext}. By default this * method will respect any explicitly set application context or application context * class before falling back to a suitable default. * @return the application context (not yet refreshed) * @see #setApplicationContextClass(Class) */ protected ConfigurableApplicationContext createApplicationContext() { Class contextClass = this.applicationContextClass; if (contextClass == null) { try { switch (this.webApplicationType) { case SERVLET: contextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS); break; case REACTIVE: contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); } } catch (ClassNotFoundException ex) { throw new IllegalStateException( \"Unable create a default ApplicationContext, please specify an ApplicationContextClass\", ex); } } return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass); } 逻辑很清楚： 先找到 context 类，然后利用工具方法将其实例化。 如果是 web 环境，则加载DEFAULT_SERVLET_WEB_CONTEXT_CLASS类。参看成员变量定义，其类名为： AnnotationConfigServletWebServerApplicationContext 此类的继承结构如图： 直接继承GenericWebApplicationContext。关于该类前文已有介绍，只要记得它是专门为 web application提供context 的就好。 refresh 在经历过 Context 的创建以及Context的一系列初始化之后，调用 Context 的 refresh 方法，真正的好戏才开始上演。 SpringApplication protected void refresh(ApplicationContext applicationContext) { Assert.isInstanceOf(AbstractApplicationContext.class, applicationContext); ((AbstractApplicationContext) applicationContext).refresh(); } 可以看到refresh直接调用的试 AbstractApplicationContext 的refresh()方法 而且查看 AnnotationConfigServletWebServerApplicationContext 本身没有实现refresh(), 找到其直接父类：ServletWebServerApplicationContext也是调用上级方法到 AbstractApplicationContext。 @Override public final void refresh() throws BeansException, IllegalStateException { try { super.refresh(); } catch (RuntimeException ex) { stopAndReleaseWebServer(); throw ex; } } AbstractApplicationContext @Override public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } 看11行 onRefresh();, AbstractApplicationContext本身onRefresh()没有实现任何东西;最后我们回归到它的实现类。 根据上面的类结构图我们找到 ServletWebServerApplicationContext ServletWebServerApplicationContext @Override protected void onRefresh() { super.onRefresh(); try { createWebServer(); } catch (Throwable ex) { throw new ApplicationContextException(\"Unable to start web server\", ex); } } 我们重点看第3行。代码第3行createWebServer根据名字是要创建一个web服务， private void createWebServer() { WebServer webServer = this.webServer; ServletContext servletContext = getServletContext(); if (webServer == null && servletContext == null) { ServletWebServerFactory factory = getWebServerFactory(); this.webServer = factory.getWebServer(getSelfInitializer()); } else if (servletContext != null) { try { getSelfInitializer().onStartup(servletContext); } catch (ServletException ex) { throw new ApplicationContextException(\"Cannot initialize servlet context\", ex); } } initPropertySources(); } 根据逻辑和进入getServletContext()可知，这里webServer和servletContext应该都是null或者servletContext不为null 先看都是null的情况 获取到了一个ServletWebServerFactory，这是一个接口我们可以找到5个实现。不用想当然是要看TomcatServletWebServerFactory TomcatServletWebServerFactory @Override public WebServer getWebServer(ServletContextInitializer... initializers) { if (this.disableMBeanRegistry) { Registry.disableRegistry(); } Tomcat tomcat = new Tomcat(); File baseDir = (this.baseDirectory != null) ? this.baseDirectory : createTempDir(\"tomcat\"); tomcat.setBaseDir(baseDir.getAbsolutePath()); Connector connector = new Connector(this.protocol); connector.setThrowOnFailure(true); tomcat.getService().addConnector(connector); customizeConnector(connector); tomcat.setConnector(connector); tomcat.getHost().setAutoDeploy(false); configureEngine(tomcat.getEngine()); for (Connector additionalConnector : this.additionalTomcatConnectors) { tomcat.getService().addConnector(additionalConnector); } prepareContext(tomcat.getHost(), initializers); return getTomcatWebServer(tomcat); } 哈哈哈！看见Tomcat了。 从第7行 Connector connector = new Connector(this.protocol) 一直到第16行完成了tomcat的connector的添加。 tomcat中的connector主要负责用来处理http请求,具体原理可以参看Tomcat的源码，此处暂且不提。 prepareContext方法有点长，重点看其中的几行： if (isRegisterDefaultServlet()) { addDefaultServlet(context); } if (shouldRegisterJspServlet()) { addJspServlet(context); addJasperInitializer(context); } context.addLifecycleListener(new StaticResourceConfigurer(context)); ServletContextInitializer[] initializersToUse = mergeInitializers(initializers); host.addChild(context); configureContext(context, initializersToUse); 前面两个分支判断添加了默认的servlet类和与jsp 相关的 servlet 类。 对所有的ServletContextInitializer进行合并后，利用合并后的初始化类对context 进行配置。 返回TomcatServletWebServerFactory，顺着getTomcatWebServer方法一直往下走，开始正式启动Tomcat。 private void initialize() throws WebServerException { logger.info(\"Tomcat initialized with port(s): \" + getPortsDescription(false)); synchronized (this.monitor) { try { addInstanceIdToEngineName(); Context context = findContext(); context.addLifecycleListener((event) -> { if (context.equals(event.getSource()) && Lifecycle.START_EVENT.equals(event.getType())) { // Remove service connectors so that protocol binding doesn't // happen when the service is started. removeServiceConnectors(); } }); // Start the server to trigger initialization listeners this.tomcat.start(); // We can re-throw failure exception directly in the main thread rethrowDeferredStartupExceptions(); try { ContextBindings.bindClassLoader(context, context.getNamingToken(), getClass().getClassLoader()); } catch (NamingException ex) { // Naming is not enabled. Continue } // Unlike Jetty, all Tomcat threads are daemon threads. We create a // blocking non-daemon to stop immediate shutdown startDaemonAwaitThread(); } catch (Exception ex) { stopSilently(); destroySilently(); throw new WebServerException(\"Unable to start embedded Tomcat\", ex); } } } this.tomcat.start();正式启动 tomcat。 现在我们回过来看看之前的那个 getSelfInitializer()方法： 是这里的 this.webServer = factory.getWebServer(getSelfInitializer()); /** * Returns the {@link ServletContextInitializer} that will be used to complete the * setup of this {@link WebApplicationContext}. * @return the self initializer * @see #prepareWebApplicationContext(ServletContext) */ private org.springframework.boot.web.servlet.ServletContextInitializer getSelfInitializer() { return this::selfInitialize; } private void selfInitialize(ServletContext servletContext) throws ServletException { prepareWebApplicationContext(servletContext); registerApplicationScope(servletContext); WebApplicationContextUtils.registerEnvironmentBeans(getBeanFactory(), servletContext); for (ServletContextInitializer beans : getServletContextInitializerBeans()) { beans.onStartup(servletContext); } } prepareWebApplicationContext方法中主要是将ServletContext设置为rootContext。 registerApplicationScope允许用户存储自定义的scope。并且将web专用的scope注册到BeanFactory中，比如(\"request\", \"session\", \"globalSession\", \"application\")。 WebApplicationContextUtils.registerEnvironmentBeans 注册web专用的environment bean（比如 (\"contextParameters\", \"contextAttributes\"））到给定的 BeanFactory 中。 beans.onStartup(servletContext)比较重要，主要用来配置 servlet、filters、listeners、context-param和一些初始化时的必要属性。 以其一个实现类ServletContextInitializer试举一例： @Override public final void onStartup(ServletContext servletContext) throws ServletException { String description = getDescription(); if (!isEnabled()) { logger.info(StringUtils.capitalize(description) + \" was not registered (disabled)\"); return; } register(description, servletContext); } "},"Chapter11/springboot.html":{"url":"Chapter11/springboot.html","title":"SpringBoot的启动流程","keywords":"","body":"SpringBoot的启动流程 new SpringApplication(Application.class) 跟踪代码到 public SpringApplication(ResourceLoader resourceLoader, Class... primarySources) { //使用的资源加载器 this.resourceLoader = resourceLoader; //主要的bean资源 primarySources【在这里是启动类所在的.class】,不能为null,如果为null，抛异常 Assert.notNull(primarySources, \"PrimarySources must not be null\"); //获取所有的主配置类//启动类的实例数组转化成list，放在LinkedHashSet集合中 this.primarySources = new LinkedHashSet<>(Arrays.asList(primarySources)); //确定项目的启动类型，这也是springboot的强大之处 //springboot1.5 只有两种类型：web环境和非web环境， springboot2.0 有三种应用类型：WebApplicationType NONE：不需要再web容器的环境下运行，也就是普通的工程 //SERVLET：基于servlet的Web项目 REACTIVE：响应式web应用reactive web Spring5版本的新特性 this.webApplicationType = WebApplicationType.deduceFromClasspath(); //每一个initailizer都是一个实现了ApplicationContextInitializer接口的实例。 //ApplicationContextInitializer是Spring IOC容器中提供的一个接口： void initialize(C applicationContext); //这个方法它会在ConfigurableApplicationContext的refresh()方法调用之前被调用（prepareContext方法中调用）,做一些容器的初始化工作。 setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); //Springboot整个生命周期在完成一个阶段的时候都会通过事件推送器(EventPublishingRunListener)产生一个事件(ApplicationEvent)， //然后再遍历每个监听器(ApplicationListener)以匹配事件对象，这是一种典型的观察者设计模式的实现具体事件推送原理请看：sb事件推送机制图 setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); //指定main函数启动所在的类，即启动类BootApplication.class this.mainApplicationClass = deduceMainApplicationClass(); } 我们来大概的看下ApplicationListener的一些实现类以及他们具体的功能简介 这些监听器的实现类都是在spring.factories文件中配置好的，代码中通过getSpringFactoriesInstances方法获取，这种机制叫做SPI机制：通过本地的注册发现获取到具体的实现类，轻松可插拔。 SpringBoot默认情况下提供了两个spring.factories文件，分别是： spring-boot-2.0.2.RELEASE.jar spring-boot-autoconfigure-2.0.2.RELEASE.jar 概括来说在创建SpringApplication实例的时候，sb会加载一些初始化和启动的参数与类，如同跑步比赛时的等待发令枪的阶段； run方法 run方法整体流程简述 /** * Run the Spring application, creating and refreshing a new * {@link ApplicationContext}. * @param args the application arguments (usually passed from a Java main method) * @return a running {@link ApplicationContext} */ public ConfigurableApplicationContext run(String... args) { //StopWatch: 简单的秒表，允许定时的一些任务，公开每个指定任务的总运行时间和运行时间。这个对象的设计不是线程安全的，没有使用同步。SpringApplication是在单线程环境下，使用安全。 StopWatch stopWatch = new StopWatch(); // 设置当前启动的时间为系统时间startTimeMillis = System.currentTimeMillis(); stopWatch.start(); // 创建一个应用上下文引用 ConfigurableApplicationContext context = null; // 异常收集，报告启动异常 Collection exceptionReporters = new ArrayList<>(); //系统设置headless模式（一种缺乏显示设备、键盘或鼠标的环境下，比如服务器）， 通过属性：java.awt.headless=true控制 configureHeadlessProperty(); //获取事件推送监器，负责产生事件，并调用支某类持事件的监听器 事件推送原理看 事件推送原理图 SpringApplicationRunListeners listeners = getRunListeners(args); //发布一个启动事件(ApplicationStartingEvent)，通过上述方法调用支持此事件的监听器 listeners.starting(); try { // 提供对用于运行SpringApplication的参数的访问。取默认实现 ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); //构建容器环境，这里加载配置文件 ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); // 对环境中一些bean忽略配置 configureIgnoreBeanInfo(environment); // 对环境中一些bean忽略配置 Banner printedBanner = printBanner(environment); // 创建容器 context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[] { ConfigurableApplicationContext.class }, context); // 准备应用程序上下文追踪源码prepareContext（）进去我们可以发现容器准备阶段做了下面的事情： // 容器设置配置环境，并且监听容器，初始化容器，记录启动日志 // 将给定的singleton对象添加到此工厂的singleton缓存中。 // 将bean加载到应用程序上下文中。 prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); // stopwatch 的作用就是记录启动消耗的时间，和开始启动的时间等信息记录下来 stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch); } // 发布一个已启动的事件 listeners.started(context); callRunners(context, applicationArguments); } catch (Throwable ex) { handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); } try { // 发布一个运行中的事件 listeners.running(context); } catch (Throwable ex) { // 启动异常，里面会发布一个失败的事件 handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); } return context; } 监听器的配置与加载 让我们忽略 Spring Boot 计 时和统计的辅助功能，直接来看 SpringApplicationRunListeners获取和使用 SpringApplicationRunL isteners可以理解为一个 SpringApplicationRunListener的容器， 它将 SpringApplicationRunListener 的集合以构造方法传入，并赋值给其 listeners成员变量，然后提供了针对 listeners 成员变量的各种遍历操作方法， 比如，遍历集合并调用对应的 starting、started、 running 等方法。 private SpringApplicationRunListeners getRunListeners(String[] args) { //构造 Class 数组 Class[] types = new Class[] { SpringApplication.class, String[].class }; //调用 SpringAppl icat ionRunL isteners 构造方法 return new SpringApplicationRunListeners(logger, getSpringFactoriesInstances(SpringApplicationRunListener.class, types, this, args)); } SpringApplicationRunListeners 构 造 方 法 的 第 二 个 参 数 便 是 SpringApplicationRunListener 的 集 合 ， SpringApplication 中 调 用 构 造 方 法 时 该 参 数 是 通 过getSpringFactoriesInstances 方法获取的，代码如下。 private Collection getSpringFactoriesInstances(Class type, Class[] parameterTypes, Object... args) { ClassLoader classLoader = getClassLoader(); // Use names and ensure unique to protect against duplicates //加 META- TNE/sprina. factori es 中对应监听器的配并将结果存 set 中(去重) Set names = new LinkedHashSet<>(SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances; } 通过方法名便可得知，getSpringFactoriesInstances 是用来获取 factories 配置文件中的注册类，并进行实例化操作。 关于通过 SpringFactoriesLoader 获取 META-INF/spring.factories 中对应的配置，前面章节已经多次提到，这里不再赘述。 SpringApplicationRunListener 的注册配置位于 spring-boot 项目中的 spring.factories 文件内，Spring Boot 默认仅有- -个监听器进行了注册，关于其功能后面会专门讲到。 我们继续看实例化监听器的方法 createSpringFactoriesInstances 的源代码。 private List createSpringFactoriesInstances(Class type, Class[] parameterTypes, ClassLoader classLoader, Object[] args, Set names) { List instances = new ArrayList<>(names.size()); for (String name : names) { try { Class instanceClass = ClassUtils.forName(name, classLoader); Assert.isAssignable(type, instanceClass); //获取有参构造器 Constructor constructor = instanceClass.getDeclaredConstructor(parameterTypes); T instance = (T) BeanUtils.instantiateClass(constructor, args); instances.add(instance); } catch (Throwable ex) { throw new IllegalArgumentException(\"Cannot instantiate \" + type + \" : \" + name, ex); } } return instances; } 在上面的代码中，实例化监听器时需要有一-个默认的构造方法， 且构造方法的参数为Class[ ] parameterTypes。我们向上追踪该参数的来源，会发现该参数的值为 Class 数组 ， 数 组 的 内 容 依 次 为 SpringApplication.class 和 String[ ].class 。 也 就 是 说 ，SpringApplicationRunL istener 的实现类必须有默认的构造方法， 且构造方法的参数必须依次为 SpringApplication 和 String[ ]类型。 SpringApplicationRunListener 源码解析 接口 SpringApplicationRunListener 是 SpringApplication 的 run 方法监听器。上节提到了SpringApplicationRunListener 通过 SpringFactoriesL oader 加载，并且必须声明一个公共构造函数，该函数接收 SpringApplication 实例和 String[ ]的参数，而且每次运行都会创建一个新的实例。 SpringApplicationRunListener 提供了-系列的方法，用户可以通过回调这些方法，在启动各个流程时加入指定的逻辑处理。下面我们对照源代码和注释来了解一下该接口都定义了哪些待实现的方法及功能。 实现类 EventPublishingRunListener EventPublishingRunL istener 是 SpringBoot 中针对 SpringApplicationRunListener 接口的唯内建实现EventPublishingRunL istener使用内置的SimpleApplicationEventMulticaster来广播在上下文刷新之前触发的事件。 默认情况下，Spring Boot在初始化过程中触发的事件也是交由EventPublishingRunListener来代理实现的。EventPublishingRunListener 的构造方法如下。 Spring Boot 完成基本的初始化之后，会遍历 SpringApplication 的所有 ApplicationListener实 例 ， 并 将 它 们 与 SimpleApplicationEventMulticaster 进 行 关 联 ， 方 便SimpleApplicationEvent-Multicaster 后续将事件传递给所有的监听器。 EventPublishingRunListener 针对不同的事件提供了不同的处理方法，但它们的处理流程基本相同。 事件推送原理 启动过程中分多个阶段或者说是多个步骤，每完成一步就会产生一个事件，并调用对应事件的监听器，这是一种标准的观察者模式，这在启动的过程中有很好的扩展性，下面我们来看看sb的事件推送原理： 构建容器环境 在：run方法中的ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments);是准备环境，里面会加载配置文件； private ConfigurableEnvironment prepareEnvironment(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) { // Create and configure the environment // 创建一个配置环境，根据前面定义的应用类型定义不同的环境 ConfigurableEnvironment environment = getOrCreateEnvironment(); // 将配置参数设置到配置环境中 configureEnvironment(environment, applicationArguments.getSourceArgs()); ConfigurationPropertySources.attach(environment); //发布一个环境装载成功的事件（ApplicationEnvironmentPreparedEvent），并调用支持此事件的监听器 这其中就有配置文件加载监听器（ConfigFileApplicationListener） listeners.environmentPrepared(environment); // 将配置环境绑定到应用程序 bindToSpringApplication(environment); if (!this.isCustomEnvironment) { environment = new EnvironmentConverter(getClassLoader()).convertEnvironmentIfNecessary(environment, deduceEnvironmentClass()); } ConfigurationPropertySources.attach(environment); return environment; } ConfigFileApplicationListener 代码有删减 加载了application.yml配置文件 public class ConfigFileApplicationListener implements EnvironmentPostProcessor, SmartApplicationListener, Ordered { private static final String DEFAULT_PROPERTIES = \"defaultProperties\"; // Note the order is from least to most specific (last one wins) private static final String DEFAULT_SEARCH_LOCATIONS = \"classpath:/,classpath:/config/,file:./,file:./config/*/,file:./config/\"; private static final String DEFAULT_NAMES = \"application\"; private static final Set NO_SEARCH_NAMES = Collections.singleton(null); private static final Bindable STRING_ARRAY = Bindable.of(String[].class); private static final Bindable> STRING_LIST = Bindable.listOf(String.class); private static final Set LOAD_FILTERED_PROPERTY; static { Set filteredProperties = new HashSet<>(); filteredProperties.add(\"spring.profiles.active\"); filteredProperties.add(\"spring.profiles.include\"); LOAD_FILTERED_PROPERTY = Collections.unmodifiableSet(filteredProperties); } /** * The \"active profiles\" property name. */ public static final String ACTIVE_PROFILES_PROPERTY = \"spring.profiles.active\"; /** * The \"includes profiles\" property name. */ public static final String INCLUDE_PROFILES_PROPERTY = \"spring.profiles.include\"; /** * The \"config name\" property name. */ public static final String CONFIG_NAME_PROPERTY = \"spring.config.name\"; /** * The \"config location\" property name. */ public static final String CONFIG_LOCATION_PROPERTY = \"spring.config.location\"; /** * The \"config additional location\" property name. */ public static final String CONFIG_ADDITIONAL_LOCATION_PROPERTY = \"spring.config.additional-location\"; @Override public void onApplicationEvent(ApplicationEvent event) { if (event instanceof ApplicationEnvironmentPreparedEvent) { onApplicationEnvironmentPreparedEvent((ApplicationEnvironmentPreparedEvent) event); } if (event instanceof ApplicationPreparedEvent) { onApplicationPreparedEvent(event); } } private void onApplicationEnvironmentPreparedEvent(ApplicationEnvironmentPreparedEvent event) { List postProcessors = loadPostProcessors(); postProcessors.add(this); AnnotationAwareOrderComparator.sort(postProcessors); for (EnvironmentPostProcessor postProcessor : postProcessors) { postProcessor.postProcessEnvironment(event.getEnvironment(), event.getSpringApplication()); } } } prepareContext private void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) { context.setEnvironment(environment); postProcessApplicationContext(context); applyInitializers(context); listeners.contextPrepared(context); if (this.logStartupInfo) { logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); } // Add boot specific singleton beans ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); beanFactory.registerSingleton(\"springApplicationArguments\", applicationArguments); if (printedBanner != null) { beanFactory.registerSingleton(\"springBootBanner\", printedBanner); } if (beanFactory instanceof DefaultListableBeanFactory) { ((DefaultListableBeanFactory) beanFactory) .setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); } if (this.lazyInitialization) { context.addBeanFactoryPostProcessor(new LazyInitializationBeanFactoryPostProcessor()); } // Load the sources //这里的sources就是获取的 new SpringApplication(Application.class) 中的Application.class Set sources = getAllSources(); Assert.notEmpty(sources, \"Sources must not be empty\"); load(context, sources.toArray(new Object[0])); listeners.contextLoaded(context); } load 方法 /** * Load beans into the application context. * @param context the context to load beans into * @param sources the sources to load */ protected void load(ApplicationContext context, Object[] sources) { if (logger.isDebugEnabled()) { logger.debug(\"Loading source \" + StringUtils.arrayToCommaDelimitedString(sources)); } BeanDefinitionLoader loader = createBeanDefinitionLoader(getBeanDefinitionRegistry(context), sources); if (this.beanNameGenerator != null) { loader.setBeanNameGenerator(this.beanNameGenerator); } if (this.resourceLoader != null) { loader.setResourceLoader(this.resourceLoader); } if (this.environment != null) { loader.setEnvironment(this.environment); } loader.load(); } protected BeanDefinitionLoader createBeanDefinitionLoader(BeanDefinitionRegistry registry, Object[] sources) { return new BeanDefinitionLoader(registry, sources); } 可以看到再这里将启动文件加载到了BeanDefinition中（这里表述不准确） @SpringBootApplication 启动类（或者配置类）已经加载完成了，那么接下来就是处理这个类。根据默认的的规则，一般我们只需要处理一个@SpringBootApplication注解。这个是源头 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { 实际上重要的只有三个Annotation： @Configuration（@SpringBootConfiguration里面还是应用了@Configuration） @EnableAutoConfiguration @ComponentScan @Configuration的作用上面我们已经知道了，被注解的类将成为一个bean配置类。 @ComponentScan的作用就是自动扫描并加载符合条件的组件，比如@Component和@Repository等，最终将这些bean定义加载到spring容器中。 @EnableAutoConfiguration 这个注解的功能很重要，借助@Import的支持，收集和注册依赖包中相关的bean定义。 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import(AutoConfigurationImportSelector.class) public @interface EnableAutoConfiguration { 如上源码，@EnableAutoConfiguration注解引入了@AutoConfigurationPackage和@Import这两个注解。@AutoConfigurationPackage的作用就是自动配置的包，@Import导入需要自动配置的组件。 进入@AutoConfigurationPackage，发现也是引入了@Import注解 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Import(AutoConfigurationPackages.Registrar.class) public @interface AutoConfigurationPackage { static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { Registrar() { } public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { AutoConfigurationPackages.register(registry, new String[]{(new AutoConfigurationPackages.PackageImport(metadata)).getPackageName()}); } public Set determineImports(AnnotationMetadata metadata) { return Collections.singleton(new AutoConfigurationPackages.PackageImport(metadata)); } } new AutoConfigurationPackages.PackageImport(metadata)).getPackageName() new AutoConfigurationPackages.PackageImport(metadata) 这两句代码的作用就是加载启动类所在的包下的主类与子类的所有组件注册到spring容器，这就是前文所说的springboot默认扫描启动类所在的包下的主类与子类的所有组件。 那问题又来了，要搜集并注册到spring容器的那些beans来自哪里？ 进入 AutoConfigurationImportSelector类， public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered { @Override public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return NO_IMPORTS; } AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations()); } ... /** * Return the auto-configuration class names that should be considered. By default * this method will load candidates using {@link SpringFactoriesLoader} with * {@link #getSpringFactoriesLoaderFactoryClass()}. * @param metadata the source metadata * @param attributes the {@link #getAttributes(AnnotationMetadata) annotation * attributes} * @return a list of candidate configurations */ protected List getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { List configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, \"No auto configuration classes found in META-INF/spring.factories. If you \" + \"are using a custom packaging, make sure that file is correct.\"); return configurations; } ... SpringFactoriesLoader.loadFactoryNames方法调用loadSpringFactories方法从所有的jar包中读取META-INF/spring.factories文件信息。 private static Map> loadSpringFactories(@Nullable ClassLoader classLoader) { MultiValueMap result = cache.get(classLoader); if (result != null) { return result; } try { Enumeration urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap<>(); while (urls.hasMoreElements()) { URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry entry : properties.entrySet()) { String factoryTypeName = ((String) entry.getKey()).trim(); for (String factoryImplementationName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) { result.add(factoryTypeName, factoryImplementationName.trim()); } } } cache.put(classLoader, result); return result; } catch (IOException ex) { throw new IllegalArgumentException(\"Unable to load factories from location [\" + FACTORIES_RESOURCE_LOCATION + \"]\", ex); } } spring.factories文件内容，其中有一个key为org.springframework.boot.autoconfigure.EnableAutoConfiguration的值定义了需要自动配置的bean，通过读取这个配置获取一组@Configuration类。 每个xxxAutoConfiguration都是一个基于java的bean配置类。实际上，这些xxxAutoConfiguratio不是所有都会被加载，会根据xxxAutoConfiguration上的@ConditionalOnClass等条件判断是否加载。 "},"Chapter11/AutoConfigurationImportSelector.html":{"url":"Chapter11/AutoConfigurationImportSelector.html","title":"AutoConfigurationImportSelector","keywords":"","body":"AutoConfigurationImportSelector 为什么要说它，要看看来自于哪里， 大家都知道SpringBoot的启动类上一般只需要一个@SpringBootApplication注解。那我们就深入瞅瞅，‘ @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import(AutoConfigurationImportSelector.class) public @interface EnableAutoConfiguration { 这里就是AutoConfigurationImportSelector的来源。 "},"Chapter11/FatJar.html":{"url":"Chapter11/FatJar.html","title":"FatJar的启动原理","keywords":"","body":"FatJar 的启动原理 SpringBoot在打包的时候会将依赖包也打进最终的Jar，变成一个可运行的FatJar。也就是会形成一个Jar in Jar的结构。 默认情况下，JDK提供的ClassLoader只能识别Jar中的class文件以及加载classpath下的其他jar包中的class文件。对于在jar包中的jar包是无法加载的。 储备知识 URLStreamHandler java中描述资源常使用URL。而URL有一个方法用于打开链接java.net.URL#openConnection()。由于URL用于表达各种各样的资源，打开资源的具体动作由java.net.URLStreamHandler这个类的子类来完成。根据不同的协议，会有不同的handler实现。而JDK内置了相当多的handler实现用于应对不同的协议。比如jar、file、http等等。URL内部有一个静态HashTable属性，用于保存已经被发现的协议和handler实例的映射。 获得URLStreamHandler有三种方法 实现URLStreamHandlerFactory接口，通过方法URL.setURLStreamHandlerFactory设置。该属性是一个静态属性，且只能被设置一次。 直接提供URLStreamHandler的子类，作为URL的构造方法的入参之一。但是在JVM中有固定的规范要求： 子类的类名必须是 Handler ，同时最后一级的包名必须是协议的名称。比如自定义了Http的协议实现，则类名必然为xx.http.Handler JVM 启动的时候，需要设置 java.protocol.handler.pkgs 系统属性，如果有多个实现类，那么中间用 | 隔开。因为JVM在尝试寻找Handler时，会从这个属性中获取包名前缀，最终使用包名前缀.协议名.Handler，使用Class.forName方法尝试初始化类，如果初始化成功，则会使用该类的实现作为协议实现。 Archive SpringBoot定义了一个接口用于描述资源，也就是org.springframework.boot.loader.archive.Archive。该接口有两个实现，分别是org.springframework.boot.loader.archive.ExplodedArchive和org.springframework.boot.loader.archive.JarFileArchive。 前者用于在文件夹目录下寻找资源，后者用于在jar包环境下寻找资源。而在SpringBoot打包的fatJar中，则是使用后者。 打包 BOOT-INF文件夹下放的程序编译class和依赖的jar包 org目录下放的是SpringBoot的启动相关包。 来看描述文件MANIFEST.MF的内容 Manifest-Version: 1.0 Spring-Boot-Classpath-Index: BOOT-INF/classpath.idx Implementation-Title: spring Implementation-Version: 0.0.1-SNAPSHOT Start-Class: io.github.firehuo.spring.Application Spring-Boot-Classes: BOOT-INF/classes/ Spring-Boot-Lib: BOOT-INF/lib/ Build-Jdk-Spec: 1.8 Spring-Boot-Version: 2.3.4.RELEASE Created-By: Maven Jar Plugin 3.2.0 Main-Class: org.springframework.boot.loader.JarLauncher 最为显眼的就是程序的启动类并不是我们项目的启动类，而是SpringBoot的JarLauncher。下面会来深究下这个类的作用。 SpringBoot启动 首先来看启动方法 public static void main(String[] args) throws Exception { new JarLauncher().launch(args); } JarLauncher继承于org.springframework.boot.loader.ExecutableArchiveLauncher。该类的无参构造方法最主要的功能就是构建了当前main方法所在的FatJar的JarFileArchive对象。下面来看launch方法。该方法主要是做了2个事情： 以FatJar为file作为入参，构造JarFileArchive对象。获取其中所有的资源目标，取得其Url，将这些URL作为参数，构建了一个URLClassLoader。 以第一步构建的ClassLoader加载MANIFEST.MF文件中Start-Class指向的业务类，并且执行静态方法main。进而启动整个程序。 通过静态方法org.springframework.boot.loader.JarLauncher#main就可以顺利启动整个程序。这里面的关键在于SpringBoot自定义的classLoader能够识别FatJar中的资源，包括有：在指定目录下的项目编译class、在指令目录下的项目依赖jar。JDK默认用于加载应用的AppClassLoader只能从jar的根目录开始加载class文件，并且也不支持jar in jar这种格式。 为了实现这个目标，SpringBoot首先从支持jar in jar中内容读取做了定制，也就是支持多个!/分隔符的url路径。SpringBoot定制了以下两个方面： 实现了一个java.net.URLStreamHandler的子类org.springframework.boot.loader.jar.Handler。该Handler支持识别多个!/分隔符，并且正确的打开URLConnection。打开的Connection是SpringBoot定制的org.springframework.boot.loader.jar.JarURLConnection实现。 实现了一个java.net.JarURLConnection的子类org.springframework.boot.loader.jar.JarURLConnection。该链接支持多个!/分隔符，并且自己实现了在这种情况下获取InputStream的方法。而为了能够在org.springframework.boot.loader.jar.JarURLConnection正确获取输入流，SpringBoot自定义了一套读取ZipFile的工具类和方法。这部分和ZIP压缩算法规范紧密相连，就不深入了。 能够读取多个!/的url后，事情就变得很简单了。上文提到的ExecutableArchiveLauncher的launch方法会以当前的FatJar构建一个JarFileArchive，并且通过该对象获取其内部所有的资源URL，这些URL包含项目编译class和依赖jar包。在构建这些URL的时候传入的就是SpringBoot定制的Handler。将获取的URL数组作为参数传递给自定义的ClassLoaderorg.springframework.boot.loader.LaunchedURLClassLoader。该ClassLoader继承自UrlClassLoader。UrlClassLoader加载class就是依靠初始参数传入的Url数组，并且尝试Url指向的资源中加载Class文件。有了自定义的Handler，再从Url中尝试获取资源就变得很容易了。 至此，SpringBoot自定义的ClassLoader就能够加载FatJar中的依赖包的class文件了。 扩展 SpringBoot提供了一个很好的思路，但是其内部实现非常复杂，特别是其自行实现了一个ZipFIle的解析器。但是本质上这些背后的工作都是为了能够读取到FatJar内部的Jar的class文件资源。也就是只要有办法能够读取这些资源其实就可以实现加载Class文件了。而依靠JDK本身提供的JarFile其实就可以做到了。而读取到所有资源后，自定义一个ClassLoader加载读取到二进制数据进而定义Class对象并不是很难的项目实现。当然，SpringBoot定制的Zip解析可以在加载类阶段避免频繁的文件解压动作，在性能上良好一些。 "},"Chapter11/CGLIB.html":{"url":"Chapter11/CGLIB.html","title":"CGLIB","keywords":"","body":"CGLIB 什么是CGLIB CGLIB（Code Generator Library）是一个强大的、高性能的代码生成库。其被广泛应用于AOP框架（Spring、dynaop）中，用以提供方法拦截操作。 Hibernate作为一个比较受欢迎的ORM框架，同样使用CGLIB来代理单端（多对一和一对一）关联（延迟提取集合使用的另一种机制）。 为什么使用CGLIB CGLIB代理主要通过对字节码的操作，为对象引入间接级别，以控制对象的访问。我们知道Java中有一个动态代理也是做这个事情的，那我们为什么不直接使用Java动态代理，而要使用CGLIB呢？ 答案是CGLIB相比于JDK动态代理更加强大，JDK动态代理虽然简单易用，但是其有一个致命缺陷是，只能对接口进行代理。如果要代理的类为一个普通类、没有接口，那么Java动态代理就没法使用了 CGLIB组成结构 CGLIB底层使用了ASM（一个短小精悍的字节码操作框架）来操作字节码生成新的类。除了CGLIB库外，脚本语言（如Groovy何BeanShell）也使用ASM生成字节码。 ASM使用类似SAX的解析器来实现高性能。我们不鼓励直接使用ASM，因为它需要对Java字节码的格式足够的了解 一个示例 cglib cglib 2.2.2 public class HelloImpl implements IHello { @Override public void sayHello() { System.out.println(\"hello\"); } } public class CgLibClient { public static void main(String[] args) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(HelloImpl.class); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { System.out.println(\"before method run...\"); Object result = methodProxy.invokeSuper(o, args); System.out.println(\"after method run...\"); return result; } }); HelloImpl hello = (HelloImpl) enhancer.create(); hello.sayHello(); } } 常用的API Enhancer Enhancer可能是CGLIB中最常用的一个类，和Java1.3动态代理中引入的Proxy类差不多。和Proxy不同的是，Enhancer既能够代理普通的class，也能够代理接口。 Enhancer创建一个被代理对象的子类并且拦截所有的方法调用（包括从Object中继承的toString和hashCode方法）。 Enhancer不能够拦截final方法，例如Object.getClass()方法，这是由于Java final方法语义决定的。基于同样的道理，Enhancer也不能对fianl类进行代理操作。 这也是Hibernate为什么不能持久化final class的原因。 @Test public void testFixedValue(){ Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(SampleClass.class); enhancer.setCallback(new FixedValue() { @Override public Object loadObject() throws Exception { return \"Hello cglib\"; } }); SampleClass proxy = (SampleClass) enhancer.create(); System.out.println(proxy.test(null)); //拦截test，输出Hello cglib System.out.println(\"=========\"); System.out.println(proxy.toString()); System.out.println(\"=========\"); System.out.println(proxy.getClass()); System.out.println(\"=========\"); System.out.println(proxy.hashCode()); } Hello cglib ========= Hello cglib ========= class cglib.SampleClass$$EnhancerByCGLIB$$a9f5b392 ========= java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Number 上述代码中，FixedValue用来对所有拦截的方法返回相同的值，从输出我们可以看出来，Enhancer对非final方法test()、toString()、hashCode()进行了拦截，没有对getClass进行拦截。 由于hashCode()方法需要返回一个Number，但是我们返回的是一个String，这解释了上面的程序中为什么会抛出异常。 Enhancer.setSuperclass用来设置父类型，从toString方法可以看出，使用CGLIB生成的类为被代理类的一个子类，形如：SampleClass$$EnhancerByCGLIB$$e3ea9b7 Enhancer.create(Object…)方法是用来创建增强对象的，其提供了很多不同参数的方法用来匹配被增强类的不同构造方法。 （虽然类的构造放法只是Java字节码层面的函数，但是Enhancer却不能对其进行操作。Enhancer同样不能操作static或者final类）。 我们也可以先使用Enhancer.createClass()来创建字节码(.class)，然后用字节码动态的生成增强后的对象。 可以使用一个InvocationHandler作为回调，使用invoke方法来替换直接访问类的方法，但是你必须注意死循环。 因为invoke中调用的任何原代理类方法，均会重新代理到invoke方法中。 @Test public void testInvocationHandler() throws Exception { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(SampleClass.class); enhancer.setCallback(new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if (method.getDeclaringClass() != Object.class && method.getReturnType() == String.class) { return \"hello cglib\"; } else { throw new RuntimeException(\"Do not know what to do\"); } } }); SampleClass proxy = (SampleClass) enhancer.create(); System.out.println(proxy.test(null)); // Assert.assertEquals(\"hello cglib\", proxy.test(null)); Assert.assertNotEquals(\"Hello cglib\", proxy.toString()); } u 为了避免这种死循环，我们可以使用MethodInterceptor，MethodInterceptor的例子在前面的hello world中已经介绍过了，这里就不浪费时间了。 有些时候我们可能只想对特定的方法进行拦截，对其他的方法直接放行，不做任何操作，使用Enhancer处理这种需求同样很简单,只需要一个CallbackFilter即可： @Test public void testCallbackFilter() throws Exception { Enhancer enhancer = new Enhancer(); CallbackHelper callbackHelper = new CallbackHelper(SampleClass.class, new Class[0]) { @Override protected Object getCallback(Method method) { if (method.getDeclaringClass() != Object.class && method.getReturnType() == String.class) { return new FixedValue() { @Override public Object loadObject() throws Exception { return \"Hello cglib\"; } }; } else { return NoOp.INSTANCE; } } }; enhancer.setSuperclass(SampleClass.class); enhancer.setCallbackFilter(callbackHelper); enhancer.setCallbacks(callbackHelper.getCallbacks()); SampleClass proxy = (SampleClass) enhancer.create(); Assert.assertEquals(\"Hello cglib\", proxy.test(null)); Assert.assertNotEquals(\"Hello cglib\", proxy.toString()); System.out.println(proxy.hashCode()); } 2080166188 ImmutableBean 通过名字就可以知道，不可变的Bean。ImmutableBean允许创建一个原来对象的包装类，这个包装类是不可变的，任何改变底层对象的包装类操作都会抛出IllegalStateException。但是我们可以通过直接操作底层对象来改变包装类对象。这有点类似于Guava中的不可变视图 为了对ImmutableBean进行测试，这里需要再引入一个bean public class SampleBean { private String value; public SampleBean() { } public SampleBean(String value) { this.value = value; } public String getValue() { return value; } public void setValue(String value) { this.value = value; } } 然后编写测试类如下： @Test(expected = IllegalStateException.class) public void testImmutableBean() throws Exception{ SampleBean bean = new SampleBean(); bean.setValue(\"Hello world\"); SampleBean immutableBean = (SampleBean) ImmutableBean.create(bean); //创建不可变类 Assert.assertEquals(\"Hello world\",immutableBean.getValue()); bean.setValue(\"Hello world, again\"); //可以通过底层对象来进行修改 Assert.assertEquals(\"Hello world, again\", immutableBean.getValue()); immutableBean.setValue(\"Hello cglib\"); //直接修改将throw exception } BeanGenerator cglib提供的一个操作bean的工具，使用它能够在运行时动态的创建一个bean。 @Test public void testBeanGenerator() throws Exception{ BeanGenerator beanGenerator = new BeanGenerator(); beanGenerator.addProperty(\"value\",String.class); Object myBean = beanGenerator.create(); Method setter = myBean.getClass().getMethod(\"setValue\",String.class); setter.invoke(myBean,\"Hello cglib\"); Method getter = myBean.getClass().getMethod(\"getValue\"); Assert.assertEquals(\"Hello cglib\",getter.invoke(myBean)); } 在上面的代码中，我们使用cglib动态的创建了一个和SampleBean相同的Bean对象，包含一个属性value以及getter、setter方法 BeanCopier cglib提供的能够从一个bean复制到另一个bean中，而且其还提供了一个转换器，用来在转换的时候对bean的属性进行操作。 @Test public void testBeanCopier() throws Exception{ BeanCopier copier = BeanCopier.create(SampleBean.class, OtherSampleBean.class, false);//设置为true，则使用converter SampleBean myBean = new SampleBean(); myBean.setValue(\"Hello cglib\"); OtherSampleBean otherBean = new OtherSampleBean(); copier.copy(myBean, otherBean, null); //设置为true，则传入converter指明怎么进行转换 Assert.assertEquals(\"Hello cglib\", otherBean.getValue()); } BulkBean 相比于BeanCopier，BulkBean将copy的动作拆分为getPropertyValues和setPropertyValues两个方法，允许自定义处理属性 @Test public void testBulkBean() throws Exception { BulkBean bulkBean = BulkBean.create(SampleBean.class, new String[]{\"getValue\"}, new String[]{\"setValue\"}, new Class[]{String.class}); SampleBean bean = new SampleBean(); bean.setValue(\"Hello world\"); Object[] propertyValues = bulkBean.getPropertyValues(bean); Assert.assertEquals(1, bulkBean.getPropertyValues(bean).length); Assert.assertEquals(\"Hello world\", bulkBean.getPropertyValues(bean)[0]); bulkBean.setPropertyValues(bean, new Object[]{\"Hello cglib\"}); Assert.assertEquals(\"Hello cglib\", bean.getValue()); } 使用注意： 避免每次进行BulkBean.create创建对象，一般将其声明为static的 应用场景：针对特定属性的get,set操作，一般适用通过xml配置注入和注出的属性，运行时才确定处理的Source,Target类，只需要关注属性名即可。 BeanMap BeanMap类实现了Java Map，将一个bean对象中的所有属性转换为一个String-to-Obejct的Java Map @Test public void testBeanMap() throws Exception { BeanGenerator generator = new BeanGenerator(); generator.addProperty(\"username\", String.class); generator.addProperty(\"password\", String.class); Object bean = generator.create(); Method setUserName = bean.getClass().getMethod(\"setUsername\", String.class); Method setPassword = bean.getClass().getMethod(\"setPassword\", String.class); setUserName.invoke(bean, \"admin\"); setPassword.invoke(bean, \"password\"); BeanMap map = BeanMap.create(bean); Assert.assertEquals(\"admin\", map.get(\"username\")); Assert.assertEquals(\"password\", map.get(\"password\")); } 我们使用BeanGenerator生成了一个含有两个属性的Java Bean，对其进行赋值操作后，生成了一个BeanMap对象，通过获取值来进行验证 keyFactory keyFactory类用来动态生成接口的实例，接口需要只包含一个newInstance方法，返回一个Object。keyFactory为构造出来的实例动态生成了Object.equals和Object.hashCode方法，能够确保相同的参数构造出的实例为单例的。 public interface SampleKeyFactory { Object newInstance(String first, int second); } 我们首先构造一个满足条件的接口，然后进行测试 @Test public void testKeyFactory() throws Exception { SampleKeyFactory keyFactory = (SampleKeyFactory) KeyFactory.create(SampleKeyFactory.class); Object key = keyFactory.newInstance(\"foo\", 42); Object key1 = keyFactory.newInstance(\"foo\", 42); Assert.assertEquals(key, key1);//测试参数相同，结果是否相等 } Mixin(混合) Mixin能够让我们将多个对象整合到一个对象中去，前提是这些对象必须是接口的实现。可能这样说比较晦涩，以代码为例： public interface Interface1 { String first(); } public interface Interface2 { String second(); } public class Class1 implements Interface1{ @Override public String first() { return \"first\"; } } public class Class2 implements Interface2 { @Override public String second() { return \"second\"; } } public interface MixinInterface extends Interface1, Interface2 { } @Test public void testMixin() throws Exception { Mixin mixin = Mixin.create(new Class[]{Interface1.class, Interface2.class, MixinInterface.class}, new Object[]{new Class1(), new Class2()}); MixinInterface mixinDelegate = (MixinInterface) mixin; Assert.assertEquals(\"first\", mixinDelegate.first()); Assert.assertEquals(\"second\", mixinDelegate.second()); } Mixin类比较尴尬，因为他要求Minix的类（例如MixinInterface）实现一些接口。既然被Minix的类已经实现了相应的接口，那么我就直接可以通过纯Java的方式实现，没有必要使用Minix类。 StringSwitcher 用来模拟一个String到int类型的Map类型。如果在Java7以后的版本中，类似一个switch语句。 @Test public void testStringSwitcher() throws Exception{ String[] strings = new String[]{\"one\", \"two\"}; int[] values = new int[]{10,20}; StringSwitcher stringSwitcher = StringSwitcher.create(strings,values,true); assertEquals(10, stringSwitcher.intValue(\"one\")); assertEquals(20, stringSwitcher.intValue(\"two\")); assertEquals(-1, stringSwitcher.intValue(\"three\")); } InterfaceMaker 正如名字所言，Interface Maker用来创建一个新的Interface @Test public void testInterfaceMarker() throws Exception{ Signature signature = new Signature(\"foo\", Type.DOUBLE_TYPE, new Type[]{Type.INT_TYPE}); InterfaceMaker interfaceMaker = new InterfaceMaker(); interfaceMaker.add(signature, new Type[0]); Class iface = interfaceMaker.create(); assertEquals(1, iface.getMethods().length); assertEquals(\"foo\", iface.getMethods()[0].getName()); assertEquals(double.class, iface.getMethods()[0].getReturnType()); } 上述的Interface Maker创建的接口中只含有一个方法，签名为double foo(int)。Interface Maker与上面介绍的其他类不同，它依赖ASM中的Type类型。由于接口仅仅只用做在编译时期进行类型检查， 因此在一个运行的应用中动态的创建接口没有什么作用。但是InterfaceMaker可以用来自动生成代码，为以后的开发做准备。 Method delegate MethodDelegate主要用来对方法进行代理 public interface BeanDelegate { String getValueFromDelegate(); } @Test public void testMethodDelegate() throws Exception { SampleBean bean = new SampleBean(); bean.setValue(\"Hello cglib\"); BeanDelegate delegate = (BeanDelegate) MethodDelegate.create(bean, \"getValue\", BeanDelegate.class); Assert.assertEquals(\"Hello cglib\", delegate.getValueFromDelegate()); } 关于Method.create的参数说明： 第二个参数为即将被代理的方法 第一个参数必须是一个无参数构造的bean。因此MethodDelegate.create并不是你想象的那么有用 第三个参数为只含有一个方法的接口。当这个接口中的方法被调用的时候，将会调用第一个参数所指向bean的第二个参数方法 缺点 为每一个代理类创建了一个新的类，这样可能会占用大量的永久代堆内存 你不能代理需要参数的方法 如果你定义的接口中的方法需要参数，那么代理将不会工作，并且也不会抛出异常；如果你的接口中方法需要其他的返回类型，那么将抛出IllegalArgumentException Constructor delegate 为了对构造函数进行代理，我们需要一个接口，这个接口只含有一个Object newInstance(…)方法，用来调用相应的构造函数 FastClass 顾明思义，FastClass就是对Class对象进行特定的处理，比如通过数组保存method引用，因此FastClass引出了一个index下标的新概念，比如getIndex(String name, Class[] parameterTypes)就是以前的获取method的方法。通过数组存储method,constructor等class信息，从而将原先的反射调用，转化为class.index的直接调用，从而体现所谓的FastClass。 @Test public void testFastClass() throws Exception { FastClass fastClass = FastClass.create(SampleBean.class); FastMethod fastMethod = fastClass.getMethod(\"getValue\", new Class[0]); SampleBean bean = new SampleBean(); bean.setValue(\"Hello world\"); Assert.assertEquals(\"Hello world\", fastMethod.invoke(bean, new Object[0])); } 注意 由于CGLIB的大部分类是直接对Java字节码进行操作，这样生成的类会在Java的永久堆中。如果动态代理操作过多，容易造成永久堆满，触发OutOfMemory异常。 "},"Chapter11/DynamicProxy.html":{"url":"Chapter11/DynamicProxy.html","title":"动态代理","keywords":"","body":"动态代理 先看看代理模式 何为动态代理？ 动态代理就是，在程序运行期，创建目标对象的代理对象，并对目标对象中的方法进行功能性增强的一种技术。 它就是JVM中，对象方法的拦截器;在生成代理对象的过程中，目标对象不变，代理对象中的方法是目标对象方法的增强方法。 可以理解为运行期间，对象中方法的动态拦截，在拦截方法的前后执行功能操作。 代理类在程序运行期间，创建的代理对象称之为动态代理对象。这种情况下，创建的代理对象，并不是事先在Java代码中定义好的。而是在运行期间，根据我们在动态代理对象中的“指示”，动态生成的。 也就是说，你想获取哪个对象的代理，动态代理就会为你动态的生成这个对象的代理对象。 动态代理可以对被代理对象的方法进行功能增强。有了动态代理的技术，那么就可以在不修改方法源码的情况下，增强被代理对象的方法的功能，在方法执行前后做任何你想做的事情。 创建代理对象的两个方法 //JDK动态代理 Proxy.newProxyInstance(三个参数); //CGLib动态代理 Enhancer.create(两个参数); 正常类创建对象的过程 动态代理创建代理对象的过程 两种常用的动态代理方式 基于接口的动态代理 提供者：JDK 必须实现InvocationHandler接口,使用JDK官方的Proxy类创建代理对象 代理的目标对象必须实现接口基于类的动态代理 提供者：第三方 CGLib 使用CGLib的Enhancer类创建代理对象 注意：如果报 asmxxxx 异常，需要导入 asm.jar包 JDK动态代理(jdk8) public interface IHello { void sayHello(); } public class HelloImpl implements IHello { @Override public void sayHello() { System.out.println(\"hello\"); } } public class HelloInvocationHandler implements InvocationHandler { private Object target; public HelloInvocationHandler(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"------插入前置通知代码-------------\"); // 执行相应的目标方法 Object rs = method.invoke(target, args); System.out.println(\"------插入后置处理代码-------------\"); return rs; } } public class HelloClient { public static void main(String[] args) throws NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException { /*第一种*/ IHello hello = new HelloImpl(); hello.sayHello(); // 生成$Proxy0的class文件 //System.getProperties().put(\"sun.misc.ProxyGenerator.saveGeneratedFiles\", \"true\"); /*第二种*/ HelloInvocationHandler helloInvocationHandler = new HelloInvocationHandler(new HelloImpl()); IHello hello1 = (IHello) Proxy.newProxyInstance(IHello.class.getClassLoader(), new Class[]{IHello.class}, helloInvocationHandler); hello1.sayHello(); /*第三种*/ // 1、获取动态代理类 Class proxy = Proxy.getProxyClass(IHello.class.getClassLoader(), IHello.class); // 2、获得代理类的构造函数，并传入参数类型InvocationHandler.class Constructor constructor = proxy.getConstructor(InvocationHandler.class); // 3、通过构造函数来创建动态代理对象，将自定义的InvocationHandler实例传入 IHello hello2 = (IHello) constructor.newInstance(new HelloInvocationHandler(new HelloImpl())); hello2.sayHello(); } } 深入源码分析 以Proxy.newProxyInstance()方法为切入点来剖析代理类的生成及代理方法的调用。 /** * loader：接口的类加载器 8/ @CallerSensitive public static Object newProxyInstance(ClassLoader loader, Class[] interfaces, InvocationHandler h) throws IllegalArgumentException { // 如果h为空直接抛出空指针异常，之后所有的单纯的判断null并抛异常，都是此方法 Objects.requireNonNull(h); // 拷贝类实现的所有接口 final Class[] intfs = interfaces.clone(); // 获取当前系统安全接口 final SecurityManager sm = System.getSecurityManager(); if (sm != null) { // Reflection.getCallerClass返回调用该方法的方法的调用类; // 进行包访问权限、类加载器权限等检查 checkProxyAccess(Reflection.getCallerClass(), loader, intfs); } /* * Look up or generate the designated proxy class. * 查找或生成指定的代理类 */ //1 Class cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. * 用指定的调用处理程序调用它的构造函数 */ try { if (sm != null) { checkNewProxyPermission(Reflection.getCallerClass(), cl); } //2 获取代理类的构造函数对象 //constructorParams是类常量，作为代理类构造函数的参数类型，常量定义如下: //private static final Class[] constructorParams = { InvocationHandler.class }; final Constructor cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) { AccessController.doPrivileged(new PrivilegedAction() { public Void run() { cons.setAccessible(true); return null; } }); } //3 根据代理类的构造函数对象来创建需要返回的代理类对象 return cons.newInstance(new Object[]{h}); } catch (IllegalAccessException|InstantiationException e) { throw new InternalError(e.toString(), e); } catch (InvocationTargetException e) { Throwable t = e.getCause(); if (t instanceof RuntimeException) { throw (RuntimeException) t; } else { throw new InternalError(t.toString(), t); } } catch (NoSuchMethodException e) { throw new InternalError(e.toString(), e); } } 可以大概看出Proxy.newProxyInstance封装了第二种方法中的1、2、3步。帮我们执行了生成代理类----获取构造器----生成代理对象这三步； Proxy.getProxyClass0()如何生成代理类？ private static Class getProxyClass0(ClassLoader loader, Class... interfaces) { // 接口数不得超过65535个，这么大，足够使用的了 if (interfaces.length > 65535) { throw new IllegalArgumentException(\"interface limit exceeded\"); } // If the proxy class defined by the given loader implementing // the given interfaces exists, this will simply return the cached copy; // otherwise, it will create the proxy class via the ProxyClassFactory //如果缓存中有代理类了直接返回，否则将由代理类工厂ProxyClassFactory创建代理类 return proxyClassCache.get(loader, interfaces); } 如果缓存中没有代理类，Proxy中的ProxyClassFactory如何创建代理类？从get()方法追踪进去看看。 public V get(K key, P parameter) { // 检查指定类型的对象引用不为空null。当参数为null时，抛出空指针异常 Objects.requireNonNull(parameter); // 清除已经被GC回收的弱引用 expungeStaleEntries(); // 将ClassLoader包装成CacheKey, 作为一级缓存的key Object cacheKey = CacheKey.valueOf(key, refQueue); // lazily install the 2nd level valuesMap for the particular cacheKey //获取得到二级缓存 ConcurrentMap> valuesMap = map.get(cacheKey); // 没有获取到对应的值 if (valuesMap == null) { ConcurrentMap> oldValuesMap //这里是防止并发，付给它一个新值 = map.putIfAbsent(cacheKey, valuesMap = new ConcurrentHashMap<>()); if (oldValuesMap != null) { valuesMap = oldValuesMap; } } // create subKey and retrieve the possible Supplier stored by that subKey from valuesMap // 根据代理类实现的接口数组来生成二级缓存key Object subKey = Objects.requireNonNull(subKeyFactory.apply(key, parameter)); Supplier supplier = valuesMap.get(subKey); Factory factory = null; // 这个循环提供了轮询机制, 如果条件为假就继续重试直到条件为真为止 while (true) { if (supplier != null) { // supplier might be a Factory or a CacheValue instance // 在这里supplier可能是一个Factory也可能会是一个CacheValue // 在这里不作判断, 而是在Supplier实现类的get方法里面进行验证 // 如果第一次的化 应该是 ProxyClassFactory V value = supplier.get(); if (value != null) { return value; } } // else no supplier in cache // or a supplier that returned null (could be a cleared CacheValue // or a Factory that wasn't successful in installing the CacheValue) // lazily construct a Factory if (factory == null) { // 新建一个Factory实例作为subKey对应的值 // 这里引入ProxyClassFactory factory = new Factory(key, parameter, subKey, valuesMap); } if (supplier == null) { // 到这里表明subKey没有对应的值, 就将factory作为subKey的值放入 supplier = valuesMap.putIfAbsent(subKey, factory); if (supplier == null) { // successfully installed Factory // 到这里表明成功将factory放入缓存 supplier = factory; } // else retry with winning supplier 否则, 可能期间有其他线程修改了值, 那么就不再继续给subKey赋值, 而是取出来直接用 } else { // 期间可能其他线程修改了值, 那么就将原先的值替换 if (valuesMap.replace(subKey, supplier, factory)) { // successfully replaced // cleared CacheEntry / unsuccessful Factory // with our Factory // 成功将factory替换成新的值 supplier = factory; } else { // retry with current supplier // 替换失败, 继续使用原先的值 supplier = valuesMap.get(subKey); } } } } get方法中Object subKey = Objects.requireNonNull(subKeyFactory.apply(key, parameter)); subKeyFactory调用apply，具体实现在ProxyClassFactory中完成。 ProxyClassFactory.apply()实现代理类创建。 private static final class ProxyClassFactory implements BiFunction[], Class>{ // prefix for all proxy class names // 统一代理类的前缀名都以$Proxy private static final String proxyClassNamePrefix = \"$Proxy\"; // next number to use for generation of unique proxy class names //使用唯一的编号给作为代理类名的一部分，如$Proxy0,$Proxy1等 private static final AtomicLong nextUniqueNumber = new AtomicLong(); @Override public Class apply(ClassLoader loader, Class[] interfaces) { Map, Boolean> interfaceSet = new IdentityHashMap<>(interfaces.length); for (Class intf : interfaces) { /* * Verify that the class loader resolves the name of this interface to the same Class object. * 验证指定的类加载器(loader)加载接口所得到的Class对象(interfaceClass)是否与intf对象相同 */ Class interfaceClass = null; try { interfaceClass = Class.forName(intf.getName(), false, loader); } catch (ClassNotFoundException e) { } if (interfaceClass != intf) { throw new IllegalArgumentException( intf + \" is not visible from class loader\"); } /* * Verify that the Class object actually represents an interface. * 验证该Class对象是不是接口 */ if (!interfaceClass.isInterface()) { throw new IllegalArgumentException( interfaceClass.getName() + \" is not an interface\"); } /* * Verify that this interface is not a duplicate. * 验证该接口是否重复 */ if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) { throw new IllegalArgumentException( \"repeated interface: \" + interfaceClass.getName()); } } // package to define proxy class in //声明代理类所在包 String proxyPkg = null; int accessFlags = Modifier.PUBLIC | Modifier.FINAL; /* * Record the package of a non-public proxy interface so that the * proxy class will be defined in the same package. Verify that * all non-public proxy interfaces are in the same package. * 验证所有非公共的接口在同一个包内；公共的就无需处理 */ for (Class intf : interfaces) { int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) { accessFlags = Modifier.FINAL; String name = intf.getName(); int n = name.lastIndexOf('.'); // 截取完整包名 String pkg = ((n == -1) ? \"\" : name.substring(0, n + 1)); if (proxyPkg == null) { proxyPkg = pkg; } else if (!pkg.equals(proxyPkg)) { throw new IllegalArgumentException( \"non-public interfaces from different packages\"); } } } if (proxyPkg == null) { // if no non-public proxy interfaces, use com.sun.proxy package //如果都是public接口，那么生成的代理类就在com.sun.proxy包下 //如果报java.io.FileNotFoundException: com\\sun\\proxy\\$Proxy0.class //(系统找不到指定的路径。)的错误，就先在你项目中创建com.sun.proxy路径 proxyPkg = ReflectUtil.PROXY_PACKAGE + \".\"; } /* * Choose a name for the proxy class to generate. * nextUniqueNumber 是一个原子类，确保多线程安全，防止类名重复，类似于：$Proxy0，$Proxy */ long num = nextUniqueNumber.getAndIncrement(); // 代理类的完全限定名，如com.sun.proxy.$Proxy0.calss String proxyName = proxyPkg + proxyClassNamePrefix + num; /* * Generate the specified proxy class. * 生成类字节码的方法（重点） */ byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); try { return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); } catch (ClassFormatError e) { /* * A ClassFormatError here means that (barring bugs in the * proxy class generation code) there was some other * invalid aspect of the arguments supplied to the proxy * class creation (such as virtual machine limitations * exceeded). */ throw new IllegalArgumentException(e.toString()); } } } 代理类创建真正在ProxyGenerator.generateProxyClass（）方法中，方法签名如下: public static byte[] generateProxyClass(final String name, Class[] interfaces, int accessFlags) { ProxyGenerator gen = new ProxyGenerator(name, interfaces, accessFlags); // 真正生成字节码的方法 final byte[] classFile = gen.generateClassFile(); // 如果saveGeneratedFiles为true 则生成字节码文件，所以在开始我们要设置这个参数 // 当然，也可以通过返回的bytes自己输出 if (saveGeneratedFiles) { java.security.AccessController.doPrivileged( new java.security.PrivilegedAction() { public Void run() { try { int i = name.lastIndexOf('.'); Path path; if (i > 0) { Path dir = Paths.get(name.substring(0, i).replace('.', File.separatorChar)); Files.createDirectories(dir); path = dir.resolve(name.substring(i+1, name.length()) + \".class\"); } else { path = Paths.get(name + \".class\"); } Files.write(path, classFile); return null; } catch (IOException e) { throw new InternalError( \"I/O exception saving generated file: \" + e); } } }); } return classFile; } 代理类生成的最终方法是ProxyGenerator.generateClassFile() private byte[] generateClassFile() { /* ============================================================ * Step 1: Assemble ProxyMethod objects for all methods to generate proxy dispatching code for. * 步骤1：为所有方法生成代理调度代码，将代理方法对象集合起来。 */ //增加 hashcode、equals、toString方法 addProxyMethod(hashCodeMethod, Object.class); addProxyMethod(equalsMethod, Object.class); addProxyMethod(toStringMethod, Object.class); // 获得所有接口中的所有方法，并将方法添加到代理方法中 for (Class intf : interfaces) { for (Method m : intf.getMethods()) { addProxyMethod(m, intf); } } /* * 验证方法签名相同的一组方法，返回值类型是否相同；意思就是重写方法要方法签名和返回值一样 */ for (List sigmethods : proxyMethods.values()) { checkReturnTypes(sigmethods); } /* ============================================================ * Step 2: Assemble FieldInfo and MethodInfo structs for all of fields and methods in the class we are generating. * 为类中的方法生成字段信息和方法信息 */ try { // 生成代理类的构造函数 methods.add(generateConstructor()); for (List sigmethods : proxyMethods.values()) { for (ProxyMethod pm : sigmethods) { // add static field for method's Method object fields.add(new FieldInfo(pm.methodFieldName, \"Ljava/lang/reflect/Method;\", ACC_PRIVATE | ACC_STATIC)); // generate code for proxy method and add it // 生成代理类的代理方法 methods.add(pm.generateMethod()); } } // 为代理类生成静态代码块，对一些字段进行初始化 methods.add(generateStaticInitializer()); } catch (IOException e) { throw new InternalError(\"unexpected I/O Exception\", e); } if (methods.size() > 65535) { throw new IllegalArgumentException(\"method limit exceeded\"); } if (fields.size() > 65535) { throw new IllegalArgumentException(\"field limit exceeded\"); } /* ============================================================ * Step 3: Write the final class file. * 步骤3：编写最终类文件 */ /* * Make sure that constant pool indexes are reserved for the following items before starting to write the final class file. * 在开始编写最终类文件之前，确保为下面的项目保留常量池索引。 */ cp.getClass(dotToSlash(className)); cp.getClass(superclassName); for (Class intf: interfaces) { cp.getClass(dotToSlash(intf.getName())); } /* * Disallow new constant pool additions beyond this point, since we are about to write the final constant pool table. * 设置只读，在这之前不允许在常量池中增加信息，因为要写常量池表 */ cp.setReadOnly(); ByteArrayOutputStream bout = new ByteArrayOutputStream(); DataOutputStream dout = new DataOutputStream(bout); try { // u4 magic; dout.writeInt(0xCAFEBABE); // u2 次要版本; dout.writeShort(CLASSFILE_MINOR_VERSION); // u2 主版本 dout.writeShort(CLASSFILE_MAJOR_VERSION); cp.write(dout); // (write constant pool) // u2 访问标识; dout.writeShort(accessFlags); // u2 本类名; dout.writeShort(cp.getClass(dotToSlash(className))); // u2 父类名; dout.writeShort(cp.getClass(superclassName)); // u2 接口; dout.writeShort(interfaces.length); // u2 interfaces[interfaces_count]; for (Class intf : interfaces) { dout.writeShort(cp.getClass( dotToSlash(intf.getName()))); } // u2 字段; dout.writeShort(fields.size()); // field_info fields[fields_count]; for (FieldInfo f : fields) { f.write(dout); } // u2 方法; dout.writeShort(methods.size()); // method_info methods[methods_count]; for (MethodInfo m : methods) { m.write(dout); } // u2 类文件属性：对于代理类来说没有类文件属性; dout.writeShort(0); // (no ClassFile attributes for proxy classes) } catch (IOException e) { throw new InternalError(\"unexpected I/O Exception\", e); } return bout.toByteArray(); } 通过addProxyMethod（）添加hashcode、equals、toString方法。 private void addProxyMethod(Method var1, Class var2) { String var3 = var1.getName(); //方法名 Class[] var4 = var1.getParameterTypes(); //方法参数类型数组 Class var5 = var1.getReturnType(); //返回值类型 Class[] var6 = var1.getExceptionTypes(); //异常类型 String var7 = var3 + getParameterDescriptors(var4); //方法签名 Object var8 = (List)this.proxyMethods.get(var7); //根据方法签名却获得proxyMethods的Value if(var8 != null) { //处理多个代理接口中重复的方法的情况 Iterator var9 = ((List)var8).iterator(); while(var9.hasNext()) { ProxyGenerator.ProxyMethod var10 = (ProxyGenerator.ProxyMethod)var9.next(); if(var5 == var10.returnType) { /*归约异常类型以至于让重写的方法抛出合适的异常类型，我认为这里可能是多个接口中有相同的方法，而这些相同的方法抛出的异常类 型又不同，所以对这些相同方法抛出的异常进行了归约*/ ArrayList var11 = new ArrayList(); collectCompatibleTypes(var6, var10.exceptionTypes, var11); collectCompatibleTypes(var10.exceptionTypes, var6, var11); var10.exceptionTypes = new Class[var11.size()]; //将ArrayList转换为Class对象数组 var10.exceptionTypes = (Class[])var11.toArray(var10.exceptionTypes); return; } } } else { var8 = new ArrayList(3); this.proxyMethods.put(var7, var8); } ((List)var8).add(new ProxyGenerator.ProxyMethod(var3, var4, var5, var6, var2, null)); /*如果var8为空，就创建一个数组，并以方法签名为key,proxymethod对象数组为value添加到proxyMethods*/ } 生成的代理对象$Proxy0.class字节码反编译： package com.sun.proxy; import com.jpeony.spring.proxy.jdk.IHello; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.lang.reflect.UndeclaredThrowableException; public final class $Proxy0 extends Proxy implements IHello // 继承了Proxy类和实现IHello接口 { // 变量，都是private static Method XXX private static Method m1; private static Method m3; private static Method m2; private static Method m0; // 代理类的构造函数，其参数正是是InvocationHandler实例，Proxy.newInstance方法就是通过通过这个构造函数来创建代理实例的 public $Proxy0(InvocationHandler paramInvocationHandler) throws { super(paramInvocationHandler); } // 以下Object中的三个方法 public final boolean equals(Object paramObject) throws { try { return ((Boolean)this.h.invoke(this, m1, new Object[] { paramObject })).booleanValue(); } catch (RuntimeException localRuntimeException) { throw localRuntimeException; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } // 接口代理方法 public final void sayHello() throws { try { this.h.invoke(this, m3, null); return; } catch (RuntimeException localRuntimeException) { throw localRuntimeException; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } public final String toString() throws { try { return ((String)this.h.invoke(this, m2, null)); } catch (RuntimeException localRuntimeException) { throw localRuntimeException; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } public final int hashCode() throws { try { return ((Integer)this.h.invoke(this, m0, null)).intValue(); } catch (RuntimeException localRuntimeException) { throw localRuntimeException; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } } // 静态代码块对变量进行一些初始化工作 static { try { // 这里每个方法对象 和类的实际方法绑定 m1 = Class.forName(\"java.lang.Object\").getMethod(\"equals\", new Class[] { Class.forName(\"java.lang.Object\") }); m3 = Class.forName(\"com.jpeony.spring.proxy.jdk.IHello\").getMethod(\"sayHello\", new Class[0]); m2 = Class.forName(\"java.lang.Object\").getMethod(\"toString\", new Class[0]); m0 = Class.forName(\"java.lang.Object\").getMethod(\"hashCode\", new Class[0]); return; } catch (NoSuchMethodException localNoSuchMethodException) { throw new NoSuchMethodError(localNoSuchMethodException.getMessage()); } catch (ClassNotFoundException localClassNotFoundException) { throw new NoClassDefFoundError(localClassNotFoundException.getMessage()); } } } 当代理对象生成后，最后由InvocationHandler的invoke()方法调用目标方法: 在动态代理中InvocationHandler是核心，每个代理实例都具有一个关联的调用处理程序(InvocationHandler)。 对代理实例调用方法时，将对方法调用进行编码并将其指派到它的调用处理程序(InvocationHandler)的invoke()方法。 所以对代理方法的调用都是通InvocationHadler的invoke来实现中，而invoke方法根据传入的代理对象， 方法和参数来决定调用代理的哪个方法。 方法签名如下: invoke(Object Proxy，Method method，Object[] args) 从反编译源码分析调用invoke()过程: 从反编译后的源码看$Proxy0类继承了Proxy类，同时实现了IHello接口，即代理类接口， 所以才能强制将代理对象转换为IHello接口，然后调用$Proxy0中的sayHello()方法。 $Proxy0中sayHello()源码: public final void sayHello() throws { try { this.h.invoke(this, m3, null); return; } catch (RuntimeException localRuntimeException) { throw localRuntimeException; } catch (Throwable localThrowable) { throw new UndeclaredThrowableException(localThrowable); } this.h.invoke(this, m3, null); this就是$Proxy0对象； m3就是m3 = Class.forName(\"com.jpeony.spring.proxy.jdk.IHello\").getMethod(\"sayHello\", new Class[0]); 即是通过全路径名，反射获取的目标对象中的真实方法加参数。 h就是Proxy类中的变量protected InvocationHandler h; 所以成功的调到了InvocationHandler中的invoke()方法，但是invoke()方法在我们自定义的MyInvocationHandler 中实现，MyInvocationHandler中的invoke()方法: @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"------插入前置通知代码-------------\"); // 执行相应的目标方法 Object rs = method.invoke(target,args); System.out.println(\"------插入后置处理代码-------------\"); return rs; } 所以，绕了半天，终于调用到了MyInvocationHandler中的invoke()方法，从上面的this.h.invoke(this, m3, null); 可以看出，MyInvocationHandler中invoke第一个参数为$Proxy0（代理对象），第二个参数为目标类的真实方法， 第三个参数为目标方法参数，因为sayHello()没有参数，所以是null。 到这里，我们真正的实现了通过代理调用目标对象的完全分析，至于InvocationHandler中的invoke()方法就是 最后执行了目标方法。到此完成了代理对象生成，目标方法调用。 所以，我们可以看到在打印目标方法调用输出结果前后所插入的前置和后置代码处理。 生成代理类的时序图如下 JDK代理的一个问题 Method invoke以下方式都可以调用 objec实例跟method方法的实例是同一个类 method是父类方法，object是子类实例 但子类方法调用父类实例是会报 java.lang.IllegalArgumentException: object is not an instance of declaring class CGLIB动态代理 CGLib动态代理是代理类去继承目标类，然后重写其中目标类的方法啊，这样也可以保证代理类拥有目标类的同名方法； 看一下CGLib的基本结构，下图所示，代理类去继承目标类，每次调用代理类的方法都会被方法拦截器拦截，在拦截器中才是调用目标类的该方法的逻辑，结构还是一目了然的； CGLib的基本使用 目标类（一个公开方法，另外一个用final修饰）： public class Dog { final public void run(String name) { System.out.println(\"狗\"+name+\"----run\"); } public void eat() { System.out.println(\"狗----eat\"); } } public class MyMethodInterceptor implements MethodInterceptor { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { System.out.println(\"这里是对目标类进行增强！！！\"); //注意这里的方法调用，不是用反射哦！！！ Object object = methodProxy.invokeSuper(obj, args); return object; } } obj: 表示增强的对象，即实现这个接口类的一个对象； method: 表示要被拦截的方法； args: 表示要被拦截方法的参数； methodProxy: 表示要触发父类的方法对象； public class CgLibProxy { public static void main(String[] args) { //在指定目录下生成动态代理类，我们可以反编译看一下里面到底是一些什么东西 System.setProperty(DebuggingClassWriter.DEBUG_LOCATION_PROPERTY, \"D:\\\\github\\\\program\\\\src\\\\main\\\\java\\\\dp\\\\cglib\"); //创建Enhancer对象，类似于JDK动态代理的Proxy类，下一步就是设置几个参数 Enhancer enhancer = new Enhancer(); //设置目标类的字节码文件 enhancer.setSuperclass(Dog.class); //设置回调函数 enhancer.setCallback(new MyMethodInterceptor()); //这里的creat方法就是正式创建代理类 Dog proxyDog = (Dog)enhancer.create(); //调用代理类的eat方法 proxyDog.eat(); } } CGLIB debugging enabled, writing to 'D:\\github\\program\\src\\main\\java\\dp\\cglib' 这里是对目标类进行增强！！！ 狗----eat 源码分析 在上面的Client代码中，通过Enhancer.create()方法创建代理对象，create()方法的源码： public Object create() { classOnly = false; argumentTypes = null; return createHelper(); } 该方法含义就是如果有必要就创建一个新类，并且用指定的回调对象创建一个新的对象实例， 使用的父类的参数的构造方法来实例化父类的部分。核心内容在createHelper()中，源码如下: private Object createHelper() { preValidate(); Object key = KEY_FACTORY.newInstance((superclass != null) ? superclass.getName() : null, ReflectUtils.getNames(interfaces), filter == ALL_ZERO ? null : new WeakCacheKey(filter), callbackTypes, useFactory, interceptDuringConstruction, serialVersionUID); this.currentKey = key; Object result = super.create(key); return result; } preValidate()方法校验callbackTypes、filter是否为空，以及为空时的处理。 通过newInstance()方法创建EnhancerKey对象，作为Enhancer父类AbstractClassGenerator.create()方法 注意这里KeyFactory的使用 创建代理对象的参数。 protected Object create(Object key) { try { ClassLoader loader = getClassLoader(); Map cache = CACHE; ClassLoaderData data = cache.get(loader); if (data == null) { synchronized (AbstractClassGenerator.class) { cache = CACHE; data = cache.get(loader); if (data == null) { Map newCache = new WeakHashMap(cache); data = new ClassLoaderData(loader); newCache.put(loader, data); CACHE = newCache; } } } this.key = key; Object obj = data.get(this, getUseCache()); if (obj instanceof Class) { return firstInstance((Class) obj); } return nextInstance(obj); } catch (RuntimeException e) { throw e; } catch (Error e) { throw e; } catch (Exception e) { throw new CodeGenerationException(e); } } 真正创建代理对象方法在nextInstance()方法中，该方法为抽象类AbstractClassGenerator的一个方法，签名如下： abstract protected Object nextInstance(Object instance) throws Exception; 在子类Enhancer中实现，实现源码如下： protected Object nextInstance(Object instance) { EnhancerFactoryData data = (EnhancerFactoryData) instance; if (classOnly) { return data.generatedClass; } Class[] argumentTypes = this.argumentTypes; Object[] arguments = this.arguments; if (argumentTypes == null) { argumentTypes = Constants.EMPTY_CLASS_ARRAY; arguments = null; } return data.newInstance(argumentTypes, arguments, callbacks); } 看看data.newInstance(argumentTypes, arguments, callbacks)方法， 第一个参数为代理对象的构成器类型，第二个为代理对象构造方法参数，第三个为对应回调对象。 最后根据这些参数，通过反射生成代理对象，源码如下： /** * Creates proxy instance for given argument types, and assigns the callbacks. * Ideally, for each proxy class, just one set of argument types should be used, * otherwise it would have to spend time on constructor lookup. * Technically, it is a re-implementation of {@link Enhancer#createUsingReflection(Class)}, * with \"cache {@link #setThreadCallbacks} and {@link #primaryConstructor}\" * * @see #createUsingReflection(Class) * @param argumentTypes constructor argument types * @param arguments constructor arguments * @param callbacks callbacks to set for the new instance * @return newly created proxy */ public Object newInstance(Class[] argumentTypes, Object[] arguments, Callback[] callbacks) { setThreadCallbacks(callbacks); try { // Explicit reference equality is added here just in case Arrays.equals does not have one if (primaryConstructorArgTypes == argumentTypes || Arrays.equals(primaryConstructorArgTypes, argumentTypes)) { // If we have relevant Constructor instance at hand, just call it // This skips \"get constructors\" machinery return ReflectUtils.newInstance(primaryConstructor, arguments); } // Take a slow path if observing unexpected argument types return ReflectUtils.newInstance(generatedClass, argumentTypes, arguments); } finally { // clear thread callbacks to allow them to be gc'd setThreadCallbacks(null); } } 最后生成代理对象： 根据上面的代码我们可以知道代理类中主要有几部分组成： 重写的父类方法， CGLIB$eat$0这种奇怪的方法， Interceptor（）方法， newInstance和get/setCallback方法 create方法生成Target的代理类，并返回代理类的实例 生成动态代理类 首先到我们指定的目录下面看一下生成的字节码文件，有三个，一个是代理类的FastClass， 一个是代理类，一个是目标类的FastClass，我们看看代理类（Dog$$EnhancerByCGLIB$$d1c4918c.class）， 名字略长~后面会仔细介绍什么是FastClass，这里简单说一下，就是给每个方法编号，通过编号找到方法，这样可以避免频繁使用反射导致效率比较低，也可以叫做FastClass机制 package dp.cglib; import java.lang.reflect.*; import net.sf.cglib.proxy.*; import net.sf.cglib.core.*; public class Dog$$EnhancerByCGLIB$$d1c4918c extends Dog implements Factory { private boolean CGLIB$BOUND; private static final ThreadLocal CGLIB$THREAD_CALLBACKS; private static final Callback[] CGLIB$STATIC_CALLBACKS; private MethodInterceptor CGLIB$CALLBACK_0; private static final Method CGLIB$eat$0$Method; private static final MethodProxy CGLIB$eat$0$Proxy; private static final Object[] CGLIB$emptyArgs; private static final Method CGLIB$finalize$1$Method; private static final MethodProxy CGLIB$finalize$1$Proxy; private static final Method CGLIB$equals$2$Method; private static final MethodProxy CGLIB$equals$2$Proxy; private static final Method CGLIB$toString$3$Method; private static final MethodProxy CGLIB$toString$3$Proxy; private static final Method CGLIB$hashCode$4$Method; private static final MethodProxy CGLIB$hashCode$4$Proxy; private static final Method CGLIB$clone$5$Method; private static final MethodProxy CGLIB$clone$5$Proxy; static void CGLIB$STATICHOOK1() { CGLIB$THREAD_CALLBACKS = new ThreadLocal(); CGLIB$emptyArgs = new Object[0]; final Class forName = Class.forName(\"dp.cglib.Dog$$EnhancerByCGLIB$$d1c4918c\"); final Class forName2; final Method[] methods = ReflectUtils.findMethods(new String[] { \"finalize\", \"()V\", \"equals\", \"(Ljava/lang/Object;)Z\", \"toString\", \"()Ljava/lang/String;\", \"hashCode\", \"()I\", \"clone\", \"()Ljava/lang/Object;\" }, (forName2 = Class.forName(\"java.lang.Object\")).getDeclaredMethods()); CGLIB$finalize$1$Method = methods[0]; CGLIB$finalize$1$Proxy = MethodProxy.create((Class)forName2, (Class)forName, \"()V\", \"finalize\", \"CGLIB$finalize$1\"); CGLIB$equals$2$Method = methods[1]; CGLIB$equals$2$Proxy = MethodProxy.create((Class)forName2, (Class)forName, \"(Ljava/lang/Object;)Z\", \"equals\", \"CGLIB$equals$2\"); CGLIB$toString$3$Method = methods[2]; CGLIB$toString$3$Proxy = MethodProxy.create((Class)forName2, (Class)forName, \"()Ljava/lang/String;\", \"toString\", \"CGLIB$toString$3\"); CGLIB$hashCode$4$Method = methods[3]; CGLIB$hashCode$4$Proxy = MethodProxy.create((Class)forName2, (Class)forName, \"()I\", \"hashCode\", \"CGLIB$hashCode$4\"); CGLIB$clone$5$Method = methods[4]; CGLIB$clone$5$Proxy = MethodProxy.create((Class)forName2, (Class)forName, \"()Ljava/lang/Object;\", \"clone\", \"CGLIB$clone$5\"); final Class forName3; CGLIB$eat$0$Method = ReflectUtils.findMethods(new String[] { \"eat\", \"()V\" }, (forName3 = Class.forName(\"dp.cglib.Dog\")).getDeclaredMethods())[0]; CGLIB$eat$0$Proxy = MethodProxy.create((Class)forName3, (Class)forName, \"()V\", \"eat\", \"CGLIB$eat$0\"); } final void CGLIB$eat$0() { super.eat(); } public final void eat() { MethodInterceptor cglib$CALLBACK_2; MethodInterceptor cglib$CALLBACK_0; if ((cglib$CALLBACK_0 = (cglib$CALLBACK_2 = this.CGLIB$CALLBACK_0)) == null) { CGLIB$BIND_CALLBACKS(this); cglib$CALLBACK_2 = (cglib$CALLBACK_0 = this.CGLIB$CALLBACK_0); } if (cglib$CALLBACK_0 != null) { cglib$CALLBACK_2.intercept((Object)this, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$eat$0$Method, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$emptyArgs, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$eat$0$Proxy); return; } super.eat(); } final void CGLIB$finalize$1() throws Throwable { super.finalize(); } protected final void finalize() throws Throwable { MethodInterceptor cglib$CALLBACK_2; MethodInterceptor cglib$CALLBACK_0; if ((cglib$CALLBACK_0 = (cglib$CALLBACK_2 = this.CGLIB$CALLBACK_0)) == null) { CGLIB$BIND_CALLBACKS(this); cglib$CALLBACK_2 = (cglib$CALLBACK_0 = this.CGLIB$CALLBACK_0); } if (cglib$CALLBACK_0 != null) { cglib$CALLBACK_2.intercept((Object)this, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$finalize$1$Method, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$emptyArgs, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$finalize$1$Proxy); return; } super.finalize(); } final boolean CGLIB$equals$2(final Object o) { return super.equals(o); } public final boolean equals(final Object o) { MethodInterceptor cglib$CALLBACK_2; MethodInterceptor cglib$CALLBACK_0; if ((cglib$CALLBACK_0 = (cglib$CALLBACK_2 = this.CGLIB$CALLBACK_0)) == null) { CGLIB$BIND_CALLBACKS(this); cglib$CALLBACK_2 = (cglib$CALLBACK_0 = this.CGLIB$CALLBACK_0); } if (cglib$CALLBACK_0 != null) { final Object intercept = cglib$CALLBACK_2.intercept((Object)this, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$equals$2$Method, new Object[] { o }, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$equals$2$Proxy); return intercept != null && (boolean)intercept; } return super.equals(o); } final String CGLIB$toString$3() { return super.toString(); } public final String toString() { MethodInterceptor cglib$CALLBACK_2; MethodInterceptor cglib$CALLBACK_0; if ((cglib$CALLBACK_0 = (cglib$CALLBACK_2 = this.CGLIB$CALLBACK_0)) == null) { CGLIB$BIND_CALLBACKS(this); cglib$CALLBACK_2 = (cglib$CALLBACK_0 = this.CGLIB$CALLBACK_0); } if (cglib$CALLBACK_0 != null) { return (String)cglib$CALLBACK_2.intercept((Object)this, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$toString$3$Method, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$emptyArgs, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$toString$3$Proxy); } return super.toString(); } final int CGLIB$hashCode$4() { return super.hashCode(); } public final int hashCode() { MethodInterceptor cglib$CALLBACK_2; MethodInterceptor cglib$CALLBACK_0; if ((cglib$CALLBACK_0 = (cglib$CALLBACK_2 = this.CGLIB$CALLBACK_0)) == null) { CGLIB$BIND_CALLBACKS(this); cglib$CALLBACK_2 = (cglib$CALLBACK_0 = this.CGLIB$CALLBACK_0); } if (cglib$CALLBACK_0 != null) { final Object intercept = cglib$CALLBACK_2.intercept((Object)this, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$hashCode$4$Method, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$emptyArgs, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$hashCode$4$Proxy); return (intercept == null) ? 0 : ((Number)intercept).intValue(); } return super.hashCode(); } final Object CGLIB$clone$5() throws CloneNotSupportedException { return super.clone(); } protected final Object clone() throws CloneNotSupportedException { MethodInterceptor cglib$CALLBACK_2; MethodInterceptor cglib$CALLBACK_0; if ((cglib$CALLBACK_0 = (cglib$CALLBACK_2 = this.CGLIB$CALLBACK_0)) == null) { CGLIB$BIND_CALLBACKS(this); cglib$CALLBACK_2 = (cglib$CALLBACK_0 = this.CGLIB$CALLBACK_0); } if (cglib$CALLBACK_0 != null) { return cglib$CALLBACK_2.intercept((Object)this, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$clone$5$Method, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$emptyArgs, Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$clone$5$Proxy); } return super.clone(); } public static MethodProxy CGLIB$findMethodProxy(final Signature signature) { final String string = signature.toString(); switch (string.hashCode()) { case -1574182249: { if (string.equals(\"finalize()V\")) { return Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$finalize$1$Proxy; } break; } case -1310345955: { if (string.equals(\"eat()V\")) { return Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$eat$0$Proxy; } break; } case -508378822: { if (string.equals(\"clone()Ljava/lang/Object;\")) { return Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$clone$5$Proxy; } break; } case 1826985398: { if (string.equals(\"equals(Ljava/lang/Object;)Z\")) { return Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$equals$2$Proxy; } break; } case 1913648695: { if (string.equals(\"toString()Ljava/lang/String;\")) { return Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$toString$3$Proxy; } break; } case 1984935277: { if (string.equals(\"hashCode()I\")) { return Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$hashCode$4$Proxy; } break; } } return null; } public Dog$$EnhancerByCGLIB$$d1c4918c() { CGLIB$BIND_CALLBACKS(this); } public static void CGLIB$SET_THREAD_CALLBACKS(final Callback[] array) { Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$THREAD_CALLBACKS.set(array); } public static void CGLIB$SET_STATIC_CALLBACKS(final Callback[] cglib$STATIC_CALLBACKS) { CGLIB$STATIC_CALLBACKS = cglib$STATIC_CALLBACKS; } private static final void CGLIB$BIND_CALLBACKS(final Object o) { final Dog$$EnhancerByCGLIB$$d1c4918c dog$$EnhancerByCGLIB$$d1c4918c = (Dog$$EnhancerByCGLIB$$d1c4918c)o; if (!dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$BOUND) { dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$BOUND = true; Object o2; if ((o2 = Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$THREAD_CALLBACKS.get()) != null || (o2 = Dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$STATIC_CALLBACKS) != null) { dog$$EnhancerByCGLIB$$d1c4918c.CGLIB$CALLBACK_0 = (MethodInterceptor)((Callback[])o2)[0]; } } } public Object newInstance(final Callback[] array) { CGLIB$SET_THREAD_CALLBACKS(array); final Dog$$EnhancerByCGLIB$$d1c4918c dog$$EnhancerByCGLIB$$d1c4918c = new Dog$$EnhancerByCGLIB$$d1c4918c(); CGLIB$SET_THREAD_CALLBACKS(null); return dog$$EnhancerByCGLIB$$d1c4918c; } public Object newInstance(final Callback callback) { CGLIB$SET_THREAD_CALLBACKS(new Callback[] { callback }); final Dog$$EnhancerByCGLIB$$d1c4918c dog$$EnhancerByCGLIB$$d1c4918c = new Dog$$EnhancerByCGLIB$$d1c4918c(); CGLIB$SET_THREAD_CALLBACKS(null); return dog$$EnhancerByCGLIB$$d1c4918c; } public Object newInstance(final Class[] array, final Object[] array2, final Callback[] array3) { CGLIB$SET_THREAD_CALLBACKS(array3); switch (array.length) { case 0: { final Dog$$EnhancerByCGLIB$$d1c4918c dog$$EnhancerByCGLIB$$d1c4918c = new Dog$$EnhancerByCGLIB$$d1c4918c(); CGLIB$SET_THREAD_CALLBACKS(null); return dog$$EnhancerByCGLIB$$d1c4918c; } default: { throw new IllegalArgumentException(\"Constructor not found\"); } } } public Callback getCallback(final int n) { CGLIB$BIND_CALLBACKS(this); Object cglib$CALLBACK_0 = null; switch (n) { case 0: { cglib$CALLBACK_0 = this.CGLIB$CALLBACK_0; break; } default: { cglib$CALLBACK_0 = null; break; } } return (Callback)cglib$CALLBACK_0; } public void setCallback(final int n, final Callback callback) { switch (n) { case 0: { this.CGLIB$CALLBACK_0 = (MethodInterceptor)callback; break; } } } public Callback[] getCallbacks() { CGLIB$BIND_CALLBACKS(this); return new Callback[] { (Callback)this.CGLIB$CALLBACK_0 }; } public void setCallbacks(final Callback[] array) { this.CGLIB$CALLBACK_0 = (MethodInterceptor)array[0]; } static { CGLIB$STATICHOOK1(); } } 我们可以看到对于eat方法，在这个代理类中对应会有eat 和CGLIB$eat$0这两个方法；其中前者则是我们使用代理类时候调用的方法，后者是在方法拦截器里面调用的， 换句话来说当我们代码调用代理对象的eat方法，然后会到方法拦截器中调用intercept方法，该方法内则通过proxy.invokeSuper调用CGLIB$eat$0这个方法， 当调用代理类的eat方法时，先判断是否已经存在实现了MethodInterceptor接口的拦截对象，如果没有的话就调用CGLIB$BIND_CALLBACKS方法来获取拦截对象 CGLIB$BIND_CALLBACKS 先从CGLIB$THREAD_CALLBACKS中get拦截对象，如果获取不到的话，再从CGLIB$STATIC_CALLBACKS来获取，如果也没有则认为该方法不需要代理。 那么拦截对象是如何设置到CGLIB$THREAD_CALLBACKS 或者 CGLIB$STATIC_CALLBACKS中的呢？ 在Jdk动态代理中拦截对象是在实例化代理类时由构造函数传入的，在cglib中是调用Enhancer的firstInstance方法来生成代理类实例并设置拦截对象的。 这里会有一个疑问，为什么不直接反射调用代理类生成的（CGLIB$g$0）来间接调用目标类的被拦截方法，而使用proxy的invokeSuper方法呢？ 这里就涉及到了另外一个点— FastClass 。 FastClass机制分析 Jdk动态代理的拦截对象是通过反射的机制来调用被拦截方法的，反射的效率比较低，所以cglib采用了FastClass的机制来实现对被拦截方法的调用。 FastClass机制就是对一个类的方法建立索引，通过索引来直接调用相应的方法，下面用一个小例子来说明一下，这样比较直观： public class test10 { public static void main(String[] args){ Test tt = new Test(); Test2 fc = new Test2(); int index = fc.getIndex(\"f()V\"); fc.invoke(index, tt, null); } } class Test{ public void f(){ System.out.println(\"f method\"); } public void g(){ System.out.println(\"g method\"); } } class Test2{ public Object invoke(int index, Object o, Object[] ol){ Test t = (Test) o; switch(index){ case 1: t.f(); return null; case 2: t.g(); return null; } return null; } public int getIndex(String signature){ switch(signature.hashCode()){ case 3078479: return 1; case 3108270: return 2; } return -1; } } 上例中，Test2是Test的Fastclass，在Test2中有两个方法getIndex和invoke。在getIndex方法中对Test的每个方法建立索引， 并根据入参（方法名+方法的描述符）来返回相应的索引。Invoke根据指定的索引，以ol为入参调用对象O的方法。 这样就避免了反射调用，提高了效率。代理类（Dog$$EnhancerByCGLIB$$de793b33）中与生成Fastclass相关的代码如下： final Class forName = Class.forName(\"dp.cglib.Dog$$EnhancerByCGLIB$$de793b33\"); CGLIB$eat$0$Method = ReflectUtils.findMethods(new String[] { \"eat\", \"()V\" }, (forName3 = Class.forName(\"dp.cglib.Dog\")).getDeclaredMethods())[0]; CGLIB$eat$0$Proxy = MethodProxy.create((Class)forName3, (Class)forName, \"()V\", \"eat\", \"CGLIB$eat$0\"); MethodProxy中会对localClass1和localClass2进行分析并生成FastClass， 然后再使用getIndex来获取方法g 和 CGLIB$g$0的索引，具体的生成过程将在后续进行介绍，这里介绍一个关键的内部类： private static class FastClassInfo { FastClass f1; // 目标类的fastclass FastClass f2; // 代理类 的fastclass int i1; //目标类的eat方法的索引 int i2; //代理类的CGLIB$eat$0方法的索引 } MethodProxy 中invokeSuper方法的代码如下： public Object invokeSuper(Object obj, Object[] args) throws Throwable { try { init(); //真正为目标类建立索引 FastClassInfo fci = fastClassInfo; return fci.f2.invoke(fci.i2, obj, args); //调用代理类FastClass中索引为i2的方法 } catch (InvocationTargetException e) { throw e.getTargetException(); } } 看init方法 private void init() { /* * Using a volatile invariant allows us to initialize the FastClass and * method index pairs atomically. * * Double-checked locking is safe with volatile in Java 5. Before 1.5 this * code could allow fastClassInfo to be instantiated more than once, which * appears to be benign. */ if (fastClassInfo == null) { synchronized (initLock) { if (fastClassInfo == null) { CreateInfo ci = createInfo; FastClassInfo fci = new FastClassInfo(); fci.f1 = helper(ci, ci.c1); fci.f2 = helper(ci, ci.c2); fci.i1 = fci.f1.getIndex(sig1); //看这里是不是和小例子类似 fci.i2 = fci.f2.getIndex(sig2); fastClassInfo = fci; createInfo = null; } } } } 当调用invokeSuper方法时，实际上是调用代理类的CGLIB$eat$0方法，CGLIB$eat$0直接调用了目标类的eat方法。所以，在第一节示例代码中我们使用invokeSuper方法来调用被拦截的目标类方法。 总结 CGLib动态代理是将继承用到了极致 这里随便画一个简单的图看看整个过程，当我们去调用方法一的时候，在代理类中会先判断是否实现了方法拦截的接口，没实现的话直接调用目标类的方法一；如果实现了那就会被方法拦截器拦截，在方法拦截器中会对目标类中所有的方法建立索引，其实大概就是将每个方法的引用保存在数组中，我们就可以根据数组的下标直接调用方法，而不是用反射；索引建立完成之后，方法拦截器内部就会调用invoke方法（这个方法在生成的FastClass中实现），在invoke方法内就是调用CGLIB$方法一$这种方法，也就是调用对应的目标类的方法一； 一般我们要添加自己的逻辑就是在方法拦截器那里。。。。 调用过程如下 生成代理类的时序图如下 JDK和CGLIB动态代理区别 JDK动态代理 利用拦截器(拦截器必须实现InvocationHanlder)加上反射机制生成一个实现代理接口的匿名类， 在调用具体方法前调用InvokeHandler来处理。 CGLIB动态代理 利用ASM开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。 何时使用JDK还是CGLIB？ 如果目标对象实现了接口，默认情况下会采用JDK的动态代理实现AOP。 如果目标对象实现了接口，可以强制使用CGLIB实现AOP。 如果目标对象没有实现了接口，必须采用CGLIB库，Spring会自动在JDK动态代理和CGLIB之间转换。 如何强制使用CGLIB实现AOP？ 添加CGLIB库(aspectjrt-xxx.jar、aspectjweaver-xxx.jar、cglib-nodep-xxx.jar) 在Spring配置文件中加入 JDK动态代理和CGLIB字节码生成的区别？ DK动态代理只能对实现了接口的类生成代理，而不能针对类。 CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法， 并覆盖其中方法实现增强，但是因为采用的是继承，所以该类或方法最好不要声明成final， 对于final类或方法，是无法继承的。 CGlib比JDK快？ 使用CGLib实现动态代理，CGLib底层采用ASM字节码生成框架，使用字节码技术生成代理类， 在jdk6之前比使用Java反射效率要高。唯一需要注意的是，CGLib不能对声明为final的方法进行代理，因为CGLib原理是动态生成被代理类的子类。 在jdk6、jdk7、jdk8逐步对JDK动态代理优化之后，在调用次数较少的情况下，JDK代理效率高于CGLIB代理效率， 只有当进行大量调用的时候，jdk6和jdk7比CGLIB代理效率低一点，但是到jdk8的时候，jdk代理效率高于CGLIB代理， 总之，每一次jdk版本升级，jdk代理效率都得到提升，而CGLIB代理消息确有点跟不上步伐。 Spring如何选择用JDK还是CGLIB？ 当Bean实现接口时，Spring就会用JDK的动态代理。 当Bean没有实现接口时，Spring使用CGlib是实现。 可以强制使用CGlib（在spring配置中加入）。 "},"Chapter11/AOP.html":{"url":"Chapter11/AOP.html","title":"AOP的实现","keywords":"","body":"Spring AOP的实现 AOP的概念 AOP(Aspect Oriented Programming)，即为面向切面编程。在软件开发中，散布于应用中多处的 功能被称为横切关注点(cross-cutting concern)，通常来说，这些横切关注点从概念上是与应用的业务逻辑分离的。 比如，声明式事务、日志、安全、缓存等等，都与业务逻辑无关，可以将这些东西抽象成为模块，采用面向切面编程的方式， 通过声明方式定义这些功能用于何处，通过预编译方式和运行期动态代理实现这些模块化横切关注点程序功能进行统一维护， 从而将横切关注点与它们所影响的对象之间分离出来，就是实现解耦。 横切关注点可以被模块化为特殊的类，这些类被称为切面(aspect)。这样做有两个优点: 每个关注点都集中于一个地方，而不是分散到多处代码中; 服务模块更简洁，因为它们只包含主要的关注点的代码（核心业务逻辑），而次要关注点的代码（日志，事务，安全等）都被转移到切面中。 AOP术语 通知(Advice) 切面类有自己要完成的工作，切面类的工作就称为通知。通知定义了切面是做什么以及何时使用。 \"做什么\"，即切面类中定义的方法是干什么的； \"何时使用\"，即5种通知类型，是在目标方法执行前，还是目标方法执行后等等； \"何处做\"，即通知定义了做什么，何时使用，但是不知道用在何处，而切点定义的就是告诉通知应该用在哪个类的哪个目标方法上，从而完美的完成横切点功能。 Spring切面定义了5种类型通知： 前置通知(Before)：在目标方法被调用之前调用通知功能。 后置通知(After)：在目标方法完成之后调用通知，不会关心方法的输出是什么。 返回通知(After-returning)： 在目标方法成功执行之后调用通知。 异常通知(After-throwing)：在目标方法抛出异常后调用通知。 环绕通知(Around)：通知包裹了被通知的方法，在被通知的方法调用之前和之后执行自定义的行为。 连接点(Join point) 在我们的应用程序中有可能有数以万计的时机可以应用通知，而这些时机就被称为连接点。 连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法时、抛出异常时、 甚至修改一个字段时。切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。 连接点是一个虚概念，可以把连接点看成是切点的集合。 下面我们看看切点是神马鬼? 切点(Poincut) 连接点谈的是一个飘渺的大范围，而切点是一个具体的位置，用于缩小切面所通知的连接点的范围。 前面说过，通知定义的是切面的\"要做什么\"和\"在何时做\"，是不是没有去哪里做，而切点就定义了\"去何处做\"。 切点的定义会匹配通知所要织入的一个或多个连接点。我们通常使用明确的类和方法名称，或者是使用 正则表达式定义所匹配的类和方法名称来指定切点。说白了，切点就是让通知找到\"发泄的地方\"。 切面(Aspect) 切面是通知和切点的结合，通知和切点共同定义了切面的全部内容。因为通知定义的是切面的 \"要做什么\"和\"在何时做\"，而切点定义的是切面的\"在何地做\"。将两者结合在一起，就可以完美的 展现切面在何时，何地，做什么(功能)。 引入(Introduction) 引入这个概念就比较高大尚，引入允许我们向现有的类添加新方法或属性。 主要目的是想在无需修改A的情况下，引入B的行为和状态。 织入(Weaving) 织入是把切面应用到目标对象并创建新的代理对象的过程。切面在指定的连接点被织入到目标对象中。 在目标对象的生命周期里有多个点可以进行织入: 编译期: 切面在目标类编译时被织入。需要特殊的编译器，是AspectJ的方式，不是spring的菜。 类加载期: 切面在目标类加载到JVM时被织入。这种方式需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ5支持这种方式。 运行期: 切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态的创建一个代理对象。而这正是Spring AOP的织入切面的方式。 各个不同的advice的拦截顺序的问题 只有一个Aspect类： 无异常：@Around（proceed()之前的部分） → @Before → 方法执行 → @Around（proceed()之后的部分） → @After → @AfterReturning 有异常：@Around（proceed(之前的部分)） → @Before → 扔异常ing → @After → @AfterThrowing （大概是因为方法没有跑完抛了异常，没有正确返回所有@Around的proceed()之后的部分和@AfterReturning两个注解的加强没有能够织入） 同一个方法有多个@Aspect类拦截： 单个Aspect肯定是和只有一个Aspect的时候的情况是一样的，但不同的Aspect里面的advice的顺序呢？？答案是不一定，像是线程一样，没有谁先谁后，除非你给他们分配优先级，同样地，在这里你也可以为@Aspect分配优先级，这样就可以决定谁先谁后了。 优先级有两种方式： 实现org.springframework.core.Ordered接口，实现它的getOrder()方法 给aspect添加@Order注解，该注解全称为：org.springframework.core.annotation.Order 不管是哪种，都是order的值越小越先执行： 注意点： 如果在同一个 aspect 类中，针对同一个 pointcut，定义了两个相同的 advice(比如，定义了两个 @Before)，那么这两个 advice 的执行顺序是无法确定的，哪怕你给这两个 advice 添加了 @Order 这个注解，也不行。这点切记。 对于@Around这个advice，不管它有没有返回值，但是必须要方法内部，调用一下 pjp.proceed();否则，Controller 中的接口将没有机会被执行，从而也导致了 @Before这个advice不会被触发。 设计原理 本篇是以ProxyFactoryBean的配置来讲的，这个是最能体现AOP思想的一个配置方式，也是我们学习AOP一个入口。 在Spring的AOP模块中，一个主要的部分就是AopProxy代理对象的生成，而对于Spring应用，可以看到，是通过配置 和调用Spring的ProxyFactoryBean来完成这个任务的。在ProxyFactoryBean中，封装了主要代理对象的生成过程。 在这个生成过程中，可以使用JDK的Proxy和CGLIB两种方式。 以ProxyFactory的设计为中心，看下整个继承关系图： 在这个类继承关系图中，可以看到完成AOP应用的类，类说明： ProxyConfig： 作为共同基类，可以看成是一个数据基类，这个数据基类为ProxyFactoryBean这样的子类提供了配置属性。 AdvisedSupport：封装了AOP对通知和通知器的相关操作，这些操作对于不同的AOP的代理对象的生成都是一样的，但对于具体的AOP代理对象的创建，AdvisedSupport把它交给它的子类们去完成。 ProxyCreatorSupport：是其子类创建AOP代理对象的一个辅助类。 AspectJProxyFactory：集成Spring和AspectJ的作用，适用于需要使用AspectJ的AOP应用。 ProxyFactoryBean：Spring的AOP应用，可以在IOC中声明式配置。 ProxyFactory：Spring的AOP应用，需要编程式使用。 配置ProxyFactoryBean 在分析Spring AOP的实现原理中，主要以ProxyFactoryBean的实现作为例子和实现的基本线索进行分析。这是因为 ProxyFactoryBean是在Spring IOC环境中创建AOP最底层的方法，也是最灵活的方法，Spring通过它完成了对AOP使用 的封装。以ProxyFactoryBean为实现入口，逐层深入，是快速理解Spring AOP实现的学习路径。 在了解ProxyFactoryBean的实现之前，先了解ProxyFactoryBean的配置和使用，在基于XML配置Spring的Bean时需要经历以下几个步骤： 定义使用的通知器Advisor，这个通知器应该作为一个Bean来定义。这个通知器实现了对目标对象进行增强的切面行为，也就是Advice通知； 定义ProxyFactoryBean：interceptorNames是指定义的通知器，可以有多个； 定义target属性，需要增强的类，从源码上看只能有一个； Spring XML关于ProxyFactoryBean使用配置： logAdvice 定义通知器Advisor： package com.jpeony.spring.aop; import org.springframework.aop.MethodBeforeAdvice; import java.lang.reflect.Method; import java.util.Arrays; import java.util.Date; public class LoggerAdvice implements MethodBeforeAdvice{ @Override public void before(Method method, Object[] args, Object target) throws Throwable { System.out.println(\"系统日志：\"+(new Date())+\":\"+\"调用了\"+method.getName()+\" :使用了参数\"+(Arrays.toString(args))); } } 定义Target： package com.jpeony.spring; public interface ConferenceService { void conference(); } package com.jpeony.spring; public class ConferenceServiceImpl implements ConferenceService { @Override public void conference() { System.out.println(\"开会......\"); } } 测试类： package com.jpeony.spring; import org.junit.Test; import org.springframework.context.ApplicationContext; import org.springframework.context.support.ClassPathXmlApplicationContext; public class TestAop { @Test public void testAop() { // 根据配置文件创建IOC容器 ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext-aop.xml\"); // 从容器中获取Bean ConferenceService conferenceService = (ConferenceService) context.getBean(\"aopMethod\"); // 调用Bean方法 conferenceService.conference(); } } 结果: > > 从打印结果，可以看到，成功对目标类实现了增强功能。 下面接着看ProxyFactoryBean是如何对Target对象起作用的，为何如此配置就能实现增强的效果。 ProxyFactoryBean生成AopProxy代理对象 在Spring AOP的使用中，我们已经了解到，可以通过ProxyFactoryBean来配置目标对象和切面行为。 这个ProxyFactoryBean是一个FactoryBean，关于FactoryBean和BeanFactory说明参考：FactoryBean和BeanFactory区别。 在ProxyFactoryBean中，通过interceptorNames属性来配置已经定义好的通知器Advisor。 虽然名字叫interceptorNames，但实际上是提供AOP应用配置通知的地方。在ProxyFactoryBean中需要为 target目标对象生成Proxy代理对象，从而为AOP横切面的编织做好准备工作。 AopProxy代理对象的生成过程： 从FactoryBean中获取对象，以getObject()方法作为入口完成的，ProxyFactoryBean需要实现FactoryBean中的 getObject()方法。对于ProxyFactoryBean来说，把需要对target目标对象增加的增强处理，都通过getObject()方法 进行封装，这些增强处理是为AOP功能实现提供服务的。 getObject()首先对通知链进行初始化，通知器链封装了一序列的拦截器，这些拦截器都要从序列中读取，然后为 代理对象的生成做好准备。在生成代理对象时，因为Spring中有singleton类型和prototype类型这两种不同的bean， 所以要对代理对象的生成做一个区分。ProxyFactoryBean的AOP实现需要依赖JDK或者CGLIB提供的Proxy特性。 ProxyFactoryBean的类图： ProxyFactoryBean.getObject()方法源码： @Override public Object getObject() throws BeansException { // 通知器链进行初始化，通知器链封装了一序列的拦截器，这些拦截器都要从序列中读取， // 然后为代理对象的生成做好准备。 initializeAdvisorChain(); // 判断是singleton还是prototype属性，选择不同的AopProxy创建实现 if (isSingleton()) { return getSingletonInstance(); } else { if (this.targetName == null) { logger.warn(\"Using non-singleton proxies with singleton targets is often undesirable. \" + \"Enable prototype proxies by setting the 'targetName' property.\"); } return newPrototypeInstance(); } } ProxyFactoryBean.initializeAdvisorChain()方法源码： private synchronized void initializeAdvisorChain() throws AopConfigException, BeansException { if (this.advisorChainInitialized) { return; } if (!ObjectUtils.isEmpty(this.interceptorNames)) { if (this.beanFactory == null) { throw new IllegalStateException(\"No BeanFactory available anymore (probably due to serialization) \" + \"- cannot resolve interceptor names \" + Arrays.asList(this.interceptorNames)); } // Globals can't be last unless we specified a targetSource using the property... if (this.interceptorNames[this.interceptorNames.length - 1].endsWith(GLOBAL_SUFFIX) && this.targetName == null && this.targetSource == EMPTY_TARGET_SOURCE) { throw new AopConfigException(\"Target required after globals\"); } // Materialize interceptor chain from bean names. // 这里添加Advisor链的调用，通过interceptorNames属性进行配置 for (String name : this.interceptorNames) { if (logger.isTraceEnabled()) { logger.trace(\"Configuring advisor or advice '\" + name + \"'\"); } if (name.endsWith(GLOBAL_SUFFIX)) { if (!(this.beanFactory instanceof ListableBeanFactory)) { throw new AopConfigException( \"Can only use global advisors or interceptors with a ListableBeanFactory\"); } addGlobalAdvisor((ListableBeanFactory) this.beanFactory, name.substring(0, name.length() - GLOBAL_SUFFIX.length())); } else { // If we get here, we need to add a named interceptor. // We must check if it's a singleton or prototype. // 如果程序在这里被调用，那么需要加入命名的拦截器Advice，并且需要检查Bean是singleton还是prototype类型。 Object advice; // singleton类型处理 if (this.singleton || this.beanFactory.isSingleton(name)) { // Add the real Advisor/Advice to the chain. advice = this.beanFactory.getBean(name); } else { // It's a prototype Advice or Advisor: replace with a prototype. // Avoid unnecessary creation of prototype bean just for advisor chain initialization. // prototype类型处理 advice = new PrototypePlaceholderAdvisor(name); } addAdvisorOnChainCreation(advice, name); } } } this.advisorChainInitialized = true; } initializeAdvisorChain()方法为Proxy代理对象配置Advisor链。通过advisorChainInitialized来判断通知器链是否初始化。 如果已经初始化，就无需再初始化，直接返回；完成初始化之后，把从IOC容器中取得的通知器加入拦截器链中，这个动作由addAdvisorOnChainCreation完成。 接下来看ProxyFactoryBean.getSingletonInstance()方法源码： private synchronized Object getSingletonInstance() { if (this.singletonInstance == null) { this.targetSource = freshTargetSource(); if (this.autodetectInterfaces && getProxiedInterfaces().length == 0 && !isProxyTargetClass()) { // Rely on AOP infrastructure to tell us what interfaces to proxy. // 根据AOP框架来判断需要代理的接口 Class targetClass = getTargetClass(); if (targetClass == null) { throw new FactoryBeanNotInitializedException(\"Cannot determine target class for proxy\"); } // 设置代理对象的接口 setInterfaces(ClassUtils.getAllInterfacesForClass(targetClass, this.proxyClassLoader)); } // Initialize the shared singleton instance. super.setFrozen(this.freezeProxy); // 使用相应的动态代理技术生产AopProxy代理对象 this.singletonInstance = getProxy(createAopProxy()); } return this.singletonInstance; } getSingletonInstance()这个方法是ProxyFactoryBean生成AopProxy代理对象的调用入口。代理对象会封装对target目标 对象的调用，也即是通过createAopProxy()返回代理对象。 代理对象生成ProxyCreatorSupport.createAopProxy()方法源码： protected final synchronized AopProxy createAopProxy() { if (!this.active) { activate(); } // 通过AopProxyFactory取得AopProxy，这个AopProxyFactory是在初始函数中定义的，使用DefaultAopProxyFactory。 return getAopProxyFactory().createAopProxy(this); } 具体对象生成在接口AopProxyFactory中的createAopProxy()，方法签名： AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException; createAopProxy方法的具体实现在DefaultAopProxyFactory中实现。 DefaultAopProxyFactory.createAopProxy()方法源码： @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class targetClass = config.getTargetClass(); // 如果targetClass是接口类，使用JDK来生成Proxy if (targetClass == null) { throw new AopConfigException(\"TargetSource cannot determine target class: \" + \"Either an interface or a target is required for proxy creation.\"); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } // 如果不是接口类，使用CGLIB生成Proxy return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } 该方法在AopProxy生成过程中，首先需要从AdvisedSupport对象中取得配置的目标对象，然后根据目标对象类型， 选择不同的代理对象生成方式。 关于AopProxy代理对象的生成，需要考虑使用哪种方式生成，如果目标对象是接口类，适合用JDK动态代理生成， 否则，适合用CGLIB动态代理生成。 JDK生成AopProxy代理对象 上面说过，AopProxy代理对象可以由JDK或CGLIB来生成，而JdkDynamicAopProxy和CglibAopProxy实现 都继承了AopProxy接口。 AopProxy继承关系图： 这里主要讨论JdkDynamicAopProxy如何生成AopProxy代理对象的。 先回顾一下ProxyFactoryBean.getSingletonInstance()方法源码： private synchronized Object getSingletonInstance() { if (this.singletonInstance == null) { this.targetSource = freshTargetSource(); if (this.autodetectInterfaces && getProxiedInterfaces().length == 0 && !isProxyTargetClass()) { // Rely on AOP infrastructure to tell us what interfaces to proxy. Class targetClass = getTargetClass(); if (targetClass == null) { throw new FactoryBeanNotInitializedException(\"Cannot determine target class for proxy\"); } setInterfaces(ClassUtils.getAllInterfacesForClass(targetClass, this.proxyClassLoader)); } // Initialize the shared singleton instance. super.setFrozen(this.freezeProxy); this.singletonInstance = getProxy(createAopProxy()); } return this.singletonInstance; } 在该法中有一个createAopProxy()，该方法具体实现在DefaultAopProxyFactory中。 DefaultAopProxyFactory.createAopProxy()方法源码： @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class targetClass = config.getTargetClass(); // 如果targetClass是接口类，使用JDK来生成Proxy if (targetClass == null) { throw new AopConfigException(\"TargetSource cannot determine target class: \" + \"Either an interface or a target is required for proxy creation.\"); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } // 如果不是接口类，使用CGLIB生成Proxy return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } 该方法会返回一个JdkDynamicAopProxy或者CglibAopProxy对象，返回的对象会传入getProxy()方法中。 this.singletonInstance = getProxy(createAopProxy()); 真正生成我们想要的AopProxy代理对象在getProxy()方法中完成。 ProxyFactoryBean.getProxy()方法源码： protected Object getProxy(AopProxy aopProxy) { return aopProxy.getProxy(this.proxyClassLoader); } getProxy()方法为AopProxy的一个接口方法，签名如下： Object getProxy(ClassLoader classLoader); 上面说过，JdkDynamicAopProxy和CglibAopProxy为具体实现，这里讨论jdk，所以看下JdkDynamicAopProxy 中的实现。 JdkDynamicAopProxy.getProxy()源码： @Override public Object getProxy(ClassLoader classLoader) { if (logger.isDebugEnabled()) { logger.debug(\"Creating JDK dynamic proxy: target source is \" + this.advised.getTargetSource()); } Class[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); // 调用jdk生成proxy的地方 return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); } 生成代理对象时，需要指明三个参数，类加载器，代理接口，Proxy回调方法所在的对象。 CGLIB生成AopProxy代理对象 关于CGLIB生成代理对象直接看CglibAopProxy关于getProxy()方法的实现，前面流程与jdk生产代理对象一样。 CglibAopProxy.getProxy()源码： @Override public Object getProxy(ClassLoader classLoader) { if (logger.isDebugEnabled()) { logger.debug(\"Creating CGLIB proxy: target source is \" + this.advised.getTargetSource()); } // 从advised中取得在IOC容器中配置的target对象 try { Class rootClass = this.advised.getTargetClass(); Assert.state(rootClass != null, \"Target class must be available for creating a CGLIB proxy\"); Class proxySuperClass = rootClass; if (ClassUtils.isCglibProxyClass(rootClass)) { proxySuperClass = rootClass.getSuperclass(); Class[] additionalInterfaces = rootClass.getInterfaces(); for (Class additionalInterface : additionalInterfaces) { this.advised.addInterface(additionalInterface); } } // Validate the class, writing log messages as necessary. // 验证代理对象的接口设置 validateClassIfNecessary(proxySuperClass, classLoader); // Configure CGLIB Enhancer... // 创建并配置Cglib的Enhancer，这个Enhancer对象是Cglib的主要操作类 Enhancer enhancer = createEnhancer(); if (classLoader != null) { enhancer.setClassLoader(classLoader); if (classLoader instanceof SmartClassLoader && ((SmartClassLoader) classLoader).isClassReloadable(proxySuperClass)) { enhancer.setUseCache(false); } } // 设置Enhancer对象，包括设置代理接口，回调方法 // 来自advised的IOC容器配置 enhancer.setSuperclass(proxySuperClass); enhancer.setInterfaces(AopProxyUtils.completeProxiedInterfaces(this.advised)); enhancer.setNamingPolicy(SpringNamingPolicy.INSTANCE); enhancer.setStrategy(new ClassLoaderAwareUndeclaredThrowableStrategy(classLoader)); Callback[] callbacks = getCallbacks(rootClass); Class[] types = new Class[callbacks.length]; for (int x = 0; x 到这里，我们生产了代理对象，通过使用AopProxy对象封装target目标对象之后， ProxyFactoryBean的getObject()方法得到的对象就不是一个普通的Java对象了，而是一个AopProxy的代理对象。 这个时候已经不会让应用直接调用target的方法实现了，而是先被AopProxy代理对象拦截，对于不同的AopProxy 代理对象的不同生成方法，拦截入口不同。比如：JDK使用的是InvocationHandler使用的是invoke入口， 而Cglib使用的是设置好的callback回调。 可以把AOP的实现部分看成是由基础设施准备和AOP运行辅助两个部分组成，这里讨论的AopProxy代理对象的生成， 可以看成是一个静态的AOP基础设施的建立过程。通过这个准备过程，把代理对象、拦截器(通知器)这些待调用的部分 都准备好，等待着AOP运行过程中对这些基础设施的使用。 对于应用触发的AOP应用，会涉及AOP框架的运行和对AOP基础设施的使用。这些动态的运行部分，是从前面提到 的拦截器的回调入口开始的，这些拦截器调用的实现原理，和AopProxy代理对象生成一样，也是AOP实现的重要组成 部分，下篇文章重点分析Spring AOP拦截器调用的实现原理。 spring中的AOP与AspectJ的区别 根据我看spring官方文档的理解（不出意外是最正确的答案）： ①选择spring的AOP还是AspectJ? spring确实有自己的AOP。功能已经基本够用了，除非你的要在接口上动态代理或者方法拦截精确到getter和setter。这些都是写奇葩的需求，一般不使用。 ②在使用AOP的时候，你是用xml还是注解的方式（@Aspect）？ 1）如果使用xml方式，不需要任何额外的jar包。 2）如果使用@Aspect方式，你就可以在类上直接一个@Aspect就搞定，不用费事在xml里配了。但是这需要额外的jar包（ aspectjweaver.jar）。因为spring直接使用AspectJ的注解功能，注意只是使用了它 的注解功能而已。并不是核心功能 ！！！ 注意到文档上还有一句很有意思的话：文档说到 是选择spring AOP还是使用full aspectJ？ 什么是full aspectJ？如果你使用\"full aspectJ\"。就是说你可以实现基于接口的动态代理，等等强大的功能。而不仅仅是aspectj的 注-解-功-能 ！！！ 如果用full AspectJ。比如说Load-Time Weaving的方式 还 需要额外的jar包 spring-instrument.jar 当然，无论是使用spring aop还是 aspectj都需要aspectjweaver.jar spring-aop.jar这两个jar包。 "},"Chapter11/PrincipleOfAOP.html":{"url":"Chapter11/PrincipleOfAOP.html","title":"AOP的原理","keywords":"","body":"AOP的原理 上文分析了AopProxy代理对象的创建过程，相当于为AOP运行做好了准备条件，这篇文章分析AOP如何运行的， 也就是如何通过拦截器调用运行AOP的。 设计原理 在Spring AOP通过JDK或CGLIB的方式生成代理对象的时候，相关的拦截器已经配置到代理对象中，拦截器在 代理对象中起作用是通过对这些方法的回调来完成的。 如果使用JDK动态代理来生成代理对象，通过InvocationHandler来设置拦截器回调； 如果使用CGLIB动态代理生成代理对象，通过DynamicAdvisedInterceptor来完成回调； JdkDynamicAopProxy的invoke拦截 上一篇文章分析了Spring中通过ProxyFactoryBean生成AopProxy代理对象的过程，以及通过JDK和CGLIB 最终生成AopProxy代理对象的实现原理。回顾一下JDK生成AopProxy代理对象的最终代码位置： @Override public Object getProxy(ClassLoader classLoader) { if (logger.isDebugEnabled()) { logger.debug(\"Creating JDK dynamic proxy: target source is \" + this.advised.getTargetSource()); } Class[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised, true); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); } Proxy.newProxyInstance(classLoader, proxiedInterfaces, this)，这里的this参数对应的是InvocationHandler对象， InvocationHandler是JDK定义的反射类的一个接口，这个接口定义了invoke方法，而这个invoke方法时作为JDK Proxy 代理对象进行拦截的回调入口出现的。 JdkDynamicAopProxy实现了InvocationHandler接口源码： final class JdkDynamicAopProxy implements AopProxy, InvocationHandler, Serializable { // ... } JdkDynamicAopProxy实现了InvocationHandler接口，也就是说Proxy代理对象方法被调用时， JdkDynamicAopProxy的invoke()方法作为Proxy对象的回调函数被触发，从而通过invoke()的具体实现来完成对 目标对象的拦截或者说功能增强的工作。 JdkDynamicAopProxy.invoke()方法源码： @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { MethodInvocation invocation; Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Class targetClass = null; Object target = null; try { if (!this.equalsDefined && AopUtils.isEqualsMethod(method)) { // The target does not implement the equals(Object) method itself. // 如果目标对象没有实现Object类的基本方法: equals() return equals(args[0]); } else if (!this.hashCodeDefined && AopUtils.isHashCodeMethod(method)) { // The target does not implement the hashCode() method itself. // 如果目标对象没有实现Object类的基本方法: hashCode() return hashCode(); } else if (method.getDeclaringClass() == DecoratingProxy.class) { // There is only getDecoratedClass() declared -> dispatch to proxy config. return AopProxyUtils.ultimateTargetClass(this.advised); } else if (!this.advised.opaque && method.getDeclaringClass().isInterface() && method.getDeclaringClass().isAssignableFrom(Advised.class)) { // Service invocations on ProxyConfig with the proxy config... // 根据代理对象的配置来调用服务 return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); } Object retVal; if (this.advised.exposeProxy) { // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; } // May be null. Get as late as possible to minimize the time we \"own\" the target, // in case it comes from a pool. // 获取目标对象 target = targetSource.getTarget(); if (target != null) { targetClass = target.getClass(); } // Get the interception chain for this method. // 获取定义好的拦截器链 List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // Check whether we have any advice. If we don't, we can fallback on direct // reflective invocation of the target, and avoid creating a MethodInvocation. // 如果没有设定拦截器，哪么就直接调用target的对应方法 if (chain.isEmpty()) { // We can skip creating a MethodInvocation: just invoke the target directly // Note that the final invoker must be an InvokerInterceptor so we know it does // nothing but a reflective operation on the target, and no hot swapping or fancy proxying. Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); } else { // We need to create a method invocation... // 如果有拦截器的设定，那么需要调用拦截器之后才调用目标对象的对应方法 // 通过构造一个ReflectiveMethodInvocation来实现，下面再看这个类的具体实现 invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain. // 根据拦截器继续执行 retVal = invocation.proceed(); } // Massage return value if necessary. Class returnType = method.getReturnType(); if (retVal != null && retVal == target && returnType != Object.class && returnType.isInstance(proxy) && !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) { // Special case: it returned \"this\" and the return type of the method // is type-compatible. Note that we can't help if the target sets // a reference to itself in another returned object. retVal = proxy; } else if (retVal == null && returnType != Void.TYPE && returnType.isPrimitive()) { throw new AopInvocationException( \"Null return value from advice does not match primitive return type for: \" + method); } return retVal; } finally { if (target != null && !targetSource.isStatic()) { // Must have come from TargetSource. targetSource.releaseTarget(target); } if (setProxyContext) { // Restore old proxy. AopContext.setCurrentProxy(oldProxy); } } } CglibAopProxy的intercept拦截 在分析CglibAopProxy的AopProxy代理对象生成的时候，我们了解到对于AOP的拦截调用，其回调是在 DynamicAdvisedInterceptor对象中实现的，这个回调的实现在intercept()方法中。 DynamicAdvisedInterceptor.intercept()方法源码： @Override public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { Object oldProxy = null; boolean setProxyContext = false; Class targetClass = null; Object target = null; try { if (this.advised.exposeProxy) { // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; } // May be null. Get as late as possible to minimize the time we // \"own\" the target, in case it comes from a pool... target = getTarget(); if (target != null) { targetClass = target.getClass(); } // 从advised中取得配置好的AOP通知链 List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); Object retVal; // Check whether we only have one InvokerInterceptor: that is, // no real advice, but just reflective invocation of the target. // 如果没有AOP通知配置，那么直接调用target对象方法 if (chain.isEmpty() && Modifier.isPublic(method.getModifiers())) { // We can skip creating a MethodInvocation: just invoke the target directly. // Note that the final invoker must be an InvokerInterceptor, so we know // it does nothing but a reflective operation on the target, and no hot // swapping or fancy proxying. Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = methodProxy.invoke(target, argsToUse); } else { // We need to create a method invocation... // 通过CglibMethodInvocation来启动Advice通知 retVal = new CglibMethodInvocation(proxy, target, method, args, targetClass, chain, methodProxy).proceed(); } retVal = processReturnType(proxy, target, method, retVal); return retVal; } finally { if (target != null) { releaseTarget(target); } if (setProxyContext) { // Restore old proxy. AopContext.setCurrentProxy(oldProxy); } } } 实现过程与JdkDynamicAopProxy回调invoke()方法类似。 目标对象方法的调用 如果没有设置拦截器，那么会对目标对象的方法直接调用。对于JdkDynamicAopProxy代理对象，从源码可以看到 对目标对象的调用方法是通过AopUtils使用反射机制在AopUtils.invokeJoinpointUsingReflection方法中实现的。 在这个调用中，首先得到调用方法的反射对象，然后使用invoke()启动对方法反射对象的调用。 AopUtils.invokeJoinpointUsingReflection()方法源码： public static Object invokeJoinpointUsingReflection(Object target, Method method, Object[] args) throws Throwable { // Use reflection to invoke the method. // 使用反射调用目标对象的方法 try { ReflectionUtils.makeAccessible(method); return method.invoke(target, args); } catch (InvocationTargetException ex) { // Invoked method threw a checked exception. // We must rethrow it. The client won't see the interceptor. throw ex.getTargetException(); } catch (IllegalArgumentException ex) { throw new AopInvocationException(\"AOP configuration seems to be invalid: tried calling method [\" + method + \"] on target [\" + target + \"]\", ex); } catch (IllegalAccessException ex) { throw new AopInvocationException(\"Could not access method [\" + method + \"]\", ex); } } CglibAopProxy调用目标对象方法： Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = methodProxy.invoke(target, argsToUse); public Object invoke(Object obj, Object[] args) throws Throwable { try { this.init(); MethodProxy.FastClassInfo fci = this.fastClassInfo; return fci.f1.invoke(fci.i1, obj, args); } catch (InvocationTargetException var4) { throw var4.getTargetException(); } catch (IllegalArgumentException var5) { if (this.fastClassInfo.i1 对目标对象的调用，通过CGLIB的MethodProxy对象来直接完成的，这个对象的使用是由CGLIB的设计决定的。 AOP拦截器链的调用 上面分析的是对目标对象的调用，下面分析如何对AOP实现目标增强调用。 由于JDK和CGLIB生成不同的AopProxy代理对象，从而构造了不同的回调方法来启动对拦截器链的调用。 但是他们对拦截器链的调用都是使用ReflectiveMethodInvocation.procced()方法来完成的，追踪源码很容易明白。 ReflectiveMethodInvocation.procced()方法源码： @Override public Object proceed() throws Throwable { // We start with an index of -1 and increment early. /** 从索引为-1的拦截器开始调用，并按序递增， * 如果拦截器链中的拦截器迭代调用完毕，这里开始调用target函数， * invokeJoinpoint()方法签名： * protected Object invokeJoinpoint() throws Throwable { * return AopUtils.invokeJoinpointUsingReflection(this.target, this.method, this.arguments); * } * 还是通过AopUtils.invokeJoinpointUsingReflection反射调用目标方法 */ if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) { return invokeJoinpoint(); } // 这里按interceptorOrInterceptionAdvice定义好的拦截器链进行调用 Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) { // Evaluate dynamic method matcher here: static part will already have // been evaluated and found to match. // 这里对拦截器进行动态匹配判断，这里是触发匹配的地方，如果和定义的PointCut匹配，那么这个Advice被执行 InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; if (dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)) { return dm.interceptor.invoke(this); } else { // Dynamic matching failed. // Skip this interceptor and invoke the next in the chain. // 如果不匹配，递归调用proceed()方法，直到所有的拦截器都被运行过为止。 // 上面代码invokeJoinpoint的if条件上对拦截器执行做了判断，如果拦截器都执行完，就执行目标方法 return proceed(); } } else { // It's an interceptor, so we just invoke it: The pointcut will have // been evaluated statically before this object was constructed. // 如果是interceptor直接调用interceptor的方法 return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); } } 以上就是整个拦截器和target目标对象被调用的过程。 配置通知器 在整个AopProxy拦截器调用的过程中，我们先回到ReflectiveMethodInvocation.procceed()方法中， 源码见上面代码。我们看下如下代码： Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); 这个interceptorOrInterceptionAdvice是获得的拦截器，它通过拦截器机制对目标对象进行增强。这个拦截来自于 interceptorsAndDynamicMethodMatchers集合中的一个元素： protected final List interceptorsAndDynamicMethodMatchers; 这个List中的拦截器是怎么生成的？ 先看回放下JdkDynamicAopProxy中的invoke()方法中的代码片段： List chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); 取得拦截器链是由advised对象完成的，也即private final AdvisedSupport advised; AdvisedSupport.getInterceptorsAndDynamicInterceptionAdvice()方法源码： public List getInterceptorsAndDynamicInterceptionAdvice(Method method, Class targetClass) { // 使用了cache，通过cache获取获取interceptor链，如果没有就是生成，否则直接返回拦截器 MethodCacheKey cacheKey = new MethodCacheKey(method); List cached = this.methodCache.get(cacheKey); if (cached == null) { // 这个interceptor由advisorChainFactory的getInterceptorsAndDynamicInterceptionAdvice()方法生成 // AdvisorChainFactory advisorChainFactory = new DefaultAdvisorChainFactory(); // 通过定义可以知道，使用DefaultAdvisorChainFactory cached = this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice( this, method, targetClass); this.methodCache.put(cacheKey, cached); } return cached; } 这个方法中取得了拦截器链，为了提高效率，设置了缓存操作。 取得拦截器链的具体实现在DefaultAdvisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice()方法中： @Override public List getInterceptorsAndDynamicInterceptionAdvice( Advised config, Method method, Class targetClass) { // This is somewhat tricky... We have to process introductions first, // but we need to preserve order in the ultimate list. // advisor链已经在config中持有，这里可以直接使用 List interceptorList = new ArrayList(config.getAdvisors().length); Class actualClass = (targetClass != null ? targetClass : method.getDeclaringClass()); boolean hasIntroductions = hasMatchingIntroductions(config, actualClass); AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); for (Advisor advisor : config.getAdvisors()) { if (advisor instanceof PointcutAdvisor) { // Add it conditionally. PointcutAdvisor pointcutAdvisor = (PointcutAdvisor) advisor; if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass)) { // 拦截器链通过AdvisorAdapterRegistry加入的 MethodInterceptor[] interceptors = registry.getInterceptors(advisor); MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); // 使用MethodMatchers.matches方法进行匹配判断 if (MethodMatchers.matches(mm, method, actualClass, hasIntroductions)) { if (mm.isRuntime()) { // Creating a new object instance in the getInterceptors() method // isn't a problem as we normally cache created chains. for (MethodInterceptor interceptor : interceptors) { interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); } } else { interceptorList.addAll(Arrays.asList(interceptors)); } } } } else if (advisor instanceof IntroductionAdvisor) { IntroductionAdvisor ia = (IntroductionAdvisor) advisor; if (config.isPreFiltered() || ia.getClassFilter().matches(actualClass)) { Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); } } else { Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); } } return interceptorList; } 判断Advisor是否符合配置要求： private static boolean hasMatchingIntroductions(Advised config, Class actualClass) { for (int i = 0; i 从源码可以看到，取得拦截器链的具体工作由DefaultAdvisorChainFactory来完成，源码逻辑： 1）首先设置一个List，长度由配置的通知器个数决定，这个配置就是XML中对ProxyFactoryBean做的InterceptNames。 2）然后通过AdvisorAdapterRegistry进行拦截器注册。 3）List中的拦截器在JDK的invoke()或CGLIB的intercept()方法中代理启动时完成切面增强。 在调用invoke()或intercept()方法中，在调用ReflectiveMethodInvocation的proceed()方法前， 通过构造器将生成的拦截器链chain作为创建ReflectiveMethodInvocation的参数，所以在procced()中 就可以直接使用拦截器链。 invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); retVal = invocation.proceed(); Advice通知的实现 经过前面的分析，我们看到AopProxy代理对象的生成，拦截器链的建立，拦截器链的调用和最终目标方法 的实现原理。但是Spring AOP定义的通知是怎样实现目标对象的增强的？ 在上面的DefaultAdvisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice()方法中： @Override public List getInterceptorsAndDynamicInterceptionAdvice( Advised config, Method method, Class targetClass) { // This is somewhat tricky... We have to process introductions first, // but we need to preserve order in the ultimate list. // advisor链已经在config中持有，这里可以直接使用 List interceptorList = new ArrayList(config.getAdvisors().length); Class actualClass = (targetClass != null ? targetClass : method.getDeclaringClass()); boolean hasIntroductions = hasMatchingIntroductions(config, actualClass); AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); for (Advisor advisor : config.getAdvisors()) { if (advisor instanceof PointcutAdvisor) { // Add it conditionally. PointcutAdvisor pointcutAdvisor = (PointcutAdvisor) advisor; if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass)) { // 拦截器链通过AdvisorAdapterRegistry加入的 MethodInterceptor[] interceptors = registry.getInterceptors(advisor); MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); // 使用MethodMatchers.matches方法进行匹配判断 if (MethodMatchers.matches(mm, method, actualClass, hasIntroductions)) { if (mm.isRuntime()) { // Creating a new object instance in the getInterceptors() method // isn't a problem as we normally cache created chains. for (MethodInterceptor interceptor : interceptors) { interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); } } else { interceptorList.addAll(Arrays.asList(interceptors)); } } } } else if (advisor instanceof IntroductionAdvisor) { IntroductionAdvisor ia = (IntroductionAdvisor) advisor; if (config.isPreFiltered() || ia.getClassFilter().matches(actualClass)) { Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); } } else { Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); } } return interceptorList; } 在上面这段代码中，GlobalAdvisorAdapterRegistry中隐藏着不少AOP实现的重要细节，它的getInterceptors方法 为AOP实现做出了很大的贡献，就是这个方法封装着advice织入实现的入口。 先看下GlobalAdvisorAdapterRegistry的源码： package org.springframework.aop.framework.adapter; public abstract class GlobalAdvisorAdapterRegistry { /** * Keep track of a single instance so we can return it to classes that request it. */ private static AdvisorAdapterRegistry instance = new DefaultAdvisorAdapterRegistry(); /** * Return the singleton {@link DefaultAdvisorAdapterRegistry} instance. */ public static AdvisorAdapterRegistry getInstance() { return instance; } /** * Reset the singleton {@link DefaultAdvisorAdapterRegistry}, removing any * {@link AdvisorAdapterRegistry#registerAdvisorAdapter(AdvisorAdapter) registered} * adapters. */ static void reset() { instance = new DefaultAdvisorAdapterRegistry(); } } 该类非常简洁，起到一个适配器的作用，同时，也是一个单例模式的应用，为Spring AOP模块提供了一个 DefaultAdvisorAdapterRegister单件，通过DefaultAdvisorAdapterRegister完成各种通知的适配和注册工作。 DefaultAdvisorAdapterRegister类的源码： package org.springframework.aop.framework.adapter; import java.io.Serializable; import java.util.ArrayList; import java.util.List; import org.aopalliance.aop.Advice; import org.aopalliance.intercept.MethodInterceptor; import org.springframework.aop.Advisor; import org.springframework.aop.support.DefaultPointcutAdvisor; @SuppressWarnings(\"serial\") public class DefaultAdvisorAdapterRegistry implements AdvisorAdapterRegistry, Serializable { // 持有一个AdvisorAdapter的List，这个List中的Adapter是与实现Spring AOP的advice增强功能相对应的 private final List adapters = new ArrayList(3); /** * Create a new DefaultAdvisorAdapterRegistry, registering well-known adapters. * 把已有的advice实现的Adapter加入进来，主要有MethodBeforeAdvice、AfterReturningAdvice、 * ThrowsAdvice这些AOP的advice封装实现 */ public DefaultAdvisorAdapterRegistry() { registerAdvisorAdapter(new MethodBeforeAdviceAdapter()); registerAdvisorAdapter(new AfterReturningAdviceAdapter()); registerAdvisorAdapter(new ThrowsAdviceAdapter()); } @Override public Advisor wrap(Object adviceObject) throws UnknownAdviceTypeException { if (adviceObject instanceof Advisor) { return (Advisor) adviceObject; } if (!(adviceObject instanceof Advice)) { throw new UnknownAdviceTypeException(adviceObject); } Advice advice = (Advice) adviceObject; if (advice instanceof MethodInterceptor) { // So well-known it doesn't even need an adapter. return new DefaultPointcutAdvisor(advice); } for (AdvisorAdapter adapter : this.adapters) { // Check that it is supported. if (adapter.supportsAdvice(advice)) { return new DefaultPointcutAdvisor(advice); } } throw new UnknownAdviceTypeException(advice); } /** * 在DefaultAdvisorChainFactory中启动getInterceptors()方法 */ @Override public MethodInterceptor[] getInterceptors(Advisor advisor) throws UnknownAdviceTypeException { List interceptors = new ArrayList(3); // 从Advisor通知器配置中取得advice通知 Advice advice = advisor.getAdvice(); // 如果通知是MethodInterceptor类型的通知，直接加入interceptors中，无需适配 if (advice instanceof MethodInterceptor) { interceptors.add((MethodInterceptor) advice); } // 对通知进行适配，使用已经配置好的Adapter:MethodBeforeAdviceAdapter、AfterReturningAdviceAdapter、 // ThrowsAdviceAdapter，从配置好的Adapter中取出封装好AOP编织功能的 for (AdvisorAdapter adapter : this.adapters) { if (adapter.supportsAdvice(advice)) { interceptors.add(adapter.getInterceptor(advisor)); } } if (interceptors.isEmpty()) { throw new UnknownAdviceTypeException(advisor.getAdvice()); } return interceptors.toArray(new MethodInterceptor[interceptors.size()]); } @Override public void registerAdvisorAdapter(AdvisorAdapter adapter) { this.adapters.add(adapter); } } 在DefaultAdvisorAdapterRegistry源码中我们看到了一序列在AOP应用中与用到的Spring AOP的advice通知相对应 的adapter适配实现，并看到了对这些adapter的具体使用。具体来说，对它们的使用主要体现在以下两个方面： 1）调用adapter的supportsAdvice方法，通过这个方法判断advice类型注册不同的AdviceInterceptor。 2）在getInterceptors方法中实现代理对象的织入功能。 以MethodBeforeAdviceAdapter为了，源码如下： class MethodBeforeAdviceAdapter implements AdvisorAdapter, Serializable { @Override public boolean supportsAdvice(Advice advice) { return (advice instanceof MethodBeforeAdvice); } @Override public MethodInterceptor getInterceptor(Advisor advisor) { MethodBeforeAdvice advice = (MethodBeforeAdvice) advisor.getAdvice(); return new MethodBeforeAdviceInterceptor(advice); } } 源码并不复杂，supportAdvice方法对Advice类型进行判断；而getInterceptor把advice从通知器中取出， 然后创建一个MethodBeforAdviceInterceptor对象将advice包裹起来并返回。 Spring AOP为了实现advice的织入，设计了特定的拦截器对这些功能进行了封装。虽然应用不会直接用 到这些拦截器，但却是advice发挥作用比不可少的准备。以MethodBeforAdviceInterceptor为例，看看它 是如何完成advice的封装的。 MethodBeforAdviceInterceptor源码： public class MethodBeforeAdviceInterceptor implements MethodInterceptor, Serializable { private MethodBeforeAdvice advice; /** * Create a new MethodBeforeAdviceInterceptor for the given advice. * @param advice the MethodBeforeAdvice to wrap */ public MethodBeforeAdviceInterceptor(MethodBeforeAdvice advice) { Assert.notNull(advice, \"Advice must not be null\"); this.advice = advice; } @Override public Object invoke(MethodInvocation mi) throws Throwable { this.advice.before(mi.getMethod(), mi.getArguments(), mi.getThis() ); return mi.proceed(); } } MethodBeforAdviceInterceptor完成的是对MethodBeforAdvice通知的封装，可以在invoke()回调方法中， 看到首先触发了advice的before回调，然后才是MethodInvocation的proceed方法调用，看到这里，就已经 和前面在ReflectiveMethodInvocation的proceed分析中联系起来了。在前面我们说过，在AopProxy代理 对象触发的ReflectiveMethodInvocation的proceed方法中，在取得拦截器以后，启动了对拦截器invoke方法 的调用。按照AOP的配置规则，ReflectiveMethodInvocation触发的拦截器invoke()方法中，最终会根据不同 的advice类型，触发Spring对不同的advice的拦截封装，比如对MethodBeforeAdvice，最终会触发 MethodBeforAdviceInterceptor的invoke方法在MethodBeforeAdviceInterceptor方法中，会先调用 advice的before方法，这就是MethodBeforeAdvice所需要的对目标对象的增强效果：在方法调用之前 完成通知增强。 如果了解了MethodBeforAdviceInterceptor的实现原理，其余类型的通知实现类似。 ProxyFactory实现AOP 在前面的分析中，我们了解了以ProxyFactoryBean为例Spring AOP的实现线索。还可以使用ProxyFactory 来实现Spring AOP的功能，只是在使用ProxyFactory的时候，需要编程式地完成AOP应用的设置。 对于ProxyFactory实现AOP功能，其实现原理与ProxyFactoryBean的实现原理是一样的，只是在最外层的表现 形式上有所不同。 ProxyFactory源码： package org.springframework.aop.framework; import org.aopalliance.intercept.Interceptor; import org.springframework.aop.TargetSource; import org.springframework.util.ClassUtils; @SuppressWarnings(\"serial\") public class ProxyFactory extends ProxyCreatorSupport { public ProxyFactory() { } public ProxyFactory(Object target) { setTarget(target); setInterfaces(ClassUtils.getAllInterfaces(target)); } public ProxyFactory(Class... proxyInterfaces) { setInterfaces(proxyInterfaces); } public ProxyFactory(Class proxyInterface, Interceptor interceptor) { addInterface(proxyInterface); addAdvice(interceptor); } public ProxyFactory(Class proxyInterface, TargetSource targetSource) { addInterface(proxyInterface); setTargetSource(targetSource); } public Object getProxy() { return createAopProxy().getProxy(); } public Object getProxy(ClassLoader classLoader) { return createAopProxy().getProxy(classLoader); } @SuppressWarnings(\"unchecked\") public static T getProxy(Class proxyInterface, Interceptor interceptor) { return (T) new ProxyFactory(proxyInterface, interceptor).getProxy(); } @SuppressWarnings(\"unchecked\") public static T getProxy(Class proxyInterface, TargetSource targetSource) { return (T) new ProxyFactory(proxyInterface, targetSource).getProxy(); } public static Object getProxy(TargetSource targetSource) { if (targetSource.getTargetClass() == null) { throw new IllegalArgumentException(\"Cannot create class proxy for TargetSource with null target class\"); } ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTargetSource(targetSource); proxyFactory.setProxyTargetClass(true); return proxyFactory.getProxy(); } } ProxyFactory没有使用FactoryBean的IOC封装，而是直接集成了ProxyCreatorSupport的功能来完成AOP的属性 "},"Chapter11/AnnotationAOP.html":{"url":"Chapter11/AnnotationAOP.html","title":"注解式AOP原理","keywords":"","body":"注解式AOP原理 基于注解的方式实现AOP需要在配置类中添加注解@EnableAspectJAutoProxy。我们就先从这个注解看一下Spring实现AOP的过程： @SpringBootApplication @EnableAspectJAutoProxy(proxyTargetClass = true, exposeProxy = true) public class TestApplication { public static void main(String[] args) { } } @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Import(AspectJAutoProxyRegistrar.class)//引入AspectJAutoProxyRegister.class对象 public @interface EnableAspectJAutoProxy { //true——使用CGLIB基于类创建代理；false——使用java接口创建代理 boolean proxyTargetClass() default false; //是否通过aop框架暴露该代理对象，aopContext能够访问. boolean exposeProxy() default false; } 可以看出@EnableAspectJAutoProxy引入了AspectJAutoProxyRegister.class对象 ，AspectJAutoProxyRegister给容器中注册一个AnnotationAwareAspectJAutoProxyCreator class AspectJAutoProxyRegistrar implements ImportBeanDefinitionRegistrar { /** * Register, escalate, and configure the AspectJ auto proxy creator based on the value * of the @{@link EnableAspectJAutoProxy#proxyTargetClass()} attribute on the importing * {@code @Configuration} class. */ @Override public void registerBeanDefinitions( AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry); AnnotationAttributes enableAspectJAutoProxy = AnnotationConfigUtils.attributesFor(importingClassMetadata, EnableAspectJAutoProxy.class); if (enableAspectJAutoProxy != null) { if (enableAspectJAutoProxy.getBoolean(\"proxyTargetClass\")) { AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); } if (enableAspectJAutoProxy.getBoolean(\"exposeProxy\")) { AopConfigUtils.forceAutoProxyCreatorToExposeProxy(registry); } } } } AopConfigUtils： @Nullable public static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary(BeanDefinitionRegistry registry) { return registerAspectJAnnotationAutoProxyCreatorIfNecessary(registry, null); } @Nullable public static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary( BeanDefinitionRegistry registry, @Nullable Object source) { //给容器中注册一个AnnotationAwareAspectJAutoProxyCreator return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class, registry, source); } @Nullable private static BeanDefinition registerOrEscalateApcAsRequired( Class cls, BeanDefinitionRegistry registry, @Nullable Object source) { Assert.notNull(registry, \"BeanDefinitionRegistry must not be null\"); if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) { BeanDefinition apcDefinition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); if (!cls.getName().equals(apcDefinition.getBeanClassName())) { int currentPriority = findPriorityForClass(apcDefinition.getBeanClassName()); int requiredPriority = findPriorityForClass(cls); if (currentPriority AnnotationAwareAspectJAutoProxyCreator 的层次结构： 这里主要关注后置处理器和自动装备BeanFactory相关的方法：SmartInstantiationAwareBeanPostProcessor(后置处理器), BeanFactoryAware(自动装配BeanFactory) AnnotationAwareAspectJAutoProxyCreator的注册流程 一切回归到refresh()方法 public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // 注册后置处理器，用于拦截bean的创建 registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); } catch (BeansException ex) { if (logger.isWarnEnabled()) { logger.warn(\"Exception encountered during context initialization - \" + \"cancelling refresh attempt: \" + ex); } // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } finally { // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); } } } refresh()方法调用了一个方法 registerBeanPostProcessors(beanFactory)，注册后置处理器，用于拦截bean的创建。 也是在这个方法中完成了AnnotationAwareAspectJAutoProxyCreator的注册。 该方法的主要逻辑在PostProcessorRegistrationDelegate类中的registerBeanPostProcessors方法中： public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) { //获取ioc容器已经定义了的需要创建对象的所有BeanPostProcessor String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); // 注册BeanPostProcessorChecker,当bean在BeanPostProcessor实例化过程中被创建时 //即当一个bean没有资格被所有BeanPostProcessor处理时，它记录一条信息消息。 int beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length; beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); // BeanPostProcessors 根据优先级进行分离，实现PriorityOrdered接口、实现Ordered接口、未实现优先级接口 List priorityOrderedPostProcessors = new ArrayList(); List internalPostProcessors = new ArrayList(); List orderedPostProcessorNames = new ArrayList(); List nonOrderedPostProcessorNames = new ArrayList(); for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp);//内部beanpostprocessor } } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { nonOrderedPostProcessorNames.add(ppName); } } //先注册实现PriorityOrdered接口的 BeanPostProcessors sortPostProcessors(priorityOrderedPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); //再注册实现Ordered接口的 BeanPostProcessors List orderedPostProcessors = new ArrayList(); for (String ppName : orderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } sortPostProcessors(orderedPostProcessors, beanFactory); //按照优先级排序 registerBeanPostProcessors(beanFactory, orderedPostProcessors); //注册 //这里注册所有常规的beanpostprocessor List nonOrderedPostProcessors = new ArrayList(); for (String ppName : nonOrderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); // 最后，重新注册所有内部beanpostprocessor。 sortPostProcessors(internalPostProcessors, beanFactory); registerBeanPostProcessors(beanFactory, internalPostProcessors); // Re-register post-processor for detecting inner beans as ApplicationListeners, // moving it to the end of the processor chain (for picking up proxies etc). beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext)); } registerBeanPostProcessors中注册BeanPostProcessor的顺序是： 注册实现了PriorityOrdered接口的BeanPostProcessor 注册实现了Ordered接口的BeanPostProcessor 注册常规的BeanPostProcessor ，也就是没有实现优先级接口的BeanPostProcessor 注册Spring内部BeanPostProcessor registerBeanPostProcessors方法中注册了所有的BeanPostProcessor，所以AnnotationAwareAspectJAutoProxyCreator也在这里注册完成。 而从AnnotationAwareAspectJAutoProxyCreator类的层次接口可以看出，该类实现了Ordered接口。 所以它是在注册实现Ordered接口的BeanPostProcessor是完成注册： BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class)； 这里调用的是AbstractBeanFactory的getBean方法，但是它实际的业务逻辑在AbstractAutowireCapableBeanFactory的doGetBean方法中实现： protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException { //实例化bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) { instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); } if (instanceWrapper == null) { instanceWrapper = createBeanInstance(beanName, mbd, args); //创建bean实例 } final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); Class beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); mbd.resolvedTargetType = beanType; // 允许post-processors修改合并的bean定义。 synchronized (mbd.postProcessingLock) { if (!mbd.postProcessed) { try { applyMergedBeanDefinitionPostProcessors(mbd, beanType, beanName); } catch (Throwable ex) { throw new BeanCreationException(mbd.getResourceDescription(), beanName, \"Post-processing of merged bean definition failed\", ex); } mbd.postProcessed = true; } } // Eagerly cache singletons to be able to resolve circular references // even when triggered by lifecycle interfaces like BeanFactoryAware. boolean earlySingletonExposure = (mbd.isSingleton() && this.allowCircularReferences && isSingletonCurrentlyInCreation(beanName)); if (earlySingletonExposure) { if (logger.isDebugEnabled()) { logger.debug(\"Eagerly caching bean '\" + beanName + \"' to allow for resolving potential circular references\"); } addSingletonFactory(beanName, new ObjectFactory() { @Override public Object getObject() throws BeansException { return getEarlyBeanReference(beanName, mbd, bean); } }); } //初始化bean实例 Object exposedObject = bean; try { populateBean(beanName, mbd, instanceWrapper); //给bean的各种属性赋值 if (exposedObject != null) { //初始化bean exposedObject = initializeBean(beanName, exposedObject, mbd); } } catch (Throwable ex) { if (ex instanceof BeanCreationException && beanName.equals(((BeanCreationException) ex).getBeanName())) { throw (BeanCreationException) ex; } else { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Initialization of bean failed\", ex); } } if (earlySingletonExposure) { Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) { if (exposedObject == bean) { exposedObject = earlySingletonReference; } else if (!this.allowRawInjectionDespiteWrapping && hasDependentBean(beanName)) { String[] dependentBeans = getDependentBeans(beanName); Set actualDependentBeans = new LinkedHashSet(dependentBeans.length); for (String dependentBean : dependentBeans) { if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) { actualDependentBeans.add(dependentBean); } } if (!actualDependentBeans.isEmpty()) { throw new BeanCurrentlyInCreationException(beanName, \"Bean with name '\" + beanName + \"' has been injected into other beans [\" + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + \"] in its raw version as part of a circular reference, but has eventually been \" + \"wrapped. This means that said other beans do not use the final version of the \" + \"bean. This is often the result of over-eager type matching - consider using \" + \"'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\"); } } } } // Register bean as disposable. try { registerDisposableBeanIfNecessary(beanName, bean, mbd); } catch (BeanDefinitionValidationException ex) { throw new BeanCreationException( mbd.getResourceDescription(), beanName, \"Invalid destruction signature\", ex); } return exposedObject; } doGetBean中的逻辑看上去很复杂，但实际上他只做了三件事： 创建bean ：createBeanInstance(beanName, mbd, args) 给bean中的属性赋值：populateBean(beanName, mbd, instanceWrapper) 初始化bean：initializeBean(beanName, exposedObject, mbd) 初始化bean时，initializeBean方法会调用BeanPostProcessor和BeanFactory以及Aware接口的相关方法。 这也是BeanPostProcessor如何发挥初始化bean的原理。 AbstractAutowireCapableBeanFactory的initializeBean方法： protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) { if (System.getSecurityManager() != null) { AccessController.doPrivileged(new PrivilegedAction() { @Override public Object run() { invokeAwareMethods(beanName, bean); return null; } }, getAccessControlContext()); } else { invokeAwareMethods(beanName, bean); //处理Aware接口的方法回调 } Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { // 应用后置处理器的postProcessBeforeInitialization方法 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } try { invokeInitMethods(beanName, wrappedBean, mbd); //执行自定义的初始化方法 } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, \"Invocation of init method failed\", ex); } if (mbd == null || !mbd.isSynthetic()) { // 执行后置处理器的postProcessAfterInitialization方法 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } 可以看出initializeBean方法主要做了四件事： 处理Aware接口的方法回调：invokeAwareMethods(beanName, bean); 应用后置处理器的postProcessBeforeInitialization方法： wrappedBean=applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); 执行自定义的初始化方法：invokeInitMethods(beanName, wrappedBean, mbd); 执行后置处理器的postProcessAfterInitialization方法： wrappedBean=applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); initializeBean方法执行成功，AnnotationAwareAspectJAutoProxyCreator注册和初始化成功。 这时再回到第3步中，BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class)执行成功，目标bean成功注册后，将注册完成的beanPostProcessor，排序后注册到BeanFactory中 sortPostProcessors(orderedPostProcessors, beanFactory); //按照优先级排序 registerBeanPostProcessors(beanFactory, orderedPostProcessors); //注册 private static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, List postProcessors) { for (BeanPostProcessor postProcessor : postProcessors) { beanFactory.addBeanPostProcessor(postProcessor); } } AnnotationAwareAspectJAutoProxyCreator调用时机： refresh()方法中，调用registerBeanPostProcessors(beanFactory)方法，注册完BeanPostProcessor后，还调用了方法finishBeanFactoryInitialization(beanFactory) ，完成BeanFactory初始化工作，并创建剩下的单实例bean。finishBeanFactoryInitialization(beanFactory)的执行过程： preInstantiateSingletons调用getBean方法，获取bean实例，执行过程getBean->doGetBean()->getSingleton()->createBean() 这里要说一下createBean中的 resolveBeforeInstantiation(beanName, mbdToUse)方法： 让beanpostprocessor有机会返回代理而不是目标bean实例 try { // 让beanpostprocessor有机会返回代理而不是目标bean实例。 Object bean = resolveBeforeInstantiation(beanName, mbdToUse); if (bean != null) { return bean; } } protected Object resolveBeforeInstantiation(String beanName, RootBeanDefinition mbd) { Object bean = null; if (!Boolean.FALSE.equals(mbd.beforeInstantiationResolved)) { // 确保此时bean类已经被解析。 if (!mbd.isSynthetic() && hasInstantiationAwareBeanPostProcessors()) { Class targetType = determineTargetType(beanName, mbd); if (targetType != null) { //拿到所有后置处理器，如果是InstantiationAwareBeanPostProcessor，就执行postProcessBeforeInstantiation bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName); if (bean != null) { bean = applyBeanPostProcessorsAfterInitialization(bean, beanName); } } } mbd.beforeInstantiationResolved = (bean != null); } return bean; } 主要看一下resolveBeforeInstantiation方法中的applyBeanPostProcessorsBeforeInstantiation 和 applyBeanPostProcessorsAfterInitialization方法，注意两个方法的后缀不同Instantiation(实例化) 和 Initialization(初始化) protected Object applyBeanPostProcessorsBeforeInstantiation(Class beanClass, String beanName) { for (BeanPostProcessor bp : getBeanPostProcessors()) { //遍历所有BeanPostProcessor if (bp instanceof InstantiationAwareBeanPostProcessor) { //如果是InstantiationAwareBeanPostProcessor类型 InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; //执行postProcessBeforeInstantiation方法 Object result = ibp.postProcessBeforeInstantiation(beanClass, beanName); if (result != null) { return result; } } } return null; } postProcessBeforeInstantiation方法是InstantiationAwareBeanPostProcessor接口中定义的方法，applyBeanPostProcessorsAfterInitialization方法会调用BeanPostProcessor接口中postProcessAfterInitialization方法。 而且InstantiationAwareBeanPostProcessor是在创建Bean实例之前先尝试用后置处理器返回对象的；BeanPostProcessor是在Bean对象创建完成初始化前后调用的。从AnnotationAwareAspectJAutoProxyCreator类的层次结构可以看出，该类实现了InstantiationAwareBeanPostProcessor接口，它会在所有bean实例创建前进行拦截，这也是AOP代理的关键所在。 Spring创建AOP代理： AnnotationAwareAspectJAutoProxyCreator实现了InstantiationAwareBeanPostProcessor接口，所以每次创建bean实例前都会调用AnnotationAwareAspectJAutoProxyCreator类的postProcessBeforeInstantiation()方法。下面就看一下该方法都做了什么。 postProcessBeforeInstantiation()是在AbstractAutoProxyCreator类中实现的： public Object postProcessBeforeInstantiation(Class beanClass, String beanName) throws BeansException { Object cacheKey = getCacheKey(beanClass, beanName); if (beanName == null || !this.targetSourcedBeans.contains(beanName)) { //判断当前bean是否在advisedBeans中（advisedBeans保存了所有需要增强的bean） if (this.advisedBeans.containsKey(cacheKey)) { return null; } //判断当前bean是否基础类型Advice、Pointcut、Advisor、AopInfrastructureBean，或是否切面（@Aspect） //是否需要跳过 if (isInfrastructureClass(beanClass) || shouldSkip(beanClass, beanName)) { this.advisedBeans.put(cacheKey, Boolean.FALSE); return null; } } //如果我们有一个自定义TargetSource，请在这里创建代理。 if (beanName != null) { TargetSource targetSource = getCustomTargetSource(beanClass, beanName); if (targetSource != null) { this.targetSourcedBeans.add(beanName); Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(beanClass, beanName, targetSource); Object proxy = createProxy(beanClass, beanName, specificInterceptors, targetSource); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } } return null; } postProcessBeforeInstantiation方法主要对当前类进行一些判断： 判断当前bean是否在advisedBeans（增强bean的集合）中----增强bean是指切入点表达式包含的类。 判断当前bean是否是基础类型的Advice、Pointcut、Advisor、AopInfrastructureBean，或者是否是切面（@Aspect） 是否需要跳过----获取候选的增强器（切面里面的通知方法）【List candidateAdvisors】，如果增强器是 AspectJPointcutAdvisor 类型的，则返回true（封装的通知方法的增强器是 InstantiationModelAwarePointcutAdvisor类型） 判断完成后会创建切入点对象，也就是创建增强bean，bean创建完成后会调用applyBeanPostProcessorsAfterInitialization方法，继而调用后置处理器的postProcessAfterInitialization方法： public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException { Object result = existingBean; for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) { //调用后置处理器的postProcessAfterInitialization方法 result = beanProcessor.postProcessAfterInitialization(result, beanName); if (result == null) { return result; } } return result; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean != null) { Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) { return wrapIfNecessary(bean, beanName, cacheKey);//包装如果需要的情况下 } } return bean; } postProcessAfterInitialization主要有调用了wrapIfNecessary(bean, beanName, cacheKey)方法：包装bean，这里会获取bean的增强器，并且生成动态代理。下面看一下wrapIfNecessary()方法： protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) { if (beanName != null && this.targetSourcedBeans.contains(beanName)) { return bean; } if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) {//不是增强类，则返回 return bean; } if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) { this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } // 如果我们有建议，创建代理。 //获取当前bean的所有增强器 Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) { this.advisedBeans.put(cacheKey, Boolean.TRUE); //将当前bean放入advisedBeans中 //创建增强bean的代理对象 Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; } this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; } wrapIfNecessary方法中主要是获取当前bean的增强器调用getAdvicesAndAdvisorsForBean方法和创建增强后的bean的代理对象调用createProxy方法，下面看看这两个方法： @Override protected Object[] getAdvicesAndAdvisorsForBean(Class beanClass, String beanName, TargetSource targetSource) { List advisors = findEligibleAdvisors(beanClass, beanName); //获取bean可用的增强器 if (advisors.isEmpty()) { return DO_NOT_PROXY; } return advisors.toArray(); } protected List findEligibleAdvisors(Class beanClass, String beanName) { List candidateAdvisors = findCandidateAdvisors();//获取所有可用的增强器 //获取当前bean可用的增强器 List eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) { eligibleAdvisors = sortAdvisors(eligibleAdvisors); //对增强器进行排序 } return eligibleAdvisors; } protected Object createProxy( Class beanClass, String beanName, Object[] specificInterceptors, TargetSource targetSource) { if (this.beanFactory instanceof ConfigurableListableBeanFactory) { AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); } ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.copyFrom(this); if (!proxyFactory.isProxyTargetClass()) { if (shouldProxyTargetClass(beanClass, beanName)) { proxyFactory.setProxyTargetClass(true); } else { evaluateProxyInterfaces(beanClass, proxyFactory); } } //获取当前bean的所有增强器 Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); proxyFactory.addAdvisors(advisors);//将增强器保存在代理工程中 proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) { proxyFactory.setPreFiltered(true); } return proxyFactory.getProxy(getProxyClassLoader()); //创建代理对象 } Spring是利用ProxyFactory代理工厂创建代理对象： ProxyFactory创建代理对象： public Object getProxy(ClassLoader classLoader) { return createAopProxy().getProxy(classLoader); } createAopProxy() 调用链路最终 */ protected final synchronized AopProxy createAopProxy() { if (!this.active) { activate(); } return getAopProxyFactory().createAopProxy(this); } public AopProxyFactory getAopProxyFactory() { return this.aopProxyFactory; } public class ProxyCreatorSupport extends AdvisedSupport { private AopProxyFactory aopProxyFactory; private final List listeners = new LinkedList<>(); /** Set to true when the first AOP proxy has been created. */ private boolean active = false; /** * Create a new ProxyCreatorSupport instance. */ public ProxyCreatorSupport() { this.aopProxyFactory = new DefaultAopProxyFactory(); } 可以看到返回的是一个DefaultAopProxyFactory对象，返回继续跟进createAopProxy的方法 public interface AopProxyFactory { /** * Create an {@link AopProxy} for the given AOP configuration. * @param config the AOP configuration in the form of an * AdvisedSupport object * @return the corresponding AOP proxy * @throws AopConfigException if the configuration is invalid */ AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException; } 到达DefaultAopProxyFactory#createAopProxy方法 @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(\"TargetSource cannot determine target class: \" + \"Either an interface or a target is required for proxy creation.\"); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } 剩余内容请看AOP的原理有详细讲解。 "},"Chapter11/Transaction.html":{"url":"Chapter11/Transaction.html","title":"spring事务","keywords":"","body":"spring事务 开启springboot事务支持 在入口类使用注解@EnableTransactionManagement开启事务 在访问数据库的service方法上添加注解@Transactional即可 流程图 TransactionManagementConfigurationSelector public class TransactionManagementConfigurationSelector extends AdviceModeImportSelector { /** * Returns {@link ProxyTransactionManagementConfiguration} or * {@code AspectJ(Jta)TransactionManagementConfiguration} for {@code PROXY} * and {@code ASPECTJ} values of {@link EnableTransactionManagement#mode()}, * respectively. */ @Override protected String[] selectImports(AdviceMode adviceMode) { switch (adviceMode) { case PROXY: return new String[] {AutoProxyRegistrar.class.getName(), ProxyTransactionManagementConfiguration.class.getName()}; case ASPECTJ: return new String[] {determineTransactionAspectClass()}; default: return null; } } private String determineTransactionAspectClass() { return (ClassUtils.isPresent(\"javax.transaction.Transactional\", getClass().getClassLoader()) ? TransactionManagementConfigUtils.JTA_TRANSACTION_ASPECT_CONFIGURATION_CLASS_NAME : TransactionManagementConfigUtils.TRANSACTION_ASPECT_CONFIGURATION_CLASS_NAME); } } 可以发现如果是PROXY 那么导入的是AutoProxyRegistrar和ProxyTransactionManagementConfiguration 如果是ASPECTJ那么就导入 org.springframework.transaction.aspectj.AspectJJtaTransactionManagementConfiguration 和 org.springframework.transaction.config.internalTransactionalEventListenerFactory 返回去看EnableTransactionManagement 发现默认是PROXY @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Import(TransactionManagementConfigurationSelector.class) public @interface EnableTransactionManagement { /** * Indicate whether subclass-based (CGLIB) proxies are to be created ({@code true}) as * opposed to standard Java interface-based proxies ({@code false}). The default is * {@code false}. Applicable only if {@link #mode()} is set to * {@link AdviceMode#PROXY}. * Note that setting this attribute to {@code true} will affect all * Spring-managed beans requiring proxying, not just those marked with * {@code @Transactional}. For example, other beans marked with Spring's * {@code @Async} annotation will be upgraded to subclass proxying at the same * time. This approach has no negative impact in practice unless one is explicitly * expecting one type of proxy vs another, e.g. in tests. */ boolean proxyTargetClass() default false; /** * Indicate how transactional advice should be applied. * The default is {@link AdviceMode#PROXY}. * Please note that proxy mode allows for interception of calls through the proxy * only. Local calls within the same class cannot get intercepted that way; an * {@link Transactional} annotation on such a method within a local call will be * ignored since Spring's interceptor does not even kick in for such a runtime * scenario. For a more advanced mode of interception, consider switching this to * {@link AdviceMode#ASPECTJ}. */ AdviceMode mode() default AdviceMode.PROXY; /** * Indicate the ordering of the execution of the transaction advisor * when multiple advices are applied at a specific joinpoint. * The default is {@link Ordered#LOWEST_PRECEDENCE}. */ int order() default Ordered.LOWEST_PRECEDENCE; } 那么我们就继续看AutoProxyRegistrar和ProxyTransactionManagementConfiguration AutoProxyRegistrar 负责依赖注入事务的相关属性配置和注入事务入口类（InfrastructureAdvisorAutoProxyCreator类）； public class AutoProxyRegistrar implements ImportBeanDefinitionRegistrar { private final Log logger = LogFactory.getLog(getClass()); /** * Register, escalate, and configure the standard auto proxy creator (APC) against the * given registry. Works by finding the nearest annotation declared on the importing * {@code @Configuration} class that has both {@code mode} and {@code proxyTargetClass} * attributes. If {@code mode} is set to {@code PROXY}, the APC is registered; if * {@code proxyTargetClass} is set to {@code true}, then the APC is forced to use * subclass (CGLIB) proxying. * Several {@code @Enable*} annotations expose both {@code mode} and * {@code proxyTargetClass} attributes. It is important to note that most of these * capabilities end up sharing a {@linkplain AopConfigUtils#AUTO_PROXY_CREATOR_BEAN_NAME * single APC}. For this reason, this implementation doesn't \"care\" exactly which * annotation it finds -- as long as it exposes the right {@code mode} and * {@code proxyTargetClass} attributes, the APC can be registered and configured all * the same. */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) { boolean candidateFound = false; Set annTypes = importingClassMetadata.getAnnotationTypes(); for (String annType : annTypes) { AnnotationAttributes candidate = AnnotationConfigUtils.attributesFor(importingClassMetadata, annType); if (candidate == null) { continue; } Object mode = candidate.get(\"mode\"); Object proxyTargetClass = candidate.get(\"proxyTargetClass\"); if (mode != null && proxyTargetClass != null && AdviceMode.class == mode.getClass() && Boolean.class == proxyTargetClass.getClass()) { candidateFound = true; if (mode == AdviceMode.PROXY) { //注册事务AOP的入口类InfrastructureAdvisorAutoProxyCreator,实际上这个AOP入口类起不了作用 AopConfigUtils.registerAutoProxyCreatorIfNecessary(registry); if ((Boolean) proxyTargetClass) { AopConfigUtils.forceAutoProxyCreatorToUseClassProxying(registry); return; } } } } if (!candidateFound && logger.isInfoEnabled()) { String name = getClass().getSimpleName(); logger.info(String.format(\"%s was imported but no annotations were found \" + \"having both 'mode' and 'proxyTargetClass' attributes of type \" + \"AdviceMode and boolean respectively. This means that auto proxy \" + \"creator registration and configuration may not have occurred as \" + \"intended, and components may not be proxied as expected. Check to \" + \"ensure that %s has been @Import'ed on the same class where these \" + \"annotations are declared; otherwise remove the import of %s \" + \"altogether.\", name, name, name)); } } } ProxyTransactionManagementConfiguration 负责注入事务相关的Bean, 包括： 事务切面Bean(BeanFactoryTransactionAttributeSourceAdvisor) TransactionAttributeSource(事务配置属性bean) TransactionInterceptor（事务拦截器bean）； @Configuration(proxyBeanMethods = false) @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public class ProxyTransactionManagementConfiguration extends AbstractTransactionManagementConfiguration { //明显是创建事务切面实例 BeanFactoryTransactionAttributeSourceAdvisor @Bean(name = TransactionManagementConfigUtils.TRANSACTION_ADVISOR_BEAN_NAME) @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public BeanFactoryTransactionAttributeSourceAdvisor transactionAdvisor( TransactionAttributeSource transactionAttributeSource, TransactionInterceptor transactionInterceptor) { BeanFactoryTransactionAttributeSourceAdvisor advisor = new BeanFactoryTransactionAttributeSourceAdvisor(); advisor.setTransactionAttributeSource(transactionAttributeSource); advisor.setAdvice(transactionInterceptor); if (this.enableTx != null) { advisor.setOrder(this.enableTx.getNumber(\"order\")); } return advisor; } @Bean @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public TransactionAttributeSource transactionAttributeSource() { return new AnnotationTransactionAttributeSource(); } 创建事务advice TransactionInterceptor @Bean @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public TransactionInterceptor transactionInterceptor(TransactionAttributeSource transactionAttributeSource) { TransactionInterceptor interceptor = new TransactionInterceptor(); interceptor.setTransactionAttributeSource(transactionAttributeSource); if (this.txManager != null) { interceptor.setTransactionManager(this.txManager); } return interceptor; } } 看到这就很清楚了，前者是注册AOP的入口类（这里注册的入口类依然是InfrastructureAdvisorAutoProxyCreator）， 后者则是创建事务AOP的组件的实例到IOC中，到这里相信不仅仅是对于事务的零配置，而是整个SpringBoot的零配置实现原理都心中有数了。 TransactionInterceptor public class TransactionInterceptor extends TransactionAspectSupport implements MethodInterceptor, Serializable { /** * Create a new TransactionInterceptor. * Transaction manager and transaction attributes still need to be set. * @see #setTransactionManager * @see #setTransactionAttributes(java.util.Properties) * @see #setTransactionAttributeSource(TransactionAttributeSource) */ public TransactionInterceptor() { } /** * Create a new TransactionInterceptor. * @param ptm the default transaction manager to perform the actual transaction management * @param tas the attribute source to be used to find transaction attributes * @since 5.2.5 * @see #setTransactionManager * @see #setTransactionAttributeSource */ public TransactionInterceptor(TransactionManager ptm, TransactionAttributeSource tas) { setTransactionManager(ptm); setTransactionAttributeSource(tas); } /** * Create a new TransactionInterceptor. * @param ptm the default transaction manager to perform the actual transaction management * @param tas the attribute source to be used to find transaction attributes * @see #setTransactionManager * @see #setTransactionAttributeSource * @deprecated as of 5.2.5, in favor of * {@link #TransactionInterceptor(TransactionManager, TransactionAttributeSource)} */ @Deprecated public TransactionInterceptor(PlatformTransactionManager ptm, TransactionAttributeSource tas) { setTransactionManager(ptm); setTransactionAttributeSource(tas); } /** * Create a new TransactionInterceptor. * @param ptm the default transaction manager to perform the actual transaction management * @param attributes the transaction attributes in properties format * @see #setTransactionManager * @see #setTransactionAttributes(java.util.Properties) * @deprecated as of 5.2.5, in favor of {@link #setTransactionAttributes(Properties)} */ @Deprecated public TransactionInterceptor(PlatformTransactionManager ptm, Properties attributes) { setTransactionManager(ptm); setTransactionAttributes(attributes); } @Override @Nullable public Object invoke(MethodInvocation invocation) throws Throwable { // Work out the target class: may be {@code null}. // The TransactionAttributeSource should be passed the target class // as well as the method, which may be from an interface. Class targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // Adapt to TransactionAspectSupport's invokeWithinTransaction... return invokeWithinTransaction(invocation.getMethod(), targetClass, invocation::proceed); } //--------------------------------------------------------------------- // Serialization support //--------------------------------------------------------------------- private void writeObject(ObjectOutputStream oos) throws IOException { // Rely on default serialization, although this class itself doesn't carry state anyway... oos.defaultWriteObject(); // Deserialize superclass fields. oos.writeObject(getTransactionManagerBeanName()); oos.writeObject(getTransactionManager()); oos.writeObject(getTransactionAttributeSource()); oos.writeObject(getBeanFactory()); } private void readObject(ObjectInputStream ois) throws IOException, ClassNotFoundException { // Rely on default serialization, although this class itself doesn't carry state anyway... ois.defaultReadObject(); // Serialize all relevant superclass fields. // Superclass can't implement Serializable because it also serves as base class // for AspectJ aspects (which are not allowed to implement Serializable)! setTransactionManagerBeanName((String) ois.readObject()); setTransactionManager((PlatformTransactionManager) ois.readObject()); setTransactionAttributeSource((TransactionAttributeSource) ois.readObject()); setBeanFactory((BeanFactory) ois.readObject()); } } TransactionInterceptor实现了MethodInterceptor；代理的入口及invoke 方法 @Override @Nullable public Object invoke(MethodInvocation invocation) throws Throwable { // Work out the target class: may be {@code null}. // The TransactionAttributeSource should be passed the target class // as well as the method, which may be from an interface. Class targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // Adapt to TransactionAspectSupport's invokeWithinTransaction... return invokeWithinTransaction(invocation.getMethod(), targetClass, invocation::proceed); } 这个方法本身没做什么事，主要是调用了父类的invokeWithinTransaction方法，注意最后一个参数，传入的是一个lambda表达式，而这个表达式中的调用的方法应该不陌生， 在分析AOP调用链时，就是通过这个方法传递到下一个切面或是调用被代理实例的方法，忘记了的可以回去看看。 TransactionAspectSupport#invokeWithinTransaction @Nullable protected Object invokeWithinTransaction(Method method, @Nullable Class targetClass, final InvocationCallback invocation) throws Throwable { // If the transaction attribute is null, the method is non-transactional. //获取事务属性类 AnnotationTransactionAttributeSource TransactionAttributeSource tas = getTransactionAttributeSource(); //获取方法上面有@Transactional注解的属性 final TransactionAttribute txAttr = (tas != null ? tas.getTransactionAttribute(method, targetClass) : null); final TransactionManager tm = determineTransactionManager(txAttr); //获取事务管理器 if (this.reactiveAdapterRegistry != null && tm instanceof ReactiveTransactionManager) { ReactiveTransactionSupport txSupport = this.transactionSupportCache.computeIfAbsent(method, key -> { if (KotlinDetector.isKotlinType(method.getDeclaringClass()) && KotlinDelegate.isSuspend(method)) { throw new TransactionUsageException( \"Unsupported annotated transaction on suspending function detected: \" + method + \". Use TransactionalOperator.transactional extensions instead.\"); } ReactiveAdapter adapter = this.reactiveAdapterRegistry.getAdapter(method.getReturnType()); if (adapter == null) { throw new IllegalStateException(\"Cannot apply reactive transaction to non-reactive return type: \" + method.getReturnType()); } return new ReactiveTransactionSupport(adapter); }); return txSupport.invokeWithinTransaction( method, targetClass, invocation, txAttr, (ReactiveTransactionManager) tm); } PlatformTransactionManager ptm = asPlatformTransactionManager(tm); final String joinpointIdentification = methodIdentification(method, targetClass, txAttr); if (txAttr == null || !(ptm instanceof CallbackPreferringPlatformTransactionManager)) { // Standard transaction demarcation with getTransaction and commit/rollback calls. TransactionInfo txInfo = createTransactionIfNecessary(ptm, txAttr, joinpointIdentification); Object retVal; try { // This is an around advice: Invoke the next interceptor in the chain. // This will normally result in a target object being invoked. // 调用proceed方法 retVal = invocation.proceedWithInvocation(); } catch (Throwable ex) { // target invocation exception //事务回滚 completeTransactionAfterThrowing(txInfo, ex); throw ex; } finally { cleanupTransactionInfo(txInfo); } if (retVal != null && vavrPresent && VavrDelegate.isVavrTry(retVal)) { // Set rollback-only in case of Vavr failure matching our rollback rules... TransactionStatus status = txInfo.getTransactionStatus(); if (status != null && txAttr != null) { retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); } } //事务提交 commitTransactionAfterReturning(txInfo); return retVal; } else { Object result; final ThrowableHolder throwableHolder = new ThrowableHolder(); // It's a CallbackPreferringPlatformTransactionManager: pass a TransactionCallback in. try { result = ((CallbackPreferringPlatformTransactionManager) ptm).execute(txAttr, status -> { TransactionInfo txInfo = prepareTransactionInfo(ptm, txAttr, joinpointIdentification, status); try { Object retVal = invocation.proceedWithInvocation(); if (retVal != null && vavrPresent && VavrDelegate.isVavrTry(retVal)) { // Set rollback-only in case of Vavr failure matching our rollback rules... retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); } return retVal; } catch (Throwable ex) { if (txAttr.rollbackOn(ex)) { // A RuntimeException: will lead to a rollback. if (ex instanceof RuntimeException) { throw (RuntimeException) ex; } else { throw new ThrowableHolderException(ex); } } else { // A normal return value: will lead to a commit. throwableHolder.throwable = ex; return null; } } finally { cleanupTransactionInfo(txInfo); } }); } catch (ThrowableHolderException ex) { throw ex.getCause(); } catch (TransactionSystemException ex2) { if (throwableHolder.throwable != null) { logger.error(\"Application exception overridden by commit exception\", throwableHolder.throwable); ex2.initApplicationException(throwableHolder.throwable); } throw ex2; } catch (Throwable ex2) { if (throwableHolder.throwable != null) { logger.error(\"Application exception overridden by commit exception\", throwableHolder.throwable); } throw ex2; } // Check result state: It might indicate a Throwable to rethrow. if (throwableHolder.throwable != null) { throw throwableHolder.throwable; } return result; } } 这个方法逻辑很清晰，一目了然，if里面就是对声明式事务的处理，先调用createTransactionIfNecessary方法开启事务，然后通过invocation.proceedWithInvocation调用下一个切面， 如果没有其它切面了，就是调用被代理类的方法，出现异常就回滚，否则提交事务，这就是Spring事务切面的执行过程。但是，我们主要要搞懂的就是在这些方法中是如何管理事务以及事务在多个方法之间是如何传播的。 "},"Chapter11/Propagation.html":{"url":"Chapter11/Propagation.html","title":"Spring事务传播","keywords":"","body":"Spring事务传播 Spring中七种Propagation类的事务属性详解 值 说明 NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 NESTED 支持当前事务，如果当前事务存在，则执行一个嵌套事务，如果当前没有事务，就新建一个事务 REQUIRED 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择 默认的传播行为 SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行 MANDATORY 支持当前事务，如果当前没有事务，就抛出异常 REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起 可以将其看成两大类，即是否支持当前事务： 支持当前事务（在同一个事务中）： PROPAGATION_REQUIRED：支持当前事务，如果不存在，就新建一个事务。 PROPAGATION_MANDATORY：支持当前事务，如果不存在，就抛出异常。 PROPAGATION_SUPPORTS：支持当前事务，如果不存在，就不使用事务。 不支持当前事务（不在同一个事务中）： PROPAGATION_NEVER：以非事务的方式运行，如果有事务，则抛出异常。 PROPAGATION_NOT_SUPPORTED：以非事务的方式运行，如果有事务，则挂起当前事务。 PROPAGATION_REQUIRES_NEW：新建事务，如果有事务，挂起当前事务（两个事务相互独立，父事务回滚不影响子事务）。 PROPAGATION_NESTED：如果当前事务存在，则嵌套事务执行（指必须依存父事务，子事务不能单独提交且父事务回滚则子事务也必须回滚，而子事务若回滚，父事务可以回滚也可以捕获异常）。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 实际上我们主要用的是PROPAGATION_REQUIRED默认属性，一些特殊业务下可能会用到PROPAGATION_REQUIRES_NEW以及PROPAGATION_NESTED。下面我会假设一个场景，并主要分析这三个属性。 Spring事务是如何传播的 其实之前有分析事务注解的解析过程，本质上是将事务封装为切面加入到AOP的执行链中，因此会调用到MethodInceptor的实现类的invoke方法， 而事务切面的Interceptor就是TransactionInterceptor，所以本篇直接从该类开始。 TransactionInterceptor#invoke @Override @Nullable public Object invoke(MethodInvocation invocation) throws Throwable { // Work out the target class: may be {@code null}. // The TransactionAttributeSource should be passed the target class // as well as the method, which may be from an interface. Class targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // Adapt to TransactionAspectSupport's invokeWithinTransaction... return invokeWithinTransaction(invocation.getMethod(), targetClass, invocation::proceed); } @Override @Nullable public Object invoke(MethodInvocation invocation) throws Throwable { // Work out the target class: may be {@code null}. // The TransactionAttributeSource should be passed the target class // as well as the method, which may be from an interface. Class targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); // Adapt to TransactionAspectSupport's invokeWithinTransaction... return invokeWithinTransaction(invocation.getMethod(), targetClass, invocation::proceed); } 这个方法本身没做什么事，主要是调用了父类的invokeWithinTransaction方法，注意最后一个参数，传入的是一个lambda表达式，而这个表达式中的调用的方法应该不陌生， 在分析AOP调用链时，就是通过这个方法传递到下一个切面或是调用被代理实例的方法，忘记了的可以回去看看。 TransactionAspectSupport#invokeWithinTransaction @Nullable protected Object invokeWithinTransaction(Method method, @Nullable Class targetClass, final InvocationCallback invocation) throws Throwable { // If the transaction attribute is null, the method is non-transactional. //获取事务属性类 AnnotationTransactionAttributeSource TransactionAttributeSource tas = getTransactionAttributeSource(); //TransactionAttributeSource#getTransactionAttribute 获取事务相关的信息(TransactionAttribute)，以注解型事务为例，看方法获取类上有没有标注@Transactional注解。 final TransactionAttribute txAttr = (tas != null ? tas.getTransactionAttribute(method, targetClass) : null); final TransactionManager tm = determineTransactionManager(txAttr); //获取事务管理器 if (this.reactiveAdapterRegistry != null && tm instanceof ReactiveTransactionManager) { ReactiveTransactionSupport txSupport = this.transactionSupportCache.computeIfAbsent(method, key -> { if (KotlinDetector.isKotlinType(method.getDeclaringClass()) && KotlinDelegate.isSuspend(method)) { throw new TransactionUsageException( \"Unsupported annotated transaction on suspending function detected: \" + method + \". Use TransactionalOperator.transactional extensions instead.\"); } ReactiveAdapter adapter = this.reactiveAdapterRegistry.getAdapter(method.getReturnType()); if (adapter == null) { throw new IllegalStateException(\"Cannot apply reactive transaction to non-reactive return type: \" + method.getReturnType()); } return new ReactiveTransactionSupport(adapter); }); return txSupport.invokeWithinTransaction( method, targetClass, invocation, txAttr, (ReactiveTransactionManager) tm); } //获取到 Spring 容器中配置的事务管理器 (PlatformTransactionManager)，后面就是真正的事务处理 PlatformTransactionManager ptm = asPlatformTransactionManager(tm); final String joinpointIdentification = methodIdentification(method, targetClass, txAttr); if (txAttr == null || !(ptm instanceof CallbackPreferringPlatformTransactionManager)) { // Standard transaction demarcation with getTransaction and commit/rollback calls. //创建事务信息(TransactionInfo)，里面包含事务管理器(PlatformTransactionManager) 以及事务相关信息(TransactionAttribute) TransactionInfo txInfo = createTransactionIfNecessary(ptm, txAttr, joinpointIdentification); Object retVal; try { // This is an around advice: Invoke the next interceptor in the chain. // This will normally result in a target object being invoked. // 调用proceed方法 retVal = invocation.proceedWithInvocation(); } catch (Throwable ex) { // target invocation exception //事务回滚 completeTransactionAfterThrowing(txInfo, ex); throw ex; } finally { cleanupTransactionInfo(txInfo); } if (retVal != null && vavrPresent && VavrDelegate.isVavrTry(retVal)) { // Set rollback-only in case of Vavr failure matching our rollback rules... TransactionStatus status = txInfo.getTransactionStatus(); if (status != null && txAttr != null) { retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); } } //事务提交 commitTransactionAfterReturning(txInfo); return retVal; } else { Object result; final ThrowableHolder throwableHolder = new ThrowableHolder(); // It's a CallbackPreferringPlatformTransactionManager: pass a TransactionCallback in. try { result = ((CallbackPreferringPlatformTransactionManager) ptm).execute(txAttr, status -> { TransactionInfo txInfo = prepareTransactionInfo(ptm, txAttr, joinpointIdentification, status); try { Object retVal = invocation.proceedWithInvocation(); if (retVal != null && vavrPresent && VavrDelegate.isVavrTry(retVal)) { // Set rollback-only in case of Vavr failure matching our rollback rules... retVal = VavrDelegate.evaluateTryFailure(retVal, txAttr, status); } return retVal; } catch (Throwable ex) { if (txAttr.rollbackOn(ex)) { // A RuntimeException: will lead to a rollback. if (ex instanceof RuntimeException) { throw (RuntimeException) ex; } else { throw new ThrowableHolderException(ex); } } else { // A normal return value: will lead to a commit. throwableHolder.throwable = ex; return null; } } finally { cleanupTransactionInfo(txInfo); } }); } catch (ThrowableHolderException ex) { throw ex.getCause(); } catch (TransactionSystemException ex2) { if (throwableHolder.throwable != null) { logger.error(\"Application exception overridden by commit exception\", throwableHolder.throwable); ex2.initApplicationException(throwableHolder.throwable); } throw ex2; } catch (Throwable ex2) { if (throwableHolder.throwable != null) { logger.error(\"Application exception overridden by commit exception\", throwableHolder.throwable); } throw ex2; } // Check result state: It might indicate a Throwable to rethrow. if (throwableHolder.throwable != null) { throw throwableHolder.throwable; } return result; } } 这个方法逻辑很清晰，一目了然，if里面就是对声明式事务的处理，先调用createTransactionIfNecessary方法开启事务，然后通过invocation.proceedWithInvocation调用下一个切面， 如果没有其它切面了，就是调用被代理类的方法，出现异常就回滚，否则提交事务，这就是Spring事务切面的执行过程。但是，我们主要要搞懂的就是在这些方法中是如何管理事务以及事务在多个方法之间是如何传播的。 Spring 事务的扩展 – TransactionSynchronization 数据库的事务是基于连接的，Spring 对于多个数据库操作的事务实现是基于 ThreadLocal。所以在事务操作当中不能使用多线程 我们回到正题， Spring 通过创建事务信息(TransactionInfo)，把数据库连接通过 TransactionSynchronizationManager#bindResource 绑定到 ThreadLocal 变量当中。 然后标注到一个事务当中的其它数据库操作就可以通过TransactionSynchronizationManager#getResource 获取到这个连接。 Spring 事务扩展 – @TransactionalEventListener 总结 本篇详细分析了事务的传播原理，另外还有隔离级别，这在Spring中没有体现，需要我们自己结合数据库的知识进行分析设置。最后我们还需要考虑声明式事务和编程式事务的优缺点，声明式事务虽然简单，但不适合用在长事务中， 会占用大量连接资源，这时就需要考虑利用编程式事务的灵活性了。总而言之，事务的使用并不是一律默认就好，接口的一致性和吞吐量与事务有着直接关系，严重情况下可能会导致系统崩溃。 "},"Chapter11/mvc.html":{"url":"Chapter11/mvc.html","title":"SpringMVC","keywords":"","body":"SpringMVC源码分析 DispatcherServlet 是否对下面的配置还有印象 SpringMVC org.SpringMVC.web.servlet.DispatcherServlet 1 上升到Springboot呢？ 我们可以在spring-boot-autoconfigure包的spring.factories下找到 org.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.MultipartAutoConfiguration,\\ org.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration,\\ springboot web类型加载的是AnnotationConfigServletWebServerApplicationContext容器 SpringMVC请求处理流程 引用spring in action上的一张图来说明了SpringMVC的核心组件和请求处理流程: DispatcherServlet是SpringMVC中的前端控制器(front controller)，负责接收request并将request转发给对应的处理组件。 HanlerMapping是SpringMVC中完成url到Controller映射的组件。DispatcherServlet接收request,然后从HandlerMapping查找处理request的controller. Cntroller处理request，并返回ModelAndView对象，Controller是SpringMVC中负责处理request的组件(类似于struts2中的Action)，ModelAndView是封装结果视图的组件。 ④ ⑤ ⑥：视图解析器解析ModelAndView对象并返回对应的视图给客户端。 "},"Chapter12/cloud.html":{"url":"Chapter12/cloud.html","title":"Part XII CLOUD篇","keywords":"","body":"第十二章 Spring Cloud CAP 分布式系统原理 常见分布式存储架构 SpringCloud 注册中心 Spring Cloud Stream ZipkinServer Spring Cloud Sleuth "},"Chapter12/cap.html":{"url":"Chapter12/cap.html","title":"CAP","keywords":"","body":"CAP CAP 定理的含义 分布式系统（distributed system）正变得越来越重要，大型网站几乎都是分布式的。 分布式系统的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理，也是理解分布式系统的起点。 本文介绍该定理。它其实很好懂，而且是显而易见的。 一、分布式系统的三个指标 1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标。 Consistency Availability Partition tolerance 它们的第一个字母分别是 C、A、P。 Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。 二、Partition tolerance 先看 Partition tolerance，中文叫做\"分区容错\"。 大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。 分区容错的意思是，区间通信可能失败。 比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信。 上图中，G1 和 G2 是两台跨区的服务器。G1 向 G2 发送一条消息，G2 可能无法收到。 系统设计的时候，必须考虑到这种情况。 一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是成立。 CAP 定理告诉我们，剩下的 C 和 A 无法同时做到。 三、Consistency Consistency 中文叫做\"一致性\"。 意思是，写操作之后的读操作，必须返回该值。举例来说，某条记录是 v0，用户向 G1 发起一个写操作，将其改为 v1。 接下来，用户的读操作就会得到 v1。这就叫一致性 问题是，用户有可能向 G2 发起读操作，由于 G2 的值没有发生变化，因此返回的是 v0。G1 和 G2 读操作的结果不一致，这就不满足一致性了。 为了让 G2 也能变为 v1，就要在 G1 写操作的时候，让 G1 向 G2 发送一条消息，要求 G2 也改成 v1。 这样的话，用户向 G2 发起读操作，也能得到 v1。 四、Availability Availability 中文叫做\"可用性\"，意思是只要收到用户的请求，服务器就必须给出回应。 用户可以选择向 G1 或 G2 发起读操作。不管是哪台服务器，只要收到请求，就必须告诉用户，到底是 v0 还是 v1，否则就不满足可用性。 五、Consistency 和 Availability 的矛盾 一致性和可用性，为什么不可能同时成立？答案很简单，因为可能通信失败（即出现分区容错）。 如果保证 G2 的一致性，那么 G1 必须在写操作时，锁定 G2 的读操作和写操作。只有数据同步后，才能重新开放读写。锁定期间，G2 不能读写，没有可用性不。 如果保证 G2 的可用性，那么势必不能锁定 G2，所以一致性不成立。 综上所述，G2 无法同时做到一致性和可用性。系统设计时只能选择一个目标。如果追求一致性，那么无法保证所有节点的可用性；如果追求所有节点的可用性，那就没法做到一致性。 六 RDBMS(mysql/oracle/sqlServer)>ACID A(Atomicity)原子性 C(Consistency)一致性 I(Isolation)独立性 D(Durability)持久性 NOSQL(redis/mongdb)========>CAP C(Consistency)强一致性 A(Availability)可用性 P(Partition tolerance)分区容错性 最多只能同时较号的满足两个 CAP理论的核心是：一个分布式系统不可能同时满足一致性、可用性、和分区容错行这三个需求，因此，根据CAP原理将NoSQL数据库分成了满足CA原则、满足CP原则和满足AP原则三大类； CA-单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。 CP-满足一致性，分区容错的系统，通常性能不是特别高 AP-满足可用性，分区容错性的系统，通常可能对一致性要求低一些 CAP的3进2 CAP理论就是说在分布式存储系统中，最多只能实现上面的两点。而由于当前的网络硬件肯定会出现延时丢包等问题，所以分区容错性是我们必须要实现的。 所以我们只能在一致性和可用性之间进行权衡，没有NoSQL系统能同时保证这三点。 "},"Chapter12/RegistrationCenter.html":{"url":"Chapter12/RegistrationCenter.html","title":"注册中心","keywords":"","body":"注册中心 Eureka的一些概念： 在Eureka的服务治理中，会涉及到下面一些概念： 服务注册：Eureka Client会通过发送REST请求的方式向Eureka Server注册自己的服务，提供自身的元数据，比如ip地址、端口、运行状况指标的url、主页地址等信息。Eureka Server接收到注册请求后，就会把这些元数据信息存储在一个双层的Map中。 服务续约：在服务注册后，Eureka Client会维护一个心跳来持续通知Eureka Server，说明服务一直处于可用状态，防止被剔除。Eureka Client在默认的情况下会每隔30秒发送一次心跳来进行服务续约。 服务同步：Eureka Server之间会互相进行注册，构建Eureka Server集群，不同Eureka Server之间会进行服务同步，用来保证服务信息的一致性。 获取服务：服务消费者（Eureka Client）在启动的时候，会发送一个REST请求给Eureka Server，获取上面注册的服务清单，并且缓存在Eureka Client本地，默认缓存30秒。同时，为了性能考虑，Eureka Server也会维护一份只读的服务清单缓存，该缓存每隔30秒更新一次。 服务调用：服务消费者在获取到服务清单后，就可以根据清单中的服务列表信息，查找到其他服务的地址，从而进行远程调用。Eureka有Region和Zone的概念，一个Region可以包含多个Zone，在进行服务调用时，优先访问处于同一个Zone中的服务提供者。 服务下线：当Eureka Client需要关闭或重启时，就不希望在这个时间段内再有请求进来，所以，就需要提前先发送REST请求给Eureka Server，告诉Eureka Server自己要下线了，Eureka Server在收到请求后，就会把该服务状态置为下线（DOWN），并把该下线事件传播出去。 服务剔除：有时候，服务实例可能会因为网络故障等原因导致不能提供服务，而此时该实例也没有发送请求给Eureka Server来进行服务下线，所以，还需要有服务剔除的机制。Eureka Server在启动的时候会创建一个定时任务，每隔一段时间（默认60秒），从当前服务清单中把超时没有续约（默认90秒）的服务剔除。 自我保护：既然Eureka Server会定时剔除超时没有续约的服务，那就有可能出现一种场景，网络一段时间内发生了异常，所有的服务都没能够进行续约，Eureka Server就把所有的服务都剔除了，这样显然不太合理。所以，就有了自我保护机制，当短时间内，统计续约失败的比例，如果达到一定阈值，则会触发自我保护的机制，在该机制下，Eureka Server不会剔除任何的微服务，等到正常后，再退出自我保护机制。 从这些概念中，就可以知道大体的流程，Eureka Client向Eureka Server注册，并且维护心跳来进行续约， 如果长时间不续约，就会被剔除。 Eureka Server之间进行数据同步来形成集群，Eureka Client从Eureka Server获取服务列表， 用来进行服务调用，Eureka Client服务重启前调用Eureka Server的接口进行下线操作。 然后调用父类的register方法注册，注册完后，会调用replicateToPeers方法， 把这个节点的注册信息告诉其它Eureka Server节点。 信息同步：每个Eureka Server同时也是Eureka Client，多个Eureka Server之间通过P2P复制的方式完成服务注册表的同步。同步时，被同步信息不会同步出去。也就是说有3个Eureka Server，Server1有新的服务信息时，同步到Server2后，Server2和Server3同步时，Server2不会把从Server1那里同步到的信息同步给Server3，只能由Server1自己同步给Server3。 Consul的流程 1、当 Producer 启动的时候，会向 Consul 发送一个 post 请求，告诉 Consul 自己的 IP 和 Port 2、Consul 接收到 Producer 的注册后，每隔10s（默认）会向 Producer 发送一个健康检查的请求，检验Producer是否健康 3、当 Consumer 发送 GET 方式请求 /api/address 到 Producer 时，会先从 Consul 中拿到一个存储服务 IP 和 Port 的临时表，从表中拿到 Producer 的 IP 和 Port 后再发送 GET 方式请求 /api/address 4、该临时表每隔10s会更新，只包含有通过了健康检查的 Producer 对比 Eureka VS ZooKeeper Eureka是基于AP原则构建，而ZooKeeper是基于CP原则构建；ZooKeeper基于CP，不保证高可用，如果zookeeper正在选举或者Zookeeper集群中半数以上机器不可用，那么将无法获得数据。 Eureka基于AP，能保证高可用，即使所有机器都挂了，也能拿到本地缓存的数据 。作为注册中心，其实配置是不经常变动的，只有发版和机器出故障时会变。 对于不经常变动的配置来说，CP是不合适的，而AP在遇到问题时可以用牺牲一致性来保证可用性，既返回旧数据，缓存数据。 所以理论上Eureka是更适合作注册中心。 而现实环境中大部分项目可能会使用ZooKeeper，那是因为集群不够大， 并且基本不会遇到用做注册中心的机器一半以上都挂了的情况，所以实际上也没什么大问题。 作为服务注册中心，Eureka比Zookeeper好在哪里 著名的CAP理论指出，一个分布式系统不可能同时满足C（一致性）A（可用性）P（分区容错性）。由于分区容错性P是分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。 因此 Zookeeper保证的是CP， Eureka则是AP Zookeeper保证CP 当向注册中心查询服务列表时，我们可用容忍注册中心返回的是几分钟以前的注册信息，但不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样一种情况，当master节点因为网络故障与其他节点时区联系时，剩余节点会重新进行leader选举。问题在于，选举leader的事件太长，30-120s，且选举期间整个zk集群都是不可用的，这就导师在选举期间注册服务瘫痪。在云部署的环境下，因网络问题使得zk集群时区master节点时较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导师的注册长期不可用是不能容忍的。 Eureka保证AP Eureka看明白了这一点，因此在设计时就优先保证可用性。Eureka各个节点都是平等的，几个节点挂掉不会影响正常节点的工作，剩余的节点依然可用提供注册和查询服务。而Eureka的客户端在向某个Eureka注册如果有时发生连接失败，则会自动切换至其它节点，只要有一台Eureka还在，救恩那个保证注册服务可用（保证可用性），只不过查到的信息可能不是最新的（不保证强一致性）。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时出现以下几种情况： Eureka不再从注册列表中移除因为长时间没收到心跳而应该过期的服务 Eureka仍然能够接受新服务的注册和查询请求，但是不会被通过不到其他节点上（即保证当前节点依然可用） 当网络稳定，当前实例新的注册信息会被同步到其他节点中 因此，Eureka可以很好的应对因网络故障导致部分节点失去联系的情况，而不会像zookeeper那样使整个服务瘫痪。 Consul VS Eureka Eureka 是一个服务发现工具。该体系结构主要是客户端/服务器，每个数据中心有一组 Eureka 服务器，通常每个可用区域一个。通常 Eureka 的客户使用嵌入式 SDK 来注册和发现服务。对于非本地集成的客户，官方提供的 Eureka 一些 REST 操作 API，其它语言可以使用这些 API 来实现对 Eureka Server 的操作从而实现一个非 jvm 语言的 Eureka Client。 Eureka 提供了一个弱一致的服务视图，尽可能的提供服务可用性。当客户端向服务器注册时，该服务器将尝试复制到其它服务器，但不提供保证复制完成。服务注册的生存时间（TTL）较短，要求客户端对服务器心跳检测。不健康的服务或节点停止心跳，导致它们超时并从注册表中删除。服务发现可以路由到注册的任何服务，由于心跳检测机制有时间间隔，可能会导致部分服务不可用。这个简化的模型允许简单的群集管理和高可扩展性。 Consul 提供了一些列特性，包括更丰富的健康检查，键值对存储以及多数据中心。Consul 需要每个数据中心都有一套服务，以及每个客户端的 agent，类似于使用像 Ribbon 这样的服务。Consul agent 允许大多数应用程序成为 Consul 不知情者，通过配置文件执行服务注册并通过 DNS 或负载平衡器 sidecars 发现。 Consul 提供强大的一致性保证，因为服务器使用 Raft 协议复制状态 。Consul 支持丰富的健康检查，包括 TCP，HTTP，Nagios / Sensu 兼容脚本或基于 Eureka 的 TTL。客户端节点参与基于 Gossip 协议的健康检查，该检查分发健康检查工作，而不像集中式心跳检测那样成为可扩展性挑战。发现请求被路由到选举出来的 leader，这使他们默认情况下强一致性。允许客户端过时读取取使任何服务器处理他们的请求，从而实现像 Eureka 这样的线性可伸缩性。 Consul 强烈的一致性意味着它可以作为领导选举和集群协调的锁定服务。Eureka 不提供类似的保证，并且通常需要为需要执行协调或具有更强一致性需求的服务运行 ZooKeeper。 Consul 提供了支持面向服务的体系结构所需的一系列功能。这包括服务发现，还包括丰富的运行状况检查，锁定，密钥/值，多数据中心联合，事件系统和 ACL。Consul 和 consul-template 和 envconsul 等工具生态系统都试图尽量减少集成所需的应用程序更改，以避免需要通过 SDK 进行本地集成。Eureka 是一个更大的 Netflix OSS 套件的一部分，该套件预计应用程序相对均匀且紧密集成。因此 Eureka 只解决了一小部分问题，可以和 ZooKeeper 等其它工具可以一起使用。 Consul 强一致性(C)带来的是： 服务注册相比 Eureka 会稍慢一些。因为 Consul 的 raft 协议要求必须过半数的节点都写入成功才认为注册成功 Leader 挂掉时，重新选举期间整个 Consul 不可用。保证了强一致性但牺牲了可用性。 Eureka 保证高可用(A)和最终一致性： 服务注册相对要快，因为不需要等注册信息 replicate 到其它节点，也不保证注册信息是否 replicate 成功 当数据出现不一致时，虽然 A, B 上的注册信息不完全相同，但每个 Eureka 节点依然能够正常对外提供服务，这会出现查询服务信息时如果请求 A 查不到，但请求 B 就能查到。如此保证了可用性但牺牲了一致性。 其它方面，eureka 就是个 servlet 程序，跑在 servlet 容器中; Consul 则是 go 编写而成。 自己的猜测 为什么consul是ac 而eureka是ap 因为consul支持k/v存储要保证数据的一直性所以 但是eureka之有服务注册。只有有服务就行。并不需要考虑一致性的问题 eureka底层原理 如上图所示，图中的这个名字叫做registry的CocurrentHashMap，就是注册表的核心结构。 从代码中可以看到，Eureka Server的注册表直接基于纯内存，即在内存里维护了一个数据结构。 各个服务的注册、服务下线、服务故障，全部会在内存里维护和更新这个注册表。 各个服务每隔30秒拉取注册表的时候，Eureka Server就是直接提供内存里存储的有变化的注册表数据给他们就可以了。 同样，每隔30秒发起心跳时，也是在这个纯内存的Map数据结构里更新心跳时间。 一句话概括：维护注册表、拉取注册表、更新心跳时间，全部发生在内存里！这是Eureka Server非常核心的一个点。 Eureka Server端多级缓存机制 而且Eureka Server为了避免同时读写内存数据结构造成的并发冲突问题，还采用了多级缓存机制来进一步提升服务请求的响应速度。 在拉取注册表的时候： 首先从ReadOnlyCacheMap里查缓存的注册表。 若没有，就找ReadWriteCacheMap里缓存的注册表。 如果还没有，就从内存中获取实际的注册表数据。 在注册表发生变更的时候： 会在内存中更新变更的注册表数据，同时过期掉ReadWriteCacheMap。 此过程不会影响ReadOnlyCacheMap提供人家查询注册表。 一段时间内（默认30秒），各服务拉取注册表会直接读ReadOnlyCacheMap 30秒过后，Eureka Server的后台线程发现ReadWriteCacheMap已经清空了，也会清空ReadOnlyCacheMap中的缓存 下次有服务拉取注册表，又会从内存中获取最新的数据了，同时填充各个缓存。 多级缓存机制的优点是什么？ 尽可能保证了内存注册表数据不会出现频繁的读写冲突问题。 并且进一步保证对Eureka Server的大量请求，都是快速从纯内存走，性能极高。 为方便大家更好的理解，同样来一张图，大家跟着图再来回顾一下这整个过程： 客户端的拉取 通过定时任务30秒拉取一次注册表,30秒发起一次心跳 "},"Chapter12/stream.html":{"url":"Chapter12/stream.html","title":"Spring Cloud Stream","keywords":"","body":"Spring Cloud Stream   SpringCloud Stream是SpringCloud对于消息中间件的进一步封装，它简化了开发人员对于消息队列的操作。 目前仅支持RabbitMQ与Kafka。 相关注释 @input: 设置输入信道名称。不设置参数，通道名称就默认为方法名 @output: 设置输出信道名称。不设置参数，通道名称就默认为方法名。 @StreamListener: 设置监听信道，用于接受来自消息队列的消息 @SendTo: 配合@StreamListener使用，在收到信息后发送反馈信息 @EnableBinding: 注解用于绑定一个或者多个接口作为参数 预设类 Sink: stream中接受消息的接口;默认input通道 Source: stream中输出消息的接口;默认output通道 Processor: stream中绑定输入输出的接口，主要用来讲发布者和订阅者绑定到一起 配置文件参数 #group 指的是queues中input.后面的.本来是随机生成.同一个组只会收到一份消息 spring.cloud.stream.bindings..group #对于RabbitMQ，destination 对应的是exchange，默认是通道的名字，group对应的是queue（带有前缀）。 # 对于kafka，destination 对应的是Topic，group就是对应的消费group。 spring.cloud.stream.bindings..destination   配置中值的是通道的名字;比如默认类Sink的通道名字是input 通道配置 @Output对MessageChannel @Input对应SubscribableChannel 默认属性   stream生成的exchang默认是topic模式。就是按照前缀匹配，发送消息给对应的队列。 * (星号): 可以（只能）匹配一个单词 # (井号): 可以匹配多个单词（或者零个） fanout: 广播模式，发送到所有的队列 direct:直传。完全匹配routingKey的队列可以收到消息。 "},"Chapter12/ZipkinServer.html":{"url":"Chapter12/ZipkinServer.html","title":"ZipkinServer","keywords":"","body":"ZipkinServer zipkin 服务端是一个单独的项目，不属于springcloud组件;github 项目地址 zipkin 下载latest released server 我下载的版本是zipkin-server-2.16.2-exec.jar 执行java -jar zipkin-server-2.16.2-exec.jar 访问http://localhost:9411 "},"Chapter12/sleuth.html":{"url":"Chapter12/sleuth.html","title":"Spring Cloud Sleuth","keywords":"","body":"Spring Cloud Sleuth   Spring Cloud Sleuth为Spring Cloud实现了分布式跟踪解决方案 相关术语 Span(跨度): 工作的基本单位。例如，发送RPC是一个新的跨度，就像发送响应到RPC一样。 Span是由一个唯一的64位ID来标识的，而另一个64位ID用于跟踪。 span还具有其他数据，如描述、时间戳事件、键值标注(标记)、导致它们的span的ID和进程ID(通常是IP地址) 可以启动和停止跨度，并跟踪其时间信息。 创建跨度后，必须在将来的某个时刻停止它。 启动跟踪的初始范围称为根跨度。 该范围的ID值等于跟踪ID。 Trace(痕迹): 一组span形成树状结构。 例如，如果运行分布式大数据存储，则可能由PUT请求形成跟踪 Annotation(注解): 用于及时记录存在的事件。常用的Annotation如下 cs(client send)：客户已发送。客户提出了请求。此注释表示Span的开始 sr(Server Received)：服务器收到;服务器端获得请求并开始处理它。从此时间戳中减去cs时间戳会显示网络延迟 ss(server send)：服务器已发送;在完成请求处理时（当响应被发送回客户端时）注释。从此时间戳中减去sr时间戳会显示服务器端处理请求所需的时间 cr(client received)：客户收到了;表示跨度的结束。客户端已成功收到服务器端的响应。从此时间戳中减去cs时间戳会显示客户端从服务器接收响应所需的全部时间。 下图显示了Span和Trace在系统中的外观以及Zipkin注释： 注释的每种颜色表示跨度（有七个跨度 - 从A到G）。 请考虑以下注释： Trace Id = X Span Id = D Client Sent 此注释表示当前跨度的Trace Id设置为X，Span Id设置为D.此外，还发生了Client Sent事件。 下图显示了跨度的父子关系： 为项目添加Sleuth   有两种模式第一种是只有Sleuth。另一种是with Zipkin via HTTP Only Sleuth (log correlation) org.springframework.cloud spring-cloud-dependencies ${release.train.version} pom import org.springframework.cloud spring-cloud-starter-sleuth We recommend that you add the dependency management through the Spring BOM so that you need not manage versions yourself. Add the dependency to spring-cloud-starter-sleuth. Sleuth with Zipkin via HTTP org.springframework.cloud spring-cloud-dependencies ${release.train.version} pom import org.springframework.cloud spring-cloud-starter-zipkin We recommend that you add the dependency management through the Spring BOM so that you need not manage versions yourself. Add the dependency to spring-cloud-starter-zipkin. "},"Chapter12/Ribbon.html":{"url":"Chapter12/Ribbon.html","title":"Ribbon","keywords":"","body":"Ribbon 详解 Ribbon的负载均衡策略及原理 负载均衡 Load Balance负载均衡是用于解决一台机器(一个进程)无法解决所有请求而产生的一种算法。像nginx可以使用负载均衡分配流量，ribbon为客户端提供负载均衡，dubbo服务调用里的负载均衡等等，很多地方都使用到了负载均衡。 使用负载均衡带来的好处很明显： 当集群里的1台或者多台服务器down的时候，剩余的没有down的服务器可以保证服务的继续使用使用了更多的机器保证了机器的良性使用，不会由于某一高峰时刻导致系统cpu急剧上升 负载均衡有好几种实现策略，常见的有： 随机 (Random) 轮询 (RoundRobin) 一致性哈希 (ConsistentHash) 哈希 (Hash) 加权（Weighted） ILoadBalance 负载均衡器 ribbon是一个为客户端提供负载均衡功能的服务，它内部提供了一个叫做ILoadBalance的接口代表负载均衡器的操作， 比如有添加服务器操作、选择服务器操作、获取所有的服务器列表、获取可用的服务器列表等等。 负载均衡器是从EurekaClient（EurekaClient的实现类为DiscoveryClient）获取服务信息，根据IRule去路由，并且根据IPing判断服务的可用性。 负载均衡器多久一次去获取一次从Eureka Client获取注册信息呢？在BaseLoadBalancer类下，BaseLoadBalancer的构造函数， 该构造函数开启了一个PingTask任务setupPingTask();，代码如下： public BaseLoadBalancer() { this.name = DEFAULT_NAME; this.ping = null; //配置默认的负载均衡规则 RoundRobinRule setRule(DEFAULT_RULE); setupPingTask(); lbStats = new LoadBalancerStats(DEFAULT_NAME); } setupPingTask()的具体代码逻辑， 它开启了ShutdownEnabledTimer执行PingTask任务，在默认情况下pingIntervalSeconds为10，即每10秒钟，向EurekaClient发送一次”ping”。 void setupPingTask() { if (canSkipPing()) { return; } if (lbTimer != null) { lbTimer.cancel(); } lbTimer = new ShutdownEnabledTimer(\"NFLoadBalancer-PingTimer-\" + name, true); lbTimer.schedule(new PingTask(), 0, pingIntervalSeconds * 1000); forceQuickPing(); } PingTask源码，即new一个Pinger对象，并执行runPinger()方法。 class PingTask extends TimerTask { public void run() { try { new Pinger(pingStrategy).runPinger(); } catch (Exception e) { logger.error(\"LoadBalancer [{}]: Error pinging\", name, e); } } } 查看Pinger的runPinger()方法，最终根据 pingerStrategy.pingServers(ping, allServers)来获取服务的可用性，如果该返回结果和之前相同，则不去向EurekaClient获取注册列表 ，如果不同则通知ServerStatusChangeListener或者changeListeners发生了改变，进行更新或者重新拉取。 public void runPinger() throws Exception { if (!pingInProgress.compareAndSet(false, true)) { return; // Ping in progress - nothing to do } // we are \"in\" - we get to Ping Server[] allServers = null; boolean[] results = null; Lock allLock = null; Lock upLock = null; try { /* * The readLock should be free unless an addServer operation is * going on... */ allLock = allServerLock.readLock(); allLock.lock(); allServers = allServerList.toArray(new Server[allServerList.size()]); allLock.unlock(); int numCandidates = allServers.length; results = pingerStrategy.pingServers(ping, allServers); final List newUpList = new ArrayList(); final List changedServers = new ArrayList(); for (int i = 0; i 完整过程是： LoadBalancerClient（RibbonLoadBalancerClient是实现类）在初始化的时候（execute方法），会通过ILoadBalance（BaseLoadBalancer是实现类）向Eureka注册中心获取服务注册列表，并且每10s一次向EurekaClient发送“ping”，来判断服务的可用性，如果服务的可用性发生了改变或者服务数量和之前的不一致，则从注册中心更新或者重新拉取。LoadBalancerClient有了这些服务注册列表，就可以根据具体的IRule来进行负载均衡。 IRule 路由 IRule接口代表负载均衡策略： RandomRule： 表示随机策略； RoundRobinRule： 表示轮询策略； WeightedResponseTimeRule： 继承了RoundRobinRule表示加权策略;开始的时候还没有权重列表，采用父类的轮询方式，有一个默认每30秒更新一次权重列表的定时任务，该定时任务会根据实例的响应时间来更新权重列表，choose方法做的事情就是，用一个(0,1)的随机double数乘以最大的权重得到randomWeight，然后遍历权重列表，找出第一个比randomWeight大的实例下标，然后返回该实例，代码略。 BestAvailableRule： 表示请求数最少策略等等; RoundRobinRule public class RoundRobinRule extends AbstractLoadBalancerRule { private AtomicInteger nextServerCyclicCounter; private static final boolean AVAILABLE_ONLY_SERVERS = true; private static final boolean ALL_SERVERS = false; private static Logger log = LoggerFactory.getLogger(RoundRobinRule.class); public RoundRobinRule() { nextServerCyclicCounter = new AtomicInteger(0); } public Server choose(ILoadBalancer lb, Object key) { if (lb == null) { log.warn(\"no load balancer\"); return null; } Server server = null; int count = 0; while (server == null && count++ reachableServers = lb.getReachableServers(); List allServers = lb.getAllServers(); int upCount = reachableServers.size(); int serverCount = allServers.size(); if ((upCount == 0) || (serverCount == 0)) { log.warn(\"No up servers available from load balancer: \" + lb); return null; } int nextServerIndex = incrementAndGetModulo(serverCount); server = allServers.get(nextServerIndex); if (server == null) { /* Transient. */ Thread.yield(); continue; } if (server.isAlive() && (server.isReadyToServe())) { return (server); } // Next. server = null; } if (count >= 10) { log.warn(\"No available alive servers after 10 tries from load balancer: \" + lb); } return server; } WeightedResponseTimeRule#choose public Server choose(ILoadBalancer lb, Object key) { if (lb == null) { return null; } Server server = null; while (server == null) { // get hold of the current reference in case it is changed from the other thread List currentWeights = accumulatedWeights; if (Thread.interrupted()) { return null; } List allList = lb.getAllServers(); int serverCount = allList.size(); if (serverCount == 0) { return null; } int serverIndex = 0; // last one in the list is the sum of all weights double maxTotalWeight = currentWeights.size() == 0 ? 0 : currentWeights.get(currentWeights.size() - 1); // No server has been hit yet and total weight is not initialized // fallback to use round robin if (maxTotalWeight = randomWeight) { serverIndex = n; break; } else { n++; } } server = allList.get(serverIndex); } if (server == null) { /* Transient. */ Thread.yield(); continue; } if (server.isAlive()) { return (server); } // Next. server = null; } return server; } 如何使用 创建具有负载均衡功能的RestTemplate实例 @Bean @LoadBalanced RestTemplate restTemplate() { return new RestTemplate(); 使用RestTemplate进行rest操作的时候，会自动使用负载均衡策略，它内部会在RestTemplate中加入LoadBalancerInterceptor这个拦截器，这个拦截器的作用就是使用负载均衡。 默认情况下会采用轮询策略，如果希望采用其它策略，则指定IRule实现，如： @Bean public IRule ribbonRule() { return new BestAvailableRule(); } 这种方式对Feign也有效。 Spring Cloud Ribbon配置详解 由于Ribbon中定义的每一个接口都有多种不同的策略，同时接口之间又有依赖关系，使得我们在用的时候不知道如何选择具体的实现策略已经组织他们的依赖关系。 Spring Cloud Ribbon的自动化配置恰好能解决这样的痛点，在引入Spring Cloud Ribbon的依赖之后，就能够自动化构建下面这些接口的实现。 IClientConfig：Ribbon的客户端配置，默认采用com. netflix.client.config. DefaultCl ientConfigImpl实现。 IRule: Ribbon 的负载均衡策略，默认采用com. netflix. loadbalancer.ZoneAvoidanceRule实现，该策略能够在多区域环境下选出最佳区域的实例进行访问。 IPing: Ribbon的实例检查策略,默认采用com. netflix. loadbalancer .NoOpPing实现，该检查策略是-一个特殊的实现，实际上它并不会检查实例是否可用，而是始终返回true,默认认为所有服务实例都是可用的。 ServerList:服务实例清单的维护机制，默认采用com. netflix.loadbalancer.ConfigurationBasedServerList实现。 ServerListFilter:服务实例清单过滤机制，默认采用org.springframework. cloud.netflix. ribbon. ZonePre ferenceServerListFilter实现，该策略能够优先过滤出与请求调用方处于同区域的服务实例。 ILoadBalancer : 负载均衡器，默认采用com. netflix. loadbalancer.ZoneAwareLoadBalancer实现，它具备了区域感知的能力。 上面这些自动化配置内容仅在没有引入SpringCloudEureka等服务治理框架时如此，在同时引入Eurcka和Ribbon依赖时， 自动化配置会有一些不同，后续我们会做详细的介绍。 通过自动化配置的实现，我们可以轻松地实现客户端负载均衡。同时，针对一些个性化需求，我们也可以方便地替换上面的这些默认实现。 只需在Spring Boot应用中创建对应的实现实例就能覆盖这些默认的配置实现。 比如下面的配置内容，由于创建了PingUrl实例，所以默认的NoOpPing就不会被创建。 @Bean public IPing ribbonPing(){ return new PingUrl(); } 改变负载均衡规则 @Bean public IRule ribbonRule(){ // 负载均衡规则改为随机 return new RandomRule(); } 另外，也可以通过使用@RibbonClient注解来实现更细粒度的客户端配置， 比如下面的代码实现了为helloService服务使用HelloConfig中的配置。 @RibbonClient(name = \"helloService\", configuration = HelloConfig.class) public class RibbonConfig { 参数配置 文件配置方式 对于Ribbon的参数配置通常有两种方式:全局配置以及指定客户端配置。 配置的属性有： NFLoadBalancerClassName : 配置ILoadBalancer的实现类 NFLoadBalancerRuleClassName : 配置IRule的实现类 NFLoadBalancerPingClassName : 配置IPing的实现类 NIWSServerListClassName: 配置ServerList的实现类 NIWSServerListFilterClassName: 配置ServerListtFilter的实现类 如配置life-base这个Ribbon Client的负载均衡规则，在yml文件中可以这样配置 life-base： ribbon： NFLoadBalancerRuleClassName：com.netflix.loadbalancer.RandomRule 全局配置的方式 只需使用ribbon.=格式进行配置即可。其中，代表了Ribbon 客户端配置的参数名， 则代表了对应参数的值。 比如，我们可以像下面这样全局配置Ribbon创建连接的超时时间: ribbon.ConnectTimeout 250 全局配置可以作为默认值进行设置，当指定客户端配置了相应key的值时，将覆盖全局配置的内容。 指定客户端的配置 方式采用.ribbon.=的格式进行配置。其中，和的含义同全局配置相同，而代表了客户端的名称， 如上文中我们在@RibbonClient 中指定的名称，也可以将它理解为是一-个服务名。为了方便理解这种配置方式，我们举一一个具体的例子: 假设，有一个服务消费者通过RestTemplate来访问hello-service服务的/hello接口， 这时我们会这样调用restTemplate.getForEntity (\"http://hello-service/hello\", String.class) .getBody();. 如果没有服务治理框架的帮助，我们需要为该客户端指定具体的实例清单，可以指定服务名来做详细的配置，具体如下: hello-service.ribbon.listOfServers-localhost: 8001. localhost:8002, 1ocalhost:8003 对于Ribbon参数的key以及value类型的定义,可以通过查看com.netflix.client.config.CommonClientConfigKey类获得更为详细的配置内容 ribbon的重试机制 默认是开启的，需要添加超时与重试的策略配置，如下： client-a: ribbon: ConnectTimeout: 250 # 连接超时时间（ms），默认值为250ms ReadTimeout: 2000 # 通信超时时间（ms），默认值为2000ms MaxAutoRetries: 1 #对第一次请求的服务的重试次数# 对同一实例重试的次数（单个集群节点服务重试的次数） MaxAutoRetriesNextServer: 1 #要重试的下一个服务的最大数量（不包括第一个服务）对同一服务不同实例重试次数（同一服务下集群个数的重试次数） OkToRetryOnAllOperations: true #是否对所有操作都进行重试 说明：这里配置的ConnectTimeout和ReadTimeout是当HTTP客户端使用的是HttpClient才生效，这个时间最终会被设置到HttpClient中。#在设置的时候需要结合hystrix的超时时间来综合考虑，针对使用的场景，设置太小会导致很多请求失败，设置太大会导致熔断控制变差。 # 开启熔断机制，超过六秒即开启熔断机制，网关内的时间排序：zuul的通信时间 > hystrix熔断时间 > retry重试时间 默认实现 接口 简述 默认实现 IClientConfig 定义ribbon中管理配置的接口 DefaultClientConfigImpl IRule 定义ribbon中负载均衡策略的接口 ZoneAvoidanceRule IPing 定义定期ping服务，检查可用性的接口 DummyPing ServerList 定义获取服务列表方法的接口 ConfigurationBasedServerList ServerListFilter 定义特定场景下，获取服务列表的方法接口 ZonePreferenceServerListFilter ILoadBalancer 定义负载均衡选择服务的核心方法接口 ZoneAwareLoadBalancer ServerListUpdater 为DynamicServerListLoadBalancer定义动态更新服务列表的接口 PollingServerListUpdater 全局配置 hystrix: command: #用于控制HystrixCommand的行为 default: execution: isolation: strategy: THREAD #控制HystrixCommand的隔离策略，THREAD->线程池隔离策略(默认)，SEMAPHORE->信号量隔离策略 thread: timeoutInMilliseconds: 1000 #配置HystrixCommand执行的超时时间，执行超过该时间会进行服务降级处理 interruptOnTimeout: true #配置HystrixCommand执行超时的时候是否要中断 interruptOnCancel: true #配置HystrixCommand执行被取消的时候是否要中断 timeout: enabled: true #配置HystrixCommand的执行是否启用超时时间 semaphore: maxConcurrentRequests: 10 #当使用信号量隔离策略时，用来控制并发量的大小，超过该并发量的请求会被拒绝 fallback: enabled: true #用于控制是否启用服务降级 circuitBreaker: #用于控制HystrixCircuitBreaker的行为 enabled: true #用于控制断路器是否跟踪健康状况以及熔断请求 requestVolumeThreshold: 20 #超过该请求数的请求会被拒绝 forceOpen: false #强制打开断路器，拒绝所有请求 forceClosed: false #强制关闭断路器，接收所有请求 requestCache: enabled: true #用于控制是否开启请求缓存 collapser: #用于控制HystrixCollapser的执行行为 default: maxRequestsInBatch: 100 #控制一次合并请求合并的最大请求数 timerDelayinMilliseconds: 10 #控制多少毫秒内的请求会被合并成一个 requestCache: enabled: true #控制合并请求是否开启缓存 threadpool: #用于控制HystrixCommand执行所在线程池的行为 default: coreSize: 10 #线程池的核心线程数 maximumSize: 10 #线程池的最大线程数，超过该线程数的请求会被拒绝 maxQueueSize: -1 #用于设置线程池的最大队列大小，-1采用SynchronousQueue，其他正数采用LinkedBlockingQueue queueSizeRejectionThreshold: 5 #用于设置线程池队列的拒绝阀值，由于LinkedBlockingQueue不能动态改版大小，使用时需要用该参数来控制线程数 实例配置只需要将全局配置中的default换成与之对应的key即可。 hystrix: command: HystrixComandKey: #将default换成HystrixComrnandKey execution: isolation: strategy: THREAD collapser: HystrixCollapserKey: #将default换成HystrixCollapserKey maxRequestsInBatch: 100 threadpool: HystrixThreadPoolKey: #将default换成HystrixThreadPoolKey coreSize: 10 配置文件中相关key的说明 HystrixComandKey对应@HystrixCommand中的commandKey属性； HystrixCollapserKey对应@HystrixCollapser注解中的collapserKey属性； HystrixThreadPoolKey对应@HystrixCommand中的threadPoolKey属性。 配置的优先级 配置文件的优先级 > java代码的配置方式 > netflix自定义的配置方式 与Eureka 整合 "},"Chapter12/Hystrix.html":{"url":"Chapter12/Hystrix.html","title":"Hystrix","keywords":"","body":"Hystrix Hystrix是什么 Hystrix是什么？官方给出的解释是：“In a distributed environment, inevitably some of the many service dependencies will fail. Hystrix is a library that helps you control the interactions between these distributed services by adding latency tolerance and fault tolerance logic. Hystrix does this by isolating points of access between the services, stopping cascading failures across them, and providing fallback options, all of which improve your system overall resiliency.” 译文：“在分布式环境中，某些服务之间的依赖失败是不可避免的。Hystrix作为一个类库，通过提供延迟控制和容错逻辑，帮助我们保证（个人认为control翻译成“保证”比“控制”更好一些）这些分布式服务之间的交互过程。Hystrix 通过“服务隔离”、阻止服务依赖导致的级联失败、并提供fallback处理，以提高我们系统的整体健壮性”。大体的意思是，通过hystrix提供的“服务隔离”、“容错处理”、“服务熔断”等方式保证了分布式环境下相互依赖的服务之间的健壮性。 Hystrix是Netflix开源的一个限流熔断的项目、主要有以下功能: 隔离（线程池隔离和信号量隔离）：限制调用分布式服务的资源使用，某一个调用的服务出现问题不会影响其他服务调用。 优雅的降级机制：超时降级、资源不足时(线程或信号量)降级，降级后可以配合降级接口返回托底数据。 融断：当失败率达到阀值自动触发降级(如因网络故障/超时造成的失败率高)，熔断器触发的快速失败会进行快速恢复。 缓存：提供了请求缓存、请求合并实现。支持实时监控、报警、控制（修改配置） 服务熔断 服务熔断的作用类似于我们家用的保险丝，当某服务出现不可用或响应超时的情况时，为了防止整个系统出现雪崩，暂时停止对该服务的调用。 服务降级 服务降级是从整个系统的负荷情况出发和考虑的，对某些负荷会比较高的情况，为了预防某些功能（业务场景）出现负荷过载或者响应慢的情况，在其内部暂时舍弃对一些非核心的接口和数据的请求，而直接返回一个提前准备好的fallback（退路）错误处理信息。这样，虽然提供的是一个有损的服务，但却保证了整个系统的稳定性和可用性。 熔断VS降级 相同点： 目标一致 都是从可用性和可靠性出发，为了防止系统崩溃； 用户体验类似 最终都让用户体验到的是某些功能暂时不可用； 不同点： 触发原因不同 服务熔断一般是某个服务（下游服务）故障引起，而服务降级一般是从整体负荷考虑； 服务熔断与降级之间存在本质的区别： 主动与被动的区别 主动与被动，主要体现在执行过程中，是否存在人为的主动干预，“服务熔断”是通过配置规则对下游服务进行健壮性分析而做出的被动熔断操作，不考虑是否为关键链路；“服务降级”是工作人员根据目前的运行状况执行的主动降级策略，用于保证主链路业务不受影响， 执行动机不同 执行动机的不同主要表现在“服务熔断”通过对下游服务的健壮性统计分析而做出的为了保证上游服务业务正常进行及资源使用的安全；“服务降级”是为了保证主链路业务不被边缘链路干扰而做出的决策，类似于“剪枝”的过程。 超时 超时（又称为 timeout）。服务响应的时间(RT)是我们在开发过程中衡量一个服务吞吐的主要指标，通常通过90线、95线、99线等规则进行对服务进行考量。在开发过程中，大家一定要非常非常关注每一个接口的RT，一般对于一个正常业务接口的请求，非高峰期情况下要求在100ms以内，高峰期情况下不能超过300ms,如果不在这个范围内的自身业务接口，就需要对这些接口进行合理的优化，除非有一些特殊接口，如依赖外部接口（其他公司的开源接口），这种严重依赖下游RT的接口可以选择通过做本地缓存的方式进行优化，如果无法做本地缓存只能只待下游提供方了（扯远了~）。 如果RT时间特别长会带来的后果是什么呢？大家可以想一下，对于java开发的服务来说，真正处理请求的是JVM（它不在本文讨论范围内），JVM是搭建在物理机上的一个容器平台，用来解析并执行java字节码文件，以实现对java的支持（早期java跨平台的主要因素），那么jvm启动后就像其他的应用程序一样是操作系统的一个进程，操作系统对于每一个进程在启动时都会分配一定的系统资源和可扩展资源限制（如：文件句柄数、CPU配额等），也就是说如果一个进行被分配的资源被全部占用时，新的请求就需要等待被占用资源的释放或者直接在操作系统层面被reject掉。 回到我们要讨论的RT，如果RT时间过长，那么直接导致每个请求占用的系统资源时间同样变长了，那么在服务接收了一定数量的请求后，其他请求就会被阻塞，此时严重影响了服务吞吐能力。所以，大家在做研发时一定要关注每一个接口的RT时间，切记不要忽略RT的抖动。RT的抖动可能暗示着服务存在着潜在风险。 资源隔离 在“超时”一节中我们聊到了资源使用问题，对，每一个进程的启动都将使用系统资源（文件句柄、CPU、IO、磁盘等）。上述资源都对于系统来说都是非常稀缺的资源，合理分配和利用操作系统的资源对于使用者来说是一个非常大的挑战。在以往的服务体系结构中，一个服务启动后，所有的“请求”共用整个服务被分配的所有资源，比如：Thread资源： 这种共享资源的方式带来很大问题，前文说道请求超时就是一个比较好的例子，当服务A对服务B存在依赖调用时，如果服务B业务处理存在大量超时，就会增加服务A中该接口占用大量的系统资源而没有及时进行释放，使得其他接口无法正常获取到系统资源而被阻塞，如果阻塞时间超过系统设置的超时时间则直接影响到了用户请求未被处理。针对上述问题，能够更合理的分配资源，hystrix提供了如何将资源进行隔离的方案：线程池隔离和信号量隔离。线程池隔离：即通过为每个下游配置一个固定配额的线程池，当服务调用下游时使用该线程池中的线程执行下游服务的调用，使用后再归还线程池，如果该下游服务出现超时或者不稳定状态时，上游服务将只占用仅有的配置的线程池数量，从而起到隔离作用。如下图4： Hystrix能做什么 Hystrix能做什么?要从其能够解决的问题才能够了解。 在复杂的分布式环境中，应用服务之间存在着错综复杂的依赖关系，依赖关系之间不可避免的会存在不稳定情况，如果主应用不与这些不稳定的外部依赖关系进行隔离，就存在被这些问题拖垮的风险。 例如：一个服务对下游服务存在30个依赖关系，下游每个服务稳定性保证99.99%（4个9，对于一般的公司已经是非常好的状态），那么对于30个 99.99%的服务来说组合起来，对于上游服务A来说其稳定性只能达到99.7%，如果有10亿次请求，那么就会存在300万次失败（现实中往往情况会更糟）。健康状况下服务依赖关系如下： 当存在一个下游依赖出现故障时，依赖状态如下 此时将间接导致上游服务的不可用，用户请求失败。随着时间的推移，失败的用户请越来越多，堆积在上游服务请求链路中 随着堆积的异常请求越来越多，到时上游服务处理请求的资源被完全占用，最终将导致上游服务的不可用。高并发情况下，一个单一的后端依赖服务的延迟将导致所有服务器实例瞬间达到饱和。任何的这种通过网络或者客户端类库访问的应用都将是产生失败的风险点。 比访问失败更糟糕的是，这些应用程序将导致请求的延迟增加，资源被占用，新的请求被加入到队列，最后将导致全系统的级联失败。 当使用第三方客户端类库——“黑盒子”时，其所有的实现细节、网络和资源配置都被隐藏在内部，且配置不尽相同时，我们根本无法对其变更进行实时监控时，将大大加重上述问题的最终结果。 这些象征着失败和延迟现象需要进行管理和隔离，以保证单一的失败依赖关系不会拖垮整个服务。 为此Hystrix做了如下事情： 防止任何单一依赖关系使用整个应用的全部用户线程资源（例如：tomcat） 降低负载、快速失败代替排队等待 提供失败处理程序（fallback）以保证用户请求正常返回 使用熔断技术（例如：隔板模式、泳道模式和断路器模式）以限制单一依赖关系带来的负面影响 通过近实时的度量统计、监控、告警，优化time-to-discovery（没想好具体怎么翻译：“及时发现问题”），大意应该是以保证hystrix能够及时发现出现问题的依赖关系。 Hystrix怎样实现它的目标 为了实现上述的目标，hystrix做了如下方案： 将所有的外部调用或者依赖项封装成一个HystrixCommand或者HystrixObservableCommand对象，并让 *Command对象分别在隔离的线程中执行。 允许用户为每一个command定义自定义的超时时间阈值，根据不同的依赖项定义用户自定义阈值，可以在某种程度上提高99.5线。 为每一个依赖项维护一个小的线程池（或者信号量），如果它满了，则对于该依赖项的新的请求将立即被拒绝，而不是进行排队等待资源的释放。 度量统计请求“成功数”、“失败数”、“超时数”以及“线程池拒绝数”等指标 如果对于某一个依赖项调用错误率超过阈值，则可以手动或者自动触发断路器断开一段时间（简称：熔断）。 当请求报错、超时、被拒绝或者熔断时，执行错误处理程序（fallback） 近实时的监控指标和配置的变更 当我们使用hystrix来包装下游依赖项后，上图中的显示的架构图变成了下面图所示，每一个依赖项彼此之间隔离，当出现延迟时并受资源的限制，并且在依赖项出现错误时， 能够决定响应是否通过容错处理程序（fallback）进行处理并返回结果。 执行过程（熔断原理） ResponseCache 首先，Response Cache功能，官方解释是：“如果针对某个command的Cache功能启用，任何请求在发起真正请求（调用熔断处理）之前，都将进行缓存命中判断，如果缓存命中，则返回缓存Response结果”。从描述中我们可以得出，该结果的返回需要依赖“Request Cache”功能的开启，至于RC功能将单独用一篇文章进行描述。 在进行RC判断之后，如果没有命中缓存，则调用CircuitBreaker进行判断进行Circuit健康检查。 Hystrix主要有4种调用方式： toObservable() 方法 ：未做订阅，只是返回一个Observable 。 observe() 方法 ：调用 #toObservable() 方法，并向 Observable 注册 rx.subjects.ReplaySubject 发起订阅。 queue() 方法 ：调用 #toObservable() 方法的基础上，调用：Observable#toBlocking() 和 BlockingObservable#toFuture() 返回 Future 对象 execute() 方法 ：调用 #queue() 方法的基础上，调用 Future#get() 方法，同步返回 #run() 的执行结果。 主要的执行逻辑： 每次调用创建一个新的HystrixCommand,把依赖调用封装在run()方法中.（其实后面有一个ResponseCache Available判断过程） 执行execute()/queue做同步或异步调用. 判断熔断器(circuit-breaker)是否打开,如果打开跳到步骤8,进行降级策略,如果关闭进入步骤. 判断线程池/队列/信号量是否跑满，如果跑满进入降级步骤8,否则继续后续步骤. 调用HystrixCommand的run方法.运行依赖逻辑,依赖逻辑调用超时,进入步骤8. 判断逻辑是否调用成功。返回成功调用结果；调用出错，进入步骤8. 计算熔断器状态,所有的运行状态(成功, 失败, 拒绝,超时)上报给熔断器，用于统计从而判断熔断器状态. getFallback()降级逻辑。以下四种情况将触发getFallback调用： run()方法抛出非HystrixBadRequestException异常。 run()方法调用超时 熔断器开启拦截调用 线程池/队列/信号量是否跑满 没有实现getFallback的Command将直接抛出异常，fallback降级逻辑调用成功直接返回，降级逻辑调用失败抛出异常. 返回执行成功结果 CircuitBreaker 如果CircuitBreaker处于断开状态，则直接fast-fail调用fallback进入错误处理程序。如果CircuitBreaker处于关闭状态，则继续向下执行。其实在整个过程中还存在一个中间状态——half open（半开）。当CircuitBreaker处于半开状态时， 将允许一个或者部分请求进行“尝试”执行下游调用，如果调用成功则将断路器进行关闭，否则进行断开。整个过程可以归纳成如下图 上图中主要描述了Hystrix断路器运行时的三个状态：打开、关闭、半开。并描述了每个状态下对于Request请求是否可以被执行的相关内容。接下来，向大家描述一下三个状态如何进行切换（即图中1、2、3）的，即什么条件下断路器才会进行状态的切换，以及切换条件的数据如何进行收集。 首先，断路器切换条件—下游服务调用执行状态（Successes（成功数）、Failures（失败数）、Rejections（拒绝数）、timeouts（超时数）），也就是说断路器的切换由上述四个指标共同决定。这里只解释一下Rejections（拒绝数），大家千万不要认为是所有的拒绝数量，切记只包含在CircuitBreaker关闭状态下线程池或者信号量满了的时候的拒绝数，不包含处于打开状态或者半开状态下的计数。 其次，断路器如何进行监控指标的数据收集，以及如何处罚状态变更？在这一节主要向大家进行简单的概述，在实现一节带着大家通过阅读源码来了解Hystrix 断路器内部如何进行数据收集。此处，只需要大家能够理解断路器在不断接收到监控数据后，通过自身Hystrix配置判断条件是否满足，如果满足则进行断开操作。 执行过程： 判断通过断路器的流量是否满足了某个阈值（配置属性） 判断该command的执行错误率是否超出了某个阈值（配置属性） 如果满足了上述两个条件，则断路器处于打开状态 当断路器处于“打开”状态时，所有针对于该command的执行都将被短路 在某个固定时间段后（配置属性），断路器处于“半开”状态，并且接下来的一个请求被允许通过断路器，被执行，如果请求失败，断路器重新回到打开状态并且持续一个新的固定时间段（配置属性），如果请求成功，则断路器变成“关闭”状态。 这里简单介绍一下四个指标的统计过程： Hystrix 断路器会按照用户的配置信息（总的时间区间/BucketsNum数量）设置每个Buckets的监控时间区间 每个Bucket内部存储四个指标（成功数、失败数、超时数、拒绝数），这四个指标仅为该Bucket所覆盖的时间区间内的统计数据 执行到下一个时间区间时，创建新的bucket，并将该时间区间的数据记录到对应的指标上。 当Bucket数量==用户设置的Buckets数量时，将第一个Bucket进行舍弃操作，并创建新的Bucket。即任何一个时间点断路器内部最多存在BucketsNum个Bucket。 Hystrix熔断相关配置 hystrix.command.default.circuitBreaker.enabled: 默认值true;该属性主要用于控制断路器功能是否生效，是否能够在断路器打开状态下路请求的执行。 hystrix.command.default.circuitBreaker.requestVolumeThreshold: 默认值 20；该属性主要表示在一个监控窗口内能够触发断路器打开的最小请求量阈值。听着很绕口是吧 ，意思就是如果监控最小窗口为1s，那么在这一秒内只有请求量超过这个阈值时，才进行错误率的判断，如果没有超过这个阈值，那么将永远都不会以触发熔断 hystrix.command.default.circuitBreaker.sleepWindowInMilliseconds： 默认值 5000；该属性表示断路器打开后，直接执行拒绝请求时间段，在此时间区间内，任何请求都将被直接Fast-Fail。只有当过了该时间段后，circuitBreaker就进入了“半开”状态，才允许一个请求通过尝试执行下游调用。如果该请求在执行过程中失败了，则circuitBreaker进入“打开”状态，如果成功了，则直接进入“关闭”。 hystrix.command.default.circuitBreaker.errorThresholdPercentage： 默认值 50； 该属性表示熔断触发条件，即触发熔断时错误百分比。一旦打到这个触发比例，断路器就进入“打开”状态，但前提是，需要整体的请求流量能够满足设置的容量——requestVolumeThreshold。 hystrix.command.default.circuitBreaker.forceClosed | forceOpen ： 默认值 false；两个属性是相互斥的两个属性，分别表示：强制处于“打开”状态、强制处于“关闭”状态，当设置成强制“打开状态”时，所有的请求将被直接Fast-Fail，阻断所有请求。当设置成强制“关闭”状态时，表示Hystrix将忽略所有的错误。 metrics.rollingStats.timeInMilliseconds： 该属性主要用来设置熔断监控的时间区间，整个熔断过程将实时统计每个时间区间内的监控数据。并计算该时间区间内的错误率（errorPercentage），如果错误率达到用户设置的阈值，则断路器将进行熔断处理，以此实现断路器熔断功能。在这里大家有时很容易产生一个疑问，这个时间区间时连续的吗？回答是：不是连续的，整个hystrix对于时间区间控制为rolling（旋转的），如下图所示: 当系统时钟通过一个Time时，就会判断n == numBuckets是否相等，如果相等，则第一个Bucket将被舍弃，同时创建一个新的Bucket，将这个Time时间内的所有维度在该Bucket内进行计数统计。 metrics.rollingStats.numBuckets: 该属性表示的上文中提到的Bucket数量 熔断器原理分析 熔断器部分主要依赖两点，状态变化和滑动窗口+bucket的统计机制 通过HystrixCommandProperties.circuitBreakerRequestVolumeThreshold())设置临界值 HystrixCommandProperties.circuitBreakerErrorThresholdPercentage():允许错误超过临界值的百分比 Then the circuit-breaker transitions from CLOSED to OPEN. While it is open, it short-circuits all requests made against that circuit-breaker. After some amount of time (HystrixCommandProperties.circuitBreakerSleepWindowInMilliseconds()), the next single request is let through (this is the HALF-OPEN state). If the request fails, the circuit-breaker returns to the OPEN state for the duration of the sleep window. If the request succeeds, the circuit-breaker transitions to CLOSED and the logic in 1. takes over again. 基于滑动窗口和桶来实现。滑动窗口相当于一个时间窗，在这个时间窗中会有很多请求进入，如果每进入一个请求就统计一次这个时间窗的请求总数会有较低的性能，所以将这个时间窗口分成 十份，每份是一个桶，时间窗滑动到每个桶结束点时就统计一下这个桶内的请求数，就可以统计出整个窗口的请求数了。bucket(桶)一般是窗口的N分之一。 整个Hystrix的流程图： 全局配置 hystrix: command: #用于控制HystrixCommand的行为 default: execution: isolation: strategy: THREAD #控制HystrixCommand的隔离策略，THREAD->线程池隔离策略(默认)，SEMAPHORE->信号量隔离策略 thread: timeoutInMilliseconds: 1000 #配置HystrixCommand执行的超时时间，执行超过该时间会进行服务降级处理 interruptOnTimeout: true #配置HystrixCommand执行超时的时候是否要中断 interruptOnCancel: true #配置HystrixCommand执行被取消的时候是否要中断 timeout: enabled: true #配置HystrixCommand的执行是否启用超时时间 semaphore: maxConcurrentRequests: 10 #当使用信号量隔离策略时，用来控制并发量的大小，超过该并发量的请求会被拒绝 fallback: enabled: true #用于控制是否启用服务降级 circuitBreaker: #用于控制HystrixCircuitBreaker的行为 enabled: true #用于控制断路器是否跟踪健康状况以及熔断请求 requestVolumeThreshold: 20 #超过该请求数的请求会被拒绝 forceOpen: false #强制打开断路器，拒绝所有请求 forceClosed: false #强制关闭断路器，接收所有请求 requestCache: enabled: true #用于控制是否开启请求缓存 collapser: #用于控制HystrixCollapser的执行行为 default: maxRequestsInBatch: 100 #控制一次合并请求合并的最大请求数 timerDelayinMilliseconds: 10 #控制多少毫秒内的请求会被合并成一个 requestCache: enabled: true #控制合并请求是否开启缓存 threadpool: #用于控制HystrixCommand执行所在线程池的行为 default: coreSize: 10 #线程池的核心线程数 maximumSize: 10 #线程池的最大线程数，超过该线程数的请求会被拒绝 maxQueueSize: -1 #用于设置线程池的最大队列大小，-1采用SynchronousQueue，其他正数采用LinkedBlockingQueue queueSizeRejectionThreshold: 5 #用于设置线程池队列的拒绝阀值，由于LinkedBlockingQueue不能动态改版大小，使用时需要用该参数来控制线程数 实例配置只需要将全局配置中的default换成与之对应的key即可。 hystrix: command: HystrixComandKey: #将default换成HystrixComrnandKey execution: isolation: strategy: THREAD collapser: HystrixCollapserKey: #将default换成HystrixCollapserKey maxRequestsInBatch: 100 threadpool: HystrixThreadPoolKey: #将default换成HystrixThreadPoolKey coreSize: 10 配置文件中相关key的说明 HystrixComandKey对应@HystrixCommand中的commandKey属性； HystrixCollapserKey对应@HystrixCollapser注解中的collapserKey属性； HystrixThreadPoolKey对应@HystrixCommand中的threadPoolKey属性。 配置的优先级 配置文件的优先级 > java代码的配置方式 > netflix自定义的配置方式 在1.2.5版本的HystrixFeignConfiguration中 默认值为true在1.3.1版本的HystrixFeignConfiguration中默认值为false feign.hystrix.enabled = true 可以启用它。 Spring cloud 启动断路器（@EnableCircuitBreaker）后可能会报如下错误 NoClassDefFoundError: com/netflix/hystrix/exception/HystrixRuntimeException 解决办法修改如下版本 --> com.netflix.hystrix hystrix-core 1.5.12 ribbon: MaxAutoRetries: 1 #最大重试次数，当Eureka中可以找到服务，但是服务连不上时将会重试 MaxAutoRetriesNextServer: 1 #切换实例的重试次数 OkToRetryOnAllOperations: true # 对所有的操作请求都进行重试，如果是get则可以，如果是post,put等操作没有实现幂等的情况下是很危险的 ConnectTimeout: 250 #请求连接的超时时间 ReadTimeout: 1000 #请求处理的超时时间 根据上面的参数计算重试的次数： MaxAutoRetries+MaxAutoRetriesNextServer+(MaxAutoRetries *MaxAutoRetriesNextServer) 即重试3次 则一共产生4次调用 。 如果在重试期间，时间超过了hystrix的超时时间，便会立即执行熔断，fallback。所以要根据上面配置的参数计算hystrix的超时时间，使得在重试期间不能达到hystrix的超时时间，不然重试机制就会没有意义 先说明一下，不要用下面这种公式来配置hystrix的超时时间，不要，不要，重要的事情说3次： hystrix超时时间的计算： (1 + MaxAutoRetries + MaxAutoRetriesNextServer) ReadTimeout 即按照以上的配置 hystrix的超时时间应该配置为 （1+1+1）3=9秒 正确的计算公式： ReadTimeout+（MaxAutoRetries * ReadTimeout），如果配置的有：MaxAutoRetriesNextServer这个属性，看下面例子： 这个hystrix的超时时间怎么配置： ReadTimeout+（MaxAutoRetries ReadTimeout）+ ReadTimeout+（MaxAutoRetries ReadTimeout）= 4000ms 那么hystrix的超时时间为：>4000ms 如果MaxAutoRetriesNextServer=1，就加1个： ReadTimeout+（MaxAutoRetries ReadTimeout）+ ReadTimeout+（MaxAutoRetries ReadTimeout）= 4000ms 如果MaxAutoRetriesNextServer=2，就加2个： ReadTimeout+（MaxAutoRetries ReadTimeout）+ ReadTimeout+（MaxAutoRetries ReadTimeout）+ ReadTimeout+（MaxAutoRetries * ReadTimeout）= 6000ms 先算出所有ribbon的超时时间+重试时间的总和，那么hystrix的超时时间大于总和，就可以保证ribbon在重试过程中不会被hystrix熔断。 当ribbon超时后且hystrix没有超时，便会采取重试机制。当OkToRetryOnAllOperations设置为false时，只会对get请求进行重试。如果设置为true，便会对所有的请求进行重试，如果是put或post等写操作，如果服务器接口没做幂等性，会产生不好的结果，所以OkToRetryOnAllOperations慎用。如果不配置ribbon的重试次数，默认会重试一次 。 一、生产环境线程池的配置的问题 生产环境里面，一个是线程池的大小怎么设置，timeout时长如果设置不合理的话，会出现很多问题 1.超时时间太短低于服务的平均响应时间，正常的功能都无法执行。 2.超时时间太长，浪费系统资源，影响系统的性能 二、如何才能获取线程池合适的配置 接口访问量的不同也导致了线程池配置的不同 在生产环境中部署一个短路器，一开始需要将一些关键配置设置的大一些，比如Timeout超时时长，线程池大小，或信号量容量（这是Hystrix资源隔离的两种方式：线程池和信号量），然后逐渐优化这些配置，直到在一个生产系统中运作良好 1.一开始先不要设置Timeout超时时长，默认就是1000ms，也就是1s 2.一开始也不要设置线程池大小，默认就是10 3.直接部署Hystrix到生产环境，如果运行的很良好，那么就让它这样运行好了 4.让Hystrix应用，24小时运行在生产环境中 5.依赖标准的监控和报警机制来捕获到系统的异常运行情况 6.在24小时之后，看一下调用延迟的占比，以及流量，来计算出让短路器生效的最小的配置数字 7.直接对Hystrix配置进行热修改，然后继续在Hystrix Dashboard上监控 8.看看修改配置后的系统表现有没有改善 三、配置经验 下面是根据系统表现优化和调整线程池大小，队列大小，信号量容量，以及timeout超时时间的经验： 1.假设对一个依赖服务的高峰调用QPS是每秒30次一开始如果默认的线程池大小是10。 2.我们想的是，理想情况下，每秒的高峰访问次数 99%的访问延时 + buffer = 30 0.2 + 4 = 10线程，10个线程每秒处理30次访问应该足够了，每个线程处理3次访问 3.此时，我们合理的timeout设置应该为300ms，也就是99.5%的访问延时，计算方法是，因为判断每次访问延时最多在250ms（TP99如果是200ms的话），再加一次重试时间50ms，就是300ms，感觉也应该足够了 4.因为如果timeout设置的太多了，比如400ms，比如如果实际上，在高峰期，还有网络情况较差的时候，可能每次调用要耗费350ms，也就是达到了最长的访问时长 5.那么每个线程处理2个请求，就会执行700ms，然后处理第三个请求的时候，就超过1秒钟了，此时会导致线程池全部被占满，都在处理请求。这个时候下一秒的30个请求再进来了，那么就会导致线程池已满，拒绝请求的情况，就会调用fallback降级机制 6.因此对于短路器来说，timeout超时一般应该设置成TP99.5，比如设置成300ms，那么可以确保说，10个线程，每个线程处理3个访问，每个访问最多就允许执行300ms，过时就timeout了。这样才能保证说每个线程都在1s内执行完，才不会导致线程池被占满，然后后续的请求过来大量的reject 7.对于线程池大小来说，一般应该控制在10个左右，20个以内，最少5个，不要太多，也不要太少。大家可能会想，每秒的高峰访问次数是30次，如果是300次，甚至是3000次，30000次呢？？？30000 * 0.2 = 6000 + buffer = 6100，一个服务器内一个线程池给6000个线程把。如果你一个依赖服务占据的线程数量太多的话，会导致其他的依赖服务对应的线程池里没有资源可以用了 6000 / 20 = 300台虚拟机也是ok的 虚拟机，4个cpu core，4G内存，虚拟机，300台 物理机，十几个cpu core，几十个G的内存，5~8个虚拟机，300个虚拟机 = 50台物理机 你要真的说是，你的公司服务的用户量，或者数据量，或者请求量，真要是到了每秒几万的QPS，3万QPS，60 * 3 = 180万访问量，1800，1亿8千，1亿，10个小时，10亿的访问量 app，系统几十台服务器去支撑，我觉得很正常QPS每秒在几千都算多的了 PS：Hystrix资源隔离线程池有一个配置，A、B线程池都有有20个线程，但某一时刻A线程池访问较多，B线程池20个线程并未全部使用，可以配置让A借用B线程池的线程 "},"Chapter12/feign.html":{"url":"Chapter12/feign.html","title":"feign","keywords":"","body":"feign Feign是一个声明式的Web服务客户端。这使得Web服务客户端的写入更加方便 要使用Feign创建一个界面并对其进行注释。它具有可插拔注释支持，包括Feign注释和JAX-RS注释。Feign还支持可插拔编码器和解码器。 Spring Cloud添加了对Spring MVC注释的支持，并在Spring Web中使用默认使用的HttpMessageConverters。Spring Cloud集成Ribbon和Eureka以在使用Feign时提供负载均衡的http客户端。 feign核心类介绍 feign.Feign.Builder 设置发送http请求的相关参数，比如http客户端，重试策略，编解码，超时时间等等 feign.Contract.Default 解析接口方法的元数据，构建http请求模板 feign.Client 发送http请求客户端，默认实现feign.Client.Default，使用的是java.net包实现的 Retryer 重试，默认实现feign.Retryer.Default，超时延迟100ms开始重试，每隔1s重试一次，重试4次 Options 超时时间，默认连接超时10s，读超时60s feign.codec.Encoder 编码器 feign.codec.Decoder 解码器 RequestInterceptor 请求拦截器，可以在发送http请求之前执行此拦截器 feign.Contract 接口以及方法元数据解析器 以上参数都可以自己扩展 HardCodedTarget 定于目标接口和url ReflectiveFeign 生成动态代理类，基于jdk的动态代理实现 feign.InvocationHandlerFactory.Default 接口方法统一拦截器创建工厂 FeignInvocationHandler 接口统一方法拦截器 ParseHandlersByName 解析接口方法元数据 SynchronousMethodHandler.Factory 接口方法的拦截器创建工厂 SynchronousMethodHandler 接口方法的拦截器，真正拦截的核心，这里真正发起http请求，处理返回结果 通过feign.Feign.Builder为我们设置http请求的相关参数，比如http客户端，重试策略，编解码，超时时间，这里都是面向接口编程实现的，我们很容易的进行扩展，比如http客户端，可以使用java原生的实现，也可以使用apache httpclient，亦可以使用okHttpClient，自己喜欢就好，其他属性亦是如此，由此看出feign的设计具有非常好的可扩展性。 ReflectiveFeign内部使用了jdk的动态代理为目标接口生成了一个动态代理类，这里会生成一个InvocationHandler(jdk动态代理原理)统一的方法拦截器，同时为接口的每个方法生成一个SynchronousMethodHandler拦截器，并解析方法上的 元数据，生成一个http请求模板。 当发起方法调用的时候，被统一的方法拦截器FeignInvocationHandler拦截，再根据不同的方法委托给不同的SynchronousMethodHandler拦截器处理。 根据每次方法调用的入参生成http请求模板，如果设置了http请求拦截器，则先经历拦截器的处理，再发起真正的http请求，得到结果后会根据方法放入返回值进行反序列化，最后返回给调用方。 如果发生了异常，会根据重试策略进行重试。 feign也整合了Hystrix，实现熔断降级的功能，其实也很简单，上面的分析我们知道了feign在方法调用的时候会经过统一方法拦截器FeignInvocationHandler的处理，而HystrixFeign则是使用了HystrixInvocationHandler代替，在方法调用的时候进行Hystrix的封装，这里需要特别说明下： Hystrix有超时时间，feign本身也有超时时间，正常来说Hystrix的超时间要大于feign的超时时间，如果是小于的话，Hytrix已经超时了，feign再等待就已经没有意义了。 再则就是feign超时的话会触发重试操作，此时要是Hytrix发生超时异常返回了，但这并不会切断feign的继续操作，什么意思呢？假设Hytrix的超时时间为1s，feign设置的超时时间为2s，而真正业务操作需要耗时3s，这时Hytrix超时异常返回，而后feign也会发生超时异常，但是feign会根据超时策略继续进行重试操作，并不会因为Hytrix的中断而中断。 所以Hytrix的超时时间一般要大于feign的总超时时间，如这个例子中要设置2 * 5(默认重试次数4 + 1)=10s，公式就是Hytrix的超时间=feign的超时时间 乘以 (feign的重试次数 + 1) spring-cloud-feign 开启@EnableFeignClients注解到底给我们做了什么事情呢？ 扫描EnableFeignClients注解上的配置信息，注册默认的配置类，这个配置类是对所有feignclient的都是生效的，即为全局的配置。 扫描带有@FeignClient注解的接口，并注册配置类（此时的配置类针对当前feignclient生效）和FeignClientFactoryBean，此bean实现了FactoryBean接口，我们知道spring有两种类型的bean对象，一种是普通的bean，另一种则是工厂bean（FactoryBean），它返回的其实是getObject方法返回的对象（更多关于FactoryBean的相关信息请查看spring官方文档）。getObject方法就是集成原生feign的核心方法，当spring注入feignclient接口时，getObject方法会被调用，得到接口的代理类。 备注：在FeignClient指定配置类时，切记不要被spring容器扫描到，不然会对全局生效。 自动加载配置类 自动加载配置类FeignAutoConfiguration，FeignClientsConfiguration，FeignRibbonClientAutoConfiguration， 这三个类为feign提供了所有的配置类，默认情况下所加载的类情况： feign.Feign.Builder 当引入了Hytrix并开启参数feign.hystrix.enabled=true后，则会加载feign.hystrix.HystrixFeign.Builder，此时feign就具备降级熔断的功能了。 feign.Client 此实现类的加载分两种情况： 使用url方式：feign.Client.Default，使用java原生的方式（java.net包）发起http请求，也可以自己扩展。 使用name方式：LoadBalancerFeignClient，集成了ribbon，实现服务发现与负载均衡，但是真正发起http请求还是java原生的方式 此处是一扩展点，当我们引入ApacheHttpClient时，http客户端就会使用apache的httpClient；当我们引入OkHttpClient时，http客户端就会使用okhttp3.OkHttpClient。 feign.Retryer 默认Retryer.NEVER_RETRY，不进行重试，这里也可以自己实现Retryer接口实现自己的重试策略，但是feign在集成了ribbon的情况下，最好保持默认不进行重试，因为ribbon也会有重试策略，如果feign也开启重试，容易产生混乱；其实在低版本中spring-cloud-feing重试默认并不是NEVER_RETRY，可能spring-cloud-feing也意识到这样做的问题，所以在D版中改成NEVER_RETRY了。 feign.Request.Options 默认设置连接超时时间是10，读超时时间是60s。这里也可以更改，分两种情况： 使用url方式：必须通过这个参数来设置，才生效 @Configuration public class MyConfig { @Bean public Request.Options options(){ Request.Options o = new Options(1000, 1000); return o; } } 然后在注解上@FeignClient指定： @FeignClient(name=\"\",url=\"\",configuration= {MyConfig.class}) 注意此类不能被spring容器扫描到，否则会对全局生效。你也可以通过注解@EnableFeignClients来全局指定 @EnableFeignClients(defaultConfiguration=MyConfig.class) 使用name方式：此时已经集成了ribbon，可以使用以下配置来设置，如果你此时也配置了Options，以下配置会被覆盖 # 对所有的feignclient生效 ribbon.ReadTimeout=10000 ribbon.ConnectTimeout=2000 # 对指定的feignclien生效 [feignclientName].ribbon.ReadTimeout=10000 [feignclientName].ribbon.ConnectTimeout=2000 如果开启Hytrix，hytrix也有超时时间设置，但是hytrix是封装在feign基础之上的，上文已有分析。 hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds=10000 你也可以关闭hytrix的超时时间 hystrix.command.default.execution.timeout.enabled=false feign.codec.Decoder 解码器，默认使用了HttpMessageConverters来实现 feign.codec.Encoder 编码器，默认使用了HttpMessageConverters来实现 feign.Contract 默认提供springmvc的注解解析，支持@RequestMapping，@RequestBody，@RequestParam，@PathVariable 最后三种也是spring-cloud-feign替换原生feign的默认实现，对springMVC的相关支持 Options对象经过getClientConfig(options, clientName) 方法，就从10s的连接时间变成了1s,60s的读取时间也变成了1s,上面源码分析可以知道： 如果我们配置了feign的超时时间，那么就会以我们配置的时间为准，如果没有配置，那么就取ribbon的超时时间，2者只能有一个生效，而ribbon默认超时时间是1秒 feign或者ribbon所配置的超时时间，最终都是在HttpUrlConnection中生效 那么，我们如何修改feign的配置时间呢？ @Configuration public class Config { @Bean public Request.Options feignRequestOptions() { Request.Options options = new Request.Options(2000, TimeUnit.MILLISECONDS, 2000, TimeUnit.MILLISECONDS, true); return options; } } Hystrix的熔断时间要大于Feign或Ribbon的connectTimeout+readTimeout feign的超时时间如果要在application.yml中配置，改如何配置呢： feign: hystrix: enabled: true client: config: default: #连接到目标的时间，此处会收到注册中心启动中的影响。设置为3秒钟，如果注册中心有明显的不在线，基本是毫秒级熔断拒绝 connectTimeout: 3000 #获取目标连接后执行的最长时间，设置为32秒，即服务最长时 readTimeout: 32000 那么有人要问了，如果feign和ribbon同时配置，那么以谁的为准 最后总结： hystrix的熔断时间配置通过yml配置没法生效，可以通过配置类的方法来修改，feign的超时时间可以通过代码或者yml配置，ribbon的超时时间可以通过yml来配置 feign和ribbon的超时时间只能二选一，只要feign的超时时间配置了，就以feign的为准，hystrix的超时时间要大于feign/riboon的connectTimeout+readTimeout的和 最后一张图来总结： 目前的学习和测试结果来看： 单纯的 Ribbon + Hystrix 搭配使用时，配置是最灵活的，两者没有相互干涉，可以自由定义 commandKey 来实现超时时间的配置 Feign + Hystrix 搭配时，由于 Feign 封装了 Hystrix 所需的 commandKey，我们不能自定义，所以同一个 FeignClient 下的服务接口不能方便的统一配置，如果有相应的业务需求，或许只能对每个特殊的接口方法做独立的超时配置（找到新方法的话再回来更新） Zuul + Hystrix 搭配时，和上述的情况相反，能对不同服务实例做不同的超时配置，但不能再细化到服务下的具体接口方法 "},"Chapter12/zuul.html":{"url":"Chapter12/zuul.html","title":"zuul","keywords":"","body":"ZUUL #网关 zuul: ribbon: eager-load: enabled: true #zuul饥饿加载 host: max-total-connections: 200 max-per-route-connections: 20 #以下两个配置也是解决zuul超时的 #和使用ribbon.ReadTimeout的区别是，如果路由配置使用service-id的方式，那么ribbon.ReadTimeout生效，如果使用url的方式，此配置生效 connect-timeout-millis: 10000 socket-timeout-millis: 10000 #配置Ribbon的超时时间 ribbon: ReadTimeout: 10000 ConnectTimeout: 10000 # MaxAutoRetries: 1 # MaxAutoRetriesNextServer: 1 hystrix: command: default: execution: isolation: strategy: SEMAPHORE #配置hystrix的超时时间 thread: timeoutInMilliseconds: 20000 访问的时候提示错误： The Hystrix timeout of 20000ms for the command uaa-service is set lower than the combination of the Ribbon read and connect timeout, 40000ms. 分析： Ribbon 总超时时间计算公式如下： ribbonTimeout = (RibbonReadTimeout + RibbonConnectTimeout) * (MaxAutoRetries + 1) * (MaxAutoRetriesNextServer + 1) 其中，MaxAutoRetries 默认为0，MaxAutoRetriesNextServer 默认为1，所以我这里的具体值为：（10000+10000）（0+1）（1+1）=40000。 而 Hystrix 超时时间为 20000 为什么不自己是手动重新加载Locator.dorefresh？非要用事件去刷新？这牵扯到内部的zuul内部组件的工作流程，不仅仅是Locator本身的一个变量，具体想要了解的还得去看源码。下面我们就来分析下zuul的源码看看为什么要这样做？ 要讲清楚zuul的事件驱动模型，还得知道spring的事件驱动模型，因为zuul的实现正是利用了spring的事件驱动模型实现的。下面看看spring提供的事件模型图： 在zuul中有这样一个实现了ApplicationListener的监听器ZuulRefreshListener ，代码如下 private static class ZuulRefreshListener implements ApplicationListener { @Autowired private ZuulHandlerMapping zuulHandlerMapping; private HeartbeatMonitor heartbeatMonitor = new HeartbeatMonitor(); @Override public void onApplicationEvent(ApplicationEvent event) { if (event instanceof ContextRefreshedEvent || event instanceof RefreshScopeRefreshedEvent || event instanceof RoutesRefreshedEvent) { this.zuulHandlerMapping.setDirty(true); } else if (event instanceof HeartbeatEvent) { if (this.heartbeatMonitor.update(((HeartbeatEvent) event).getValue())) { this.zuulHandlerMapping.setDirty(true); } } } } 由此可知在发生ContextRefreshedEvent和RoutesRefreshedEvent事件时会执行this.zuulHandlerMapping.setDirty(true); public void setDirty(boolean dirty) { this.dirty = dirty; if (this.routeLocator instanceof RefreshableRouteLocator) { ((RefreshableRouteLocator) this.routeLocator).refresh(); } } 这样在spring容器启动完成后就刷新了路由规则。因此我们如果要主动刷新路由规则，只需要发布一个RoutesRefreshedEvent事件即可，代码如下 public void refreshRoute() { RoutesRefreshedEvent routesRefreshedEvent = new RoutesRefreshedEvent(routeLocator); this.publisher.publishEvent(routesRefreshedEvent); logger.info(\"刷新了路由规则......\"); } 我们知道spring-cloud-zuul是依赖springMVC来注册路由的，而springMVC又是在建立在servlet之上的（这里微服务专家杨波老师写过一篇文章讲述其网络模型，可以参考看看），在servlet3.0之前使用的是thread per connection方式处理请求，就是每一个请求需要servlet容器为其分配一个线程来处理，直到响应完用户请求，才被释放回容器线程池，如果后端业务处理比较耗时，那么这个线程将会被一直阻塞，不能干其他事情，如果耗时请求比较多时，servlet容器线程将被耗尽，也就无法处理新的请求了，所以Netflix还专门开发了一个熔断的组件Hystrix 来保护这样的服务，防止其因后端的一些慢服务耗尽资源，造成服务不可用。 不过在servlet3.0出来之后支持异步servlet了，可以把业务操作放到独立的线程池里面去， 这样可以尽快释放servlet线程，springMVC本身也支持异步servlet了 "},"Chapter13/architecture.html":{"url":"Chapter13/architecture.html","title":"Part XIII 架构篇","keywords":"","body":"第十三章 架构篇 SAAS MicroService 高并发情况下，我们系统是如何支撑大量的请求的 微服务与SOA区别 "},"Chapter13/saas.html":{"url":"Chapter13/saas.html","title":"SAAS","keywords":"","body":"SAAS 云服务的三种模式 IaaS（Infrastructure as a Service基础设施即服务） 是指把IT基础设施作为一种服务通过网络对外提供。 在这种服务模型中，用户不用自己构建一个数据中心， 而是通过租用的方式来使用基础设施服务 ，包括服务器、存储和网络等。在使用模式上， IaaS与传统的主机托管有相似之处， 但是在服务的灵活性、扩展性和成本等方面IaaS具有很强的优势 PaaS是（Platform as a Service）的缩写，是指平台即服务。 把服务器平台作为一种服务提供的商业模式， 通过网络进行程序提供的服务称之为SaaS(Software as a Service)， 而云计算时代相应的服务器平台或者开发环境作为服务进行提供就成为了PaaS(Platform as a Service) SaaS即Software-as-a-Service（软件即服务） 是随着互联网技术的发展和应用软件的成熟， 在21世纪开始兴起的一种完全创新的软件应用模式。 传统模式下，厂商通过License将软件产品部署到企业内部多个客户终端实现交付。 SaaS定义了一种新的交付方式，也使得软件进一步回归服务本质。 企业部署信息化软件的本质是为了自身的运营管理服务， 软件的表象是一种业务流程的信息化，本质还是第一种服务模式， SaaS改变了传统软件服务的提供方式， 减少本地部署所需的大量前期投入，进一步突出信息化软件的服务属性， 或成为未来信息化软件市场的主流交付模式。 企业开发 服务器(cpu、存储、网络) 操作系统、数据库、运行环境 应用 云服务 IAAS PAAS SAAS 基础设施即服务 平台即服务 应用即服务 阿里云 阿里云 微盟 多租户saas平台的数据库方案 一、对多租户的理解 多租户定义：多租户技术或称多重租赁技术，简称SaaS， 是一种软件架构技术，是实现如何在多用户环境下（此处的多用户一般是面向企业用户）共用相同的系统或程序组件， 并且可确保各用户间数据的隔离性。 简单讲：在一台服务器上运行单个应用实例， 它为多个租户（客户）提供服务。从定义中我们可以理解： 多租户是一种架构，目的是为了让多用户环境下使用同一套程序， 且保证用户间数据隔离。那么重点就很浅显易懂了， 多租户的重点就是同一套程序下实现多用户数据的隔离。 对于实现方式，我们下面会讨论到。 在了解详细一点：在一个多租户的结构下， 应用都是运行在同样的或者是一组服务器下， 这种结构被称为“单实例”架构（Single Instance）， 单实例多租户。多个租户的数据是保存在相同位置， 依靠对数据库分区来实现隔离操作。既然用户都在运行相同的应用实例，服务运行在服务供应商的服务器上，用户无法去进行定制化的操作， 所以这对于对该产品有特殊需要定制化的客户就无法适用，所以多租户适合通用类需求的客户。那么缺点来了，多租户下无法实现用户的定制化操作。 在翻阅多租户的资料时，还有一个名词与之相对应， 那就是单租户SaaS架构（也被称作多实例架构（Multiple Instance））。单租户架构与多租户的区别在于，单租户是为每个客户单独创建各自的软件应用和支撑环境。 单租户SaaS被广泛引用在客户需要支持定制化的应用场合，而这种定制或者是因为地域，抑或是他们需要更高的安全控制。通过单租户的模式， 每个客户都有一份分别放在独立的服务器上的数据库和操作系统，或者使用强的安全措施进行隔离的虚拟网络环境中。因为本篇主要是讨论多租户，所以单租户的相关知识就简单了解一下，不做过多的阐述了。 二、多租户数据隔离的三种方案 在当下云计算时代，多租户技术在共用的数据中心以单一系统架构与服务提供多数客户端相同甚至可定制化的服务，并且仍可以保障客户的数据隔离。目前各种各样的云计算服务就是这类技术范畴，例如阿里云数据库服务（RDS）、阿里云服务器等等。 多租户在数据存储上存在三种主要的方案，分别是： 1. 独立数据库 这是第一种方案，即一个租户一个数据库，这种方案的用户数据隔离级别最高，安全性最好，但成本较高。 优点： 为不同的租户提供独立的数据库，有助于简化数据模型的扩展设计，满足不同租户的独特需求；如果出现故障，恢复数据比较简单。 缺点： 增多了数据库的安装数量，随之带来维护成本和购置成本的增加。 这种方案与传统的一个客户、一套数据、一套部署类似，差别只在于软件统一部署在运营商那里。如果面对的是银行、医院等需要非常高数据隔离级别的租户，可以选择这种模式，提高租用的定价。如果定价较低，产品走低价路线，这种方案一般对运营商来说是无法承受的。 2. 共享数据库，独立 Schema 这是第二种方案，即多个或所有租户共享Database，但是每个租户一个Schema（也可叫做一个user）。底层库比如是：DB2、ORACLE等，一个数据库下可以有多个SCHEMA 优点： 为安全性要求较高的租户提供了一定程度的逻辑数据隔离，并不是完全隔离；每个数据库可支持更多的租户数量。 缺点： 如果出现故障，数据恢复比较困难，因为恢复数据库将牵涉到其他租户的数据； 如果需要跨租户统计数据，存在一定困难。 3. 共享数据库，共享 Schema，共享数据表 这是第三种方案，即租户共享同一个Database、同一个Schema，但在表中增加TenantID多租户的数据字段。这是共享程度最高、隔离级别最低的模式。 即每插入一条数据时都需要有一个客户的标识。这样才能在同一张表中区分出不同客户的数据。 优点： 三种方案比较，第三种方案的维护和购置成本最低，允许每个数据库支持的租户数量最多。 缺点： 隔离级别最低，安全性最低，需要在设计开发时加大对安全的开发量； 数据备份和恢复最困难，需要逐表逐条备份和还原。 如果希望以最少的服务器为最多的租户提供服务，并且租户接受牺牲隔离级别换取降低成本，这种方案最适合。 在SaaS实施过程中，有一个显著的考量点，就是如何对应用数据进行设计，以支持多租户，而这种设计的思路，是要在数据的共享、安全隔离和性能间取得平衡。 三、选择合理的实现模式 衡量三种模式主要考虑的因素是隔离还是共享。 成本角度因素 隔离性越好，设计和实现的难度和成本越高，初始成本越高。共享性越好，同一运营成本下支持的用户越多，运营成本越低。 安全因素 要考虑业务和客户的安全方面的要求。安全性要求越高，越要倾向于隔离。 从租户数量上考虑 主要考虑下面一些因素 系统要支持多少租户？上百？上千还是上万？可能的租户越多，越倾向于共享。 平均每个租户要存储数据需要的空间大小。存贮的数据越多，越倾向于隔离。 每个租户的同时访问系统的最终用户数量。需要支持的越多，越倾向于隔离。 是否想针对每一租户提供附加的服务，例如数据的备份和恢复等。这方面的需求越多， 越倾向于隔离 技术储备 共享性越高，对技术的要求越高。 四、建议和意见 试用版和正式版 试用版由于是免费的使用共享数据库和共享数据表 正式版及用户花钱购买了我们就可以选择第二种甚至是第一种 RBAC 介绍 (权限) RBAC是基于角色的访问控制（Role-Based Access Control） 在RBAC中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。 这样管理都是层级相互依赖的， 权限赋予给角色，而把角色又赋予用户，这样的权限设计很清楚，管理起来很方便。 常见的认证机制 HTTP Basic Auth Cookie Auth OAuth OAUTH协议为用户资源的授权提供了一个安全的、开放而又简易的标准。与以往的授权方式不同之处是OAUTH的授权不会使第三方触及到用户的帐号信息（如用户名与密码），即第三方无需使用用户的用户名与密码就可以申请获得该用户资源的授权，因此OAUTH是安全的。oAuth是Open Authorization的简写。 Token Auth 支持跨域访问: Cookie是不允许垮域访问的，这一点对Token机制是不存在的，前提是传输的用户认证信息通过HTTP头传输 无状态(也称：服务端可扩展行): Token机制在服务端不需要存储session信息，因为Token 自身包含了所有登录用户的信息， 只需要在客户端的cookie或本地介质存储状态信息 更适用CDN: 可以通过内容分发网络请求你服务端的所有资料（如：javascript，HTML,图片等），而你的服务端只要提供API即可 去耦: 不需要绑定到一个特定的身份验证方案。Token可以在任何地方生成，只要在你的API被调用的时候，你可以进行Token生成调用即可 更适用于移动应用: 当你的客户端是一个原生平台（iOS, Android，Windows 8等）时，Cookie是不被支持的（你需要通过Cookie容器进行处理）， 这时采用Token认证机制就会简单得多。 CSRF: 因为不再依赖于Cookie，所以你就不需要考虑对CSRF（跨站请求伪造）的防范。 TOKEN 认证过程： 客户端使用用户名跟密码请求登录 服务端收到请求，去验证用户名与密码 验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端 客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里或者 Local Storage 里 客户端每次向服务端请求资源的时候需要带着服务端签发的 Token 服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向客户端返回请求的数据 因为token是被签名的，所以我们可以认为一个可以解码认证通过的token是由我们系统发放的，其中带的信息是合法有效的 saas.docx "},"Chapter13/MicroService.html":{"url":"Chapter13/MicroService.html","title":"微服务","keywords":"","body":"微服务 微服务架构的问题？4步曲 这么多服务，客户端如何去访问 这么多服务，服务之间怎么通信 这么多服务，怎么治理 这么多服务，服务挂了怎么办 服务是好的，网络出问题了怎么办 SpringCloud SpringCloud 不是一种技术，是一个生态或者说是一站式解决方案 spring cloud netflix 一站式解决方案 api: ---zuul feign -- httpClient 同步阻塞 服务注册与发现 eureka 熔断机制 Hystrix dubbo + zookeeper ApI ： 没有 借助被人的或者自己写 Dubbo 异步非阻塞 RPC 服务注册与发现 zookeeper 熔断机制 没有借用 hystrix springcloud alibab 下一代服务标准 目前又推出了新一代的服务标准Service Mesh （服务网格） Istio 作为目前众多 Service Mesh 中最闪耀的新星 解决了网络问题 "},"Chapter14/permission.html":{"url":"Chapter14/permission.html","title":"Part XIV 权限篇","keywords":"","body":"第十四章 权限篇 shiroSession "},"Chapter14/shiroSession.html":{"url":"Chapter14/shiroSession.html","title":"shiroSession","keywords":"","body":"shiro中出现不同请求session不同的现象 Shiro提供了三个默认实现： DefaultSessionManager：DefaultSecurityManager使用的默认实现，用于JavaSE环境； ServletContainerSessionManager：DefaultWebSecurityManager使用的默认实现，用于Web环境，其直接使用Servlet容器的会话； DefaultWebSessionManager：用于Web环境的实现，可以替代ServletContainerSessionManager，自己维护着会话，直接废弃了Servlet容器的会话管理。 遇到的坑： 在web环境下用ini文件配置shiro时，如果不指定SecurityManager时， shiro会默认创建DefaultSecurityManager对象，这样会导致在web环境下，发不同的请求生成的session不同,导致登录功能失效。 因为DefaultSessionManager在源码中是和本地线程绑定的，而web环境中一个请求会创建一个线程，从而导致session都不同。 当使用DefaultWebSessionManager时，shiro中的session和web中的session是一致的。 而DefaultSessionManager的session和web中的session是不一致的。 完整解决办法 ShiroConfiguration.java import com.fire.shiro.AuthRealm; import com.fire.shiro.RedisManager; import com.fire.shiro.RedisSessionDAO; import lombok.extern.slf4j.Slf4j; import org.apache.shiro.mgt.SecurityManager; import org.apache.shiro.spring.LifecycleBeanPostProcessor; import org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor; import org.apache.shiro.spring.web.ShiroFilterFactoryBean; import org.apache.shiro.web.mgt.DefaultWebSecurityManager; import org.apache.shiro.web.servlet.SimpleCookie; import org.apache.shiro.web.session.mgt.DefaultWebSessionManager; import org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator; import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.DependsOn; @Slf4j public abstract class ShiroConfiguration { abstract ShiroFilterFactoryBean shiroFilter(SecurityManager manager); // 配置自定义的权限登录器 @Bean public AuthRealm authRealm() { AuthRealm authRealm = new AuthRealm(); return authRealm; } // 配置核心安全事务管理器 @Bean @ConditionalOnMissingBean public SecurityManager securityManager(AuthRealm authRealm, RedisSessionDAO redisSessionDAO) { //, EhCacheManager ehcacheManager DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setRealm(authRealm); // 缓存授权 //manager.setCacheManager(ehcacheManager); DefaultWebSessionManager sessionManager = new DefaultWebSessionManager(); // sessionManager.setGlobalSessionTimeout(7200000); sessionManager.setSessionDAO(redisSessionDAO); sessionManager.setSessionIdCookie(new SimpleCookie(\"ShiroSession\")); manager.setSessionManager(sessionManager); return manager; } @Bean public RedisSessionDAO redisSessionDAO(RedisManager redisManager) { RedisSessionDAO redisSessionDAO = new RedisSessionDAO(); redisSessionDAO.setRedisManager(redisManager); // redisSessionDAO.setRedisTemplate(redisTemplate); return redisSessionDAO; } @Bean public RedisManager redisManager(@Value(\"${session.redis.host}\") final String redisHost, @Value(\"${session.redis.port}\") final int redisPort, @Value(\"${session.redis.password}\") final String redisPassword) { log.info(\"redisHost: {}, redisPort: {}, redisPassword: {}\", redisHost, redisPort, redisPassword); RedisManager redisManager = new RedisManager(); redisManager.setHost(redisHost); redisManager.setPort(redisPort); redisManager.setPassword(redisPassword); redisManager.setExpire(1800); return redisManager; } // 注解使用需要下面三个bean @Bean public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() { return new LifecycleBeanPostProcessor(); } @Bean @DependsOn(\"lifecycleBeanPostProcessor\") public DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator() { DefaultAdvisorAutoProxyCreator creator = new DefaultAdvisorAutoProxyCreator(); creator.setProxyTargetClass(true); return creator; } @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(SecurityManager manager) { AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); advisor.setSecurityManager(manager); return advisor; } } ProdShiroConfiguration.java import com.fire.shiro.AuthRealm; import org.apache.shiro.cache.ehcache.EhCacheManager; import org.apache.shiro.mgt.SecurityManager; import org.apache.shiro.spring.web.ShiroFilterFactoryBean; import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Profile; import javax.servlet.Filter; import java.util.HashMap; import java.util.LinkedHashMap; import java.util.Map; @Configuration @Profile({\"sandbox\", \"prod\"}) public class ProdShiroConfiguration extends ShiroConfiguration { @Bean public ShiroFilterFactoryBean shiroFilter(SecurityManager manager) { ShiroFilterFactoryBean bean = new ShiroFilterFactoryBean(); bean.setSecurityManager(manager); // 配置登录的url和登录成功的url bean.setLoginUrl(\"/login\"); bean.setUnauthorizedUrl(\"/illegal\"); Map filters = new HashMap<>(); bean.setFilters(filters); // 配置访问权限 LinkedHashMap filterChainDefinitionMap = new LinkedHashMap<>(); filterChainDefinitionMap.put(\"/\", \"anon\"); filterChainDefinitionMap.put(\"/nonuser\", \"anon\"); filterChainDefinitionMap.put(\"/**\", \"authc\"); bean.setFilterChainDefinitionMap(filterChainDefinitionMap); return bean; } // 配置自定义的权限登录器 @Bean public AuthRealm authRealm() { AuthRealm authRealm = new AuthRealm(); return authRealm; } @Bean public EhCacheManager ehCacheCacheManager() { EhCacheManager ehCacheCacheManager = new EhCacheManager(); return ehCacheCacheManager; } @Bean @ConditionalOnMissingBean public EhCacheManager shiroCacheManager(EhCacheManager ehCacheCacheManager) { EhCacheManager shiroCacheManager = new EhCacheManager(); shiroCacheManager.setCacheManager(ehCacheCacheManager.getCacheManager()); return shiroCacheManager; } } RedisSessionDAO.java import lombok.extern.slf4j.Slf4j; import org.apache.shiro.session.Session; import org.apache.shiro.session.UnknownSessionException; import org.apache.shiro.session.mgt.eis.AbstractSessionDAO; import java.io.Serializable; import java.util.Collection; import java.util.HashSet; import java.util.List; import java.util.Set; @Slf4j public class RedisSessionDAO extends AbstractSessionDAO { private RedisManager redisManager; /** * The Redis key prefix for the sessions */ private String keyPrefix = \"firehuo:\"; @Override public void update(Session session) throws UnknownSessionException { this.saveSession(session); } /** * save session * * @throws UnknownSessionException */ private void saveSession(Session session) throws UnknownSessionException { if (session == null || session.getId() == null) { log.error(\"session or session id is null\"); return; } byte[] key = getByteKey(session.getId()); byte[] value = SerializeUtils.serialize(session); session.setTimeout(redisManager.getExpire() * 1000); this.redisManager.set(key, value, redisManager.getExpire()); } @Override public void delete(Session session) { if (session == null || session.getId() == null) { log.error(\"session or session id is null\"); return; } redisManager.del(this.getByteKey(session.getId())); } @Override public Collection getActiveSessions() { log.info(\"getActiveSessions session start\"); Set sessions = new HashSet<>(); try { List keys = redisManager.scan(this.keyPrefix + \"*\"); if (keys != null && keys.size() > 0) { for (String key : keys) { Session s = (Session) SerializeUtils.deserialize(redisManager.get(key.getBytes())); sessions.add(s); } } log.info(\"getActiveSessions session end\"); } catch (Exception e) { log.error(\"getActiveSessions is error\", e); } return sessions; } @Override protected Serializable doCreate(Session session) { Serializable sessionId = this.generateSessionId(session); this.assignSessionId(session, sessionId); this.saveSession(session); return sessionId; } @Override protected Session doReadSession(Serializable sessionId) { if (sessionId == null) { log.error(\"session id is null\"); return null; } Session s = (Session) SerializeUtils.deserialize(redisManager.get(this.getByteKey(sessionId))); return s; } /** * 获得byte[]型的key */ private byte[] getByteKey(Serializable sessionId) { String preKey = this.keyPrefix + sessionId; return preKey.getBytes(); } public void setRedisManager(RedisManager redisManager) { this.redisManager = redisManager; //初始化redisManager this.redisManager.init(); } } "},"Chapter15/longin.html":{"url":"Chapter15/longin.html","title":"Part XV 登录篇","keywords":"","body":"第十五章 登录篇 初识SSO 搭建基础服务 Service配置及管理 Service配置及管理 客户端接入 "},"Chapter15/cas1.html":{"url":"Chapter15/cas1.html","title":"初识SSO","keywords":"","body":"初识SSO 一、初识CAS 首先我们来说一下CAS，CAS全称为Central Authentication Service即中央认证服务，是一个企业多语言单点登录的解决方案，并努力去成为一个身份验证和授权需求的综合平台。 CAS是由Yale大学发起的一个企业级的、开源的项目，旨在为Web应用系统提供一种可靠的单点登录解决方法（属于 Web SSO ）。 CAS协议至少涉及三方：客户端Web浏览器，请求身份验证的Web应用程序和CAS服务器。 它也可能涉及后端服务，如数据库服务器，它没有自己的HTTP接口，但与Web应用程序进行通信。 主要特征： 多种的协议的支持，包括CAS (v1、v2、v3)、SAML(v1、v2)、OAuth、OpenID、OpenID Connect和WS-Federation Passive Requestor 多种认证机制，可以通过JAAS，LDAP，RDBMS，X.509，Radius，SPNEGO，JWT，Remote，Trusted，BASIC，Apache Shiro，MongoDB，Pac4J等进行身份验证 可以通过WS-FED，Facebook，Twitter，SAML IdP，OpenID，OpenID Connect，CAS等代理委派认证 多种形式的授权包括ABAC, Time/Date, REST, Internet2’s Grouper等 同时也支持HAZELCAST、EHCache、JPA、MycCache、Apache IGITE、MangGDB、ReDIS、DimoDoB、Couchbase等实现高可用多个集群部署 各种丰富的客户端，像常见的Java、Python、Node、PHP、C#、Perl等等 SSO简介 单点登录（ Single Sign-On , 简称 SSO ）是多个相关但独立的软件系统访问控制的一个属性。通过使用该属性，用户登录与单个ID和密码来访问所连接的一个或多个系统，而不使用不同的用户名或密码，或在某些配置中无缝登录在每个系统上，它是比较流行的服务于企业业务整合的一种解决方案。总结一句话，SSO 使得在多个应用系统中，用户只需要 登录一次 就可以访问所有相互信任的应用系统。 举个栗子：阿里巴巴旗下的淘宝网，你在浏览器里登录了，打开阿里云或者天猫就会发现可以不用在登录了，这里就是使用了SSO。 在SSO体系中，主要包括三部分： User （多个） Web 应用（多个） SSO 认证中心（ 1 个） 而SSO的实现基本核心原则如下： 所有的登录都在 SSO 认证中心进行 SSO 认证中心通过一些方法来告诉 Web 应用当前访问用户究竟是不是已通过认证的用户 SSO 认证中心和所有的 Web 应用建立一种信任关系， SSO 认证中心对用户身份正确性的判断会通过某种方法告之 Web 应用，而且判断结果必须被 Web 应用信任。 SSO原理 登录 上面介绍我们知道，在SSO中有一个独立的认证中心，只有认证中心能接受用户的用户名密码等安全信息，其他系统不提供登录入口，只接受认证中心的间接授权。那其他的系统如何访问受保护的资源？这里就是通过认证中心间接授权通过令牌来实现，当SSO验证了用户信息的正确性后，就会创建授权令牌，在接下来的跳转过程中，授权令牌作为参数发送给各个子系统，子系统拿到令牌，即得到了授权，可以借此创建局部会话，局部会话登录方式与单系统的登录方式相同。 上面是一张SSO登录原理图，下面我们来分析一下具体的流程： 首先用户访问系统1受保护的资源，系统1发现未登陆，跳转至SSO认证中心，并将自己的参数传递过去 SSO认证中心发现用户未登录，将用户引导至登录页面 用户输入用户名和密码提交至SSO认证中心 SSO认证中心校验用户信息，创建用户与SSO认证中心之间的会话，称为全局会话，同时创建授权令牌 SSO认证中心带着令牌跳转会最初的请求地址（系统1） 系统1拿到令牌，去SSO认证中心校验令牌是否有效 SSO认证中心校验令牌，返回有效，注册系统1的地址 系统1使用该令牌创建与用户的会话，称为局部会话，返回给用户受保护资源 用户访问系统2受保护的资源 系统2发现用户未登录，跳转至SSO认证中心，并将自己的地址作为参数传递过去 SSO认证中心发现用户已登录，跳转回系统2的地址，并附上令牌 系统2拿到令牌，去SSO认证中心校验令牌是否有效 SSO认证中心校验令牌，返回有效，注册系统2地址 系统2使用该令牌创建与用户的局部会话，返回给用户受保护资源 用户登录成功之后，会与SSO认证中心及各个子系统建立会话，用户与SSO认证中心建立的会话称为全局会话，用户与各个子系统建立的会话称为局部会话，局部会话建立之后，用户访问子系统受保护资源将不再通过SSO认证中心，全局会话与局部会话有如下约束关系： 局部会话存在，全局会话一定存在 全局会话存在，局部会话不一定存在 全局会话销毁，局部会话必须销毁 注销 既然有登陆那么就自然有注销，单点登录也要单点注销，在一个子系统中注销，所有子系统的会话都将被销毁。原理图如下： SSO认证中心一直监听全局会话的状态，一旦全局会话销毁，监听器将通知所有注册系统执行注销操作 同样的我们也来分析一下具体的流程： 用户向系统1发起注销请求 系统1根据用户与系统1建立的会话id拿到令牌，向SSO认证中心发起注销请求 SSO认证中心校验令牌有效，销毁全局会话，同时取出所有用此令牌注册的系统地址 SSO认证中心向所有注册系统发起注销请求 各注册系统接收SSO认证中心的注销请求，销毁局部会话 SSO认证中心引导用户至登录页面 CAS 的基本原理 在上面的文章中，我们介绍过，在CAS的结构中主要分两部分，一部分是CAS Server，另一部分是CAS Client。 CAS Server：CAS Server 负责完成对用户的认证工作 , 需要独立部署 , CAS Server 会处理用户名 / 密码等凭证(Credentials)。 CAS Client：负责处理对客户端受保护资源的访问请求，需要对请求方进行身份认证时，重定向到 CAS Server 进行认证。（原则上，客户端应用不再接受任何的用户名密码等 Credentials ）。 CAS协议 CAS协议是一个简单而强大的基于票据的协议，它涉及一个或多个客户端和一台服务器。即在CAS中，通过TGT(Ticket Granting Ticket)来获取 ST(Service Ticket)，通过ST来访问具体服务。 其中主要的关键概念： TGT（Ticket Granting Ticket）是存储在TGCcookie中的代表用户的SSO会话。 该ST（Service Ticket），作为参数在GET方法的URL中，代表由CAS服务器授予访问CASified应用程序（包含CAS客户端的应用程序）具体用户的权限。 CAS基本协议模式 结合官方的流程图，我们可以知道，CAS中的单点登录流程： 我们可以发现这里的流程与上面SSO执行的基本是一致，只是CAS协议流程图中，更加清楚的指定了我们在访问过程当中的各种情况，在SSO中令牌也是我们在CAS中的ST(Service Ticket)。 首先用户访问受保护的资源，权限没有认证，所以会把请求的URL以参数跳转到CAS认证中心，CAS认证中心发现没有SSO session，所以弹出登录页面，输入用户信息，提交到CAS认证中心进行信息的认证，如果信息正确，CAS认证中心就会创建一个SSO session和CASTGC cookie，这个CASTGC cookie包含了TGT，而用户session则以TGT为key创建，同时服务端会分发一个ST返回给用户。用户拿到了ST后，访问带参数ST的资源地址，同时应用将ST发送给CAS认证中心，CAS认证中心对ST进行校验，同时判断相应的cookie（包含TGT）是否正确（通过先前设定的key），判断ST是否是有效的，结果会返回一个包含成功信息的XML给应用。应用在建立相应的session和cookie跳转到浏览器，用户再通过浏览器带cookie去应用访问受保护的资源地址，cookie和后端session验证成功便可以成功访问到信息。 第二次访问应用时，浏览器就会携带相应的cookie信息，后台session验证用户是否登录，与一般单系统应用登录模式一样。 当我们访问其他的应用，与前面的步骤也是基本相同，首先用户访问受保护的资源，跳转回浏览器，浏览器含有先前登录的CASTGC cookie，CASTGC cookie包含了TGT并发送到CAS认证中心，CAS认证中心校验TGT是否有效，如果有效分发浏览器一个带ST参数的资源地址URL，应用程序拿到ST后，再发送给CAS认证中心，如果认证了ST有效后，结果会返回一个包含成功信息的XML给应用。同样的步骤，应用在建立相应的session和cookie跳转到浏览器，用户再通过浏览器带cookie去应用访问受保护的资源地址，验证session成功便可以成功访问到信息。 CAS代理模式 CAS协议的最强大的功能之一就是CAS服务可以作为另一个CAS服务的代理的能力，传输用户身份。 同样的我们还是结合官方的流程图来分析： 加载代理应用 首先用户访问代理地址，权限没有认证，所以会把请求的URL以参数跳转到CAS认证中心，CAS认证中心发现没有SSO session，所以弹出登录页面，输入用户信息，提交到CAS认证中心进行信息的认证，如果信息正确，CAS认证中心就会创建一个SSO session——CASTGC cookie，这个CASTGC cookie包含了TGT，这个TGT作为一个用户session，它会分发一个ST返回给用户。用户拿到了ST后，访问带参数ST的代理地址，代理地址将ST发送给CAS认证中心并且带一个pgtUrl，这是请求一个PGT的回调URL。CAS认证中通过调用回调PGT URL将TGT和PGTIOU传递给代理地址，代理地址匹配存储PGTIOU和PGT并执行下一步，然后CAS返回一个PGTIOU给代理匹配刚刚存储是PGTIOU与PGT是否一致。后面就通过PGTIOU查找PGT，然后代理地址在建立相应的session cookie跳转到浏览器，用户再通过浏览器带cookie去访问代理地址。 通过代理访问应用 通过代理访问应用，在浏览器中通过携带相应的cookie去访问代理，然后验证session cookie的有效性，然后再代理地址通过PGT跳转到CAS认证中心，认证中心再通过代理地址访问应用，相同的步骤，应用发送ST给CAS认证中心，检验proxy ticket是否有效，如何有效返回给应用一个XML信息。在应用中查询代理URL是否可信赖，阻止代理用户非法的行为。然后应用再建立相应的session cookie跳转到代理地址，代理地址再带cookie去访问应用，并验证是否正确。如果正确，则应用响应代理地址的请求，代理地址再把请求发送给用户。 CAS术语概念 CAS 系统中的票据： TGC、TGT 、 ST 、 PGT 、 PGTIOU 、 PT 。 (1)、TGC（ticket-granting cookie） 授权的票据证明，由 CAS Server 通过 SSL 方式发送给终端用户，存放用户身份认证凭证的Cookie，在浏览器和CAS Server间通讯时使用，并且只能基于安全通道传输（Https），是CAS Server用来明确用户身份的凭证。 (2)、TGT（Ticket Grangting Ticket） TGT是CAS为用户签发的登录票据，拥有了TGT，用户就可以证明自己在CAS成功登录过。TGT封装了Cookie值以及此Cookie值对应的用户信息。用户在CAS认证成功后，CAS生成Cookie（叫TGC），写入浏览器，同时生成一个TGT对象，放入自己的缓存，TGT对象的ID就是Cookie的值。当HTTP再次请求到来时，如果传过来的有CAS生成的Cookie，则CAS以此Cookie值为key查询缓存中有无TGT ，如果有的话，则说明用户之前登录过，如果没有，则用户需要重新登录。 (3)、ST（Service Ticket） ST是CAS为用户签发的访问某一service的票据。用户访问service时，service发现用户没有ST，则要求用户去CAS获取ST。用户向CAS发出获取ST的请求，如果用户的请求中包含Cookie，则CAS会以此Cookie值为key查询缓存中有无TGT，如果存在TGT，则用此TGT签发一个ST，返回给用户。用户凭借ST去访问service，service拿ST去CAS验证，验证通过后，允许用户访问资源。 (4)、PGT（Proxy Granting Ticket） Proxy Service的代理凭据。用户通过CAS成功登录某一Proxy Service后，CAS生成一个PGT对象，缓存在CAS本地，同时将PGT的值（一个UUID字符串）回传给Proxy Service，并保存在Proxy Service里。Proxy Service拿到PGT后，就可以为Target Service（back-end service）做代理，为其申请PT。 (5)、PGTIOU（Proxy Granting Ticket I Owe You） PGTIOU是CAS协议中定义的一种附加票据，它增强了传输、获取PGT的安全性。 PGT的传输与获取的过程：Proxy Service调用CAS的serviceValidate接口验证ST成功后，CAS首先会访问pgtUrl指向的Https URL，将生成的 PGT及PGTIOU传输给proxy service，proxy service会以PGTIOU为key，PGT为value，将其存储在Map中；然后CAS会生成验证ST成功的XML消息，返回给Proxy Service，XML消息中含有PGTIOU，proxy service收到XML消息后，会从中解析出PGTIOU的值，然后以其为key，在Map中找出PGT的值，赋值给代表用户信息的Assertion对象的pgtId，同时在Map中将其删除。 (6)、PT（Proxy Ticket） PT是用户访问Target Service（back-end service）的票据。如果用户访问的是一个Web应用，则Web应用会要求浏览器提供ST，浏览器就会用Cookie去CAS获取一个ST，然后就可以访问这个Web应用了。如果用户访问的不是一个Web应用，而是一个C/S结构的应用，因为C/S结构的应用得不到Cookie，所以用户不能自己去CAS获取ST，而是通过访问proxy service的接口，凭借proxy service的PGT去获取一个PT，然后才能访问到此应用。 TGT、ST、PGT、PT之间关系 ST是TGT签发的。用户在CAS上认证成功后，CAS生成TGT，用TGT签发一个ST，ST的ticketGrantingTicket属性值是TGT对象，然后把ST的值redirect到客户应用。 PGT是ST签发的。用户凭借ST去访问Proxy service，Proxy service去CAS验证ST（同时传递PgtUrl参数给CAS），如果ST验证成功，则CAS用ST签发一个PGT，PGT对象里的ticketGrantingTicket是签发ST的TGT对象。 PT是PGT签发的。Proxy service代理back-end service去CAS获取PT的时候，CAS根据传来的pgt参数，获取到PGT对象，然后调用其grantServiceTicket方法，生成一个PT对象。 其他概念 KDC(Key Distribution Center)----------密钥发放中心； Authentication Service (AS) --------- 认证服务，索取Crendential ，发放 TGT； Ticket-Granting Service (TGS) --------- 票据授权服务，索取TGT ，发放ST。 "},"Chapter15/cas2.html":{"url":"Chapter15/cas2.html","title":"搭建基础服务","keywords":"","body":"搭建基础服务 参考文章: https://blog.csdn.net/Anumbrella/article/details/81045885 搭建CAS基础服务 准备 首先CAS官方文档地址：https://apereo.github.io/cas/5.3.x/index.html，在后面我们可能随时会用到。 然后我们从Geting Started开始，在文档里面告诉我们部署CAS，推荐我们使用WAR Overlay method的方法，利用覆盖机制来组合CAS原始工件和本地自定义方法来达到自定义CAS的要求。 It is recommended to build and deploy CAS locally using the WAR Overlay method. This approach does not require the adopter to explicitly download any version of CAS, but rather utilizes the overlay mechanism to combine CAS original artifacts and local customizations to further ease future upgrades and maintenance. 官方给了两种编译方式，一种是Maven、另一种是Gradle，这里使用Maven安装部署。 具体的详情可以参考：https://apereo.github.io/cas/5.3.x/installation/Maven-Overlay-Installation.html 在开始之前，我们需要配置好电脑环境，笔者当前的环境为： JDK 1.8 Maven 3.5.3 Tomcat 8.5（官方推荐Tomcat至少要8版本以上） 下载代码打包 我们需要下载打包成WAR的代码架子，地址为： https://github.com/apereo/cas-overlay-template。 这里我们使用的CAS当前版本5.3.x，因为6.x是基于Gradle和JDK11的。然后我们进入代码根目录下打开pom.xml文件，添加国内的maven镜像源地址，加快下载包的速度，因为CAS需要的包有点多，并且很大，如果为原来的地址，速度非常慢。 maven-ali http://maven.aliyun.com/nexus/content/groups/public// true true always fail 更改pom.xml文件后，我们到项目的跟目录下，执行mvn clean package命令，接着就会去下载相应的jar包。 当执行完毕后，我们便可在项目根目录下的target目录下发现生成的cas.war包。然后我们将其放入Tomcat目录下的webapps下面。接着在浏览器里访问http://localhost:8080/cas/login，可以发现CAS出现登录界面。 默认账号：casuser 默认密码：Mellon 目前的配置仅有这一个用户。输入用户名和密码，登录成功！ 这样我们就完成了CAS的登录过程，基本的CAS服务搭建就实现了。 我们在登录界面可以发现，弹出了两个提示框，如下图红框所圈： 我们仔细阅读可以发现，这里主要说了： 一、我们的登录不是安全的，并没有使用HTTPS协议，这里我们使用了HTTP。 二、提示我们这里用户验证方式是静态文件写死的，指定就是我们的用户名和密码（casuser/Mellon）,这个只适合demo使用。 接下来我们就主要围绕这两点来解决问题。 配置证书 生成证书 接下来我们便开始解决这两个问题，首先是HTTPS，我们知道使用HTTPS是需要证书的，所以接下来我们便制作一个证书。 使用JDK自带的工具keytool PS C:\\Users\\fire> keytool -genkey -alias caskeystore -keypass 123456 -keyalg RSA -keystore thekeystore 输入密钥库口令: 再次输入新口令: 您的名字与姓氏是什么? [Unknown]: passport.fire.com 您的组织单位名称是什么? [Unknown]: fire 您的组织名称是什么? [Unknown]: fire 您所在的城市或区域名称是什么? [Unknown]: fire 您所在的省/市/自治区名称是什么? [Unknown]: fire 该单位的双字母国家/地区代码是什么? [Unknown]: cn CN=passport.fire.com, OU=fire, O=fire, L=fire, ST=fire, C=cn是否正确? [否]: y PS C:\\Users\\fire> ls 目录: C:\\Users\\fire Mode LastWriteTime Length Name ---- ------------- ------ ---- -a---- 2020/3/26 15:45 2235 thekeystore 首先输入密钥库口令，然后在输入名字与姓氏时为为具体路由地址，就是待会CAS认证服务器的地址（这里以passport.fire.com为例）， 而其余的根据具体情况填写即可，然后就会在当前目录下生成证书。 注意：-keypass 123456 应该和密钥库口令保持一直否则tomcat会报 Caused by: java.security.UnrecoverableKeyException: Cannot recover key 下面是另一种方式 PS C:\\Users\\fire> keytool -genkey -alias caskeystore -keyalg RSA -keystore firekeystore 输入密钥库口令: 再次输入新口令: 您的名字与姓氏是什么? [Unknown]: password.fire.com 您的组织单位名称是什么? [Unknown]: fire 您的组织名称是什么? [Unknown]: fire 您所在的城市或区域名称是什么? [Unknown]: fire 您所在的省/市/自治区名称是什么? [Unknown]: fire 该单位的双字母国家/地区代码是什么? [Unknown]: cn CN=password.fire.com, OU=fire, O=fire, L=fire, ST=fire, C=cn是否正确? [否]: y 输入 的密钥口令 (如果和密钥库口令相同, 按回车): 再次输入新口令: Warning: JKS 密钥库使用专用格式。建议使用 \"keytool -importkeystore -srckeystore firekeystore -destkeystore firekeystore -deststor etype pkcs12\" 迁移到行业标准格式 PKCS12。 密钥库口令应该和 的密钥口令 一致 导出数字证书 PS C:\\Users\\fire> keytool -export -alias caskeystore -keystore thekeystore -rfc -file cas.crt 输入密钥库口令: 存储在文件 中的证书 Warning: JKS 密钥库使用专用格式。建议使用 \"keytool -importkeystore -srckeystore thekeystore -destkeystore thekeystore -deststoretype pkcs12\" 迁移到行业标准格式 PKCS12。 PS C:\\Users\\fire> ls 目录: C:\\Users\\liuyi27 Mode LastWriteTime Length Name -a---- 2020/3/26 15:52 1266 cas.crt -a---- 2020/3/26 15:45 2235 thekeystore 输入先前的密钥库口令，然后在当前目录下生成具体的cas.crt数字证书 将数字证书导入jdk下的jre里 windows keytool -import -alias caskeystore -keystore %JAVA_HOME%\\jre\\lib\\security\\cacerts -file cas.crt -trustcacerts -storepass changeit Unix sudo keytool -import -alias caskeystore -keystore $JAVA_HOME/jre/lib/security/cacerts -file cas.crt -trustcacerts -storepass changeit 这里导入JDK时需要默认密码changeit，在命令中已经配置好了。如果没有该密码，则会报java.io.IOException: Keystore was tampered with, or password was incorrect错误。 PS C:\\Users\\fire> keytool -import -alias caskeystore -keystore D:\\Java\\jdk1.8.0_161\\jre\\lib\\security\\cacerts -file cas.crt -trustcacerts -storepass changeit 所有者: CN=passport.fire.com, OU=fire, O=fire, L=fire, ST=fire, C=cn 发布者: CN=passport.fire.com, OU=fire, O=fire, L=fire, ST=fire, C=cn 序列号: 2e8ca677 有效期为 Thu Mar 26 15:45:54 CST 2020 至 Wed Jun 24 15:45:54 CST 2020 证书指纹: MD5: C4:6E:6F:A9:B9:21:50:59:52:42:D1:EB:EC:51:83:05 SHA1: 14:B8:CB:ED:1D:69:10:83:7A:F6:0F:E2:9C:48:BB:BA:92:16:6F:34 SHA256: 08:07:18:D6:B9:B9:E5:E0:37:2B:EF:1C:0C:7E:87:69:9F:23:2D:96:0E:5A:90:60:11:76:EB:9E:C1:54:A5:B3 签名算法名称: SHA256withRSA 主体公共密钥算法: 2048 位 RSA 密钥 版本: 3 扩展: #1: ObjectId: 2.5.29.14 Criticality=false SubjectKeyIdentifier [ KeyIdentifier [ 0000: 11 7A 98 FE 3C 69 C8 55 F9 96 D0 94 5D 2E 7B 52 .z.. 已经存在。 C:\\Users\\lenovo>keytool -genkey -alias caskeystore -keypass 123456 -keyalg RSA -keystore thekeystore 输入密钥库口令: keytool 错误: java.lang.Exception: 未生成密钥对, 别名 已经存在 解决办法：删除caskeystore即可 2、出现以下错误是因为已经导入过一次证书，别名 已经存在。 C:\\Users\\lenovo>keytool -import -alias caskeystore -keystore %JAVA_HOME%\\jre\\lib\\security\\cacerts -file cas.crt -trustcacerts -storepass changeit keytool 错误: java.lang.Exception: 证书未导入, 别名 已经存在 解决办法：执行以下命令： keytool -delete -alias caskeystore -keystore %JAVA_HOME%\\jre\\lib\\security\\cacerts -storepass changeit 配置DNS(修改host) 这里我的CAS服务端是部署在本地的，所以需要做一个本地映射。 127.0.0.1 passport.fire.com 配置Tomcat 编辑Tomcat目录Conf下的server.xml文件。 将8443的端口配置文件打开，配置如下（添加前面刚刚生成的keystore的地址和密匙）： or --> 更改配置 我们知道cas-overlay-template是采用配置覆盖的策略来进行自定义的，因此我们可以通过覆盖或者继承某些类重写某些方法实现自定义的需求。将我们先前的cas.war解压，或者直接去Tomcat下webapps目录下面，打开WEB-INF目录下的classes里面的application.properties文件。 可以看到该文件就是对CAS认证服务器的配置，我们现在要自定义，打开我们的项目，新建src/main/resources文件夹，同时将刚才的application.properties文件复制到该目录下。 现在我们就可以通过配置application.properties文件来实现对CAS服务器的自定义。 除了上面我们刚刚在Tomcat里面配置证书，我们还可以直接将证书配置到CAS服务信息里面，打开application.properties文件，我们可以发现在开头有配置信息如下： https://blog.csdn.net/Anumbrella/article/details/81045885 "},"Chapter15/cas3.html":{"url":"Chapter15/cas3.html","title":"Service配置及管理","keywords":"","body":"Service配置及管理 首先我们要明白CAS中的Service的概念是什么， 我们在第一节就讲解了在CAS系统中， 主要分为三部分，User、Web应用、SSO认证中心。 User就是我们普通用户，Web应用就是需要接入SSO认证中心的应用也就是这里的Service， 而SSO认证中心就是CAS服务端。 简单来说就是CAS分为服务端和客户端 ，而Service就是指具体的多个客户端（CAS Clients）。 而这里的服务管理（Service Management）就是CAS服务管理工具允许CAS服务器管理员声明和配置哪些服务（Service，CAS客户端）可以在哪些方面使用CAS。 服务管理工具的核心组件是服务注册表，它存储一个或多个注册服务。 接下来先介绍我们的第一个知识点——Service配置！ Service配置 我们刚刚提及到在CAS中， 服务管理工具中的服务注册表当中存储着一个或多个注册服务， 而这些Service中包含着各个行为的元数据，通过配置这些数据我们可以控制这些Service的行为。 主要行为包括一些几点： 授权服务 - 控制哪些服务可以参与CAS SSO会话。 强制身份验证 - 为强制身份验证提供管理控制。 属性发布 - 为服务提供用户详细信息以进行授权和个性化。 代理控制 - 通过授予/拒绝代理身份验证功能进一步限制授权服务。 主题控制 - 定义用于特定服务的备用CAS主题。 在Service中配置属性主要包括以下这些信息： 上图介绍了一些在Service中常用的配置项， 对于各个配置属性的含义可以参考具体文档，服务配置。 在这些配置中，比较常使用的主要是： 服务访问策略——(accessStrategy)，具体可以查看：服务策略配置。 服务属性配置——(properties)，具体查看：服务属性配置。 服务到期政策——(expirationPolicy)，具体查看：服务到期配置。 对CAS元数据的配置信息有了大致的了解后，我们需要配置其存储方式， 就像我们前面介绍的多种认证方式一样，用户信息提供了多种方式， 这里的Service存储方式也是提供了多种的解决方案。 推荐使用JSON、YAML、MongoDb、Redis、JPA这几种方式来存储使用，这里也将使用这几种方式来介绍。 JSON 这种方式也是CAS默认初始化使用的，注册表在应用程序上下文初始化时从JSON配置文件中读取服务定义，期望在配置的目录位置内找到JSON文件。 首先添加依赖包： org.apereo.cas cas-server-support-json-service-registry ${cas.version} 在resources/services文件夹下面新建web-10000001.json，具体内容如下： { \"@class\" : \"org.apereo.cas.services.RegexRegisteredService\", \"serviceId\" : \"^(https|imaps|http)://.*\", \"name\" : \"web\", \"id\" : 10000001, \"evaluationOrder\" : 10 } 注意: Json文件名字规则为${name}-${id}.json，id必须为Json文件内容中的id一致。 Json文件解释： @class：必须为org.apereo.cas.services.RegisteredService的实现类，对其他属性进行一个json反射对象，常用的有RegexRegisteredService，匹配策略为id的正则表达式 serviceId：唯一的服务id name： 服务名称，会显示在默认登录页 id：全局唯一标志 description：服务描述，会显示在默认登录页 evaluationOrder： 匹配争取时的执行循序，最好是比1大的数字 因为在CAS服务中，默认是提供了默认的Service配置项，所以如果添加的Json配置没起作用，可以尝试注释掉默认启动Json，在pom.xml文件里面进行配置，如下： org.apache.maven.plugins maven-war-plugin 2.6 cas false false false ${manifestFileToUse} org.apereo.cas cas-server-webapp${app.server} **/services/*.json 然后在配置文件application.properties下添加配置： cas: serviceRegistry: # Service Registry(服务注册) initFromJson: true #开启识别Json文件，默认false #watcherEnabled: true #自动扫描服务配置，默认开启 schedule: repeatInterval: 120000 #120秒扫描一遍 startDelay: 15000 #延迟15秒开启 json: # Json配置 location: classpath:/services authn: accept: users: casuser::Mellon Jpa 最后介绍一下Jpa这种方式，通过Java持久层API来实现数据保存到数据库， 可以是Mysql、Oracle、SQL Service这种方式可能使用的比较多一些。 同样的先添加依赖： org.apereo.cas cas-server-support-jpa-service-registry ${cas.version} 添加配置信息： cas: serviceRegistry: # Service Registry(服务注册) initFromJson: false #开启识别Json文件，默认false #watcherEnabled: true #自动扫描服务配置，默认开启 schedule: repeatInterval: 120000 #120秒扫描一遍 startDelay: 15000 #延迟15秒开启 json: # Json配置 location: classpath:/services jpa: # Jpa配置 user: root password: 123 driverClass: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/cas? dialect: org.hibernate.dialect.MySQL5Dialect failFastTimeout: 1 healthQuery: isolateInternalQueries: false leakThreshold: 10 batchSize: 1 ddlAuto: update #设置配置的服务，一直都有，不会给清除掉 ， 第一次使用，需要配置为 create #create-drop 重启cas服务的时候，就会给干掉 #create 没有表就创建，有就不创建 #none 一直都有 #update 更新 autocommit: true idleTimeout: 5000 pool: suspension: false minSize: 6 maxSize: 18 maxWait: 2000 timeoutMillis: 1000 更详细的驱动信息可以参考文档： https://apereo.github.io/cas/5.3.x/installation/JDBC-Drivers.html 启动服务后，还是和前面一样，提示CAS的服务记录是空的，没有定义服务。 希望通过CAS进行认证的应用程序必须在服务记录中明确定义。。。暂时可以忽略掉，马上我们就来配置服务管理。 然后我们去数据库发现，新增了一些表，这就是CAS将服务信息保存到数据库使用的表。 注意这里应该生成的表是5个 registeredserviceimplcontact registeredservice_contacts regexregisteredserviceproperty regexregisteredservice 但是可能由于默认的引擎选择的是MyISAM所以导致 registeredserviceimpl_props不能生成.把其他引擎改成innodb就行 或者手动创建表也行并设置属性cas.serviceRegistry.jpa.ddlAuto=node CREATE TABLE `regexregisteredservice` ( `expression_type` varchar(50) COLLATE utf8mb4_general_ci NOT NULL DEFAULT 'regex', `id` bigint NOT NULL AUTO_INCREMENT, `access_strategy` longblob, `attribute_release` longblob, `description` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `evaluation_order` int NOT NULL, `expiration_policy` longblob, `informationUrl` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `logo` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `logout_type` int DEFAULT NULL, `logout_url` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `mfa_policy` longblob, `name` varchar(255) COLLATE utf8mb4_general_ci NOT NULL, `privacyUrl` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `proxy_policy` longblob, `public_key` longblob, `required_handlers` longblob, `responseType` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `serviceId` varchar(255) COLLATE utf8mb4_general_ci NOT NULL, `theme` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `username_attr` longblob, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci; CREATE TABLE `regexregisteredserviceproperty` ( `id` bigint NOT NULL AUTO_INCREMENT, `property_values` longblob, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci; CREATE TABLE `registeredservice_contacts` ( `AbstractRegisteredService_id` bigint NOT NULL, `contacts_id` bigint NOT NULL, `contacts_ORDER` int NOT NULL, PRIMARY KEY (`AbstractRegisteredService_id`,`contacts_ORDER`), UNIQUE KEY `UK_s7mf4a23wejqx62tt4vh3tgwi` (`contacts_id`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci; CREATE TABLE `registeredserviceimplcontact` ( `id` bigint NOT NULL AUTO_INCREMENT, `department` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `email` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, `name` varchar(255) COLLATE utf8mb4_general_ci NOT NULL, `phone` varchar(255) COLLATE utf8mb4_general_ci DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci; CREATE TABLE `registeredserviceimpl_props` ( `AbstractRegisteredService_id` bigint(20) NOT NULL, `properties_id` bigint(20) NOT NULL, `properties_KEY` varchar(255) NOT NULL, PRIMARY KEY (`AbstractRegisteredService_id`,`properties_KEY`), UNIQUE KEY `UK_i2mjaqjwxpvurc6aefjkx5x97` (`properties_id`), CONSTRAINT `FK1xan7uamsa94y2451jgksjkj4` FOREIGN KEY (`properties_id`) REFERENCES `regexregisteredserviceproperty` (`id`), CONSTRAINT `FK5ghaknoplphay7reury7n3vcm` FOREIGN KEY (`AbstractRegisteredService_id`) REFERENCES `regexregisteredservice` (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; "},"Chapter15/cas4.html":{"url":"Chapter15/cas4.html","title":"Service配置及管理","keywords":"","body":"自定义登录界面和表单信息 自定义用户界面 在上一节中我们讲解了关于Service配置和管理，在Service的配置中， 我们可以配置theme参数。 比如，我们在使用上一节的代码中使用Json来存储Service配置， 在web-10000001.json文件中，我们添加指定主题的参数为fire。 配置如下： { \"@class\" : \"org.apereo.cas.services.RegexRegisteredService\", \"serviceId\" : \"^(https|imaps|http)://.*\", \"name\" : \"web\", \"id\" : 10000001, \"evaluationOrder\" : 10, \"accessStrategy\" : { \"@class\" : \"org.apereo.cas.services.DefaultRegisteredServiceAccessStrategy\", \"enabled\" : true, \"ssoEnabled\" : true }, \"theme\": \"fire\" } 接着我们在src/main目录下新建fire.properties文件， 文件名与主题参数一致。在官网中推荐我们在配置文件中写法为： cas.standard.css.file=/themes/[theme_name]/css/cas.css cas.javascript.file=/themes/[theme_name]/js/cas.js cas.admin.css.file=/themes/[theme_name]/css/admin.css 这里采用的写法会把CAS系统中自带的页面样式完全覆盖， 如果我们只想自定义一部分页面，可以采用自定义部分样式的写法。 anumbrella.javascript.file=/themes/fire/js/cas.js anumbrella.standard.css.file=/themes/fire/css/cas.css 比如这里我只想自定义登录页面，其他页面不变，可以采用上面的写法。 所以fire.properties文件的内容如下： fire.javascript.file=/themes/fire/js/cas.js fire.standard.css.file=/themes/fire/css/cas.css fire.login.images.path=/themes/fire/images cas.standard.css.file=/css/cas.css cas.javascript.file=/js/cas.js cas.admin.css.file=/css/admin.css fire.login.images.path=/themes/fire/images 为要在html页面使用到的图片路径，所以这里自定义图片的地址。 接着我们在src\\main\\resources文件下新建static和templates文件夹， 同时在static文件夹下新建themes/anumbrella文件夹， 在templates目录下新建anumbrella文件夹。 继续在static/themes/anumbrella下新建css、js、images这三个文件夹， 把需要的css、js、图片放入这下面 。接着我们在templates/anumbrella目录下新建casLoginView.html文件。 注意：这里的casLoginView.html文件不能乱命名，必须为casLoginView.html。这里是覆盖登录页面所以命名为casLoginView.html，如果要覆盖退出页面则是casLogoutView.html。 "},"Chapter15/cas5.html":{"url":"Chapter15/cas5.html","title":"客户端接入","keywords":"","body":"客户端接入 我们使用这个简单的基于Spring Boot的用户登录系统，将给它来接入CAS系统，这里的登录验证是使用的spring-boot-starter-data-jpa，来验证表user中的用户信息。现在我们介入CAS客户端代码，首先导入依赖。 org.jasig.cas.client cas-client-core 3.5.1 首先我们在application.properties中添加配置， 这里也就是我们上面讲解过的配置信息。 spring: cas: sign-out-filters: /* # 监听退出的接口，即所有接口都会进行监听 auth-filters: /* # 需要拦截的认证的接口 validate-filters: /* request-wrapper-filters: /* assertion-filters: /* ignore-filters: /test # 表示忽略拦截的接口，也就是不用进行拦截 cas-server-login-url: https://sso.fire.com:8443/login cas-server-url-prefix: https://sso.fire.com:8443/ redirect-after-validation: true use-session: true server-name: https://gateway.fire.com:8000 然后再新建bean类SpringCasAutoconfig，读取配置文件中的信息。 "},"Chapter16/system.html":{"url":"Chapter16/system.html","title":"Part XVI 系统篇","keywords":"","body":"第十六章 系统篇 查看cpu核数 防火墙 系统内核 Linux中的零拷贝技术 "},"Chapter16/cpu.html":{"url":"Chapter16/cpu.html","title":"查看cpu核数","keywords":"","body":"linux下怎么查看cpu核数 查看CPU型号 ```shell script $ cat /proc/cpuinfo | grep name | sort | uniq model name : Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz ###### 查看物理CPU数目 ```shell script $ cat /proc/cpuinfo | grep \"physical id\" physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 physical id : 0 physical id : 1 所有physical id都是0，可知有1个物理CPU。这里有0又有1所以是两个物理cpu。 也用管道排序去重后直接输出物理cpu的个数； ```shell script $ cat /proc/cpuinfo | grep \"physical id\" | sort | uniq | wc -l 2 ###### 查看核数和逻辑CPU数目 ```shell script $ cat /proc/cpuinfo | grep \"core id\" | sort | uniq | wc -l 8 $ cat /proc/cpuinfo | grep \"processor\" | sort | uniq | wc -l 32 由图可知：2颗物理CPU，8核32线程； 如果不想自己算，也可以直接lscpu ```shell script $ lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 32 On-line CPU(s) list: 0-31 Thread(s) per core: 2 Core(s) per socket: 8 Socket(s): 2 NUMA node(s): 2 Vendor ID: GenuineIntel CPU family: 6 Model: 63 Stepping: 2 CPU MHz: 2400.001 BogoMIPS: 4799.30 Virtualization: VT-x L1d cache: 32K L1i cache: 32K L2 cache: 256K L3 cache: 20480K NUMA node0 CPU(s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30 NUMA node1 CPU(s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31 主要是看下面几个 ```shell script CPU(s): 32 #32个逻辑cpu Thread(s) per core: 2 Core(s) per socket: 8 Socket(s): 2 #2个物理cpu "},"Chapter16/firewall.html":{"url":"Chapter16/firewall.html","title":"防火墙","keywords":"","body":"防火墙 问题:老是关闭防火墙太麻烦，所以选择彻底关闭防火墙，发现每次都记不住命令! 下面是red hat/CentOs7关闭防火墙的命令! 1:查看防火状态 systemctl status firewalld service iptables status 2:暂时关闭防火墙 systemctl stop firewalld service iptables stop 3:永久关闭防火墙 systemctl disable firewalld chkconfig iptables off 4:重启防火墙 systemctl enable firewalld service iptables restart 5:永久关闭后重启 //暂时还没有试过 chkconfig iptables on 重要的事情说三遍,强烈建议使用第二种方法!第二种方法!第二!; 开放端口的方法： 方法一：命令行方式 1. 开放端口命令： /sbin/iptables -I INPUT -p tcp --dport 8080 -j ACCEPT 2.保存：/etc/rc.d/init.d/iptables save 3.重启服务：/etc/init.d/iptables restart 4.查看端口是否开放：/sbin/iptables -L -n 方法二：直接编辑/etc/sysconfig/iptables文件 1.编辑/etc/sysconfig/iptables文件：vi /etc/sysconfig/iptables 加入内容并保存：-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT 2.重启服务：/etc/init.d/iptables restart 3.查看端口是否开放：/sbin/iptables -L -n 但是我用方法一一直保存不上，查阅网上发现直接修改文件不需要iptables save，重启下iptables 重新加载下配置。iptables save 是将当前的iptables写入到/etc/sysconfig/iptables。我不save直接restart也不行，所以还是方法二吧 查询端口是否有进程守护用如下命令grep对应端口，如80为端口号 例：netstat -nalp|grep 80 现在Linux服务器只打开了22端口，用putty.exe测试一下是否可以链接上去。 可以链接上去了，说明没有问题。 最后别忘记了保存 对防火墙的设置 通过命令：service iptables save 进行保存 iptables -A INPUT -p tcp --dport 22 -j ACCEPT iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT 针对这2条命令进行一些讲解吧 -A 参数就看成是添加一条 INPUT 的规则 -p 指定是什么协议 我们常用的tcp 协议，当然也有udp 例如53端口的DNS 到时我们要配置DNS用到53端口 大家就会发现使用udp协议的 而 --dport 就是目标端口 当数据从外部进入服务器为目标端口 反之 数据从服务器出去 则为数据源端口 使用 --sport -j 就是指定是 ACCEPT 接收 或者 DROP 不接收 "},"Chapter16/core.html":{"url":"Chapter16/core.html","title":"系统内核","keywords":"","body":"系统内核 一、查看Linux内核版本命令（两种方法）： 1、cat /proc/version 2、uname -a 二、查看Linux系统版本的命令（3种方法）： 1、lsb_release -a 即可列出所有版本信息： 这个命令适用于所有的Linux发行版，包括Redhat、SuSE、Debian…等发行版。 2、cat /etc/redhat-release 这种方法只适合Redhat系的Linux： 3、cat /etc/issue 此命令也适用于所有的Linux发行版。 三、Linux查看版本多少位 1、getconf LONG_BIT "},"Chapter16/zerocopy.html":{"url":"Chapter16/zerocopy.html","title":"Linux中的零拷贝技术","keywords":"","body":"Linux中的零拷贝技术 引言 传统的 Linux 操作系统的标准 I/O 接口是基于数据拷贝操作的，即 I/O 操作会导致数据在操作系统内核地址空间的缓冲区和应用程序地址空间定义的缓冲区之间进行传输。这样做最大的好处是可以减少磁盘 I/O 的操作，因为如果所请求的数据已经存放在操作系统的高速缓冲存储器中，那么就不需要再进行实际的物理磁盘 I/O 操作。但是数据传输过程中的数据拷贝操作却导致了极大的 CPU 开销，限制了操作系统有效进行数据传输操作的能力。 零拷贝（ zero-copy ）这种技术可以有效地改善数据传输的性能，在内核驱动程序（比如网络堆栈或者磁盘存储驱动程序）处理 I/O 数据的时候，零拷贝技术可以在某种程度上减少甚至完全避免不必要 CPU 数据拷贝操作。现代的 CPU 和存储体系结构提供了很多特征可以有效地实现零拷贝技术，但是因为存储体系结构非常复杂，而且网络协议栈有时需要对数据进行必要的处理，所以零拷贝技术有可能会产生很多负面的影响，甚至会导致零拷贝技术自身的优点完全丧失。 为什么需要零拷贝技术 如今，很多网络服务器都是基于客户端 - 服务器这一模型的。在这种模型中，客户端向服务器端请求数据或者服务；服务器端则需要响应客户端发出的请求，并为客户端提供它所需要的数据。随着网络服务的逐渐普及，video 这类应用程序发展迅速。当今的计算机系统已经具备足够的能力去处理 video 这类应用程序对客户端所造成的重负荷，但是对于服务器端来说，它应付由 video 这类应用程序引起的网络通信量就显得捉襟见肘了。而且，客户端的数量增长迅速，那么服务器端就更容易成为性能瓶颈。而对于负荷很重的服务器来说，操作系统通常都是引起性能瓶颈的罪魁祸首。举个例子来说，当数据“写”操作或者数据“发送”操作的系统调用发出时，操作系统通常都会将数据从应用程序地址空间的缓冲区拷贝到操作系统内核的缓冲区中去。操作系统这样做的好处是接口简单，但是却在很大程度上损失了系统性能，因为这种数据拷贝操作不单需要占用 CPU 时间片，同时也需要占用额外的内存带宽。 一般来说，客户端通过网络接口卡向服务器端发送请求，操作系统将这些客户端的请求传递给服务器端应用程序，服务器端应用程序会处理这些请求，请求处理完成以后，操作系统还需要将处理得到的结果通过网络适配器传递回去。 下边这一小节会跟读者简单介绍一下传统的服务器是如何进行数据传输的，以及这种数据传输的处理过程存在哪些问题有可能会造成服务器的性能损失。 Linux中传统服务器进行数据传输的流程 Linux 　中传统的 I/O 操作是一种缓冲 I/O，I/O 过程中产生的数据传输通常需要在缓冲区中进行多次的拷贝操作。一般来说，在传输数据的时候，用户应用程序需要分配一块大小合适的缓冲区用来存放需要传输的数据。应用程序从文件中读取一块数据，然后把这块数据通过网络发送到接收端去。用户应用程序只是需要调用两个系统调用 read() 和 write() 就可以完成这个数据传输操作，应用程序并不知晓在这个数据传输的过程中操作系统所做的数据拷贝操作。对于 Linux 操作系统来说，基于数据排序或者校验等各方面因素的考虑，操作系统内核会在处理数据传输的过程中进行多次拷贝操作。在某些情况下，这些数据拷贝操作会极大地降低数据传输的性能。 当应用程序需要访问某块数据的时候，操作系统内核会先检查这块数据是不是因为前一次对相同文件的访问而已经被存放在操作系统内核地址空间的缓冲区内，如果在内核缓冲区中找不到这块数据，Linux 操作系统内核会先将这块数据从磁盘读出来放到操作系统内核的缓冲区里去。如果这个数据读取操作是由 DMA 完成的，那么在 DMA 进行数据读取的这一过程中，CPU 只是需要进行缓冲区管理，以及创建和处理 DMA ，除此之外，CPU 不需要再做更多的事情，DMA 执行完数据读取操作之后，会通知操作系统做进一步的处理。Linux 操作系统会根据 read() 系统调用指定的应用程序地址空间的地址，把这块数据存放到请求这块数据的应用程序的地址空间中去，在接下来的处理过程中，操作系统需要将数据再一次从用户应用程序地址空间的缓冲区拷贝到与网络堆栈相关的内核缓冲区中去，这个过程也是需要占用 CPU 的。数据拷贝操作结束以后，数据会被打包，然后发送到网络接口卡上去。在数据传输的过程中，应用程序可以先返回进而执行其他的操作。之后，在调用 write() 系统调用的时候，用户应用程序缓冲区中的数据内容可以被安全的丢弃或者更改，因为操作系统已经在内核缓冲区中保留了一份数据拷贝，当数据被成功传送到硬件上之后，这份数据拷贝就可以被丢弃。 从上面的描述可以看出，在这种传统的数据传输过程中，数据至少发生了四次拷贝操作，即便是使用了 DMA 来进行与硬件的通讯，CPU 仍然需要访问数据两次。在 read() 读数据的过程中，数据并不是直接来自于硬盘，而是必须先经过操作系统的文件系统层。在 write() 写数据的过程中，为了和要传输的数据包的大小相吻合，数据必须要先被分割成块，而且还要预先考虑包头，并且要进行数据校验和操作。 什么是零拷贝？ 简单一点来说，零拷贝就是一种避免 CPU 将数据从一块存储拷贝到另外一块存储的技术。针对操作系统中的设备驱动程序、文件系统以及网络协议堆栈而出现的各种零拷贝技术极大地提升了特定应用程序的性能，并且使得这些应用程序可以更加有效地利用系统资源。这种性能的提升就是通过在数据拷贝进行的同时，允许 CPU 执行其他的任务来实现的。零拷贝技术可以减少数据拷贝和共享总线操作的次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效地提高数据传输效率。而且，零拷贝技术减少了用户应用程序地址空间和操作系统内核地址空间之间因为上下文切换而带来的开销。进行大量的数据拷贝操作其实是一件简单的任务，从操作系统的角度来说，如果 CPU 一直被占用着去执行这项简单的任务，那么这将会是很浪费资源的；如果有其他比较简单的系统部件可以代劳这件事情，从而使得 CPU 解脱出来可以做别的事情，那么系统资源的利用则会更加有效。综上所述，零拷贝技术的目标可以概括如下： 避免数据拷贝 避免操作系统内核缓冲区之间进行数据拷贝操作。 避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作。 用户应用程序可以避开操作系统直接访问硬件存储。 数据传输尽量让 DMA 来做。 将多种操作结合在一起 避免不必要的系统调用和上下文切换。 需要拷贝的数据可以先被缓存起来。 对数据进行处理尽量让硬件来做。 前文提到过，对于高速网络来说，零拷贝技术是非常重要的。这是因为高速网络的网络链接能力与 CPU 的处理能力接近，甚至会超过 CPU 的处理能力。如果是这样的话，那么 CPU 就有可能需要花费几乎所有的时间去拷贝要传输的数据，而没有能力再去做别的事情，这就产生了性能瓶颈，限制了通讯速率，从而降低了网络链接的能力。一般来说，一个 CPU 时钟周期可以处理一位的数据。举例来说，一个 1 GHz 的处理器可以对 1Gbit/s 的网络链接进行传统的数据拷贝操作，但是如果是 10 Gbit/s 的网络，那么对于相同的处理器来说，零拷贝技术就变得非常重要了。对于超过 1 Gbit/s 的网络链接来说，零拷贝技术在超级计算机集群以及大型的商业数据中心中都有所应用。然而，随着信息技术的发展，1 Gbit/s，10 Gbit/s 以及 100 Gbit/s 的网络会越来越普及，那么零拷贝技术也会变得越来越普及，这是因为网络链接的处理能力比 CPU 的处理能力的增长要快得多。传统的数据拷贝受限于传统的操作系统或者通信协议，这就限制了数据传输性能。零拷贝技术通过减少数据拷贝次数，简化协议处理的层次，在应用程序和网络之间提供更快的数据传输方法，从而可以有效地降低通信延迟，提高网络吞吐率。零拷贝技术是实现主机或者路由器等设备高速网络接口的主要技术之一。 现代的 CPU 和存储体系结构提供了很多相关的功能来减少或避免 I/O 操作过程中产生的不必要的 CPU 数据拷贝操作，但是，CPU 和存储体系结构的这种优势经常被过高估计。存储体系结构的复杂性以及网络协议中必需的数据传输可能会产生问题，有时甚至会导致零拷贝这种技术的优点完全丧失。在下一章中，我们会介绍几种 Linux 操作系统中出现的零拷贝技术，简单描述一下它们的实现方法，并对它们的弱点进行分析。 零拷贝技术分类 零拷贝技术的发展很多样化，现有的零拷贝技术种类也非常多，而当前并没有一个适合于所有场景的零拷贝技术的出现。对于 Linux 来说，现存的零拷贝技术也比较多，这些零拷贝技术大部分存在于不同的 Linux 内核版本，有些旧的技术在不同的 Linux 内核版本间得到了很大的发展或者已经渐渐被新的技术所代替。本文针对这些零拷贝技术所适用的不同场景对它们进行了划分。概括起来，Linux 中的零拷贝技术主要有下面这几种： 直接 I/O：对于这种数据传输方式来说，应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输：这类零拷贝技术针对的是操作系统内核并不需要对数据进行直接处理的情况，数据可以在应用程序地址空间的缓冲区和磁盘之间直接进行传输，完全不需要 Linux 操作系统内核提供的页缓存的支持。 在数据传输的过程中，避免数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间进行拷贝。有的时候，应用程序在数据进行传输的过程中不需要对数据进行访问，那么，将数据从 Linux 的页缓存拷贝到用户进程的缓冲区中就可以完全避免，传输的数据在页缓存中就可以得到处理。在某些特殊的情况下，这种零拷贝技术可以获得较好的性能。Linux 中提供类似的系统调用主要有 mmap()，sendfile() 以及 splice()。 对数据在 Linux 的页缓存和用户进程的缓冲区之间的传输过程进行优化。该零拷贝技术侧重于灵活地处理数据在用户进程的缓冲区和操作系统的页缓存之间的拷贝操作。这种方法延续了传统的通信方式，但是更加灵活。在　 Linux 　中，该方法主要利用了写时复制技术。 前两类方法的目的主要是为了避免应用程序地址空间和操作系统内核地址空间这两者之间的缓冲区拷贝操作。这两类零拷贝技术通常适用在某些特殊的情况下，比如要传送的数据不需要经过操作系统内核的处理或者不需要经过应用程序的处理。第三类方法则继承了传统的应用程序地址空间和操作系统内核地址空间之间数据传输的概念，进而针对数据传输本身进行优化。我们知道，硬件和软件之间的数据传输可以通过使用 DMA 来进行，DMA 　进行数据传输的过程中几乎不需要　 CPU 　参与，这样就可以把 CPU 解放出来去做更多其他的事情，但是当数据需要在用户地址空间的缓冲区和　 Linux 　操作系统内核的页缓存之间进行传输的时候，并没有类似　 DMA 　这种工具可以使用，CPU 　需要全程参与到这种数据拷贝操作中，所以这第三类方法的目的是可以有效地改善数据在用户地址空间和操作系统内核地址空间之间传递的效率。 Linux 中的直接 I/O 如果应用程序可以直接访问网络接口存储，那么在应用程序访问数据之前存储总线就不需要被遍历，数据传输所引起的开销将会是最小的。应用程序或者运行在用户模式下的库函数可以直接访问硬件设备的存储，操作系统内核除了进行必要的虚拟存储配置工作之外，不参与数据传输过程中的其它任何事情。直接 I/O 使得数据可以直接在应用程序和外围设备之间进行传输，完全不需要操作系统内核页缓存的支持。关于直接 I/O 技术的具体实现细节可以参看 developerWorks 上的另一篇文章”Linux 中直接 I/O 机制的介绍” ，本文不做过多描述。 针对数据传输不需要经过应用程序地址空间的零拷贝技术 利用 mmap() 在 Linux 中，减少拷贝次数的一种方法是调用 mmap() 来代替调用 read，比如： tmp_buf = mmap(file, len); write(socket, tmp_buf, len); 首先，应用程序调用了 mmap() 之后，数据会先通过 DMA 拷贝到操作系统内核的缓冲区中去。接着，应用程序跟操作系统共享这个缓冲区，这样，操作系统内核和应用程序存储空间就不需要再进行任何的数据拷贝操作。应用程序调用了 write() 之后，操作系统内核将数据从原来的内核缓冲区中拷贝到与 socket 相关的内核缓冲区中。接下来，数据从内核 socket 缓冲区拷贝到协议引擎中去，这是第三次数据拷贝操作。 通过使用 mmap() 来代替 read(), 已经可以减半操作系统需要进行数据拷贝的次数。当大量数据需要传输的时候，这样做就会有一个比较好的效率。但是，这种改进也是需要代价的，使用 mma()p 其实是存在潜在的问题的。当对文件进行了内存映射，然后调用 write() 系统调用，如果此时其他的进程截断了这个文件，那么 write() 系统调用将会被总线错误信号 SIGBUS 中断，因为此时正在执行的是一个错误的存储访问。这个信号将会导致进程被杀死，解决这个问题可以通过以下这两种方法： 为 SIGBUS 安装一个新的信号处理器，这样，write() 系统调用在它被中断之前就返回已经写入的字节数目，errno 会被设置成 success。但是这种方法也有其缺点，它不能反映出产生这个问题的根源所在，因为 BIGBUS 信号只是显示某进程发生了一些很严重的错误。 第二种方法是通过文件租借锁来解决这个问题的，这种方法相对来说更好一些。我们可以通过内核对文件加读或者写的租借锁，当另外一个进程尝试对用户正在进行传输的文件进行截断的时候，内核会发送给用户一个实时信号：RT_SIGNAL_LEASE 信号，这个信号会告诉用户内核破坏了用户加在那个文件上的写或者读租借锁，那么 write() 系统调用则会被中断，并且进程会被 SIGBUS 信号杀死，返回值则是中断前写的字节数，errno 也会被设置为 success。文件租借锁需要在对文件进行内存映射之前设置。 使用 mmap 是 POSIX 兼容的，但是使用 mmap 并不一定能获得理想的数据传输性能。数据传输的过程中仍然需要一次 CPU 拷贝操作，而且映射操作也是一个开销很大的虚拟存储操作，这种操作需要通过更改页表以及冲刷 TLB （使得 TLB 的内容无效）来维持存储的一致性。但是，因为映射通常适用于较大范围，所以对于相同长度的数据来说，映射所带来的开销远远低于 CPU 拷贝所带来的开销。 sendfile() 为了简化用户接口，同时还要继续保留 mmap()/write() 技术的优点：减少 CPU 的拷贝次数，Linux 在版本 2.1 中引入了 sendfile() 这个系统调用。 sendfile() 不仅减少了数据拷贝操作，它也减少了上下文切换。首先：sendfile() 系统调用利用 DMA 引擎将文件中的数据拷贝到操作系统内核缓冲区中，然后数据被拷贝到与 socket 相关的内核缓冲区中去。接下来，DMA 引擎将数据从内核 socket 缓冲区中拷贝到协议引擎中去。如果在用户调用 sendfile () 系统调用进行数据传输的过程中有其他进程截断了该文件，那么 sendfile () 系统调用会简单地返回给用户应用程序中断前所传输的字节数，errno 会被设置为 success。如果在调用 sendfile() 之前操作系统对文件加上了租借锁，那么 sendfile() 的操作和返回状态将会和 mmap()/write () 一样。 sendfile() 系统调用不需要将数据拷贝或者映射到应用程序地址空间中去，所以 sendfile() 只是适用于应用程序地址空间不需要对所访问数据进行处理的情况。相对于 mmap() 方法来说，因为 sendfile 传输的数据没有越过用户应用程序 / 操作系统内核的边界线，所以 sendfile () 也极大地减少了存储管理的开销。但是，sendfile () 也有很多局限性，如下所列： sendfile() 局限于基于文件服务的网络应用程序，比如 web 服务器。据说，在 Linux 内核中实现 sendfile() 只是为了在其他平台上使用 sendfile() 的 Apache 程序。 由于网络传输具有异步性，很难在 sendfile () 系统调用的接收端进行配对的实现方式，所以数据传输的接收端一般没有用到这种技术。 基于性能的考虑来说，sendfile () 仍然需要有一次从文件到 socket 缓冲区的 CPU 拷贝操作，这就导致页缓存有可能会被传输的数据所污染。 带有 DMA 收集拷贝功能的 sendfile() 上小节介绍的 sendfile() 技术在进行数据传输仍然还需要一次多余的数据拷贝操作，通过引入一点硬件上的帮助，这仅有的一次数据拷贝操作也可以避免。为了避免操作系统内核造成的数据副本，需要用到一个支持收集操作的网络接口，这也就是说，待传输的数据可以分散在存储的不同位置上，而不需要在连续存储中存放。这样一来，从文件中读出的数据就根本不需要被拷贝到 socket 缓冲区中去，而只是需要将缓冲区描述符传到网络协议栈中去，之后其在缓冲区中建立起数据包的相关结构，然后通过 DMA 收集拷贝功能将所有的数据结合成一个网络数据包。网卡的 DMA 引擎会在一次操作中从多个位置读取包头和数据。Linux 2.4 版本中的 socket 缓冲区就可以满足这种条件，这也就是用于 Linux 中的众所周知的零拷贝技术，这种方法不但减少了因为多次上下文切换所带来开销，同时也减少了处理器造成的数据副本的个数。对于用户应用程序来说，代码没有任何改变。首先，sendfile() 系统调用利用 DMA 引擎将文件内容拷贝到内核缓冲区去；然后，将带有文件位置和长度信息的缓冲区描述符添加到 socket 缓冲区中去，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，DMA 引擎会将数据直接从内核缓冲区拷贝到协议引擎中去，这样就避免了最后一次数据拷贝。 通过这种方法，CPU 在数据传输的过程中不但避免了数据拷贝操作，理论上，CPU 也永远不会跟传输的数据有任何关联，这对于 CPU 的性能来说起到了积极的作用：首先，高速缓冲存储器没有受到污染；其次，高速缓冲存储器的一致性不需要维护，高速缓冲存储器在 DMA 进行数据传输前或者传输后不需要被刷新。然而实际上，后者实现起来非常困难。源缓冲区有可能是页缓存的一部分，这也就是说一般的读操作可以访问它，而且该访问也可以是通过传统方式进行的。只要存储区域可以被 CPU 访问到，那么高速缓冲存储器的一致性就需要通过 DMA 传输之前冲刷新高速缓冲存储器来维护。而且，这种数据收集拷贝功能的实现是需要硬件以及设备驱动程序支持的。 splice() splice() 是　 Linux 　中与 mmap() 和　 sendfile() 类似的一种方法。它也可以用于用户应用程序地址空间和操作系统地址空间之间的数据传输。splice() 适用于可以确定数据传输路径的用户应用程序，它不需要利用用户地址空间的缓冲区进行显式的数据传输操作。那么，当数据只是从一个地方传送到另一个地方，过程中所传输的数据不需要经过用户应用程序的处理的时候，spice() 就成为了一种比较好的选择。splice() 可以在操作系统地址空间中整块地移动数据，从而减少大多数数据拷贝操作。而且，splice() 进行数据传输可以通过异步的方式来进行，用户应用程序可以先从系统调用返回，而操作系统内核进程会控制数据传输过程继续进行下去。splice() 可以被看成是类似于基于流的管道的实现，管道可以使得两个文件描述符相互连接，splice 的调用者则可以控制两个设备（或者协议栈）在操作系统内核中的相互连接。 splice() 系统调用和 sendfile() 非常类似，用户应用程序必须拥有两个已经打开的文件描述符，一个用于表示输入设备，一个用于表示输出设备。与 sendfile() 不同的是，splice() 允许任意两个文件之间互相连接，而并不只是文件到 socket 进行数据传输。对于从一个文件描述符发送数据到 socket 这种特例来说，一直都是使用 sendfile() 这个系统调用，而 splice 一直以来就只是一种机制，它并不仅限于 sendfile() 的功能。也就是说，sendfile() 只是 splice() 的一个子集，在 Linux 2.6.23 中，sendfile() 这种机制的实现已经没有了，但是这个 API 以及相应的功能还存在，只不过 API 以及相应的功能是利用了 splice() 这种机制来实现的。 在数据传输的过程中，splice() 机制交替地发送相关的文件描述符的读写操作，并且可以将读缓冲区重新用于写操作。它也利用了一种简单的流控制，通过预先定义的水印（ watermark ）来阻塞写请求。有实验表明，利用这种方法将数据从一个磁盘传输到另一个磁盘会增加 30% 到 70% 的吞吐量，数据传输的过程中， CPU 的负载也会减少一半。 Linux 2.6.17 内核引入了 splice() 系统调用，但是，这个概念在此之前 ] 其实已经存在了很长一段时间了。1988 年，Larry McVoy 提出了这个概念，它被看成是一种改进服务器端系统的 I/O 性能的一种技术，尽管在之后的若干年中经常被提及，但是 splice 系统调用从来没有在主流的 Linux 操作系统内核中实现过，一直到 Linux 2.6.17 版本的出现。splice 系统调用需要用到四个参数，其中两个是文件描述符，一个表示文件长度，还有一个用于控制如何进行数据拷贝。splice 系统调用可以同步实现，也可以使用异步方式来实现。在使用异步方式的时候，用户应用程序会通过信号 SIGIO 来获知数据传输已经终止。splice() 系统调用的接口如下所示： long splice(int fdin, int fdout, size_t len, unsigned int flags); 调用 splice() 系统调用会导致操作系统内核从数据源 fdin 移动最多 len 个字节的数据到 fdout 中去，这个数据的移动过程只是经过操作系统内核空间，需要最少的拷贝次数。使用 splice() 系统调用需要这两个文件描述符中的一个必须是用来表示一个管道设备的。不难看出，这种设计具有局限性，Linux 的后续版本针对这一问题将会有所改进。参数 flags 用于表示拷贝操作的执行方法，当前的 flags 有如下这些取值： SPLICE_F_NONBLOCK：splice 操作不会被阻塞。然而，如果文件描述符没有被设置为不可被阻塞方式的 I/O ，那么调用 splice 有可能仍然被阻塞。 SPLICE_F_MORE：告知操作系统内核下一个 splice 系统调用将会有更多的数据传来。 SPLICE_F_MOVE：如果输出是文件，这个值则会使得操作系统内核尝试从输入管道缓冲区直接将数据读入到输出地址空间，这个数据传输过程没有任何数据拷贝操作发生。 Splice() 系统调用利用了 Linux 提出的管道缓冲区（ pipe buffer ）机制，这就是为什么这个系统调用的两个文件描述符参数中至少有一个必须要指代管道设备的原因。为了支持 splice 这种机制，Linux 在用于设备和文件系统的 file_operations 结构中增加了下边这两个定义： ssize_t (*splice_write)(struct inode *pipe, strucuct file *out, size_t len, unsigned int flags); ssize_t (*splice_read)(struct inode *in, strucuct file *pipe, size_t len, unsigned int flags); 这两个新的操作可以根据 flags 的设定在 pipe 和 in 或者 out 之间移动 len 个字节。Linux 文件系统已经实现了具有上述功能并且可以使用的操作，而且还实现了一个 generic_splice_sendpage() 函数用于和 socket 之间的接合。 对应用程序地址空间和内核之间的数据传输进行优化的零拷贝技术 前面提到的几种零拷贝技术都是通过尽量避免用户应用程序和操作系统内核缓冲区之间的数据拷贝来实现的，使用上面那些零拷贝技术的应用程序通常都要局限于某些特殊的情况：要么不能在操作系统内核中处理数据，要么不能在用户地址空间中处理数据。而这一小节提出的零拷贝技术保留了传统在用户应用程序地址空间和操作系统内核地址空间之间传递数据的技术，但却在传输上进行优化。我们知道，数据在系统软件和硬件之间的传递可以通过 DMA 传输来提高效率，但是对于用户应用程序和操作系统之间进行数据传输这种情况来说，并没有类似的工具可以使用。本节介绍的技术就是针对这种情况提出来的。 利用写时复制 在某些情况下，Linux 操作系统内核中的页缓存可能会被多个应用程序所共享，操作系统有可能会将用户应用程序地址空间缓冲区中的页面映射到操作系统内核地址空间中去。如果某个应用程序想要对这共享的数据调用　 write() 系统调用，那么它就可能破坏内核缓冲区中的共享数据，传统的 write() 系统调用并没有提供任何显示的加锁操作，Linux 中引入了写时复制这样一种技术用来保护数据。 什么是写时复制 写时复制是计算机编程中的一种优化策略，它的基本思想是这样的：如果有多个应用程序需要同时访问同一块数据，那么可以为这些应用程序分配指向这块数据的指针，在每一个应用程序看来，它们都拥有这块数据的一份数据拷贝，当其中一个应用程序需要对自己的这份数据拷贝进行修改的时候，就需要将数据真正地拷贝到该应用程序的地址空间中去，也就是说，该应用程序拥有了一份真正的私有数据拷贝，这样做是为了避免该应用程序对这块数据做的更改被其他应用程序看到。这个过程对于应用程序来说是透明的，如果应用程序永远不会对所访问的这块数据进行任何更改，那么就永远不需要将数据拷贝到应用程序自己的地址空间中去。这也是写时复制的最主要的优点。 写时复制的实现需要 MMU 的支持，MMU 需要知晓进程地址空间中哪些特殊的页面是只读的，当需要往这些页面中写数据的时候，MMU 就会发出一个异常给操作系统内核，操作系统内核就会分配新的物理存储空间，即将被写入数据的页面需要与新的物理存储位置相对应。 写时复制的最大好处就是可以节约内存。不过对于操作系统内核来说，写时复制增加了其处理过程的复杂性。 数据传输的实现及其局限性 数据发送端 对于数据传输的发送端来说，实现相对来说是比较简单的，对与应用程序缓冲区相关的物理页面进行加锁，并将这些页面映射到操作系统内核的地址空间，并标识为“ write only ”。当系统调用返回的时候，用户应用程序和网络堆栈就都可以读取该缓冲区中的数据。在操作系统已经传送完所有的数据之后，应用程序就可以对这些数据进行写操作。如果应用程序尝试在数据传输完成之前对数据进行写操作，那么就会产生异常，这个时候操作系统就会将数据拷贝到应用程序自己的缓冲区中去，并且重置应用程序端的映射。数据传输完成之后，对加锁的页面进行解锁操作，并重置 COW 标识。 数据接收端 对于数据接收端来说，该技术的实现则需要处理复杂得多的情况。如果 read() 系统调用是在数据包到达之前发出的，并且应用程序是被阻塞的，那么 read() 系统调用就会告知操作系统接收到的数据包中的数据应该存放到什么地方去。在这种情况下，根本没有必要进行页面重映射，网络接口卡可以提供足够的支持让数据直接存入用户应用程序的缓冲区中去。如果数据接收是异步的，在 read() 系统调用发出之前，操作系统不知道该把数据写到哪里，因为它不知道用户应用程序缓冲区的位置，所以操作系统内核必须要先把数据存放到自己的缓冲区中去。 局限性 写时复制技术有可能会导致操作系统的处理开销很大．所有相关的缓冲区都必须要进行页对齐处理，并且使用的 MMU 页面一定要是整数个的。对于发送端来说，这不会造成什么问题。但是对于接收端来说，它需要有能力处理更加复杂的情况。首先，数据包的尺寸大小要合适，大小需要恰到好处能够覆盖一整页的数据，这就限制了那些 MTU 大小大于系统内存页的网络，比如 FDDI 和 ATM。其次，为了在没有任何中断的情况下将页面重映射到数据包的流，数据包中的数据部分必须占用整数个页面。对于异步接收数据的情况来说，为了将数据高效地移动到用户地址空间中去，可以使用这样一种方法：利用网络接口卡的支持，传来的数据包可以被分割成包头和数据两部分，数据被存放在一个单独的缓冲区内，虚拟存储系统然后就会将数据映射到用户地址空间缓冲区去。使用这种方法需要满足两个先决条件，也就是上面提到过的：一是应用程序缓冲区必须是页对齐的，并且在虚拟存储上是连续的；二是传来的数据有一页大小的时候才可以对数据包进行分割。事实上，这两个先决条件是很难满足的。如果应用程序缓冲区不是页对齐的，或者数据包的大小超过一个页，那么数据就需要被拷贝。对于数据发送端来说，就算数据在传输的过程中对于应用程序来说是写保护的，应用程序仍然需要避免使用这些忙缓冲区，这是因为写时拷贝操作所带来的开销是很大的。如果没有端到端这一级别的通知，那么应用程序很难会知道某缓冲区是否已经被释放还是仍然在被占用。 这种零拷贝技术比较适用于那种写时复制事件发生比较少的情况，因为写时复制事件所产生的开销要远远高于一次 CPU 拷贝所产生的开销。实际情况中，大多数应用程序通常都会多次重复使用相同的缓冲区，所以，一次使用完数据之后，不要从操作系统地址空间解除页面的映射，这样会提高效率。考虑到同样的页面可能会被再次访问，所以保留页面的映射可以节省管理开销，但是，这种映射保留不会减少由于页表往返移动和 TLB 冲刷所带来的开销，这是因为每次页面由于写时复制而进行加锁或者解锁的时候，页面的只读标志都要被更改。 缓冲区共享 还有另外一种利用预先映射机制的共享缓冲区的方法也可以在应用程序地址空间和操作系统内核之间快速传输数据。采用缓冲区共享这种思想的架构最先在 Solaris 上实现，该架构使用了“ fbufs ”这个概念。这种方法需要修改 API。应用程序地址空间和操作系统内核地址空间之间的数据传递需要严格按照 fbufs 体系结构来实现，操作系统内核之间的通信也是严格按照 fbufs 体系结构来完成的。每一个应用程序都有一个缓冲区池，这个缓冲区池被同时映射到用户地址空间和内核地址空间，也可以在必要的时候才创建它们。通过完成一次虚拟存储操作来创建缓冲区，fbufs 可以有效地减少由存储一致性维护所引起的大多数性能问题。该技术在 Linux 中还停留在实验阶段。 为什么要扩展 Linux I/O API 传统的 Linux 输入输出接口，比如读和写系统调用，都是基于拷贝的，也就是说，数据需要在操作系统内核和应用程序定义的缓冲区之间进行拷贝。对于读系统调用来说，用户应用程序呈现给操作系统内核一个预先分配好的缓冲区，内核必须把读进来的数据放到这个缓冲区内。对于写系统调用来说，只要系统调用返回，用户应用程序就可以自由重新利用数据缓冲区。 为了支持上面这种机制，Linux 需要能够为每一个操作都进行建立和删除虚拟存储映射。这种页面重映射的机制依赖于机器配置、cache 体系结构、TLB 未命中处理所带来的开销以及处理器是单处理器还是多处理器等多种因素。如果能够避免处理 I/O 请求的时候虚拟存储 / TLB 操作所产生的开销，则会极大地提高 I/O 的性能。fbufs 就是这样一种机制。使用 fbufs 体系结构就可以避免虚拟存储操作。由数据显示，fbufs 这种结构在 DECStation™ 5000/200 这个单处理器工作站上会取得比上面提到的页面重映射方法好得多的性能。如果要使用 fbufs 这种体系结构，必须要扩展 Linux API，从而实现一种有效而且全面的零拷贝技术。 快速缓冲区（ Fast Buffers ）原理介绍 I/O 数据存放在一些被称作 fbufs 的缓冲区内，每一个这样的缓冲区都包含一个或者多个连续的虚拟存储页。应用程序访问 fbuf 是通过保护域来实现的，有如下这两种方式： 如果应用程序分配了 fbuf，那么应用程序就有访问该 fbuf 的权限 如果应用程序通过 IPC 接收到了 fbuf，那么应用程序对这个 fbuf 也有访问的权限 对于第一种情况来说，这个保护域被称作是 fbuf 的“ originator ”；对于后一种情况来说，这个保护域被称作是 fbuf 的“ receiver ”。 传统的 Linux I/O 接口支持数据在应用程序地址空间和操作系统内核之间交换，这种交换操作导致所有的数据都需要进行拷贝。如果采用 fbufs 这种方法，需要交换的是包含数据的缓冲区，这样就消除了多余的拷贝操作。应用程序将 fbuf 传递给操作系统内核，这样就能减少传统的 write 系统调用所产生的数据拷贝开销。同样的，应用程序通过 fbuf 来接收数据，这样也可以减少传统 read 系统调用所产生的数据拷贝开销。如下图所示： I/O 子系统或者应用程序都可以通过 fbufs 管理器来分配 fbufs。一旦分配了 fbufs，这些 fbufs 就可以从程序传递到 I/O 子系统，或者从 I/O 子系统传递到程序。使用完后，这些 fbufs 会被释放回 fbufs 缓冲区池。 fbufs 在实现上有如下这些特性，如图 9 所示： fbuf 需要从 fbufs 缓冲区池里分配。每一个 fbuf 都存在一个所属对象，要么是应用程序，要么是操作系统内核。fbuf 可以在应用程序和操作系统之间进行传递，fbuf 使用完之后需要被释放回特定的 fbufs 缓冲区池，在 fbuf 传递的过程中它们需要携带关于 fbufs 缓冲区池的相关信息。 每一个 fbufs 缓冲区池都会和一个应用程序相关联，一个应用程序最多只能与一个 fbufs 缓冲区池相关联。应用程序只有资格访问它自己的缓冲区池。 fbufs 不需要虚拟地址重映射，这是因为对于每个应用程序来说，它们可以重新使用相同的缓冲区集合。这样，虚拟存储转换的信息就可以被缓存起来，虚拟存储子系统方面的开销就可以消除。 I/O 子系统（设备驱动程序，文件系统等）可以分配 fbufs，并将到达的数据直接放到这些 fbuf 里边。这样，缓冲区之间的拷贝操作就可以避免。 前面提到，这种方法需要修改 API，如果要使用 fbufs 体系结构，应用程序和 Linux 操作系统内核驱动程序都需要使用新的 API，如果应用程序要发送数据，那么它就要从缓冲区池里获取一个 fbuf，将数据填充进去，然后通过文件描述符将数据发送出去。接收到的 fbufs 可以被应用程序保留一段时间，之后，应用程序可以使用它继续发送其他的数据，或者还给缓冲区池。但是，在某些情况下，需要对数据包内的数据进行重新组装，那么通过 fbuf 接收到数据的应用程序就需要将数据拷贝到另外一个缓冲区内。再者，应用程序不能对当前正在被内核处理的数据进行修改，基于这一点，fbufs 体系结构引入了强制锁的概念以保证其实现。对于应用程序来说，如果 fbufs 已经被发送给操作系统内核，那么应用程序就不会再处理这些 fbufs。 fbufs 存在的一些问题 管理共享缓冲区池需要应用程序、网络软件、以及设备驱动程序之间的紧密合作。对于数据接收端来说，网络硬件必须要能够将到达的数据包利用 DMA 传输到由接收端分配的正确的存储缓冲区池中去。而且，应用程序稍微不注意就会更改之前发到共享存储中的数据的内容，从而导致数据被破坏，但是这种问题在应用程序端是很难调试的。同时，共享存储这种模型很难与其他类型的存储对象关联使用，但是应用程序、网络软件以及设备驱动程序之间的紧密合作是需要其他存储管理器的支持的。对于共享缓冲区这种技术来说，虽然这种技术看起来前景光明，但是这种技术不但需要对 API 进行更改，而且需要对驱动程序也进行更改，并且这种技术本身也存在一些未解决的问题，这就使得这种技术目前还只是出于试验阶段。在测试系统中，这种技术在性能上有很大的改进，不过这种新的架构的整体安装目前看起来还是不可行的。这种预先分配共享缓冲区的机制有时也因为粒度问题需要将数据拷贝到另外一个缓冲区中去。 总结 本系列文章介绍了 Linux 中的零拷贝技术，本文是其中的第二部分。本文对第一部分文章中提出的 Linux 操作系统上出现的几种零拷贝技术进行了更详细的介绍，主要描述了它们各自的优点，缺点以及适用场景。对于网络数据传输来说，零拷贝技术的应用受到了很多体系结构方面因素的阻碍，包括虚拟存储体系结构以及网络协议体系结构等。所以，零拷贝技术仍然只是在某些很特殊的情况中才可以应用，比如文件服务或者使用某种特殊的协议进行高带宽的通信等。但是，零拷贝技术在磁盘操作中的应用的可行性就高得多了，这很可能是因为磁盘操作具有同步的特点，以及数据传输单元是按照页的粒度来进行的。 针对 Linux 操作系统平台提出并实现了很多种零拷贝技术，但是并不是所有这些零拷贝技术都被广泛应用于现实中的操作系统中的。比如，fbufs 体系结构，它在很多方面看起来都很吸引人，但是使用它需要更改 API 以及驱动程序，它还存在其他一些实现上的困难，这就使得 fbufs 还只是停留在实验的阶段。动态地址重映射技术只是需要对操作系统做少量修改，虽然不需要修改用户软件，但是当前的虚拟存储体系结构并不能很好地支持频繁的虚拟地址重映射操作。而且为了保证存储的一致性，重映射之后还必须对 TLB 和一级缓存进行刷新。事实上，利用地址重映射实现的零拷贝技术适用的范围是很小的，这是因为虚拟存储操作所带来的开销往往要比 CPU 拷贝所产生的开销还要大。此外，为了完全消除 CPU 访问存储，通常都需要额外的硬件来支持，而这种硬件的支持并不是很普及，同时也是非常昂贵的。 本系列文章的目的是想帮助读者理清这些出现在 Linux 操作系统中的零拷贝技术都是从何种角度来帮助改善数据传输过程中遇到的性能问题的。关于各种零拷贝技术的具体实现细节，本系列文章没有做详细描述。同时，零拷贝技术一直是在不断地发展和完善当中的，本系列文章并没有涵盖 Linux 上出现的所有零拷贝技术。 相关主题 Zero Copy I: User-Mode Perspective, Linux Journal (2003), http://www.linuxjournal.com/node/6345描述了几种 Linux 操作系统中存在的零拷贝技术。 在 http://www.ibm.com/developerworks/cn/aix/library/au-tcpsystemcalls/这篇文章中可以找到关于 TCP 系统调用序列的介绍。 关于 Linux 中直接 I/O 技术的设计与实现请参考 developerWorks 上的文章 ”Linux 中直接 I/O 机制的介绍”。 http://lwn.net/Articles/28548/上可以找到 Linux 上零拷贝技术中关于用户地址空间访问技术的介绍。 http://articles.techrepublic.com.com/5100-10878_11-1044112.html?tag=content;leftCol，这篇文章描述了如何利用 sendfile() 优化数据传输的介绍。 关于 splice() 系统调用的介绍，可以参考 http://lwn.net/Articles/178199/。 http://en.scientificcommons.org/43480276这篇文章介绍了如何通过在操作系统内核中直接传输数据来提高 I/O 的性能以及 CPU 的可用性，并详细描述了 splice 系统调用如何在用户应用程序不参与的情况下进行异步数据传输。 在 Understanding the Linux Kernel(3rd Edition) 中，有关于 Linux 内核的实现。 http://pages.cs.wisc.edu/~cao/cs736/slides/cs736-class18.ps这篇文章提出并详细介绍了 fbufs 这种用于 I/O 缓冲区管理的机制，以及基于共享存储的数据传输机制。 http://www.cs.inf.ethz.ch/group/stricker/sada/archive/chihaia.pdf，这篇文章列举了一些 Linux 中出现的零拷贝技术，并在测试系统上实现了 fbufs 技术。 "},"Chapter16/Binary.html":{"url":"Chapter16/Binary.html","title":"二进制","keywords":"","body":"二进制 二进制补码计算原理详解 二进制负数的在计算机中采用补码的方式表示。很多人很好奇为什么使用补码，直接使用原码表示多好，看上去更加直观和易于计算。然而事实告诉我们，这种直观只是我们人类的一厢情愿罢了，在计算机看来，补码才是它们最想要的。那么，为什么计算机使用补码更好，又是如何通过补码来计算数值的呢？ 我看过网络上很多解释补码的文章，几乎一致的回答就是符号位不变，其他各位逐位求反再加一。在此我想说，这些都不是根本原理。谁都知道这么求，数电第一章就明确写了怎么求，关键是为什么这么算，其中的原理是什么？ 本文主要的内容就是深入讲解补码的原理，其中内容有相互引用成分及计算机基础要求，不适合初学者阅读。当然，随便看看无所谓啦。 1.什么是补码 这个没有找到官方定义，只进行个人定义。 个人定义：补码是计算机中用来表示负数，使得负数能够使用加法器参与加法运算的一种码。 加减是计算机中最常用的运算，加法一般使用加法器来实现，减法则使用减法器实现。那有什么办法可以将减法变为加法，这样就可以让系统只实现加法即可，答案就是补码。 理解补码最简单的例子就是时钟。 例1： 假如一个时钟现在显示的是10点钟，如何将它调到6点钟？ 解：有两种方法，一是向后拨8个小时，二是向前拨4个小时 在这个例子中，8 和 4 互为补数，也就是说4的补码是8,8的补码是4，而这个时钟的模就是12 注意：可能有人会想，在往后调8个小时虽然也调到了6点，但是他实际上比原来日期多了12小时。是的，的确如此，但是你的时钟有地方存储了这多余的12个小时吗？答案是没有，所以在你调完后，你没有记录这12个小时，换句话说，你把这溢出的12个小时自动舍弃了。当第二个人来查看闹钟时间的时候，他看到的时间就是准的。 例2： 一个数的数值是11，他的模是16，那么他的补码是多少？ 解：16-11 = 5，即补码就是5。 2.模 关于模没有找到固定的定义，简单来说模就是一个循环的周期，在例1中，时钟的一个周期就是12，所以模是12。一周的模是7天，一天的模是24小时。模是补码的一个重要概念。 3.使用补码运算 例3 模为32，使用补码运算该算式16-13，13 - 16。 解 16-13 = (16 + （32-13）)% 32 = 35 % 32 = 3 13-16 = (13 + （32-16）)% 32 = 29 % 32 = 29 4.使用补码进行二进制运算 有看过数电基础的都应该记得，第一章就有说明二进制补码是如何运算的。正数的补码即为自己，负数的补码为符号位不变，其余逐位求反再加1。 使用该定义，先通过例子求出数值，再对例子进行详细讲解，为什么可以使用负数的补码来运算。 例4 通过二进制求15-11的值 要想让减法变加法，必须转换算式为 15 + （-11） 15为正数，符号位为0，二进制表示为 01111 -11为负数，符号位为1，负数原码为 11011 ，补码符号位不变，其余逐位求反再 +1，即 10101 所以 15 + （-11） = 01111 + 10101 = 100100 舍弃第一位溢出位，即00100，即+4 这是一个最简单的补码算法运算的例子，却有很多不解之处。 1.为什么负数 （-11 ）逐位求反再+1就可以代表原来的数？ 2.为什么第一位舍弃 3.为什么符号位能够参与运算 先看问题1，-11 先不考虑符号，观察11的二进制表示，11使用二进制表示是 1011，将1011 逐位求反 得到 0100，因为是逐位求反，1011 +0100 = 1111 ，而1111 + 1 = 10000，发现了什么？10000是四位2二进制数的模。为什么10000是4位二进制数的模呢？原理也很简单，4位寄存器最高能表示什么数？即 （二进制）1111 = （十进制） 15 , 15 +1 =16 = （二进制）10000,后四位寄存器清0，所以模为16。 所以不管几位二进制数，取反后得到的值加原码会刚好的到所有的位都是1的二进制数，再加一就刚好进位得到模。所以取反加一是无论如何都能取到补码的。比如 ：（原）101 + （补)((反)010 +1） = 8 ，（原）10 + （补）（（反）01 +1）= 4。 所以，负数符号位不变，其余逐位求反 +1 只是算出补码最简单的方法，而不是理论基础。 前面提到过，使用补码代替原码，计算后模掉溢出得到数值就可以得到计算的值了。 再看问题2，为什么高位舍弃，这个问题其实在例1中已经做了说明，因为你没有存储这个高位的空间，用最简单的解释来解释就是，一个4位的寄存器，只能存储数据的后四位，第一位进位没地方放，就像例1中多出来的12小时，没有地方存储，那么就丢弃了，反正结果正确就行。可能有人会问，你说丢弃就丢弃吗，丢了你怎么保证是正确的？ 重新再看一次例4，现在考虑符号位，将符号位加入运算。 15 - 11 = 15 + （-11），其中 -11 为 11011（加了一位符号位，所以模为 16 * 2^1） 他的补码为 10101 = 21 所以 15 -11 变为 15 +21 = 36 （01111 + 10101） ，原先是减一个数，变换后却变成了一个比减数更大的加数了，能不溢出吗。所以要模掉溢出位 36 % 32 = 4 。仔细观察被模数36 = 1 00100 你会发现，在第二位之前的所有高位都是32的倍数，所以用32进行模运算就会全部清0，这就是为什么高位可以直接舍弃的原因，因为高位永远会是模的倍数，在模的过程中会被清0。 最后看问题3，为什么符号位能够参与运算，可能在这前面一直被符号位困扰，为什么参与运算就没有问题，看了这里的解释，我相信你讲不会再被困扰。 符号位对于我对二进制补码的理解产生了很大的阻碍，我不明白为什么符号位可以参与运算，而且算出来还是对的。花了不少时间理解了符号位的含义。 首先放弃符号位就是计算机表示专门定义出来表示正数和负数用的这个思维，换成这样一种理解思路：我们观察计算机中一个二进制数，它没有专门的符号位，但是正数和负数在计算机中存储第一位二进制数会存在差异，如果它的第一位是0，那么他就是正数原码，如果它的第一位是1，那么他就是负数补码，我们通过观察第一位是0还是1即可知道这个数是正数还是负数。 通过以下例子，可以发现符号位其实是算出来的。 例5：计算机运算 9 - 12 解：9 转五位二进制为 01001 //模为32（100000） 因为12是减数，所以将12转为补码变为加数，方便参与加法运算 12 = 01100(原) = 10011 + 1 = 10100(补) 01001 + 10100 = 11101 11101% 100000= 11101 从结果看，我们可以发现，第一位为1，第一位如果是1，那么这个数值是负数补码，由此可知，这个数是一个负数，只是在计算机中，他表示为11101，反求原码，即可得到 （11101 - 1） 反 = 00011 = 十进制数 3，由此可知，11101表示的是负数的3 从这个例子当中可以看到，在运算中完全没有考虑符号位，仅用单纯的加法运算进行运算，而得到的结果11101也没有人为去添加一个符号位，符号位是计算所得的，所以他才能参与运算。我们应该反过来看符号位，正是在补码运算中，存在第一位为0则是正数，为1则是负数这个规律，我们才方便地认定第一位为符号位，符号位跟人为定义没有任何关系（人为定义的符号位不可能满足数学规则参与数学运算）。 补充：在例5中可以发现，我们在第一位之前补了一个0，将四位二进制数补成五位二进制数来计算，为什么要补这个0呢？为了说明这个问题，我们来算一个错误的运算 例6：计算机运算 0 - 12 解 ： 我们仅仅用四位二进制数（四位寄存器）来算 12 = （二进制）1100 12转为二进制后是一个四位二进制数，它的模为10000 因为12是减数，所以将12转为补码变为加数，方便参与加法运算 12 = 1100(原) = 0011 + 1 = 0100(补) 0000 + 0100 = 0100 = 4 ，第一位是0，说明是正数原码， 即运算结果为 +4 4 != -12 运算出错 通过结果发现，只用四位二进制数运算，结果是错误的，为什么会产生这种错误呢？因为寄存器位数不够，四位寄存器只能存储 0 -15 的数值，没有多余的寄存器位置用来表示这个数的正负，我们虽然算出了符号位，但是计算后所得到的符号位却没有地方存放，导致符号位缺失，我们又误将四位二进制数结果的第一位当做符号位来判断数的正负，才会出现错误的结果。 通过例5我们可以知道，符号位是计算后得出来的，所以我们要预先要在寄存器第一位留出一个位置存放计算后得出的符号位，所以我们先在第一位前面补一个0占位。 由此可知： 要想计算结果正确， 需要满足 ： 寄存器位数 >= max(操作数1，操作数2,....) 的二进制位数加一位 最后，附上一张五位二进制数运算转换关系图，自己体会二进制数如何带着所谓的符号位在计算机中运算： 有如下几点： 1：五位二进制数的所有正数都在圆环的黑线上，而负数都在圆环的红线上，如果在黑线上，那么六位寄存器第一位必为0，而在红线上，那么第一位必为1，我们也是通过这个位来判断数值的正负 2：五位二进制数运算结果取模后如果在黑线上，那么为正数，否则为负数 3：尝试写几个数字转为五位二进制数或者写几个算式对号入座，观察是否符合，加深理解 "},"Chapter17/netty.html":{"url":"Chapter17/netty.html","title":"Part XVII netty篇","keywords":"","body":"netty netty是基于java NIO的。所以这里先讲NIO。 NIO基础知识 NIO示例 NIO与IO "},"Chapter17/io.html":{"url":"Chapter17/io.html","title":"IO","keywords":"","body":"IO 基础 Java IO 是一个庞大的知识体系 Java IO流是一个庞大的生态环境，其内部提供了很多不同的输入流和输出流，细分下去还有字节流和字符流，甚至还有缓冲流提高 IO 性能， 转换流将字节流转换为字符流······看到这些就已经对 IO 产生恐惧了，在日常开发中少不了对文件的 IO 操作，虽然 apache 已经提供了 Commons IO 这种封装好的组件，但面对特殊场景时， 我们仍需要自己去封装一个高性能的文件 IO 工具类，本文将会解析 Java IO 中涉及到的各个类，以及讲解如何正确、高效地使用它们。 什么是流 我们知道任何一个文件都是以二进制形式存在于设备中，计算机就只有 0 和 1，你能看见的东西全部都是由这两个数字组成，你看这篇文章时，这篇文章也是由01组成，只不过这些二进制串经过各种转换演变成一个个文字、一张张图片跃然屏幕上。 而流就是将这些二进制串在各种设备之间进行传输，如果你觉得有些抽象，我举个例子就会好理解一些： 下图是一张图片，它由01串组成，我们可以通过程序把一张图片拷贝到一个文件夹中， 把图片转化成二进制数据集，把数据一点一点地传递到文件夹中 , 类似于水的流动 , 这样整体的数据就是一个数据流 IO 流读写数据的特点： 顺序读写: 读写数据时，大部分情况下都是按照顺序读写，读取时从文件开头的第一个字节到最后一个字节，写出时也是也如此（RandomAccessFile 可以实现随机读写） 字节数组: 读写数据时本质上都是对字节数组做读取和写出操作，即使是字符流，也是在字节流基础上转化为一个个字符，所以字节数组是 IO 流读写数据的本质。 流的分类 根据数据流向不同分类：输入流 和 输出流 输入流：从磁盘或者其它设备中将数据输入到进程中 输出流：将进程中的数据输出到磁盘或其它设备上保存 图示中的硬盘只是其中一种设备，还有非常多的设备都可以应用在IO流中，例如：打印机、硬盘、显示器、手机······ 根据处理数据的基本单位不同分类：字节流 和 字符流 字节流：以字节（8 bit）为单位做数据的传输 字符流：以字符为单位（1字符 = 2字节）做数据的传输 字符流的本质也是通过字节流读取，Java 中的字符采用 Unicode 标准，在读取和输出的过程中，通过以字符为单位，查找对应的码表将字节转换为对应的字符。 面对字节流和字符流，很多读者都有疑惑：什么时候需要用字节流，什么时候又要用字符流？ 我这里做一个简单的概括，你可以按照这个标准去使用： 字符流只针对字符数据进行传输，所以如果是文本数据，优先采用字符流传输；除此之外，其它类型的数据（图片、音频等），最好还是以字节流传输。 根据这两种不同的分类，我们就可以做出下面这个表格，里面包含了 IO 中最核心的 4 个顶层抽象类： 数据流向 / 数据类型字节流字符流输入流InputStreamReader输出流OutputStreamWriter 现在看 IO 是不是有一些思路了，不会觉得很混乱了，我们来看这四个类下的所有成员。 看到这么多的类是不是又开始觉得混乱了，不要慌，字节流和字符流下的输入流和输出流大部分都是一一对应的，有了上面的表格支撑，我们不需要再担心看见某个类会懵逼的情况了。 看到 Stream 就知道是字节流，看到 Reader / Writer 就知道是字符流。 这里还要额外补充一点：Java IO 提供了字节流转换为字符流的转换类，称为转换流。 转换流 / 数据类型字节流与字符流之间的转换 （输入）字节流 => InputStreamReader** => 字符流 （输出）字符流 => OutputStreamWriter => 字节流 注意字节流与字符流之间的转换是有严格定义的： 输入流：可以将字节流 => 字符流 输出流：可以将字符流 => 字节流 为什么在输入流不能字符流 => 字节流，输出流不能字节流 => 字符流？ 在存储设备上，所有数据都是以字节为单位存储的，所以输入到内存时必定是以字节为单位输入，输出到存储设备时必须是以字节为单位输出， 字节流才是计算机最根本的存储方式，而字符流是在字节流的基础上对数据进行转换，输出字符，但每个字符依旧是以字节为单位存储的。 节点流和处理流 在这里需要额外插入一个小节讲解节点流和处理流。 节点流：节点流是真正传输数据的流对象，用于向特定的一个地方（节点）读写数据，称为节点流。例如 FileInputStream 处理流：处理流是对节点流的封装，使用外层的处理流读写数据，本质上是利用节点流的功能，外层的处理流可以提供额外的功能。处理流的基类都是以 Filter 开头。 上图将 ByteArrayInputStream 封装成 DataInputStream，可以将输入的字节数组转换为对应数据类型的数据。例如希望读入int类型数据，就会以2个字节为单位转换为一个数字。 Java IO 的核心类 File Java 提供了 File类，它指向计算机操作系统中的文件和目录，通过该类只能访问文件和目录，无法访问内容。 它内部主要提供了 3种操作： 访问文件的属性：绝对路径、相对路径、文件名······ 文件检测：是否文件、是否目录、文件是否存在、文件的读/写/执行权限······ 操作文件：创建目录、创建文件、删除文件······ 上面举例的操作都是在开发中非常常用的，File 类远不止这些操作，更多的操作可以直接去 API 文档中根据需求查找。 访问文件的属性 API功能String getAbsolutePath()返回该文件处于系统中的绝对路径名String getPath()返回该文件的相对路径，通常与 new File() 传入的路径相同String getName()返回该文件的文件名 文件检测 API功能boolean isFIle()校验该路径指向是否一个文件boolean isDirectory()校验该路径指向是否一个目录boolean isExist()校验该路径指向的文件/目录是否存在boolean canWrite()校验该文件是否可写boolean canRead()校验该文件是否可读boolean canExecute()校验该文件/目录是否可以被执行 操作文件 API功能mkdirs()递归创建多个文件夹，路径中间有可能某些文件夹不存在createNewFile()创建新文件，它是一个原子操作，有两步：检查文件是否存在、创建新文件delete()删除文件或目录，删除目录时必须保证该目录为空 多了解一些 文件的读/写/执行权限，在 Windows 中通常表现不出来，而在 Linux 中可以很好地体现这一点，原因是 Linux 有严格的用户权限分组，不同分组下的用户对文件有不同的操作权限，所以这些方法在 Linux 下会比在 Windows 下更好理解。下图是 redis 文件夹中的一些文件的详细信息，被红框标注的是不同用户的执行权限： r（Read）：代表该文件可以被当前用户读，操作权限的序号是 4 w（Write）：代表该文件可以被当前用户写，操作权限的序号是 2 x（Execute）：该文件可以被当前用户执行，操作权限的序号是 1 Java IO 流对象 回顾流的分类有2种： 根据数据流向分为输入流和输出流 根据数据类型分为字节流和字符流 所以，本小节将以字节流和字符流作为主要分割点，在其内部再细分为输入流和输出流进行讲解。 字节流对象 字节流对象大部分输入流和输出流都是成双成对地出现，所以学习的时候可以将输入流和输出流一一对应的流对象关联起来，输入流和输出流只是数据流向不同，而处理数据的方式可以是相同的。 注意不要认为用什么流读入数据，就需要用对应的流写出数据，在 Java 中没有这么规定，下图只是各个对象之间的一个对应关系，不是两个类使用时必须强制关联使用。 下面有非常多的类，我会介绍基类的方法，了解这些方法是非常有必要的，子类的功能基于父类去扩展，只有真正了解父类在做什么，学习子类的成本就会下降。 InputStream InputStream 是字节输入流的抽象基类，提供了通用的读方法，让子类使用或重写它们。下面是 InputStream 常用的重要的方法。 重要方法功能public abstract int read()从输入流中读取下一个字节，读到尾部时返回 -1public int read(byte b[])从输入流中读取长度为 b.length 个字节放入字节数组 b 中public int read(byte b[], int off, int len)从输入流中读取指定范围的字节数据放入字节数组 b 中public void close()关闭此输入流并释放与该输入流相关的所有资源 还有其它一些不太常用的方法，我也列出来了。 其它方法功能public long skip(long n)跳过接下来的 n 个字节，返回实际上跳过的字节数public long available()返回下一次可读取（跳过）且不会被方法阻塞的字节数的估计值public synchronized void mark(int readlimit)标记此输入流的当前位置，对 reset() 方法的后续调用将会重新定位在 mark() 标记的位置，可以重新读取相同的字节public boolean markSupported()判断该输入流是否支持 mark() 和 reset() 方法，即能否重复读取字节public synchronized void reset()将流的位置重新定位在最后一次调用 mark() 方法时的位置 ByteArrayInputStream ByteArrayInputStream 内部包含一个 buf 字节数组缓冲区，该缓冲区可以从流中读取的字节数，使用 pos 指针指向读取下一个字节的下标位置，内部还维护了一个count 属性，代表能够读取 count 个字节。 必须保证 pos 严格小于 count，而 count 严格小于 buf.length 时，才能够从缓冲区中读取数据 FileInputStream 文件输入流，从文件中读入字节，通常对文件的拷贝、移动等操作，可以使用该输入流把文件的字节读入内存中，然后再利用输出流输出到指定的位置上。 PipedInputStream 管道输入流，它与 PipedOutputStream 成对出现，可以实现多线程中的管道通信。PipedOutputStream 中指定与特定的 PipedInputStream 连接，PipedInputStream 也需要指定特定的 PipedOutputStream 连接，之后输出流不断地往输入流的 buffer 缓冲区写数据，而输入流可以从缓冲区中读取数据。 ObjectInputStream 对象输入流，用于对象的反序列化，将读入的字节数据反序列化为一个对象，实现对象的持久化存储。 PushBackInputStream 它是 FilterInputStream 的子类，是一个处理流，它内部维护了一个缓冲数组buf。 在读入字节的过程中可以将读取到的字节数据回退给缓冲区中保存，下次可以再次从缓冲区中读出该字节数据。所以PushBackInputStream 允许多次读取输入流的字节数据，只要将读到的字节放回缓冲区即可。 需要注意的是如果回推字节时，如果缓冲区已满，会抛出 IOException 异常。 它的应用场景：对数据进行分类规整。 假如一个文件中存储了数字和字母两种类型的数据，我们需要将它们交给两种线程各自去收集自己负责的数据，如果采用传统的做法，把所有的数据全部读入内存中，再将数据进行分离，面对大文件的情况下，例如1G、2G，传统的输入流在读入数组后，由于没有缓冲区，只能对数据进行抛弃，这样每个线程都要读一遍文件。 使用 PushBackInputStream 可以让一个专门的线程读取文件，唤醒不同的线程读取字符： 第一次读取缓冲区的数据，判断该数据由哪些线程读取 回退数据，唤醒对应的线程读取数据 重复前两步 关闭输入流 到这里，你是否会想到 AQS 的 Condition 等待队列，多个线程可以在不同的条件上等待被唤醒。 BufferedInputStream 缓冲流，它是一种处理流，对节点流进行封装并增强，其内部拥有一个 buffer 缓冲区，用于缓存所有读入的字节，当缓冲区满时，才会将所有字节发送给客户端读取，而不是每次都只发送一部分数据，提高了效率。 DataInputStream 数据输入流，它同样是一种处理流，对节点流进行封装后，能够在内部对读入的字节转换为对应的 Java 基本数据类型。 SequenceInputStream 将两个或多个输入流看作是一个输入流依次读取，该类的存在与否并不影响整个 IO 生态，在程序中也能够做到这种效果 StringBufferInputStream 将字符串中每个字符的低 8 位转换为字节读入到字节数组中，目前已过期 InputStream 总结： InputStream 是所有输入字节流的抽象基类 ByteArrayInputStream 和 FileInputStream 是两种基本的节点流，他们分别从字节数组 和 本地文件中读取数据 DataInputStream、BufferedInputStream 和 PushBackInputStream 都是处理流，对基本的节点流进行封装并增强 PipiedInputStream 用于多线程通信，可以与其它线程公用一个管道，读取管道中的数据。 ObjectInputStream 用于对象的反序列化，将对象的字节数据读入内存中，通过该流对象可以将字节数据转换成对应的对象 OutputStream OutputStream 是字节输出流的抽象基类，提供了通用的写方法，让继承的子类重写和复用。 方法功能public abstract void write(int b)将指定的字节写出到输出流，写入的字节是参数 b 的低 8 位public void write(byte b[])将指定字节数组中的所有字节写入到输出流当中public void write(byte b[], int off, int len)指定写入的起始位置 offer，字节数为 len 的字节数组写入到输出流当中public void flush()刷新此输出流，并强制写出所有缓冲的输出字节到指定位置，每次写完都要调用public void close()关闭此输出流并释放与此流关联的所有系统资源 OutputStream 中大多数的类和 InputStream 是对应的，只不过数据的流向不同而已。从上面的图可以看出： OutputStream 是所有输出字节流的抽象基类 ByteArrayOutputStream 和 FileOutputStream 是两种基本的节点流，它们分别向字节数组和本地文件写出数据 DataOutputStream、BufferedOutputStream 是处理流，前者可以将字节数据转换成基本数据类型写出到文件中；后者是缓冲字节数组，只有在缓冲区满时，才会将所有的字节写出到目的地，减少了 IO 次数。 PipedOutputStream 用于多线程通信，可以和其它线程共用一个管道，向管道中写入数据 ObjectOutputStream 用于对象的序列化，将对象转换成字节数组后，将所有的字节都写入到指定位置中 PrintStream 在 OutputStream 基础之上提供了增强的功能，即可以方便地输出各种类型的数据（而不仅限于byte型）的格式化表示形式，且 PrintStream 的方法从不抛出 IOEception，其原理是写出时将各个数据类型的数据统一转换为 String 类型，我会在讲解完 字符流对象 # # "},"Chapter17/iomodel.html":{"url":"Chapter17/iomodel.html","title":"IO模型","keywords":"","body":"Java IO读写原理 无论是Socket的读写还是文件的读写，在Java层面的应用开发或者是linux系统底层开发，都属于输入input和输出output的处理，简称为IO读写。在原理上和处理流程上，都是一致的。区别在于参数的不同。 用户程序进行IO的读写，基本上会用到read&write两大系统调用。可能不同操作系统，名称不完全一样，但是功能是一样的。 先强调一个基础知识：read系统调用，并不是把数据直接从物理设备，读数据到内存。write系统调用，也不是直接把数据，写入到物理设备。 read系统调用，是把数据从内核缓冲区复制到进程缓冲区；而write系统调用，是把数据从进程缓冲区复制到内核缓冲区。这个两个系统调用，都不负责数据在内核缓冲区和磁盘之间的交换。底层的读写交换，是由操作系统kernel内核完成的 内核缓冲与进程缓冲区 缓冲区的目的，是为了减少频繁的系统IO调用。大家都知道，系统调用需要保存之前的进程数据和状态等信息，而结束调用之后回来还需要恢复之前的信息，为了减少这种损耗时间、也损耗性能的系统调用，于是出现了缓冲区。 有了缓冲区，操作系统使用read函数把数据从内核缓冲区复制到进程缓冲区，write把数据从进程缓冲区复制到内核缓冲区中。等待缓冲区达到一定数量的时候，再进行IO的调用，提升性能。至于什么时候读取和存储则由内核来决定，用户程序不需要关心。 在linux系统中，系统内核也有个缓冲区叫做内核缓冲区。每个进程有自己独立的缓冲区，叫做进程缓冲区。 所以，用户程序的IO读写程序，大多数情况下，并没有进行实际的IO操作，而是在读写自己的进程缓冲区。 java IO读写的底层流程 用户程序进行IO的读写，基本上会用到系统调用read&write，read把数据从内核缓冲区复制到进程缓冲区，write把数据从进程缓冲区复制到内核缓冲区，它们不等价于数据在内核缓冲区和磁盘之间的交换。 首先看看一个典型Java 服务端处理网络请求的典型过程： 客户端请求: Linux通过网卡，读取客户断的请求数据，将数据读取到内核缓冲区。 获取请求数据: 服务器从内核缓冲区读取数据到Java进程缓冲区。 服务器端业务处理: Java服务端在自己的用户空间中，处理客户端的请求。 服务器端返回数据: ava服务端已构建好的响应，从用户缓冲区写入系统缓冲区。 发送给客户端: Linux内核通过网络 I/O ，将内核缓冲区中的数据，写入网卡，网卡通过底层的通讯协议，会将数据发送给目标客户端。 四种主要的IO模型 服务器端编程经常需要构造高性能的IO模型，常见的IO模型有四种： 同步阻塞IO（Blocking IO） 首先，解释一下这里的阻塞与非阻塞： 阻塞IO，指的是需要内核IO操作彻底完成后，才返回到用户空间，执行用户的操作。阻塞指的是用户空间程序的执行状态，用户空间程序需等到IO操作彻底完成。传统的IO模型都是同步阻塞IO。在java中，默认创建的socket都是阻塞的。 其次，解释一下同步与异步： 同步IO，是一种用户空间与内核空间的调用发起方式。同步IO是指用户空间线程是主动发起IO请求的一方，内核空间是被动接受方。异步IO则反过来，是指内核kernel是主动发起IO请求的一方，用户线程是被动接受方。 同步非阻塞IO（Non-blocking IO） 非阻塞IO，指的是用户程序不需要等待内核IO操作完成后，内核立即返回给用户一个状态值，用户空间无需等到内核的IO操作彻底完成，可以立即返回用户空间，执行用户的操作，处于非阻塞的状态。 简单的说：阻塞是指用户空间（调用线程）一直在等待，而且别的事情什么都不做；非阻塞是指用户空间（调用线程）拿到状态就返回，IO操作可以干就干，不可以干，就去干的事情。 非阻塞IO要求socket被设置为NONBLOCK。 强调一下，这里所说的NIO（同步非阻塞IO）模型，并非Java的NIO（New IO）库。 IO多路复用（IO Multiplexing） 即经典的Reactor设计模式，有时也称为异步阻塞IO，Java中的Selector和Linux中的epoll都是这种模型。 异步IO（Asynchronous IO） 异步IO，指的是用户空间与内核空间的调用方式反过来。用户空间线程是变成被动接受的，内核空间是主动调用者。 这一点，有点类似于Java中比较典型的模式是回调模式，用户空间线程向内核空间注册各种IO事件的回调函数，由内核去主动调用。 同步阻塞IO（Blocking IO） 在linux中的Java进程中，默认情况下所有的socket都是blocking IO。在阻塞式 I/O 模型中，应用程序在从IO系统调用开始，一直到到系统调用返回，这段时间是阻塞的。返回成功后，应用进程开始处理用户空间的缓存数据。 举个栗子，发起一个blocking socket的read读操作系统调用，流程大概是这样： 当用户线程调用了read系统调用，内核（kernel）就开始了IO的第一个阶段：准备数据。很多时候，数据在一开始还没有到达（比如，还没有收到一个完整的Socket数据包），这个时候kernel就要等待足够的数据到来。 当kernel一直等到数据准备好了，它就会将数据从kernel内核缓冲区，拷贝到用户缓冲区（用户内存），然后kernel返回结果。 从开始IO读的read系统调用开始，用户线程就进入阻塞状态。一直到kernel返回结果后，用户线程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在内核进行IO执行的两个阶段，用户线程都被block了。 BIO的优点： 程序简单，在阻塞等待数据期间，用户线程挂起。用户线程基本不会占用 CPU 资源。 BIO的缺点： 一般情况下，会为每个连接配套一条独立的线程，或者说一条线程维护一个连接成功的IO流的读写。在并发量小的情况下，这个没有什么问题。但是，当在高并发的场景下，需要大量的线程来维护大量的网络连接，内存、线程切换开销会非常巨大。因此，基本上，BIO模型在高并发场景下是不可用的。 同步非阻塞NIO（None Blocking IO） 在linux系统下，可以通过设置socket使其变为non-blocking。NIO 模型中应用程序在一旦开始IO系统调用，会出现以下两种情况： 在内核缓冲区没有数据的情况下，系统调用会立即返回，返回一个调用失败的信息。 在内核缓冲区有数据的情况下，是阻塞的，直到数据从内核缓冲复制到用户进程缓冲。复制完成后，系统调用返回成功，应用进程开始处理用户空间的缓存数据。 举个栗子。发起一个non-blocking socket的read读操作系统调用，流程是这个样子： 在内核数据没有准备好的阶段，用户线程发起IO请求时，立即返回。用户线程需要不断地发起IO系统调用。 内核数据到达后，用户线程发起系统调用，用户线程阻塞。内核开始复制数据。它就会将数据从kernel内核缓冲区，拷贝到用户缓冲区（用户内存），然后kernel返回结果。 用户线程才解除block的状态，重新运行起来。经过多次的尝试，用户线程终于真正读取到数据，继续执行。 NIO的特点： 应用程序的线程需要不断的进行 I/O 系统调用，轮询数据是否已经准备好，如果没有准备好，继续轮询，直到完成系统调用为止。 NIO的优点：每次发起的 IO 系统调用，在内核的等待数据过程中可以立即返回。用户线程不会阻塞，实时性较好。 NIO的缺点：需要不断的重复发起IO系统调用，这种不断的轮询，将会不断地询问内核，这将占用大量的 CPU 时间，系统资源利用率较低。 总之，NIO模型在高并发场景下，也是不可用的。一般 Web 服务器不使用这种 IO 模型。一般很少直接使用这种模型，而是在其他IO模型中使用非阻塞IO这一特性。java的实际开发中，也不会涉及这种IO模型。 再次说明，Java NIO（New IO） 不是IO模型中的NIO模型，而是另外的一种模型，叫做IO多路复用模型（ IO multiplexing ）。 IO多路复用模型(I/O multiplexing） 如何避免同步非阻塞NIO模型中轮询等待的问题呢？这就是IO多路复用模型。 IO多路复用模型，就是通过一种新的系统调用，一个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是内核缓冲区可读/可写），内核kernel能够通知程序进行相应的IO系统调用。 目前支持IO多路复用的系统调用，有 select，epoll等等。select系统调用，是目前几乎在所有的操作系统上都有支持，具有良好跨平台特性。epoll是在linux 2.6内核中提出的，是select系统调用的linux增强版本。 IO多路复用模型的基本原理就是select/epoll系统调用，单个线程不断的轮询select/epoll系统调用所负责的成百上千的socket连接，当某个或者某些socket网络连接有数据到达了，就返回这些可以读写的连接。因此，好处也就显而易见了——通过一次select/epoll系统调用，就查询到到可以读写的一个甚至是成百上千的网络连接。 举个栗子。发起一个多路复用IO的的read读操作系统调用，流程是这个样子： 在这种模式中，首先不是进行read系统调动，而是进行select/epoll系统调用。当然，这里有一个前提，需要将目标网络连接，提前注册到select/epoll的可查询socket列表中。然后，才可以开启整个的IO多路复用模型的读流程。 进行select/epoll系统调用，查询可以读的连接。kernel会查询所有select的可查询socket列表，当任何一个socket中的数据准备好了，select就会返回。当用户进程调用了select，那么整个线程会被block（阻塞掉）。 用户线程获得了目标连接后，发起read系统调用，用户线程阻塞。内核开始复制数据。它就会将数据从kernel内核缓冲区，拷贝到用户缓冲区（用户内存），然后kernel返回结果。 用户线程才解除block的状态，用户线程终于真正读取到数据，继续执行。 多路复用IO的特点： IO多路复用模型，建立在操作系统kernel内核能够提供的多路分离系统调用select/epoll基础之上的。多路复用IO需要用到两个系统调用（system call）， 一个select/epoll查询调用，一个是IO的读取调用。 和NIO模型相似，多路复用IO需要轮询。负责select/epoll查询调用的线程，需要不断的进行select/epoll轮询，查找出可以进行IO操作的连接。 另外，多路复用IO模型与前面的NIO模型，是有关系的。对于每一个可以查询的socket，一般都设置成为non-blocking模型。只是这一点，对于用户程序是透明的（不感知）。 多路复用IO的优点： 用select/epoll的优势在于，它可以同时处理成千上万个连接（connection）。与一条线程维护一个连接相比，I/O多路复用技术的最大优势是：系统不必创建线程，也不必维护这些线程，从而大大减小了系统的开销。 Java的NIO（new IO）技术，使用的就是IO多路复用模型。在linux系统上，使用的是epoll系统调用。 多路复用IO的缺点： 本质上，select/epoll系统调用，属于同步IO，也是阻塞IO。都需要在读写事件就绪后，自己负责进行读写，也就是说这个读写过程是阻塞的。 如何充分的解除线程的阻塞呢？那就是异步IO模型。 异步IO模型（asynchronous IO） 如何进一步提升效率，解除最后一点阻塞呢？这就是异步IO模型，全称asynchronous I/O，简称为AIO。 AIO的基本流程是：用户线程通过系统调用，告知kernel内核启动某个IO操作，用户线程返回。kernel内核在整个IO操作（包括数据准备、数据复制）完成后，通知用户程序，用户执行后续的业务操作。 kernel的数据准备是将数据从网络物理设备（网卡）读取到内核缓冲区；kernel的数据复制是将数据从内核缓冲区拷贝到用户程序空间的缓冲区。 当用户线程调用了read系统调用，立刻就可以开始去做其它的事，用户线程不阻塞。 内核（kernel）就开始了IO的第一个阶段：准备数据。当kernel一直等到数据准备好了，它就会将数据从kernel内核缓冲区，拷贝到用户缓冲区（用户内存）。 kernel会给用户线程发送一个信号（signal），或者回调用户线程注册的回调接口，告诉用户线程read操作完成了。 用户线程读取用户缓冲区的数据，完成后续的业务操作。 异步IO模型的特点： 在内核kernel的等待数据和复制数据的两个阶段，用户线程都不是block(阻塞)的。用户线程需要接受kernel的IO操作完成的事件，或者说注册IO操作完成的回调函数，到操作系统的内核。所以说，异步IO有的时候，也叫做信号驱动 IO 。 异步IO模型缺点： 需要完成事件的注册与传递，这里边需要底层操作系统提供大量的支持，去做大量的工作。 目前来说， Windows 系统下通过 IOCP 实现了真正的异步 I/O。但是，就目前的业界形式来说，Windows 系统，很少作为百万级以上或者说高并发应用的服务器操作系统来使用。 而在 Linux 系统下，异步IO模型在2.6版本才引入，目前并不完善。所以，这也是在 Linux 下，实现高并发网络编程时都是以 IO 复用模型模式为主。 "},"Chapter17/nioBase.html":{"url":"Chapter17/nioBase.html","title":"NIO基础知识","keywords":"","body":"NIO基础知识 Buffer 以下是Java NIO里关键的Buffer实现 ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer MappedByteBuffer -> ByteBuffer Buffer的属性 capacity 容量 缓冲区能够容纳的数据元素的最大数量;作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。 一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。这一容量在缓冲区创建时被设定，并且永远不能被改变 position 位置 下一个要被读或写的元素的索引。位置会自动由相应的 get( )和 put( )函数更新。 当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1. 当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0. 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。 limit 上界 缓冲区的第一个不能被读或写的元素。或者说，缓冲区中现存元素的计数。 在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。 当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position） 三个属性的关系 position和limit的含义取决于Buffer处在读模式还是写模式。不管Buffer处在什么模式，capacity的含义总是一样的。 新创建的 ByteBuffer 位置被设为 0，而且容量和上界被设为 10，刚好经过缓冲区能够容纳的最后一个字节。标记最初未定义。容量是固定的，但另外的三个属性可以在使用缓冲区时改变。 这里引出一个新的属性mark mark 标记 一个备忘位置。调用 mark( )来设定 mark = postion。调用 reset( )设定 position =mark。标记在设定前是未定义的(undefined)。 这四个属性之间总是遵循以下关系： 0 Channel 通道（Channel）是 java.nio 的第二个主要创新。它们既不是一个扩展也不是一项增强，而是全新、极好的 Java I/O 示例，提供与 I/O 服务的直接连接。 Channel 用于在字节缓冲区和位于通道另一侧的实体（通常是一个文件或套接字）之间有效地传输数据。 多数情况下，通道与操作系统的文件描述符（File Descriptor）和文件句柄（File Handle）有着一对一的关系。虽然通道比文件描述符更广义， 但您将经常使用到的多数通道都是连接到开放的文件描述符的。Channel 类提供维持平台独立性所需的抽象过程，不过仍然会模拟现代操作系统本身的 I/O 性能。 通道是一种途径，借助该途径，可以用最小的总开销来访问操作系统本身的 I/O 服务。缓冲区则是通道内部用来发送和接收数据的端点。 Channel的实现 FileChannel: 从文件中读写数据。 DatagramChannel: 能通过UDP读写网络中的数据。 SocketChannel: 能通过TCP读写网络中的数据。 ServerSocketChannel: 可以监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。 Selector 为什么使用Selector 从最基础的层面来看，选择器提供了询问通道是否已经准备好执行每个 I/0 操作的能力。例如，我们需要了解一个 SocketChannel 对象是否还有更多的字节需要读取，或者我们需要知道ServerSocketChannel 是否有需要准备接受的连接。 在与 SelectableChannel 联合使用时，选择器提供了这种服务，但这里面有更多的事情需要去了解。就绪选择的真正价值在于潜在的大量的通道可以同时进行就绪状态的检查。调用者可以轻松地决定多个通道中的哪一个准备好要运行。 有两种方式可以选择：被激发的线程可以处于休眠状态，直到一个或者多个注册到选择器的通道就绪，或者它也可以周期性地轮询选择器，看看从上次检查之后，是否有通道处于就绪状态。 如果您考虑一下需要管理大量并发的连接的网络服务器(webserver)的实现，就可以很容易地想到如何善加利用这些能力。 乍一看，好像只要非阻塞模式就可以模拟就绪检查功能，但实际上还不够。非阻塞模式同时还会执行您请求的任务，或指出它无法执行这项任务。这与检查它是否能够执行某种类型的操作是不同的。 举个例子，如果您试图执行非阻塞操作，并且也执行成功了，您将不仅仅发现 read( )是可以执行的，同时您也已经读入了一些数据。就下来您就需要处理这些数据了。 效率上的要求使得您不能将检查就绪的代码和处理数据的代码分离开来，至少这么做会很复杂。 即使简单地询问每个通道是否已经就绪的方法是可行的，在您的代码或一个类库的包里的某些代码需要遍历每一个候选的通道并按顺序进行检查的时候，仍然是有问题的。 这会使得在检查每个通道是否就绪时都至少进行一次系统调用，这种代价是十分昂贵的，但是主要的问题是，这种检查不是原子性的。 列表中的一个通道都有可能在它被检查之后就绪，但直到下一次轮询为止，您并不会觉察到这种情况。最糟糕的是，您除了不断地遍历列表之外将别无选择。您无法在某个您感兴趣的通道就绪时得到通知。 这就是为什么传统的监控多个 socket 的 Java 解决方案是为每个 socket 创建一个线程并使得线程可以在 read( )调用中阻塞，直到数据可用。这事实上将每个被阻塞的线程当作了 socket 监控器，并将 Java 虚拟机的线程调度当作了通知机制。 这两者本来都不是为了这种目的而设计的。程序员和 Java 虚拟机都为管理所有这些线程的复杂性和性能损耗付出了代价，这在线程数量的增长失控时表现得更为突出。 真正的就绪选择必须由操作系统来做。操作系统的一项最重要的功能就是处理 I/O 请求并通知各个线程它们的数据已经准备好了。选择器类提供了这种抽象，使得 Java 代码能够以可移植的方式，请求底层的操作系统提供就绪选择服务。 仅用单个线程来处理多个Channels的好处是，只需要更少的线程来处理通道。事实上，可以只用一个线程处理所有的通道。对于操作系统来说，线程之间上下文切换的开销很大，而且每个线程都要占用系统的一些资源（如内存）。因此，使用的线程越少越好。 但是，需要记住，现代的操作系统和CPU在多任务方面表现的越来越好，所以多线程的开销随着时间的推移，变得越来越小了。实际上，如果一个CPU有多个内核，不使用多任务可能是在浪费CPU能力。不管怎么说，关于那种设计的讨论应该放在另一篇不同的文章中。在这里，只要知道使用Selector能够处理多个通道就足够了。 1 通道在被注册到一个选择器上之前，必须先设置为非阻塞模式（通过调用 configureBlocking(false)）。 调用可选择通道的 register( )方法会将它注册到一个选择器上。如果您试图注册一个处于阻塞状态的通道，register( )将抛出未检查的 IllegalBlockingModeException 异常。 此外，通道一旦被注册，就不能回到阻塞状态。试图这么做的话，将在调用 configureBlocking( )方法时将抛出IllegalBlockingModeException 异常。 选择器才是提供管理功能的对象，而不是可选择通道对象。选择器对象对注册到它之上的通道执行就绪选择，并管理选择键。、 Pipe Java NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。 这里是Pipe原理的图示： Path Path接口是java NIO2的一部分。首次在java 7中引入。Path接口在java.nio.file包下，所以全称是java.nio.file.Path。 java中的Path表示文件系统的路径。可以指向文件或文件夹。也有相对路径和绝对路径之分。绝对路径表示从文件系统的根路径到文件或是文件夹的路径。相对路径表示从特定路径下访问指定文件或文件夹的路径。相对路径的概念可能有点迷糊。不用担心，我将在本文的后面详细介绍相关细节。 不要将文件系统的path和操作系统的环境变量path搞混淆。java.nio.file.Path接口和操作系统的path环境变量没有任何关系。 在很多方面，java.nio.file.Path接口和java.io.File有相似性，但也有一些细微的差别。在很多情况下，可以用Path来代替File类。 Files java NIO Files类(java.nio.file.Files) 提供了操作文件的相关方法。本篇文章将会覆盖大多数常用的方法。Files类包含了很多方法，如果你需要的功能在文中没有提及，需要自己查阅JavaDoc文档确认，也许Files类提供了相应方法(译者注：但本文中没有涉及) java.nio.file.Files类需要和java.nio.file.Path一起使用，在学习Files类前，你需要掌握Path类的相关用法。 "},"Chapter17/NIOExample.html":{"url":"Chapter17/NIOExample.html","title":"NIO示例","keywords":"","body":"NIO示例 ByteBuffer //定义一个容量为10的buffer ByteBuffer bbf = ByteBuffer.allocate(10); //依次在0-4填充H、e、l、l、o字节码。如果不转换成byte将以字符形式存入 bbf.put((byte) 'H').put((byte) 'e').put((byte) 'l').put((byte) 'l').put((byte) 'o'); //修改第0位为M，填充第5位为w； bbf.put(0, (byte) 'M').put((byte) 'w'); // bbf.limit(bbf.position()).position(0); // bbf.flip(); 读取文件 RandomAccessFile aFile = new RandomAccessFile(\"data/nio-data.txt\", \"rw\"); FileChannel inChannel = aFile.getChannel(); //create buffer with capacity of 48 bytes ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = inChannel.read(buf); //read into buffer. while (bytesRead != -1) { buf.flip(); //make buffer ready for read while (buf.hasRemaining()) { System.out.print((char) buf.get()); // read 1 byte at a time } buf.clear(); //make buffer ready for writing bytesRead = inChannel.read(buf); } aFile.close(); 读取中文文件 RandomAccessFile aFile = new RandomAccessFile(\"data/nio-data.txt\", \"rw\"); FileChannel inChannel = aFile.getChannel(); ByteBuffer buf = ByteBuffer.allocate(48); int byteRead = inChannel.read(buf); while (byteRead != -1) { System.out.println(\"Read\" + byteRead); buf.flip(); byte[] bytes = new byte[byteRead]; int index = 0; while (buf.hasRemaining()) { bytes[index] = buf.get(); index++; } System.out.println(new String(bytes, \"utf-8\")); buf.clear(); byteRead = inChannel.read(buf); } aFile.close(); Channel数据转换 RandomAccessFile fromFile = new RandomAccessFile(\"fromFile.txt\", \"rw\"); FileChannel fromChannel = fromFile.getChannel(); RandomAccessFile toFile = new RandomAccessFile(\"toFile.txt\", \"rw\"); FileChannel toChannel = toFile.getChannel(); long position = 0; long count = fromChannel.size(); toChannel.transferFrom(fromChannel, position, count); Selector 与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。 Selector selector = Selector.open(); ServerSocketChannel channel = ServerSocketChannel.open(); //通道在被注册到一个选择器上之前，必须先设置为非阻塞模式 channel.configureBlocking(false); SelectionKey key = channel.register(selector, SelectionKey.OP_READ); while (true) { int readyChannels = selector.select(); if (readyChannels == 0) continue; Set selectedKeys = selector.selectedKeys(); Iterator keyIterator = selectedKeys.iterator(); while (keyIterator.hasNext()) { SelectionKey key1 = (SelectionKey) keyIterator.next(); if (key1.isAcceptable()) { // a connection was accepted by a ServerSocketChannel. } else if (key1.isConnectable()) { // a connection was established with a remote server. } else if (key1.isReadable()) { // a channel is ready for reading } else if (key1.isWritable()) { // a channel is ready for writing } keyIterator.remove(); } } pipe //通过Pipe.open()方法打开管道 Pipe pipe = Pipe.open(); //要向管道写数据，需要访问sink通道。像这样： Pipe.SinkChannel sinkChannel = pipe.sink(); //通过调用SinkChannel的write()方法，将数据写入SinkChannel,像这样： String newData = \"New String to write to file...\" + System.currentTimeMillis(); ByteBuffer buf = ByteBuffer.allocate(48); buf.clear(); buf.put(newData.getBytes()); buf.flip(); while(buf.hasRemaining()) { sinkChannel.write(buf); } //从读取管道的数据，需要访问source通道，像这样： Pipe.SourceChannel sourceChannel = pipe.source(); //调用source通道的read()方法来读取数据，像这样 read()方法返回的int值会告诉我们多少字节被读进了缓冲区。 ByteBuffer rBuf = ByteBuffer.allocate(48); int bytesRead = sourceChannel.read(rBuf); SocketChannel(client) SocketChannel 模拟连接导向的流协议（如 TCP/IP //打开 SocketChannel SocketChannel socketChannel = SocketChannel.open(); //可以设置 SocketChannel 为非阻塞模式（non-blocking mode）.设置之后，就可以在异步模式下调用connect(), read() 和write()了。 socketChannel.configureBlocking(false); //连接到互联网上的某台服务器。 socketChannel.connect(new InetSocketAddress(\"http://jenkov.com\", 80)); while (!socketChannel.finishConnect()) { //wait, or do something else... //要从SocketChannel中读取数据 ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = socketChannel.read(buf); //写入 SocketChannel String newData = \"New String to write to file...\" + System.currentTimeMillis(); ByteBuffer wBuf = ByteBuffer.allocate(48); wBuf.clear(); wBuf.put(newData.getBytes()); wBuf.flip(); while (buf.hasRemaining()) { //Write()方法无法保证能写多少字节到SocketChannel。所以，我们重复调用write()直到Buffer没有要写的字节为止。 socketChannel.write(buf); } } //关闭 SocketChannel socketChannel.close(); ServerSocketChannel ServerSocketChannel 是一个可以监听新进来的TCP连接的通道, 就像标准IO中的ServerSocket一样 boolean go = true; //ServerSocketChannel 是一个可以监听新进来的TCP连接的通道, 就像标准IO中的ServerSocket一样 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(9999)); //ServerSocketChannel可以设置成非阻塞模式。在非阻塞模式下，accept() 方法会立刻返回，如果还没有新进来的连接,返回的将是null。 因此，需要检查返回的SocketChannel是否是null.如： serverSocketChannel.configureBlocking(false); while (go) { //通过 ServerSocketChannel.accept() 方法监听新进来的连接。当 accept()方法返回的时候,它返回一个包含新进来的连接的 SocketChannel。因此, accept()方法会一直阻塞到有新连接到达。 SocketChannel socketChannel = serverSocketChannel.accept(); if (socketChannel != null) { //do something with socketChannel... } //do something with socketChannel... } //通过调用ServerSocketChannel.close() 方法来关闭ServerSocketChannel serverSocketChannel.close(); DatagramChannel 与面向流的的 socket 不同，DatagramChannel 可以发送单独的数据报给不同的目的地址。 同样，DatagramChannel 对象也可以接收来自任意地址的数据包。每个到达的数据报都含有关于它来自何处的信息（源地址）。 Java NIO中的DatagramChannel是一个能收发UDP包的通道。因为UDP是无连接的网络协议，所以不能像其它通道那样读取和写入。它发送和接收的是数据包。 DatagramChannel channel = DatagramChannel.open(); //打开的 DatagramChannel可以在UDP端口9999上接收数据包。 channel.socket().bind(new InetSocketAddress(9999)); //receive()方法会将接收到的数据包内容复制到指定的Buffer. 如果Buffer容不下收到的数据，多出的数据将被丢弃。 ByteBuffer buf = ByteBuffer.allocate(48); buf.clear(); channel.receive(buf); //发送数据 String newData = \"New String to write to file...\" + System.currentTimeMillis(); ByteBuffer sendBuf = ByteBuffer.allocate(48); sendBuf.clear(); sendBuf.put(newData.getBytes()); sendBuf.flip(); //这个例子发送一串字符到”jenkov.com”服务器的UDP端口80。 因为服务端并没有监控这个端口，所以什么也不会发生。也不会通知你发出的数据包是否已收到，因为UDP在数据传送方面没有任何保证。 int bytesSent = channel.send(buf, new InetSocketAddress(\"jenkov.com\", 80)); //可以将DatagramChannel“连接”到网络中的特定地址的。由于UDP是无连接的，连接到特定地址并不会像TCP通道那样创建一个真正的连接。而是锁住DatagramChannel ，让其只能从特定地址收发数据。 channel.connect(new InetSocketAddress(\"jenkov.com\", 80)); //当连接后，也可以使用read()和write()方法，就像在用传统的通道一样。只是在数据传送方面没有任何保证。这里有几个例子： int bytesRead = channel.read(buf); int bytesWritten = channel.write(sendBuf); SelectSockets 它创建了 ServerSocketChannel 和 Selector 对象，并将通道注册到选择器上。我们不在注册的键中保存服务器 socket 的引用，因为它永远不会被注销。 这个无限循环在最上面先调用了 select( )，这可能会无限期地阻塞。当选择结束时，就遍历选择键并检查已经就绪的通道。 public class SelectSockets { public static int PORT_NUMBER = 1234; public static void main(String[] argv) throws Exception { new SelectSockets().go(argv); } public void go(String[] argv) throws Exception { int port = PORT_NUMBER; if (argv.length > 0) { // Override default listen port port = Integer.parseInt(argv[0]); } System.out.println(\"Listening on port \" + port); // Allocate an unbound server socket channel ServerSocketChannel serverChannel = ServerSocketChannel.open(); // Get the associated ServerSocket to bind it with ServerSocket serverSocket = serverChannel.socket(); // Create a new Selector for use below Selector selector = Selector.open(); // Set the port the server channel will listen to serverSocket.bind(new InetSocketAddress(port)); // Set nonblocking mode for the listening socket serverChannel.configureBlocking(false); // Register the ServerSocketChannel with the Selector serverChannel.register(selector, SelectionKey.OP_ACCEPT); while (true) { // This may block for a long time. Upon returning, the // selected set contains keys of the ready channels. int n = selector.select(); if (n == 0) { continue; // nothing to do } // Get an iterator over the set of selected keys Iterator it = selector.selectedKeys().iterator(); // Look at each key in the selected set while (it.hasNext()) { SelectionKey key = (SelectionKey) it.next(); // Is a new connection coming in? if (key.isAcceptable()) { ServerSocketChannel server = (ServerSocketChannel) key.channel(); SocketChannel channel = server.accept(); registerChannel(selector, channel, SelectionKey.OP_READ); sayHello(channel); } // Is there data to read on this channel? if (key.isReadable()) { readDataFromSocket(key); } // Remove key from selected set; it's been handled it.remove(); } } } // ---------------------------------------------------------- /** * Register the given channel with the given selector for the given * operations of interest */ protected void registerChannel(Selector selector, SelectableChannel channel, int ops) throws Exception { if (channel == null) { return; // could happen } // Set the new channel nonblocking channel.configureBlocking(false); // Register it with the selector channel.register(selector, ops); } // ---------------------------------------------------------- // Use the same byte buffer for all channels. A single thread is // servicing all the channels, so no danger of concurrent acccess. private ByteBuffer buffer = ByteBuffer.allocateDirect(1024); /** * Sample data handler method for a channel with data ready to read. * * @param key A SelectionKey object associated with a channel determined by * the selector to be ready for reading. If the channel returns * 142 * an EOF condition, it is closed here, which automatically * invalidates the associated key. The selector will then * de-register the channel on the next select call. */ protected void readDataFromSocket(SelectionKey key) throws Exception { SocketChannel socketChannel = (SocketChannel) key.channel(); int count; buffer.clear(); // Empty buffer // Loop while data is available; channel is nonblocking while ((count = socketChannel.read(buffer)) > 0) { buffer.flip(); // Make buffer readable // Send the data; don't assume it goes all at once while (buffer.hasRemaining()) { socketChannel.write(buffer); } // WARNING: the above loop is evil. Because // it's writing back to the same nonblocking // channel it read the data from, this code can // potentially spin in a busy loop. In real life // you'd do something more useful than this. buffer.clear(); // Empty buffer } if (count SelectSocketsThreadPool public class SelectSocketsThreadPool extends SelectSockets { private static final int MAX_THREADS = 5; private ThreadPool pool = new ThreadPool(MAX_THREADS); // ------------------------------------------------------------- public static void main(String[] argv) throws Exception { new SelectSocketsThreadPool().go(argv); } // ------------------------------------------------------------- /** * Sample data handler method for a channel with data ready to read. This * method is invoked from the go( ) method in the parent class. This handler * delegates to a worker thread in a thread pool to service the channel, * then returns immediately. * * @param key A SelectionKey object representing a channel determined by the * selector to be ready for reading. If the channel returns an * EOF condition, it is closed here, which automatically * invalidates the associated key. The selector will then * de-register the channel on the next select call. */ protected void readDataFromSocket(SelectionKey key) throws Exception { WorkerThread worker = pool.getWorker(); if (worker == null) { // No threads available. Do nothing. The selection // loop will keep calling this method until a // thread becomes available. This design could // be improved. return; } // Invoking this wakes up the worker thread, then returns worker.serviceChannel(key); } // --------------------------------------------------------------- /** * A very simple thread pool class. The pool size is set at construction * time and remains fixed. Threads are cycled through a FIFO idle queue. */ private class ThreadPool { List idle = new LinkedList(); ThreadPool(int poolSize) { // Fill up the pool with worker threads for (int i = 0; i 0) { worker = (WorkerThread) idle.remove(0); } } return (worker); } /** * Called by the worker thread to return itself to the idle pool. */ void returnWorker(WorkerThread worker) { synchronized (idle) { idle.add(worker); } } } /** * A worker thread class which can drain channels and echo-back the input. * Each instance is constructed with a reference to the owning thread pool * object. When started, the thread loops forever waiting to be awakened to * service the channel associated with a SelectionKey object. The worker is * tasked by calling its serviceChannel( ) method with a SelectionKey * object. The serviceChannel( ) method stores the key reference in the * thread object then calls notify( ) to wake it up. When the channel has * 147 * been drained, the worker thread returns itself to its parent pool. */ private class WorkerThread extends Thread { private ByteBuffer buffer = ByteBuffer.allocate(1024); private ThreadPool pool; private SelectionKey key; WorkerThread(ThreadPool pool) { this.pool = pool; } // Loop forever waiting for work to do public synchronized void run() { System.out.println(this.getName() + \" is ready\"); while (true) { try { // Sleep and release object lock this.wait(); } catch (InterruptedException e) { e.printStackTrace(); // Clear interrupt status this.interrupted(); } if (key == null) { continue; // just in case } System.out.println(this.getName() + \" has been awakened\"); try { drainChannel(key); } catch (Exception e) { System.out.println(\"Caught '\" + e + \"' closing channel\"); // Close channel and nudge selector try { key.channel().close(); } catch (IOException ex) { ex.printStackTrace(); } key.selector().wakeup(); } key = null; // Done. Ready for more. Return to pool this.pool.returnWorker(this); } } /** * Called to initiate a unit of work by this worker thread on the * provided SelectionKey object. This method is synchronized, as is the * run( ) method, so only one key can be serviced at a given time. * Before waking the worker thread, and before returning to the main * selection loop, this key's interest set is updated to remove OP_READ. * This will cause the selector to ignore read-readiness for this * channel while the worker thread is servicing it. */ synchronized void serviceChannel(SelectionKey key) { this.key = key; key.interestOps(key.interestOps() & (~SelectionKey.OP_READ)); this.notify(); // Awaken the thread } /** * 148 * The actual code which drains the channel associated with the given * key. This method assumes the key has been modified prior to * invocation to turn off selection interest in OP_READ. When this * method completes it re-enables OP_READ and calls wakeup( ) on the * selector so the selector will resume watching this channel. */ void drainChannel(SelectionKey key) throws Exception { SocketChannel channel = (SocketChannel) key.channel(); int count; buffer.clear(); // Empty buffer // Loop while data is available; channel is nonblocking while ((count = channel.read(buffer)) > 0) { buffer.flip(); // make buffer readable // Send the data; may not go all at once while (buffer.hasRemaining()) { channel.write(buffer); } // WARNING: the above loop is evil. // See comments in superclass. buffer.clear(); // Empty buffer } if (count "},"Chapter17/NIO-IO.html":{"url":"Chapter17/NIO-IO.html","title":"NIO与IO","keywords":"","body":"Java NIO与IO Java NIO和IO的主要区别 IO NIO 面向流 面向缓冲 阻塞IO 非阻塞IO 无 选择器 面向流与面向缓冲 Java NIO和IO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。 此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。 但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。 阻塞与非阻塞IO Java IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 选择器（Selectors） Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 NIO和IO如何影响应用程序的设计 对NIO或IO类的API调用。 数据处理。 用来处理数据的线程数。 "},"Chapter17/Zero-Copy.html":{"url":"Chapter17/Zero-Copy.html","title":"深入理解Linux, NIO和Netty中的零拷贝","keywords":"","body":"深入理解Linux, NIO和Netty中的零拷贝 零拷贝 Wikipedia上对零拷贝的解释如下： “Zero-copy” describes computer operations in which the CPU does not perform the task of copying data from one memory area to another. This is frequently used to save CPU cycles and memory bandwidth when transmitting a file over a network. “零复制”描述了计算机操作，其中CPU不执行将数据从一个存储区域复制到另一个存储区域的任务。 通过网络传输文件时，通常用于节省CPU周期和内存带宽。 零拷贝防止了数据在内存中的复制，可以提升网络传输的性能。由此产生两个疑问： 为什么会出现数据的复制？ 零拷贝真的是0次数据复制吗？ Linux系统中的零拷贝 前篇文章Linux中的零拷贝技术已经讲过Linux的零拷贝。这里复习下。 内核空间：计算机内存被分为用户空间和内核空间。内核空间运行OS内核代码，并可以访问所有内存，机器指令和硬件资源，具有最高的权限。内核空间是属于操作系统的。不是cpu的硬件。 用户空间：即内核以外的所有空间，用于正常用户进程运行。用户空间的进程无权访问内核空间，只能通过内核暴露的接口----系统调用(system calls)去访问内核的一小部分。如果用户进程请求执行系统调用，需要给内核发送系统中断(software interrupt)，内核会分派相应的中断处理器处理请求。 DMA：Direct Memory Access(DMA)是来应对CPU与硬盘之间速度量级不匹配的问题的，它允许某些硬件子系统访问独立于CPU的主内存。如果没有DMA，CPU进行IO操作的整个过程都是阻塞的，无法执行其他工作，这会使计算机陷入假死状态。 如果有DMA介入，IO过程变成这样：CPU启动DMA传输，期间它可以执行其他操作；DMA控制器(DMAC)在传输完成后，会给CPU发送中断信号，这时CPU便可以处理传输好的数据。 传统的网络传输 网络IO的一个常见场景是，将文件从硬盘读取出来，并通过网卡发送至网络。以下是简单的伪代码： // 从硬盘读取数据 File.read(fileDesc, buf, len); // 发送数据到网络 Socket.write(socket, buf, len); 代码层面，这是一个非常简单的操作，但是深入到系统层面，我们来看看背后发生了什么： 由于用户空间无法直接访问文件系统，所以，这个场景涉及到了三个模块的交互：用户空间，内核空间和硬件。 用户发起read()系统调用(syscall)，请求硬盘数据。此时，会发生一次上下文切换(context switch)。 DMA从硬盘读取文件，这时，产生一次复制：硬盘–>DMA缓冲区。 DMA将数据复制到用户空间，read()调用返回。此时，发生一次上下文切换以及一次数据复制：DMA缓冲区–>用户空间。 用户发起write()系统调用，请求发送数据。此时发生一次上下文切换和一次数据复制：用户空间–>DMA缓冲区。 DMA将数据复制到网卡，以备网络发送。此时发生第四次数据复制：DMA缓冲区–>套接字缓冲区 write()调用返回，再次发生上下文切换。 数据流如下： 可以发现，其中共涉及到了4次上下文切换以及4次数据复制。对于单纯的网络文件发送，有很多不必要的开销。 sendfile传输 对于上述场景，我们发现从DMA缓冲到用户空间，和从用户空间到套接字缓冲的两次CPU复制是完全没必要的，零拷贝由此而生。针对这种情况，Linux内核提供了sendfile系统调用。 如果用sendfile()执行上述请求，系统流程可以简化如下： sendfile()系统调用，可以实现数据在DMA内部的复制，而不需要将数据copy到用户空间。由此，上下文切换次数减为2次，数据复制次数减为3次。这已经实现了用户空间的零拷贝。 这里有一个问题：为什么DMA内部会出现一次复制(此次复制需要CPU参与)？这是因为，早期的网卡，要求被发送的数据在物理空间上是连续的，所以，需要有Socket Buffer。 但是如果网卡本身支持收集操作(scatter-gather)，即可以从不连续的内存地址聚集并发送数据，那么还可以进一步优化。 网卡支持scatter-gather的sendfile传输 在Linux内核版本2.4之后对此做了优化，如果计算机网卡支持收集操作，sendfile()操作可以省去到Socket Buffer的数据复制，取而代之的是，直接将数据位置和长度的描述符(descriptors)，传递给Socket Buffer： 借由网卡的支持，上下文切换的次数为2次，数据复制的次数也降低为2次。而这两次的数据复制是必须的，也就是说，数据在内存中的复制已经完全避免。 对于从硬盘向网络发送文件的场景，如果网卡支持收集操作，那么sendfile()系统调用，真正意义上的做到了零拷贝 内存映射(mmap) 对于“网络发送文件”的情况，用sendfile()系统调用可以极大地提高性能(据测试吞吐量可达传统方式的三倍)。但有一点不足的是，它只支持“读取->发送”这一“连贯操作”，所以，sendfile()一般用于处理一些静态网络资源，如果要对数据进行额外的操作，它无能为力。 内存映射(Memory mapping–mmap)对此提供了解决方案。mmap是一种内存映射文件的方法，它可以将一个文件映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中的虚拟地址的对应。如此一来，用户进程可以采用指针读写操作这一段内存，而内核空间对这段区域的修改也直接反映到用户空间。简而言之，mmap实现了用户空间和内核空间数据的共享。可以猜到，如果使用mmap系统调用，上文中所述场景的步骤如下： 用户发起mmap()系统调用，DMA直接将数据复制到用户空间和内核空间的共享虚拟内存，之后，用户便可以正常操作数据。期间进行了2次上下文切换，1次数据复制。接下来往网卡发送数据的流程，与前面一样采用write()系统调用。 数据流如下： 可以看到，相比于传统的方式，mmap省去了一次数据的复制，广义上也可以称之为零拷贝。与此同时，它还使得用户可以自定义地操作数据，这是相较于sendfile的优势所在。 不过，如果数据量很小(比如KB级别)，使用mmap的效率反而不如单纯的read系统调用高。这是因为mmap虽然避免了多余的复制，但是增加了OS维护此共享内存的成本。 NIO中的零拷贝 从1.4版本开始，JDK引入了NIO，提供了对零拷贝的支持。由于JVM是运行在OS之上的，其功能只是对系统底层api的封装，如果OS本身不支持零拷贝(mmap/sendfile)，那JVM对此也无能为力。 JDK对零拷贝的封装，主要体现在FileChannel这个类上。 map()的签名如下： public abstract class FileChannel extends AbstractInterruptibleChannel implements SeekableByteChannel, GatheringByteChannel, ScatteringByteChannel { public abstract MappedByteBuffer map(MapMode mode, long position, long size) throws IOException; } 以下引自方法注释： Maps a region of this channel’s file directly into memory…For most operating systems, mapping a file into memory is more expensive than reading or writing a few tens of kilobytes of data via the usual read and write methods. From the standpoint of performance it is generally only worth mapping relatively large files into memory. 将此通道的文件区域直接映射到内存中...对于大多数操作系统，将文件映射到内存中比通过常规读写方法读取或写入几十KB数据要昂贵得多。 从性能的角度来看，通常仅需要将较大的文件映射到内存中。 map()方法可以直接将一个文件映射到内存中。来简单看看FileChannelImpl中方法的具体实现： public class FileChannelImpl extends FileChannel { public MappedByteBuffer map(MapMode mode, long position, long size) throws IOException { ... synchronized (positionLock) { ... try { // 实际调用的是调用map0方法 addr = map0(imode, mapPosition, mapSize); } catch (OutOfMemoryError x) { // An OutOfMemoryError may indicate that we've exhausted // memory so force gc and re-attempt map System.gc(); ... } } ... } // Creates a new mapping private native long map0(int prot, long position, long length) throws IOException; } 最终调用的是一个native的map0()方法。solaris版的方法的源码在FileChannelImpl.c中： JNIEXPORT jlong JNICALL Java_sun_nio_ch_FileChannelImpl_map0(JNIEnv *env, jobject this, jint prot, jlong off, jlong len) { ... // 发现，内部果然是通过mmap系统调用来实现的 mapAddress = mmap64( 0, /* Let OS decide location */ len, /* Number of bytes to map */ protections, /* File permissions */ flags, /* Changes are shared */ fd, /* File descriptor of mapped file */ off); /* Offset into file */ if (mapAddress == MAP_FAILED) { if (errno == ENOMEM) { JNU_ThrowOutOfMemoryError(env, \"Map failed\"); return IOS_THROWN; } return handle(env, -1, \"Map failed\"); } return ((jlong) (unsigned long) mapAddress); } 最终map()方法会返回一个MappedByteBuffer，熟悉NIO的同学估计对这个类不会陌生，大名鼎鼎的DirectByteBuffer便是它的子类。它引用了一块独立于JVM之外的内存，不受GC机制所管制，需要自己来管理创建与销毁的操作。 transferTo()方法 mmap系统调用有了Java版的马甲，那sendfile呢？来看看FileChannel的transferTo()方法，签名如下： public abstract class FileChannel extends AbstractInterruptibleChannel implements SeekableByteChannel, GatheringByteChannel, ScatteringByteChannel { public abstract long transferTo(long position, long count, WritableByteChannel target) throws IOException; } 以下引自方法注释： Transfers bytes from this channel’s file to the given writable byte channel… This method is potentially much more efficient than a simple loop that reads from this channel and writes to the target channel. Many operating systems can transfer bytes directly from the filesystem cache to the target channel without actually copying them. 后半句其实隐式地说明了，如果操作系统支持“transfer without copying”，transferTo()方法就能做到相应的支持。来看看FileChannelImpl中方法的实现： public long transferTo(long position, long count, WritableByteChannel target) throws IOException { ... // Attempt a direct transfer, if the kernel supports it // 如果内核支持，采用直接传送的方式 if ((n = transferToDirectly(position, icount, target)) >= 0) return n; // Attempt a mapped transfer, but only to trusted channel types // 尝试使用mmap传送方式 // 其实这里也用到了mmap，由于上面已经简要介绍过，故不再展开 if ((n = transferToTrustedChannel(position, icount, target)) >= 0) return n; // Slow path for untrusted targets // 传统的传送方式 return transferToArbitraryChannel(position, icount, target); } 由注释可以看出来，sendfile()调用应该就发生在transferToDirectly()方法中，我们进去看看： private long transferToDirectly(long position, int icount, WritableByteChannel target) throws IOException { if (!transferSupported) return IOStatus.UNSUPPORTED; // 一系列检查判断 ... if (nd.transferToDirectlyNeedsPositionLock()) { synchronized (positionLock) { long pos = position(); try { // 调用的是transferToDirectlyInternal()方法 return transferToDirectlyInternal(position, icount, target, targetFD); } finally { position(pos); } } } else { // 调用的是transferToDirectlyInternal()方法 return transferToDirectlyInternal(position, icount, target, targetFD); } } private long transferToDirectlyInternal(long position, int icount, WritableByteChannel target, FileDescriptor targetFD) throws IOException { try { begin(); ti = threads.add(); if (!isOpen()) return -1; do { // 转到native方法transferTo0() n = transferTo0(fd, position, icount, targetFD); } while ((n == IOStatus.INTERRUPTED) && isOpen()); ... return IOStatus.normalize(n); } finally { threads.remove(ti); end (n > -1); } } // Transfers from src to dst, or returns -2 if kernel can't do that private native long transferTo0(FileDescriptor src, long position, long count, FileDescriptor dst); 可见，最终transferTo()方法还是需要委托给native的方法transferTo0()来完成调用，此方法的源码依然在FileChannelImpl.c中： JNIEXPORT jlong JNICALL Java_sun_nio_ch_FileChannelImpl_transferTo0(JNIEnv *env, jobject this, jobject srcFDO, jlong position, jlong count, jobject dstFDO) { jint srcFD = fdval(env, srcFDO); jint dstFD = fdval(env, dstFDO); #if defined(__linux__) off64_t offset = (off64_t)position; // 果然，内部确实是sendfile()系统调用 jlong n = sendfile64(dstFD, srcFD, &offset, (size_t)count); ... return n; #elif defined (__solaris__) sendfilevec64_t sfv; size_t numBytes = 0; jlong result; sfv.sfv_fd = srcFD; sfv.sfv_flag = 0; sfv.sfv_off = (off64_t)position; sfv.sfv_len = count; // 果然，内部确实是sendfile()系统调用 result = sendfilev64(dstFD, &sfv, 1, &numBytes); /* Solaris sendfilev() will return -1 even if some bytes have been * transferred, so we check numBytes first. */ ... return result; ... 果不其然，最终方法还是通过sendfile()系统调用来达到传输的目的。注意，由于sendfile()只适用于往Socket Buffer发送数据，所以，通过零拷贝技术来提升性能，只能用于网络发送数据的场景。 什么意思呢？如果单纯的用transferTo()把数据从硬盘上的一个文件写入到另一个文件中，是没有性能提升效果的， SendFile and transferTo in Java 我正在使用CentOs内核版本2.6.32。我计划使用NIO在有和没有transferTo（sendFile）的情况下进行测试。我的测试是将1GB文件从一个目录复制到另一个目录。但是，由于使用transferTo（），我没有发现任何显着的性能改进。请让我知道文件到文件sendFile是否确实在Linux内核中有效或仅文件到套接字有效吗？我需要为sendFile启用任何功能吗？ private static void doCopyNIO(String inFile, String outFile) { FileInputStream fis = null; FileOutputStream fos = null; FileChannel cis = null; FileChannel cos = null; long len = 0, pos = 0; try { fis = new FileInputStream(inFile); cis = fis.getChannel(); fos = new FileOutputStream(outFile); cos = fos.getChannel(); len = cis.size(); /*while (pos sendfile（）syscall将文件发送到套接字。splice（）syscall可以用于文件 Most efficient way to copy a file in Linux 我在与OS无关的文件管理器中工作，并且正在寻找为Linux复制文件的最有效方法。Windows有一个内置函数CopyFileEx（），但是据我所知，Linux没有这种标准函数。所以我想我必须实现自己的。 明显的方法是fopen / fread / fwrite，但是有没有更好（更快）的方法呢？我还必须有能力每隔一段时间停止一次，以便可以更新文件进度菜单的“到目前为止”。 不幸的是，您不能sendfile()在这里使用，因为目的地不是套接字。（名称sendfile()来自send()+“文件”）。 对于零复制，可以splice()按照@Dave的建议使用。（除非它不是零拷贝；它将是从源文件的页面缓存到目标文件的页面缓存的“一个副本”。） 但是...（a）splice()是特定于Linux的；（b）只要正确使用便携式接口，几乎可以肯定也可以做到。 简而言之，将open()+ read()+write()与一个小的临时缓冲区一起使用。我建议8K。因此，您的代码将如下所示： int in_fd = open(\"source\", O_RDONLY); assert(in_fd >= 0); int out_fd = open(\"dest\", O_WRONLY); assert(out_fd >= 0); char buf[8192]; while (1) { ssize_t read_result = read(in_fd, &buf[0], sizeof(buf)); if (!read_result) break; assert(read_result > 0); ssize_t write_result = write(out_fd, &buf[0], read_result); assert(write_result == read_result); } 通过此循环，您将把8k从in_fd页面缓存复制到CPU L1缓存中，然后将其从L1缓存写入out_fd页面缓存中。然后，您将用文件中的下一个8K块覆盖L1缓存的该部分，依此类推。最终结果是，输入的数据buf根本不会真正存储在主存储器中（除非最后一次存储）。从系统RAM的角度来看，这与使用“ zero-copy”一样好splice()。另外，它非常适合任何POSIX系统。 请注意，此处的小缓冲区是关键。典型的现代CPU的L1数据缓存具有32K左右，因此，如果将缓冲区设置得太大，这种方法会比较慢。可能慢得多。因此，将缓冲区保持在“几千字节”范围内。 当然，除非您的磁盘子系统非常快，否则内存带宽可能不是您的限制因素。因此，我建议posix_fadvise让内核知道您要做什么： posix_fadvise(in_fd, 0, 0, POSIX_FADV_SEQUENTIAL); 这将向Linux内核暗示其预读机制应该非常积极。 我还建议使用posix_fallocate来为目标文件预分配存储空间。这将提前告诉您是否会用完磁盘。对于具有现代文件系统（例如XFS）的现代内核，它将有助于减少目标文件中的碎片。 我最后建议的是mmap。由于TLB颠簸，它通常是最慢的方法。（非常新的具有“透明大页面”的内核可能会减轻这种情况；我最近没有尝试过。但是它过去肯定很糟糕。因此，mmap如果您有很多时间进行基准测试和使用新内核，那么我只会打扰测试。） 注释中存在一个问题，即splice从一个文件到另一个文件是否为零拷贝。Linux内核开发人员将此称为“页面窃取”。的手册页splice和内核源代码中的注释均表示该SPLICE_F_MOVE标志应提供此功能。 不幸的是，支持SPLICE_F_MOVE被猛拉在2.6.21（早在2007年），从来没有更换。（内核源代码中的注释从未更新。）如果您搜索内核源代码，您会发现SPLICE_F_MOVE实际上并没有在任何地方引用它。我可以找到的最后一条消息（来自2008年）说，它是“等待替换”。 底线是splice从一个文件到另一个调用memcpy以移动数据；它不是零拷贝。这并不比在使用小缓冲区read/的用户空间中做的更好write，所以您最好还是坚持使用标准的可移植接口。 如果曾经将“页面窃取”重新添加到Linux内核中，那么的好处splice将更大。（甚至在今天，当目的地是套接字时，您也会得到真正的零副本，从而splice更具吸引力。）但是，出于这个问题的目的，splice并不能为您带来多少好处。 Netty中的零拷贝 分析完了Linux内核和JVM层面的零拷贝，再来看看Netty中的零拷贝又是怎么回事。 类似的，由于Netty是构建在NIO之上的一个高性能网络IO框架，它也支持系统层面的零拷贝。举一个简单的例子，DefaultFileRegion类可以进行高效的网络文件传输，因为它封装了NIO中FileChannel的transferTo()方法： public class DefaultFileRegion extends AbstractReferenceCounted implements FileRegion { private FileChannel file; public long transferTo(WritableByteChannel target, long position) throws IOException { long count = this.count - position; if (count 0) { transferred += written; } return written; } } 那是不是Netty中所谓的零拷贝，完全依赖于系统支持呢？其实，零拷贝在Netty中还有另外一层意义：防止JVM中不必要的内存复制。 Netty in Action第5.1节是这么介绍ByteBuf API的： Transparent zero-copy is achieved by a built-in composite buffer type. 通过内置的composite buffer实现了透明的零拷贝，什么意思呢？Netty将物理上的多个Buffer组合成了一个逻辑上完整的CompositeByteBuf，它一般用在需要合成多个Buffer的场景。这在网络编程中很常见，如一个完整的http请求常常会被分散到多个Buffer中。用CompositeByteBuf很容易将多个分散的Buffer组装到一起，而无需额外的复制： ByteBuf header = Unpooled.buffer();// 模拟http请求头 ByteBuf body = Unpooled.buffer();// 模拟http请求主体 CompositeByteBuf httpBuf = Unpooled.compositeBuffer(); // 这一步，不需要进行header和body的额外复制，httpBuf只是持有了header和body的引用 // 接下来就可以正常操作完整httpBuf了 httpBuf.addComponents(header, body); 反观JDK的实现ByteBuffer是如何完成这一需求的： ByteBuffer header = ByteBuffer.allocate(1024);// 模拟http请求头 ByteBuffer body = ByteBuffer.allocate(1024);// 模拟http请求主体 // 需要创建一个新的ByteBuffer来存放合并后的buffer信息，这涉及到复制操作 ByteBuffer httpBuffer = ByteBuffer.allocate(header.remaining() + body.remaining()); // 将header和body放入新创建的Buffer中 httpBuffer.put(header); httpBuffer.put(body); httpBuffer.flip(); 相比于JDK，Netty的实现更合理，省去了不必要的内存复制，可以称得上是JVM层面的零拷贝。除此之外，整个ByteBuf的API都贯穿了零拷贝的设计理念： 尽量避免Buffer复制带来的开销。比如关于派生缓冲区(Derived buffers)的操作，duplicate()(复制)，slice()(切分)，order()(排序)等，虽然都会返回一个新的ByteBuf实例， 但它们只是具有自己独立的读索引、写索引和标记索引而已，内部存储(Buffer数据)是共享的，也就是过程中并没有复制操作。由此带来的一个负面影响是，使用这些操作的时候需要注意：修改原对象会影响派生对象， 修改派生对象也会影响原对象。 总结 由于Linux系统中内核空间和用户空间的区别，数据的读取和发送需要有内存中的复制。mmap系统调采用内存映射的方式，让内核空间和用户控件共享同一块内存，省去了从内核空间往用户空间复制的开销。sendfile系统调用可以将文件直接从硬盘经由DMA传输到套接字缓冲区，而无需经过用户空间。如果网卡支持收集操作(scatter-gather)，那么可以做到真正意义上的零拷贝。 NIO中FileChannel的map()和transferTo()方法封装了底层的mmap和sendfile系统调用，从而在Java语言上提供了系统层面零拷贝的支持。 Netty通过封装，也可以支持系统级别的零拷贝。此外，Netty中的零拷贝有另一层应用层面的含义：设计良好的ByteBuf API，防止了JVM内部不必要的Buffer复制。 "},"Chapter18/computer.html":{"url":"Chapter18/computer.html","title":"Part XVIII 计算机原理篇","keywords":"","body":"计算机原理 位、字、字符、字节 "},"Chapter18/bit.html":{"url":"Chapter18/bit.html","title":"位、字、字符、字节","keywords":"","body":"字、字符、字节 术语 位: (bit)数据存储的最小单位。每个二进制数字0或者1就是1个位; 字节: (byte)简写 B或b;8个位构成一个字节;即:1 byte (字节)= 8 bit(位); 1 KB = 1024 B(字节) 字符: 2、a、A、中、+、*、の......等计算机中使用的字母、数字、字和符号都表示一个字符; 一般 utf-8 编码下，一个汉字 字符 占用 3 个 字节； 一般 gbk 编码下，一个汉字 字符 占用 2 个 字节； 图文 "},"Chapter20/appendix.html":{"url":"Chapter20/appendix.html","title":"附录","keywords":"","body":"附录 翻墙 标记语言Markdown 电子书 文本编辑器 sql开发工具 linux链接工具 文件服务器 git Navicat 15 Keygen注册机 JetBrains全家桶激活 "},"Chapter20/OverTheWall.html":{"url":"Chapter20/OverTheWall.html","title":"翻墙","keywords":"","body":"翻墙工具 1.1 蓝灯(lantern) 先附上地址 https://github.com/getlantern/lantern "},"Chapter20/Markdown.html":{"url":"Chapter20/Markdown.html","title":"标记语言Markdown","keywords":"","body":"Markdown   Markdown是一种可以使用普通文本编辑器编写的标记语言， 通过简单的标记语法，它可以使普通文本内容具有一定的格式。 编写工具推荐 Typora Typora 是一款支持实时预览的 Markdown 文本编辑器。 它有 OS X、Windows、Linux 三个平台的版本，并且由于仍在测试中， 是完全免费的。 Typora 首先是一个 Markdown 文本编辑器， 它支持且仅支持 Markdown 语法的文本编辑。 在 Typora 官网 上他们将 Typora 描述为 「A truly minimal markdown editor. 」。 认识Markdown https://sspai.com/post/36610 Markdown常用语法 缩进 一个汉字占两个空格大小，所以使用四个空格就可以达到首行缩进两个汉字的效果。有如下几种方法：     一个空格大小的表示：&ensp ; 或 &#8194 ; ，此时只要在相应需要缩进的段落前加上 4个 如上的标记即可，注意要带上分号。   两个空格的大小表示：&emsp ; 或 &#8195 ; ，同理，使用2个即可缩进2个汉字，推荐使用该方式。     不换行空格：&nbsp ; 或 & # 160; ，使用4个即可。注意！此时的分号为英文分号，但是不推荐使用此方法，太麻烦！ 　　使用全角空格(切换快捷键shift+空格)。然后在全角输入状态下直接使用空格键就ok了。看输入法了我感觉不习惯。 换行 两个回车即可 或者使用 (<>里没有空格) 加粗 **内容** 内容 　(*与内容之间没有空格) 倾斜 *内容* 内容 　　 (*与内容之间没有空格) 列表 在Markdown 中我们只需要在文字前加上 - 或 * 即可变为无序列表 有序列表则直接在文字前加1. 2. 3. 符号要和文字之间加上一个字符的空格 * 1 * 2 * 3 1. a 2. b 3. c 5. d 1 2 3 a b c d 注意*号和.后要加空格后有空格不然不生效 插入图片 ![title](/path/to/img.jpg) 表格 语法如下 标题1 标题2 标题3 文本1 文本2 文本3 文本4 文本5 文本6 对其方式 左对齐 居中 右对齐 文本1 文本2 文本3 文本4 文本5 文本6 表格内文本 左对齐 居中 右对齐 （斜体）文本1 （加粗）文本2 （删除线）文本3 简书 $\\color{red}{红色字}$ （加粗斜体）文本6 下载链接 超链接文字 "},"Chapter20/gitbook.html":{"url":"Chapter20/gitbook.html","title":"电子书","keywords":"","body":"gitbook   不用猜了本书就是用gitbook写的。 首先安装git Git-2.25.0-32-bit.exe下载 GitBook 是一个基于 Node.js 的命令行工具。 安装 npm install gitbook-cli -g 创建新书 gitbook init   进入一个新的目录(电子书命名目录)执行本命令就可初始化电子书 启动 gitbook serve 访问 http://localhost:4000 可能的问题   启动是可能会报如下错误 You already have a server listening on 35729 You should stop it and try again.   问题定位：端口被占用，关掉即可   步骤： 打开CMD，输入netstat -ano|findstr 35729 查询占用端口的pid。接着kill掉应用程序就行。 gitbook部署到github   以目录名为Attacking-Java-Rookie下面的章节目录为Chapter*格式 编译成html cd Attacking-Java-Rookie mkdir content cp *.md content cp -r Chapter* content gitbook serve ./content ./docs   每次启动的时候，都要敲长长的命令，很不方便，所以，我们就需要把命名简短化，具体就是去写成 npm 脚本 npm init -y   生成一个package.json文件，在package.json添加或修改一下代码 \"scripts\": { \"build\": \"gitbook build ./content ./docs\" }   然后执行命令运行 npm run build   这样 html 内容被编译好之后就会被保存到 docs 文件夹中 部署到github pages   把文件push到github。   到仓库配置（settings）下的Options页面。往下拉。到达Github Pages 一项。 Source一项设置为 master branch /docs folder。意思就是 master 分支的 docs 文件夹。 当然master branch 也可以只是页面效果不一样而已可以自己尝试下。   最后完整版脚本release.sh #!/bin/sh echo \"开始同步数据\" echo \"git pull\" git pull echo \"同步数据完成\" echo \"开始构建\" echo \"rm -rf content\" rm -rf content echo \"rm -rf docs\" rm -rf docs echo \"旧目录删除完成\" echo \"创建content文件夹\" echo \"mkdir content\" mkdir content echo \"创建新文件夹完成;开始移动文件\" echo \"cp -r *.md ./content\" cp -r *.md ./content echo \"cp -r Chapter* ./content\" cp -r Chapter* ./content echo \"cp -r image ./content\" cp -r image ./content echo \"cp -r file ./content\" cp -r file ./content echo \"移动文件完成;开始构建\" echo \"npm run build\" npm run build echo \"构建完成;开始上传新docs文件夹\" echo \"git add .\" git add . echo \"git commit -am 'release book'\" git commit -am \"release book\" echo \"git push\" git push echo \"上传完成\" gitboot配置 介绍一下gitbook中book.json的一些实用配置和插件 全局配置 title 设置书本的标题 \"title\" : \"Gitbook Use\" author 作者的相关信息 \"author\" : \"mingyue\" description 本书的简单描述 \"description\" : \"记录Gitbook的配置和一些插件的使用\" language Gitbook使用的语言, 版本2.6.4中可选的语言如下： en, ar, bn, cs, de, en, es, fa, fi, fr, he, it, ja, ko, no, pl, pt, ro, ru, sv, uk, vi, zh-hans, zh-tw 例如，配置使用简体中文 \"language\" : \"zh-hans\" links 在左侧导航栏添加链接信息 \"links\" : { \"sidebar\" : { \"Home\" : \"https://www.baidu.com\" } } styles 自定义页面样式， 默认情况下各generator对应的css文件 \"styles\": { \"website\": \"styles/website.css\", \"ebook\": \"styles/ebook.css\", \"pdf\": \"styles/pdf.css\", \"mobi\": \"styles/mobi.css\", \"epub\": \"styles/epub.css\" } 插件列表plugins 在book.json中添加以下内容。然后执行gitbook install 配置使用的插件 \"plugins\": [ \"-search\", \"back-to-top-button\", \"expandable-chapters-small\", \"insert-logo\" ] 其中\"-search\"中的 - 符号代表去除默认自带的插件 Gitbook默认自带有5个插件： highlight： 代码高亮 search： 导航栏查询功能（不支持中文） sharing：右上角分享功能 font-settings：字体设置（最上方的\"A\"符号） livereload：为GitBook实时重新加载 配置插件的属性 例如配置insert-logo的属性： \"pluginsConfig\": { \"insert-logo\": { \"url\": \"images/logo.png\", \"style\": \"background: none; max-height: 30px; min-height: 30px\" } } 一些实用的插件 回到顶部 { \"plugins\": [ \"back-to-top-button\" ] } code 代码添加行号&复制按钮（可选） { \"plugins\" : [ \"code\" ] } 如果想去掉复制按钮，在book.json的插件配置块更新： { \"plugins\" : [ \"code\" ], \"pluginsConfig\": { \"code\": { \"copyButtons\": false } } } copy-code-button 代码块复制按钮 { \"plugins\": [\"copy-code-button\"] } 将logo插入到导航栏上方中 { \"plugins\": [ \"insert-logo\" ], \"pluginsConfig\": { \"insert-logo\": { \"url\": \"images/logo.png\", \"style\": \"background: none; max-height: 30px; min-height: 30px\" } } } 支持中文搜索, 在使用此插件之前，需要将默认的search和lunr 插件去掉。 { \"plugins\": [ \"-lunr\", \"-search\", \"search-pro\" ] } 大神的文章 https://www.cnblogs.com/mingyue5826/p/10307051.html "},"Chapter20/Txt.html":{"url":"Chapter20/Txt.html","title":"文本编辑器","keywords":"","body":"文本编辑器 "},"Chapter20/sql.html":{"url":"Chapter20/sql.html","title":"sql开发工具","keywords":"","body":"数据库 "},"Chapter20/xshell.html":{"url":"Chapter20/xshell.html","title":"linux链接工具","keywords":"","body":"shell "},"Chapter20/DecompileTool.html":{"url":"Chapter20/DecompileTool.html","title":"反编译工具","keywords":"","body":"反编译工具 使用Luyten，能够成功反编译所有之前用jd-gui无法反编译的文件，这个工具提供有.exe后缀的可执行文件，也有.jar版本 luyten https://github.com/deathmarine/Luyten 可以用Jadx: https://github.com/skylot/jadx试试， "},"Chapter20/redis-desktop-manager.html":{"url":"Chapter20/redis-desktop-manager.html","title":"redis-desktop-manager","keywords":"","body":"redis-desktop-manager-0.8.8.384 redis-desktop-manager-0.8.8.384.exe 下载 "},"Chapter20/file.html":{"url":"Chapter20/file.html","title":"文件服务器","keywords":"","body":"一个上传和下载文件的网址 https://www.filedropper.com/ "},"Chapter20/git.html":{"url":"Chapter20/git.html","title":"git","keywords":"","body":"git github删除提交历史 尝试 运行 git checkout --orphan latest_branch 添加所有文件git add -A 提交更改git commit -am \"commit message\" 删除分支git branch -D master 将当前分支重命名git branch -m master 最后，强制更新存储库。git push -f origin master 清理远程已删除本地还存在的分支 使用命令 git remote show origin，可以查看remote地址，远程分支，还有本地分支与之相对应关系等信息 清理远程已删除本地还存在的分支 git fetch --prune origin 或者 git fetch -p 或者 git pull -p $ git remote show origin "},"Chapter20/Navicat.html":{"url":"Chapter20/Navicat.html","title":"Navicat 15 Keygen注册机","keywords":"","body":"Navicat 15 Keygen注册机使用 1.点击\"补丁“按钮给主程序打补丁 2.给Hosts打补丁屏蔽注册服务器 3.选择正确的语言和版本，点击”生成“产生SN 4.在手工注册界面点击”激活“按钮，激活码会自动填入注册界面 5.完工 http://www.filedropper.com/navicat15zhucejijb51 https://www.jb51.net/database/712269.html "},"Chapter20/JetBrains.html":{"url":"Chapter20/JetBrains.html","title":"JetBrains全家桶激活","keywords":"","body":"JetBrains 2020.2全家桶激活 2020.2 安装打开ideaIU-2020.2.exe 选择免费->进入 2020 版本文件夹中的激活补丁 jetbrains-agent.zip 拖入 IDEA 界面中 补丁，拖不进去，请重启 IDEA ，新建一个Java 项目，写一个空的 main 方法，再次试试 补丁，拖不进去，请重启 IDEA ，新建一个Java 项目，写一个空的 main 方法，再次试试 注意：激活补丁，拖进去之后，激活补丁的位置不要更换，不要删除。否则激活之后还会失效， 注意：激活补丁，拖进去，等于是程序，帮你去写一个激活补丁的位置，你换补丁jar位置，或者，删除补丁jar，就找不到激活补丁了 拖入补丁后会弹框，点击 restart 重启 idea： 配置助手会提示您，需要使用哪种激活方式，这里我们选择默认的 Activation Code，通过注册码来激活，点击为IDEA安装： 安装参数 LFq51qqupnaiTNn39w6zATiOTxZI2JYuRJEBlzmUDv4zeeNlXhMgJZVb0q5QkLr+CIUrSuNB7ucifrGXawLB4qswPOXYG7+ItDNUR/9UkLTUWlnHLX07hnR1USOrWIjTmbytcIKEdaI6x0RskyotuItj84xxoSBP/iRBW2EHpOc 点击是，重启 IDEA 即可 进入 IDEA 界面后，点击 Help -> Register 查看： 可以看到，已经成功激活至 2089 年，为确保不失效，请勿随意更新 https://justcode.ikeepstudying.com/2020/09/2020-09-09-%E4%BA%B2%E6%B5%8B%E6%9C%89%E6%95%88%EF%BC%9Aintellij-idea-2020-2-%E6%9C%80%E6%96%B0%E5%85%A8%E5%AE%B6%E6%A1%B6%E7%B3%BB%E5%88%97%E4%BA%A7%E5%93%81%E6%BF%80%E6%B4%BB%E7%A0%B4%E8%A7%A3/#viewSource "}}